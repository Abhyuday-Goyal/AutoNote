<<<<<<< HEAD
1 
 
 
 
 
 
 
 
 
Effects of Rising Temperatures and Drought on Rice Production  
In the United States of America  
By: Nishkal Hundia  
Dr. Lars Olson, AREC280  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 2 
 
Abstract  
This paper aims to investigate the impact of climate elements, notably escalating 
temperatures and drought occurrences, on rice farming in the United States. By analyzing 
extensive datasets covering several decades, the research aims to discern patterns in  
temperature changes during critical phases of rice cultivation and the prevalence of drought 
events. The findings reveal a trend of steadily rising average temperatures during rice -growing 
months over recent decades. While maximum temperatures remained re latively consistent, the 
consistent increase in minimum temperatures led to a narrower temperature range, potentially 
placing strain on rice crops known to thrive within specific temperature thresholds. 
Temperature variances and rice yield growth, while being statistically significant , did not impact 
the yield growth much  at either local or national scales. Conversely, the influence of drought 
emerges as a significant and distinct factor affecting rice yield. There is an evident negative 
relationship betwee n yield growth and drought severity. A notable reduction in yield growth is 
observed as drought severity intensifies, displaying statistical significance across both localized 
and national levels. These outcomes emphasize the susceptibility of rice farming  in the US to 
drought events, highlighting the substantial challenge faced in maintaining consistent 
productivity.  
Significance Statement  
 Rice is a vital food crop on a global scale and analyzing the impact of climate change and 
drought on yield growth ca n reveal invaluable insights that will help improve yield in the future. 
Examining drought's impact on US rice yield highlights a critical correlation with climate change. 
Understanding these dynamics is essential for safeguarding rice production against 
environmental challenges especially because rice is considered to be a highly drought -
susceptible plant.  3 
 
Introduction  
 Rice is among the worlds most important food crops. Research into rice is crucial for the 
development of technologies that will increase productivity for farmers who rely on rice for 
their livelihood (*Zeigler & Barclay, 2008). Rice is a major staple among two -thirds of the 
worlds population (*Sen et al., 2020).  By the worlds standards, per capita rice consumption in 
the United States is  not very large, but it has been consistently increasing over the last several 
decades, reaching a level of 21.0 lb per capita annually currently (*Batres -Marquez et al., 2009). 
United States exports about 40 -45 percent of its rice crop each year, mostly t o Mexico, Central 
America, South America, the Caribbean, etc., and accounts for around 5 percent of the annual 
volume of global rice trade ( USDA ERS - Trade , n.d.). Rice is also a source of nutrition and health 
benefits for the U.S. population, as it is lo w in fat and sodium, cholesterol -free, and rich in 
complex carbohydrates and essential vitamins and minerals ( USDA ERS - Rice Sector at a 
Glance , n.d.).  
 Drought is the leading threat to agricultural food production, especially in the cultivation 
of rice, a semi -aquatic plant (*Oladosu et al., 2019). Rice is considered one of the most drought -
susceptible plants due to its small root system, thin cuticular wax, and swift stomata closure  
(*Sahebi et al., 2018). Drought is a very devastating and costly natura l disaster. From 1980 to 
2022, droughts caused extensive losses in the United States ($344.9B CPI -adjusted economic 
losses) accounting for roughly 13% of total losses from weather and climate disasters 
(noaa.gov, 2023). Similarly, increasing temperatures c an also have a drastic impact on rice 
production. Most of the rice in the US is currently cultivated in regions where temperatures are 
above the optimal temperature for the growth of rice, which is 22 -28 degrees Celsius or 71 -87 
degrees Fahrenheit (Krishna n et al., 2011). Any further increase in mean temperature or 
episodes of high temperatures during sensitive stages may reduce rice yields drastically.  This 4 
 
study aims to explore the impacts of drought and changing temperatures on the Yield Growth 
of Rice in the United States of America over time.  
Results:  
 To understand the potential impacts of changing temperatures, we will first assess 
trends over time of various temperature variables. Figure 1 shows the variation of the mean 
temperature during the rice -growing and harvesting months in the US (April - August) from 
1960 to 2020. As can be seen, there has been a steady increase in overall mean temperatures 
over the last few decades. Figure 2 shows the changes in the mean of the maximum (T max) and 
minimum ( Tmin) temperatures from 1960 to 2020 during the rice -growing and harvesting 
months in the US. The graph on the left in Figure 2 shows T min increasing significantly over time 
whereas the graph on the right shows T max not varying as much. This explains the d ecrease in 
the difference between T max and T min from 1960 to 2020 as shown in Figure 3.  
Figure 4 shows a graph of growth in the Yield of rice at the county level vs the change in 
the difference between T max and T min of their respective years. The graph al so contains a linear 
regression line which is almost horizontal . Fitting a linear regression model on the county  level  
showed that, although statistically significant,  change in the difference between minimum and 
maximum temperatures did not cause much cha nge in  yield growth, where a 1-degree  
Fahrenheit increase in  the differences leads to a 0.4% reduction in yield, as can be seen in Table 
1. Not much changed when fitting to country -level data as can be seen in Table 2 , with a 1 -
degree increase in the difference leading to a 1.9% reduction in yield. The country level case is 
also represented by Figure 4.  Moreover, changes in the mean temperature over time also 
proved  to be statistically significant when trying to model yield growth, as shown in Tables 1  
and 2 , but yet again, its impact on the value of yield growth was not major.  5 
 
Drought is another factor that was tested for any correlation to the yield growth of rice 
over time. Figure 5 represents the variation of the mean Palmer Drought Severity Index d uring 
the rice sowing and growing months (March -July) by year. It shows consistent repetition of 
drought occurrences over time in the US and the almost horizontal linear regression line is a 
further indication of the same. Figure 6 shows the graph of yield  growth vs the Change in Mean 
Palmer drought severity index. The blue line represents a linear regression line which shows a 
clear fall in yield growth with increasing drought severity index. The change in the value of 
drought severity is revealed to be st atistically significant at both the county level and the 
country level as revealed in Tables 1 and 2 respectively , with a 1.3% decrease in yield growth 
with a 1 -point change in the drought severity index at county level and a 2.2% decrease at the 
country l evel. The country level condition is represented in Figure 6.  
Discussion:  
The results show the intricate relationship between climate variables like rising 
temperatures and drought, and their impact on rice production in the United States. The 
increase in mean temperatures during crucial rice -growing months over the past few decades, 
as seen in Figures 1 and 2, presents a concerning trend. While the maximum temperatures 
(Tmax) have not changed as much, the consistent rise in minimum temperatures (T min) lead s to a 
reduction in the gap between the T max and T min as seen in Figure 3. This narrowing temperature 
range can be indicative of potential stress on rice crops , as these crops can only thrive within a 
specific temperature band (Krishnan et al., 2011).  
The correlation between change in temperature difference and rice yield growth, 
although statistically significant, was not very evident . This does not directly agree with the 
literature reviewed, but the statistical significance of temperature variables  sugge sts a complex 6 
 
relationship where other factors might interplay or mitigate the impact of temperature 
variations on rice yields.  
The impact of drought on rice yield growth, however, appears more pronounced and  is 
also statistically significant. The Palmer D rought Severity Index, one of the most widely used 
drought indicators, was used to measure the impact of drought on rice yield (*Wang et al., 
2022). demonstrates a recurring pattern of drought occurrences during the crucial rice sowing 
and growing months a s seen in Figure 5. The analysis depicted in Figure 6 shows a clear 
negative correlation between yield growth and change in drought severity at the country level. 
As the severity of drought increases, the yield growth reduces considerably, revealing a stro ng 
correlation  at both state and country levels, as represented in Tables 1 and 2. These results 
confirm the fact that rice is highly susceptible to drought , as suggested by Sahebi et. al.,  and 
further research must be done to find viable solutions to prev ent these recurring droughts from 
affecting rice yield in the US in the future.  
These findings show the vulnerability of rice cultivation in the US to drought events. 
While temperature fluctuations show a less direct  impact on yield growth, the effects of 
drought are starkly evident. As rice cultivation heavily relies on water availability, the recurring 
and consistent occurrence of drought poses a major challenge to sustained productivity. The 
lack of a clear statistical relationship between temperature va riations and yield growth could 
suggest the existence of adaptive mechanisms within rice cultivation practices or the need for 
further analysis that takes into account additional variables. Furthermore, while the 
temperature variations might not exhibit a direct linear correlation with yield growth, their 
indirect impacts on rice growth might still be of interest and warrant further investigation.  
Materials/Methods  7 
 
 This paper relied on county -level month -wise rice yield data obtained from the United 
State s Department of Agriculture (USDA) along with temperature and precipitation data 
collected over the years from 1960 -2020 by counties in the US (National Centers for 
Environmental Information (NCEI), 2023) . It also uses Palmer Drought Severity Index data at  the 
state level collected from 1960 -2020 (National Centers for Environmental Information (NCEI), 
2023). The Palmer Drought Severity Index was used because it is one of the most widely used 
indicators for drought monitoring and research (*Wang et al., 2022 ). All datasets were joined 
together in R. To graph how temperature has changed over time during the rice growing and 
harvesting months, the mean of the temperature was taken for these months and then a line 
plot was created by setting Year to the x -axis and the mean temperature to the y -axis. 
Similarly, changes in mean minimum and maximum temperatures were also graphed with 
Year as the x -axis. Linear regression lines were made for each of these graphs to show the 
trend over time. Drought severity was a lso graphed in a similar way over time. Scatterplots and 
linear regression models were created to investigate the relationship between temperature 
changes and drought severity with changes in the yield growth of rice. Yield growth was 
obtained by the formu la log(Yield)  log(lag(Yield)). Linear regression models were created at 
both the county level and at the country level. Since drought data was only available at the 
state level, every county in a state was assigned the same value for the drought index. F or the 
country level evaluation, mean was taken for all three explanatory variables: Change in 
Temperature, Change in the Difference between Minimum and Maximum Temperatures, and 
the Palmer Drought Severity Index as well as Yield Growth to get values at th e country level. A 
linear model was then fit to these datasets and summaries were generated which constitute 
tables 1 and 2.  8 
 
Figure 1 : Graph showing the variation of mean temperatures during crucial rice -growing and 
harvesting months (April -August) from 19 60-2020. The blue line in the figure represents a linear 
line of best fit for temperature values based on time.  
 
 
 
 
 
 
 
Figure 2 : Graphs showing the variation of the mean of the minimum (left) and maximum (right) 
temperatures during rice-growing and harvesting months (April -August) from 1960 -2020. They 
also include a blue linear line of best fit for temperature values based on time.  
Figure 3 : Graph showing the variation in the difference between the mean maximum and 
minimum temperatures during the rice -growing and harvesting months (April -August) from 
1960 -2020. Also includes a linear line of best fit for the difference in temperatures based on 
time.  
 
 
9 
 
Figure 4 : Graph showing the variation of yield growth of rice in the US b ased on the difference 
in mean minimum and maximum temperatures during the rice -growing and harvesting months. 
Also includes a linear line of best fit for the yield growth based on the difference in 
temperature.  
Figure 5 : Graph showing the variation in the  mean Palmer Drought Severity Index from 1960 -
2020 during the rice growing and sowing months (March -July). Also includes a linear line of best 
for the drought severity based on time.  
Figure 6 : Graph showing the variation of yield growth of rice in the US based on change in the 
mean drought severity during the rice -growing and sowing months. Also includes a linear line of 
best fit for the yield growth based on the drought severity.
10 
 
Table 1 : Yield growth of rice in the United States based on the change in mean temperature, change in 
drought severity, and change in the difference between minimum and maximum temperatures. Yield growth 
is given by log(Yield)  log(lag(Yield)). Temperature variables are at the co unty level while drought variable is 
at the state level.
 
Table 2 : Yield growth of rice in the United States based on the change in mean temperature, change in 
drought severity, and change in the difference between minimum and maximum temperatures. Yield  
growth is given by log(Yield)  log(lag(Yield)). All variables are at the country level. Values for the 
variables are the mean taken by year.  
11 
 
References  
1. *Batres -Marquez, S. P., Jensen, H. H., & Upton, J. (2009). Rice Consumption in the 
United States: Recent Evidence from Food Consumption Surveys . Journal of the 
American Dietetic Association , 109(10), 1719 –1727. 
https://doi.org/10.1016/j.jada.2009.07.010  
2. Krishnan, P., Ramakrishnan, B., Reddy, K. R., & Reddy, V. R. (2011, January 1). Chapter 
three - High -Temperature Effects on Rice Growth, Yield, and Grain Quality  (D. L. Sparks, 
Ed.). ScienceDirect; Academic Press.  
https://www.sciencedirect.com/science/article/abs/pii/B9780123876898000047  
3. noaa.gov. (2023). Billion -Dollar Weather and Climate Disasters | National Centers for 
Environmental Information  (NCEI). Www.ncei.noaa.gov. 
https://www.ncei.noaa.gov/access/billions/state -summary/US  
4. National Centers for Environmental Information (NCEI). (2023, June 23). NOAA Monthly 
U.S. Climate Divisional Database (NCLIMDIV) . 
https://www.ncei.noaa.gov/acce ss/metadata/landing -
page/bin/iso?id=gov.noaa.ncdc:C00005  
5. *Oladosu, Y., Rafii, M. Y., Samuel, C., Fatai, A., Magaji, U., Kareem, I., Kamarudin, Z. S., 
Muhammad, I., & Kolapo, K. (2019). Drought Resistance in Rice from Conventional to 
Molecular Breeding: A R eview. International Journal of Molecular Sciences , 20(14), 3519. 
https://doi.org/10.3390/ijms20143519  
6. *Sahebi, M., Hanafi, M. M., Rafii, M. Y., Mahmud, T. M. M., Azizi, P., Osman, M., Abiri, 
R., Tahe ri, S., Kalhori, N., Shabanimofrad, M., Miah, G., & Atabaki, N. (2018). 
Improvement of Drought Tolerance in Rice (Oryza sativa L.): Genetics, Genomic Tools, 
and the WRKY Gene Family . BioMed Research International , 2018, 1 –20. 
https://doi.org/10.1155/2018/3158474  
7. *Sen, S., Chakraborty, R., & Kalita, P. (2020). Rice - not just a staple food: A 
comprehensive review on its phytochemicals and therapeutic potential. Trends in Food 
Science and Technology , 97,  265–285. https://doi.org/10.1016/j.tifs.2020.01.022  
8. USDA ERS - Trade. (n.d.). Www.ers.usda.gov . 
https://www.ers.usda.gov/topics/crops/rice/trad e/ 
9. USDA ERS - Rice Sector at a Glance. (n.d.). Www.ers.usda.gov. 
https://www.ers.usda.gov/topics/crops/rice/rice -sector -at-a-glance/  
10. *Wang, Z., Yang, Y., Zhang, C., Guo,  H., & Hou, Y. (2022). Historical and future Palmer 
Drought Severity Index with improved hydrological modeling. Journal of Hydrology , 610, 
127941. https://doi.org/10.1016/j.jhydrol.2022.127941  
11. *Zeigler, R. S., & Barclay, A. (2008). The Relevance of Rice. Rice, 1(1), 3 –10. 
https://doi.org/10.1007/s12284 -008-9001 -z 
=======
Linear Algebra
Joe EricksonTable of Contents
1.Euclidean Vectors
1.1 Groups, Rings, and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Real Euclidean Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Located Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 The Dot Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5 The Norm of a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12
1.6 Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.Matrices and Systems
2.1 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3 Row and Column Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34
2.4 The Inverse of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.5 Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.6 Homogeneous Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.Vector Spaces
3.1 The Vector Space Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.3 Subspace Sums and Direct Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.4 Linear Combinations and Spans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.5 Linear Independence and Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .81
3.6 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
3.7 Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.8 The Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .99
4.Linear Mappings
4.1 Linear Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2 Images and Null Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.3 Matrix Representations of Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.4 Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .128
4.5 The Rank-Nullity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.6 Dimension and Rank Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.7 Compositions of Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.8 The Inverse of a Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.9 Properties of Invertible Operators and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1555.Determinants
5.1 Determinants of Low Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.2 Determinants of Arbitrary Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
5.3 Applications of Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.4 Determinant Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.5 Permutations and the Symmetric Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.6 The Leibniz Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.Eigen Theory
6.1 Eigenvectors and Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
6.2 The Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.3 Applications of the Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.4 Similar Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
6.5 The Theory of Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
6.6 Diagonalization Methods and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
6.7 Matrix Limits and Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .231
6.8 The Cayley-Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232
7.Inner Product Spaces
7.1 Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .237
7.2 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
7.3 Orthogonal Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
7.4 Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .259
8.Operator Theory
8.1 The Adjoint of a Linear Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8.2 Self-Adjoint and Unitary Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
8.3 Normal Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .276
8.4 The Spectral Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
9.Canonical Forms
9.1 Generalized Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .284
9.2 Jordan Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
10. The Geometry of Vector Spaces
10.1 Convex Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
A.Appendix
Symbol Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2931
1
Euclidean Vectors
1.1 – Groups, Rings and Fields
It is assumed that the reader is well familiar with sets and functions. Given a set S, a
binary operation from S×StoSis a function ∗:S×S→S, so that for each ( a, b)∈S×S
we have ∗(a, b)∈S. As is customary we will usually write a∗binstead of ∗(a, b). The operation
∗iscommutative if, for every a, b∈S,
∗(a, b) =a∗b=b∗a=∗(b, a),
andassociative if, for every a, b, c∈S,
∗(a,∗(b, c)) =a∗(b∗c) = (a∗b)∗c=∗(∗(a, b), c).
Common binary operations are addition of real numbers, + : R×R→R, and multiplication
of real numbers, ·:R×R→R, both of which are commutative and associative. Recall that
subtraction and division of real numbers is neither commutative nor associative, and indeed
a÷bis not even defined in the case when b= 0!
Linear algebra is foremost the study of vector spaces, and the functions between vector
spaces called mappings. However, underlying every vector space is a structure known as a field,
and underlying every field there is what is known as a ring. Thus we begin with the definition
of a ring and proceed thence.
Definition 1.1. Aring is a triple (R,+,·)consisting of a set Rof objects, along with binary
operations addition + :R×R→Randmultiplication ·:R×R→Rsubject to the following
axioms:
R1.a+b=b+afor any a, b∈R.
R2.a+ (b+c) = (a+b) +cfor any a, b, c∈R.
R3.There exists some 0∈Rsuch that a+ 0 = afor any a∈R.
R4.For each a∈Rthere exists some −a∈Rsuch that −a+a= 0.
R5.a·(b·c) = (a·b)·cfor any a, b, c∈R.
R6.a·(b+c) =a·b+a·cfor any a, b, c∈R.2
As in elementary algebra it is common practice to denote multiplication by omitting the
symbol ·and employing juxtaposition:
ab=a·b, a (bc) =a·(b·c), a(b+c) =a·(b+c),
and so on.
We call the object −ain Axiom R4 the additive identity ofa. From Axioms R1 and R4
we see that
(−a) +a=a+ (−a) = 0 .
As a matter of convenience we define a subtraction operation as follows:
a−b=a+ (−b),
so that
a−a= 0
obtains just as in elementary algebra.
Definition 1.2. A ring (R,+,·)iscommutative if it satisfies the additional axiom
R7.a·b=b·afor all a, b∈R.
Definition 1.3. A commutative ring (R,+,·)is aunitary commutative ring if it satisfies
the additional axiom
R8.There exists some 1∈Rsuch that a·1 =afor any a∈R.
A ring that satisfies Axiom R8 but not R7 is simply called a unitary ring , but we will have
no need for such an entity.
Definition 1.4. Let(R,+,·)be a unitary ring. The multiplicative inverse of an object
a∈Ris an object a−1∈Rfor which
a·a−1=a−1·a= 1.
We now have all the necessary pieces in place in order to give the following simple definition
of a field.
Definition 1.5. Afield is a unitary commutative ring (R,+,·)for which 1̸= 0, and every
a∈Rsuch that a̸= 0has a multiplicative inverse.
To summarize, a field is a set of objects F, together with binary operations +and·onF,
that are subject to the following field axioms :
F1.a+b=b+afor any a, b∈F.
F2.a+ (b+c) = (a+b) +cfor any a, b, c∈F.
F3.There exists some 0∈Fsuch that a+ 0 = afor any a∈F.
F4.For each a∈Fthere exists some −a∈Fsuch that −a+a= 0.
F5.a·(b·c) = (a·b)·cfor any a, b, c∈F.
F6.a·(b+c) =a·b+a·cfor any a, b, c∈F.
F7.a·b=b·afor all a, b∈F.
F8.There exists some 0̸= 1∈Fsuch that a·1 =afor any a∈F.3
F9.For each 0̸=a∈Fthere exists some a−1∈Fsuch that aa−1= 1.
Commonly encountered fields are the set of real numbers Runder the usual operations of
addition and multiplication, and also the set of complex numbers C. Many results in linear
algebra (but not all) are applicable to both the fields RandC, in which case we will employ
the symbol Fto denote either. That is, anywhere Fappears one can safely substitute either R
orCas desired. Throughout these notes a scalar will be taken to be an object belonging to a
field. Throughout the remainder of this chapter all scalars will be real numbers.
Example 1.6. The set of integers Zunder the usual operations of addition and multiplication
satisfies all the field axioms save for one: F9, the axiom that requires every nonzero element in
a set of objects to have a multiplicative inverse that also is an element of the set of objects. The
multiplicative inverse for 2 ∈Zis 2−1, and of course 2−1= 1/2 does not belong to Z. Therefore
Zis not a field under the usual operations of addition and multiplication.
In contrast, the set of rational numbers
Q=p
q:p, q∈Zandq̸= 0
is a field under the usual operations of addition and multiplication, since the reciprocal of any
nonzero rational number is also a rational number. ■
Example 1.7. Afinite field is a field that contains a finite number of elements. One example
is the set Z2={0,1}, with a binary operation + defined by
0 + 0 = 0 ,0 + 1 = 1 ,1 + 0 = 1 ,1 + 1 = 0 ,
and a binary operation ·defined by
0·0 = 0 ,0·1 = 0 ,1·0 = 0 ,1·1 = 1 .
Note that the only departure from “usual” addition and multiplication in evidence is 1 + 1 = 0.
It is straightforward, albeit tedious, to directly verify that each of the nine field axioms are
satisfied. ■4
1.2 – Real Euclidean Space
LetRdenote the set of real numbers. Given a positive integer n, we define real Euclidean
n-space , or simply n-space , to be the set
Rn={(x1, x2, . . . , x n) :xi∈Rfor 1≤i≤n}. (1.1)
Any ordered list of nobjects is called an n-tuple , and the n-tuple ( x1, x2, . . . , x n) of real
numbers, when regarded as an element of Rn, is called a point inn-space. Each value xiin
(x1, x2, . . . , x n) is called a coordinate of the point, with x1being the “first” coordinate, x2the
“second” coordinate, and so on. If xis a point in Rn, we write x∈Rnand take this as meaning
x= (x1, x2, . . . , x n)
for some real numbers x1, x2, . . . , x n. Ifxi= 0 for all 1 ≤i≤n, then we obtain the point
(0,0, . . . , 0) called the origin .
Euclidean 2-space is more commonly known as the plane , which is the set
R2={(x1, x2) :x1, x2∈R},
with each point ( x1, x2) in the plane (or “on the plane”) being a 2-tuple usually called an
ordered pair . Euclidean 3-space is customarily called simply space , which is the set
R3={(x1, x2, x3) :x1, x2, x3∈R},
with each point ( x1, x2, x3) in space being a 3-tuple usually called an ordered triple .
It is natural to assign a geometrical interpretation to the notion of a point on a plane or
in space. Specifically, in the case of a point p= (p1, p2)∈R2(i.e. a point pon a plane), it is
convenient to think of pas being “located” somewhere on the plane relative to the origin (0 ,0).
Exactly how the coordinates p1andp2of the point pare used to determine a location for pon
the plane depends on the coordinate system being used. In R2the rectangular and polar
coordinate systems are most commonly employed. In R3there are the rectangular, cylindrical,
and spherical coordinate systems, among others. Unless otherwise specified, we will always use
the rectangular coordinate system! For those who may not have encountered the rectangular
coordinate system in R3, Figure 2 should suffice to make its workings known. In the figure the
x1x2
p
p1p2
p1
x1x2
p
p2
Figure 1. At left: p= (p1, p2) in the rectangular coordinate system. At right:
p= (p1, p2) in the polar coordinate system.5
p1p2p3
x1x2x3
p= (p1, p2, p3)
p1p2p3
x1x2x3
p= (p1, p2, p3)
Figure 2. Stereoscopic image of R3with p= (p1, p2, p3) in the rectangular
coordinate system.
positive xi-axis is labeled for i= 1,2,3, and so the point p= (p1, p2, p3) shown has coordinates
pi>0 for each i.
It will be convenient to designate operations that allow for “adding” points, as well as
“multiplying” them by real numbers and “subtracting” them. The definitions for these operations
make use of the operations of addition and multiplication of real numbers which are taken to be
understood.
Definition 1.8. Letp= (p1, p2, . . . , p n)andq= (q1, q2, . . . , q n)be points in Rn, and c∈R.
Then we define the sum p+qofpandqto be the point
p+q= (p1+q1, p2+q2, . . . , p n+qn),
and the scalar multiple cpofpbycto be the point
cp= (cp1, cp2, . . . , cp n).
Defining −p= (−p1,−p2, . . . ,−pn), thedifference p−qofpandqis given to be
p−q=p+ (−q).6
1.3 – Located Vectors
Alocated vector inn-space is an ordered pair of points p, q∈Rn. We denote such an
ordered pair by#„pqrather than ( p, q), both to help distinguish it from a point in R2(which is an
ordered pair of numbers ), and also to reinforce the natural geometric interpretation of a located
vector as an “arrow” in n-space that starts at pand ends at q. We call ptheinitial point of#„pq, and qtheterminal point , and say that#„pqis “located at p.” If the initial point pis at the
origin (0 ,0, . . . , 0), then the located vector#„pqis called a position vector (a vector located at
the origin).
The situation in R2will be illustrative. In Figure 3 it can be seen that, if p= (p1, p2) and
q= (q1, q2), then#„pqmay be characterized as an arrow with initial point pthat decomposes into
a horizontal translation of q1−p1and a vertical translation of q2−p2.
Two located vectors#„pqand# „uvareequivalent , written#„pq∼# „uv, ifq−p=v−u. Again
considering the situation in R2, ifp= (p1, p2),q= (q1, q2),u= (u1, u2), and v= (v1, v2), then
#„pq∼# „uv⇔q−p=v−u
⇔(q1, q2)−(p1, p2) = (v1, v2)−(u1, u2)
⇔(q1−p1, q2−p2) = (v1−u1, v2−u2)
⇔q1−p1=v1−u1and q2−p2=v2−u2.
Thus#„pq∼# „uvinR2if and only if the arrows corresponding to the two located vectors decompose
into the same horizontal and vertical translations.
Ifo= (0, . . . , 0) is the origin in Rn,p= (p1, . . . , p n), and q= (q1, . . . , q n), then
#„pq∼#               „o(q−p).
This is verified by direct calculation:
q−p= (q1−p1, q2−p2) = (q1−p1, q2−p2)−(0,0) = ( q−p)−o.
Thus, any arbitrary location vector#„pqis equivalent to some position vector, and in the exercises
it will be established that the position vector equivalent to#„pqmust be unique.
xy
#„pq
pq
p1 q1p2q2
q1−p1q2−p2
Figure 3. A vector in the plane R2located at p.7
xy
#„ovv
o
Figure 4. Equivalent located vectors, all belonging to v.
Definition 1.9. Let#„ovbe a position vector in Rn. The equivalence class of#„ov, denoted by
v, is the set of all located vectors that are equivalent to#„ov. That is,
v={#„pq:#„ov∼#„pq}.
The equivalence class vof a located vector#„ovis also called the vector v.
The symbol vis usually handwritten as# „v. Ifv= (v1, . . . , v n), then it is common to denote
vby either
[v1, . . . , v n] or
v1...
vn
.
The row format exhibited in the first symbol will be used throughout this chapter, but later
on the column format of the second symbol will be favored. Thus v= [v1, . . . , v n] is the set of
located vectors that are equivalent to the position vector having v= (v1, . . . , v n) as its terminal
point. A vector of the form [ v1, . . . , v n], where the ithcoordinate viis a real number for
each 1 ≤i≤n, is called a Euclidean vector (orcoordinate vector ) to distinguish it from
the more abstract notion of vector that will be introduced in Chapter 3. Put another way, a
Euclidean vector is an equivalence class of located vectors in a Euclidean space Rn, and it is
fully determined by nreal-valued coordinates v1, . . . , v n.
The Euclidean zero vector is the vector 0whose coordinates are all equal to 0; thus if
0∈Rn, then
0= [ 0,0, . . . , 0|{z}
nzeros].
A useful way to think of a vector v̸=0in Euclidean n-space is as an arrow with a fixed
length and direction, but varying location. For instance we can take the located vector#„ov,
naturally depicted as an arrow with initial point at the origin oand terminal point at the point
v, and move the arrow around in a way that preserves its length and direction. See Figure 4.
Remark. If a located vector#„pqis equivalent to#„ov, then strictly speaking we say that#„pqbelongs
to the equivalence class of located vectors known as vector v. However, sometimes the symbol#„pqitself may be used to represent the vector v, which is in keeping with the common practice8
xy
uv
u+v
uu+v
o xy
uv
vu
uu+v
v
o
Figure 5. The geometry of vector addition.
in mathematics of letting any member of an equivalence class be a representative of that class.
Other times we may be given a located vector#„pqin a situation when location is irrelevant, and
so refer to#„pqas simply a vector.
Example 1.10. InR3letp= (2,−3,4) and q= (−5,−2,8). Find v= (v1, v2, v3) so that#„pq∼#„ov, where o= (0,0,0).
Solution. By definition#„pq∼#„ovmeans q−p=v−o, or
(−5,−2,8)−(2,−3,4) = ( v1, v2, v3)−(0,0,0) = ( v1, v2, v3).
Thus we have
v= (v1, v2, v3) = (−5−2,−2−(−3),8−4) = (−7,1,4).
It follows from this calculation that the located vector#„pqbelongs to the equivalence class
of located vectors known as the vector v= [−7,1,4]. The symbol#„pqitself could be used to
represent the vector [ −7,1,4], and we may even say that#„pqand [−7,1,4] are the “same vector”
if location in R3is unimportant. ■
As with points we define operations that allow for adding and subtracting Euclidean vectors,
and also multiplying them by real numbers.
Definition 1.11. Letu= [u1, . . . , u n]andv= [v1, . . . , v n]be Euclidean vectors in Rn, and
c∈R. Then we define the sum u+vofuandvto be the vector
u+v= [u1+v1, . . . , u n+vn],
and the scalar multiple cvofvbycto be the vector
cv= [cv1, . . . , cv n].
Defining −v= (−1)v, thedifference u−vofuandvis given to be
u−v=u+ (−v).9
There is some geometrical significance to the sum of two vectors, and it suffices to consider
the situation in R2to appreciate it. Define vectors u= [u1, u2] and v= [v1, v2] in the plane.
One representative of uis the located vector# „ou. As for v, from
(u+v)−u=u+v−u=v=v−o
we have#                „u(u+v)∼#„ov,
and so#                „u(u+v)is a located vector—in fact the only located vector—having initial point uthat
can represent v. Finally, a representative for u+vis the located vector
#                „o(u+v).
Now, if the located vectors# „ou,#                „u(u+v), and#                „o(u+v)are all drawn as arrows in R2, they will
be seen to form a triangle such as the one at left in Figure 5. Indeed if#„ov, also representing v,
and#                „v(u+v)
—easily seen to be another representative of u—are also drawn as arrows, then a parallelogram
such as the one at right in Figure 5 results. In the figure, it should be pointed out, the various
located vectors are labeled only by the vector ( uorv) that they represent.
After this section we will refer to located vectors only infrequently, and instead focus mostly
on vectors. Until Chapter 3 the vectors will be strictly of the Euclidean variety, viewed naturally
as arrows in Rnwhich have length and direction but no particular location. Also we will often
use the symbol Rnto denote the set of all Euclidean vectors of the form [ x1, . . . , x n], rather
than the set of all points ( x1, . . . , x n). That is,
Rn={[x1, . . . , x n] :xi∈Rfor 1≤i≤n}.
There is no substantive difference between this definition for Rnand the one given by equation
(1.1); there is only a difference in interpretation.
Definition 1.12. Two vectors u,vareparallel if there exists some scalar c̸= 0such that
u=cv.10
1.4 – The Dot Product
We have established operations that add and subtract vectors, and also multiply them by
real numbers. Now we define a way of “multiplying” vectors that is known as the dot product.1
Definition 1.13. Letu= [u1, . . . , u n]andv= [v1, . . . , v n]be two vectors in Rn. Then the dot
product ofuandvis the real number
u·v=u1v1+u2v2+···+unvn=nX
i=1uivi.
Thus, if uandvare vectors in R2, then
u·v= [u1, u2]·[v1, v2] =u1v1+u2v2.
Some properties of the dot product now follow.
Theorem 1.14. For any vectors u,v,w∈Rnand scalar c,
1.u·v=v·u
2.u·(v+w) =u·v+u·w
3. (cu)·v=c(u·v) =u·(cv)
4.u·u>0ifu̸=0
Proof. Proof of (2):
u·(v+w) = [u1, . . . , u n]· 
[v1, . . . , v n] + [w1, . . . , w n]
= [u1, . . . , u n]·[v1+w1, . . . , v n+wn]
=nX
i=1ui(vi+wi) =nX
i=1(uivi+uiwi)
=nX
i=1uivi+nX
i=1uiwi=u·v+u·w,
using the established summation propertyP(ai+bi) =Pai+Pbi.
Proofs for the other dot product properties are left to the exercises. ■
Definition 1.15. Two vectors u,vareorthogonal , written u⊥v, ifu·v= 0.
Orthogonal vectors are also said to be perpendicular , and in the next section we shall see
that this means precisely what we expect: the vectors form a right angle.
Example 1.16. Find two mutually perpendicular vectors in R3that are each perpendicular to
v= [2,−1,3]
1The dot product is also called the “scalar product” in some books.11
Solution. We need to find vectors u= [u1, u2, u3] and w= [w1, w2, w3] such that
u·w=u·v=w·v= 0.
From this we obtain a system of equations:


2u1− u2+ 3 u3= 0
2w1−w2+ 3w3= 0
u1w1+u2w2+u3w3= 0
There are six variables but only three equations, and so we can expect that there are an infinite
number of solutions. To satisfy the third equation we may choose, quite arbitrarily, to let
u1w1= 1,u2w2=−2, and u3w3= 1, so that
w1=1
u1, w 2=−2
u2,and w3=1
u3. (1.2)
Substituting these into the system’s second equation yields
2
u1+2
u2+3
u3= 0. (1.3)
Now, from the system’s first equation we have u2= 2u1+ 3u3, which we substitute into (1.3) to
obtain2
u1+2
2u1+ 3u3+3
u3= 0.
From this, with a little algebra, we obtain a quadratic equation:
2u2
3+ 5u1u3+ 2u2
1= 0.
We employ the quadratic formula to solve this equation for u3:
u3=−5u1±p
25u2
1−4(2)(2 u2
1)
2(2)=−5u1±3|u1|
4.
If we set u1= 1 (again an arbitrary choice we’re free to make), then we find that
u3=−5±3
4=−2,−1
2.
If we choose u3=−2, then we have
u2= 2u1+ 3u3= 2(1) + 3( −2) =−4,
and so u= [1,−4,−2]. Also from (1.2) we have
w=1
u1,−2
u2,1
u3
=
1,1
2,−1
2
.
Therefore
[1,−4,−2] and
1,1
2,−1
2
are two mutually perpendicular vectors that are each perpendicular to [2 ,−1,3]. There are
infinitely many other possibilities. ■12
1.5 – The Norm of a Vector
Definition 1.17. Thenorm of a vector v∈Rnis∥v∥=√v·v.
Ifv= [v1, . . . , v n], then
∥v∥=p
[v1, . . . , v n]·[v1, . . . , v n] =rXn
i=1v2
i (1.4)
The norm of a vector is also known as the vector’s magnitude orlength . Consider a located
vector#„ovin the plane, which is a convenient representative of the vector v= [v1, v2]. In §1.2 we
saw that#„ovmay be depicted as an arrow that starts at the origin o= (0,0) and ends at the
point v= (v1, v2). How long is the arrow? The answer is given by the conventional (Euclidean)
distance d(o, v) between oandvthat is derived from the familiar Pythagorean Theorem:
d(o, v) =p
(v1−0)2+ (v2−0)2=p
v2
1+v2
2.
On the other hand from (1.4) we have
∥v∥=p
v2
1+v2
2,
and so ∥v∥=d(o, v), the length of the arrow#„ovrepresenting v. Note that if#„pq∼#„ov, where
p= (p1, p2) and q= (q1, q2), then
d(p, q) =p
(q1−p1)2+ (q2−p2)2=p
v2
1+v2
2=d(o, v) =∥v∥
since q1−p1=v1andq2−p2=v2, and so it does not matter which located vector we choose
to represent v: the length of the arrow will be the same! These truths stay true in R3using
the usual Euclidean conception of distance in three-dimensional space. In fact, in light of the
following definition they remain true in Rnfor all n.
Definition 1.18. Letx,y∈Rn. The distance d(x,y), between xandyis given by
d(x,y) =∥x−y∥.
Thus if x= [x1, . . . , x n] and y= [y1, . . . , y n], then
d(x,y) =rXn
i=1(xi−yi)2,
which reduces to the usual formula for the distance between points xandywhen nequals 2 or
3. That is, d(x,y) =d(x, y) inR2orR3.
Remark. From now on we will frequently use the bold-faced symbol xfor the vector [x1, . . . , x n]
to represent the point x= (x1, . . . , x n). The logic of doing this is thus: a point xis naturally
identified with its corresponding position vector# „ox, and# „oxis naturally identified with x. Such
“vectorization” of points allows for a uniform notation in the statement of momentous results
in vector calculus and the sciences. Moreover it places everything under consideration in the
setting of a “vector space,” which is the main object of study in linear algebra. So it must be13
vu
ou
vcx
ou
vc v
Figure 6.
remembered: depending on context, x= [x1, . . . , x n] may be interpreted as a vector, a located
vector, or a point!2
We are now in a position to justify Definition 1.15, by which we mean ground the definition
in more familiar geometric soil. Suppose u,v∈Rnare orthogonal vectors, which is to say
u·v= 0 and (since the dot product is commutative) v·u= 0. Recall that located vectors
representing u,vandu+vmay be chosen so that their corresponding arrows form a triangle,
as at left in Figure 5. A triangle is a planar figure so it does not matter if the located vectors
are in an n-space for some n >2: we can always orient the situation so that it lies on a plane.
Now,∥u+v∥is the length of the longest side of the triangle, and ∥u∥and∥v∥are the lengths
of the shorter sides. From the calculation
∥u+v∥2=p
(u+v)·(u+v)2
= (u+v)·(u+v)
= (u+v)·u+ (u+v)·v=u·u+v·u+u·v+v·v
=∥u∥2+∥v∥2,
it can be seen that the lengths of the triangle’s sides obey the Pythagorean Theorem, and so it
must be that the triangle is a right triangle. That is, the sides formed by the located vectors
representing uandvmust meet at a right angle and therefore be perpendicular! It is in this
sense that orthogonal vectors are also said to be “perpendicular.”
Proposition 1.19. Ifu,v∈Rnare orthogonal vectors, then ∥u+v∥2=∥u∥2+∥v∥2.
The proof has already been furnished above.
Definition 1.20. Letv̸=0. The orthogonal projection of uonto v,projvu, is given by
projvu=u·v
v·v
v.
Once again it should help to ground the definition in geometry, because ultimately it is
geometry that motivates the definition. Let u,v∈Rnwithv̸=0. We represent these vectors
by located vectors with common initial point oas at left in Figure 6. For any c∈Rletvc=cv.
We wish to find the value for cso that the vector xrepresented by located vector#   „vcuat right in
Figure 6 is orthogonal to v. This means cmust be such that x·v= 0, and since vc+x=uwe
obtain
(u−vc)·v= 0
2It was Henri Poincar´ e who said “Mathematics is the art of giving the same name to different things.”14
and thus
u·v−vc·v=u·v−(cv)·v= 0.
Since ( cv)·v=c(v·v) we finally arrive at
c=u·v
v·v. (1.5)
Now, consider the right side of Figure 6 again. It can be seen that the vector vc, as pictured,
would be the shadow that uwould cast upon vwere a light to be directed upon the scene
from directly overhead. It is in this sense that vcis a projection of uontov—in particular
theorthogonal projection, since the “light rays” casting the “shadow” are perpendicular to v.
Multiplying both sides of equation (1.5) by vgives
vc=u·v
v·v
v,
which is projvuas given in Definition 1.20.
Lemma 1.21. Ifu,v∈Rn,v̸=0, and cis as in (1.5), then u−cvis orthogonal to v.
Proof. Taking the dot product,
(u−cv)·v=u·v−c(v·v) =u·v−u·v
v·v
(v·v) =u·v−u·v= 0,
we immediately conclude that u−cv⊥v. ■
It’s a worthwhile exercise to verify that if u⊥v, then u⊥avfor any scalar a. The lemma
will be used to prove the following.
Theorem 1.22 (Schwarz Inequality ).Ifu,v∈Rn, then |u·v| ≤ ∥u∥∥v∥.
Proof. Suppose u,v∈Rn. Ifu=0orv=0, then
|u·v|=|0|= 0 = ∥u∥∥v∥,
which affirms the theorem’s conclusion. So, suppose u,v̸=0, and let c∈Rbe given by (1.5).
Now,
(u−cv)·(cv) =c[(u−cv)·v] =c(0) = 0 ,
where ( u−cv)·v= 0 by Lemma 1.21. Thus u−cvandcvare orthogonal, and by Proposition
1.19
∥u∥2=∥(u−cv) +cv∥2=∥u−cv∥2+∥cv∥2.
Since∥u−cv∥2≥0, this implies that ∥cv∥2≤ ∥u∥2. However,
∥cv∥2=c2∥v∥2=u·v
v·v2
(v·v) =(u·v)2
v·v=(u·v)2
∥v∥2,
and so from ∥cv∥2≤ ∥u∥2we obtain
(u·v)2
∥v∥2≤ ∥u∥2,
whence comes ( u·v)2≤ ∥u∥2∥v∥2. Taking the square root of both sides completes the proof. ■15
From the Schwarz inequality we have
− ∥u∥∥v∥ ≤u·v≤ ∥u∥∥v∥,
and thus
−1≤u·v
∥u∥∥v∥≤1
for any u,v̸= 0. This observation justifies the following definition.
Definition 1.23. Letu,v∈Rnbe nonzero vectors. The angle between uandvis the number
θ∈[0, π]for which
cosθ=u·v
∥u∥∥v∥. (1.6)
Since the function cos: [0, π]→[−1,1] is one-to-one and onto, and the fraction in (1.6) only
takes values in [ −1,1], there will always exist a unique value θ∈[0, π] that satisfies (1.6). From
Definition 1.23 we have a new formula for the dot product:
u·v=∥u∥∥v∥cosθ. (1.7)
Some textbooks give this formula as the definition of the dot product, but it is less desirable
since the idea of a dot product is then founded on a geometric notion of angle that becomes
problematic to visualize in Rnforn >3. However it is worthwhile verifying that the definition
of angle between vectors, as given here, agrees with our geometric intuition. For the sake of
simplicity we can assume that uandvare nonzero vectors in R2, though the situation does not
alter in Rnforn >2 since two vectors can always be represented by coplanar located vectors.3
The approach will be to let θbe the geometric angle between uandv, and then show that (1.7)
must necessarily follow.
Let 0 < θ < π . The vectors u,v, and u−vmay be represented by located vectors that form
the triangle in Figure 7 (for convenience we depict θas an acute angle).
By the Law of Cosines we obtain
∥u−v∥2=∥u∥2+∥v∥2−2∥u∥∥v∥cosθ,
and since we’re assuming that u= [u1, u2] and v= [v1, v2], we obtain u−v= [u1−v1, u2−v2]
so that
(u1−v1)2+ (u2−v2)2= (u2
1+u2
2) + (v2
1+v2
2)−2∥u∥∥v∥cosθ,
3This is because two located vectors can be defined by three points p,q, and r, such as#„pqand#„pr, and three
points define a plane.
uu−v
v θ
Figure 7.16
and hence
∥u∥∥v∥cosθ=u1v1+u2v2=u·v.
In the cases when θ= 0 or θ=πwe find that v=ku= [ku1, ku 2] for some nonzero scalar
k; that is, uandvare parallel vectors, and we have
∥u∥∥v∥cosθ=∥u∥∥ku∥cosθ=|k|(u2
1+u2
2) cosθ. (1.8)
Ifθ= 0, then k >0 so that |k|=kandcosθ= 1; and if θ=π, then k <0 so that |k|=−k
and cos θ=−1. In either case, from (1.8) we obtain
∥u∥∥v∥cosθ=k(u2
1+u2
2) = [u1, u2]·[ku1, ku 2] =u·v
as desired.
Example 1.24. Letu= [2,−1,5] and v= [−1,1,1].
(a) Find ∥u∥and∥v∥.
(b) Find projvu, the orthogonal projection of uontov.
(c) Find projuv, the orthogonal projection of vontou.
(d) Find the angle between uandvto the nearest tenth of a degree.
Solution.
(a) We have
∥u∥=p
22+ (−1)2+ 52=√
30 and ∥v∥=p
(−1)2+ 12+ 12=√
3.
(b) Since
u·v= (2)( −1) + (−1)(1) + (5)(1) = 2 and v·v=∥v∥2= (√
3)2= 3,
we have
projvu=u·v
v·v
v=2
3[−1,1,1] =
−2
3,2
3,2
3
.
(c) Since
v·u=u·v= 2 and u·u=∥u∥2= (√
30)2= 30,
we have
projuv=v·u
u·u
u=2
30[2,−1,5] =2
15,−1
15,1
3
.
(d) By definition,
cosθ=u·v
∥u∥∥v∥=2√
30√
3=2
3√
10,
and thus
θ= cos−12
3√
10
≈77.8◦.
■
Example 1.25. Find the measure of the angle θbetween the diagonal of a cube and the
diagonal of one of its faces, as shown in Figure 8.17
θ θ
Figure 8.
Solution. It will be convenient to regard the cube as existing in R3, with edges of length 1,
and the vertex where the two diagonals meet situated at the origin (0 ,0,0). We can then set
up coordinate axes such that the cube diagonal has endpoints (0 ,0,0) and (1 ,1,1), and the
face diagonal has endpoints (0 ,0,0) and (0 ,1,1). Thus the diagonals can be characterized as
positions vectors u= [1,1,1] and v= [0,1,1]. Now,
cosθ=u·v
∥u∥∥v∥=[1,1,1]·[0,1,1]√
12+ 12+ 12√
02+ 12+ 12=2√
6,
and so
θ= cos−12√
6
≈35.264◦
is the angle’s measure. ■
Problems
1.Find the measure of the angle θbetween the diagonal of a cube and one of its edges, as
shown below.
θ18
1.6 – Lines and Planes
InR2a line Lis typically defined to be the solution set to an equation of the form ax+by=c
for constants a, b, c∈R, where aandbare not both zero. That is, Lis the set of points
{(x, y) :ax+by=C},
andax+by=Cis called the Cartesian equation (oralgebraic equation ) for L. InRnfor
n >2 we can still speak geometrically of lines, of course, but it becomes impossible to define the
line using a single Cartesian equation. The most convenient remedy for this is to use vectors,
thereby motivating the following definition.
Definition 1.26. Letp,v∈Rnwithv̸=0. The line through pand parallel to v∈Rnis the
set of vectors of the form
{p+tv:t∈R}.
Aparametric equation (orparametrization ) of a line L={p+tv:t∈R} ⊆Rnis any
vector-valued function x:R→Rngiven by
x(t) =˜p+t˜v
for some ˜p∈Land vector ˜vparallel to v. (Here tis called a parameter .) Thus we find that
x(t) =p+tv
is one parametrization for L, but there are infinitely many others in existence.
Given a parametrization x(t) =p+tvfor some line in Rn, the vector p= [p1, . . . , p n] may
more naturally be thought of as the position vector#„opof the point p= (p1, . . . , p n), and so in
everyday speech pmay be referred to as a point even though mathematically it is handled as a
vector. The same applies to the vector
x(t) = [x1(t), . . . , x n(t)]
for each t∈R: we may regard it, if desired, as the position vector of the point
x(t) = (x1(t), . . . , x n(t)),
and so refer to it as a point. In contrast, for each t∈Rthe vector tvmay be thought of as a
localized vector (i.e. an arrow) with initial point at pand terminal point located at another
point on the line.
Definition 1.27. Theline segment inRnwith endpoints p,q∈Rnis the set of vectors of
the form
{p+t(q−p) :t∈[0,1]}.
A natural parametrization for a line segment with endpoints pandqis the vector-valued
function x: [0,1]→Rngiven by
x(t) =p+t(q−p), (1.9)19
though it is frequently the case in applications that other parametrizations may be considered.
In(1.9) we have x(0) = pandx(1) = q, and so as tincreases from 0 to 1 we see that we “travel”
along the line segment from ptoq. However, the alternative parametrization
x(t) =q+t(p−q)
reverses the direction of travel.
Example 1.28. Find a parametrization x(t) of the line containing the points p= (2,−6,9)
andq= (0,8,1), such that x(1) = pandx(−2) =q.
Solution. We must have x(t) =p+f(t)(q−p) for some function fsuch that f(1) = 0 and
f(−2) = 1. The simplest such function is a linear one, which is to say f(t) =mt+bfor constants
mandb. With the condition f(1) = 0 we obtain b=−m, so that f(t) =m(t−1). With the
condition f(−2) = 1 we obtain 1 = m(−2−1), or m=−1/3, and hence b= 1/3. Now we have
x(t) =p+ 
−1
3t+1
3
(q−p)
forp= [2,−6,9] and q= [0,8,1], giving
x(t) =4
3,−4
3,19
3
+t2
3,−14
3,8
3
.
Other answers are possible if we choose fto be a nonlinear function. ■
InR3a line Pis sometimes defined to be the solution set to an equation of the form
ax+by+cz=dfor constants a, b, c, d ∈R, where a,b,care not all zero. That is, Pis the set
of points
{(x, y, z ) :ax+by+cz=d},
where ax+by+cz=dis the Cartesian equation forP. InRnforn >3 we may still wish
to conceive of planes, but it is no longer possible to define the plane using a single Cartesian
equation. The following definition uses vectors to define the notion of a plane for all Rnwith
n≥3.
Definition 1.29. Letu,v∈Rnbe nonzero, nonparallel vectors. The plane through point
p∈Rnand parallel to u,vis the set of vectors of the form
{p+su+tv:s, t∈R}.
Aparametric equation (orparametrization ) of a plane P={p+su+tv:t∈R} ⊆Rn
is any vector-valued function x:R2→Rngiven by
x(s, t) =˜p+s˜u+t˜v
for some ˜p∈Pand vectors ˜uand˜vparallel to uandv, respectively. (Here and sandtare
called the parameters .) Thus
x(s, t) =p+su+tv (1.10)
is one parametrization for Pamong infinitely many.
Anormal vector for a plane Phaving parametrization (1.10) is a nonzero vector nsuch
thatn·u= 0 and n·v= 0. A line Lis said to be orthogonal toPifLis parallel to n. IfL20
is orthogonal to Pandp∈L∩P(i.e.pis the point of intersection between LandP), then the
distance between any point q∈LandPis the length of the line segment pq.
Example 1.30. Find both a parametric and Cartesian equation for the plane Pcontaining the
point (0 ,0,0) that is orthogonal to the line Lhaving parametric equation
x(t) = [3 ,−2,1] +t[2,1,−3].
Solution. By definition any normal vector nforPmust be parallel to L, which in turn means
thatnmust be parallel to a direction vector of L. Since [2 ,1,−3] is an obvious direction vector
ofL, we may let n= [2,1,−3]. Geometrically speaking, since Pcontains the point o= (0,0,0),
Pwill consist precisely of those points ( x, y, z ) for which the vector [ x, y, z ]−[0,0,0] = [ x, y, z ]
is orthogonal to n. Since
n·[x, y, z ] = 0 ⇔[2,1,−3]·[x, y, z ] = 0 ⇔2x+y−3z= 0,
we conclude that 2 x+y−3z= 0 is a Cartesian equation for P.
To find a parametric equation, we use the Cartesian equation to find two other points on P
besides (0 ,0,0), such as p= (1,−2,0) and q= (0,3,1). Now let
u=p−0= [1,−2,0] and v=q−0= [0,3,1].
A parametric equation for Pisx(s, t) =0+su+tv, or
x(s, t) =s[1,−2,0] +t[0,3,1]
fors, t∈R. ■
Example 1.31. Find a normal vector for the plane 3 x+ 2y−2z= 3.
Solution. We first find three points on the plane that are not collinear. This can be done by
substituting values for xandyin the equation, say, and then solving for z. In this way we find
points (0 ,0,1/7), (1 ,1,1), and (1 ,2,2).
Example 1.32. Find the distance between the point q= (1,−2,4) and the plane 3 x+2y−2z= 3.
Solution. Letting x=y= 0 in the plane’s equation gives z= 1/7, so p= (0,0,1/7) is a point
on the plane. Let
v=#„pq=q−p=
5,2,−22
7
.
A normal vector for the plane is n= [4,−4,7]. We project vonton:
projn(v) =v·n
n·n
n=−10
81[4,−4,7].
The magnitude of this vector,
D=∥projn(v)∥=10
9,
is the sought-after distance. ■21
Problems
1.LetL1be the line given by x(t) = [1 ,1,1] +t[2,1,−1], and let L2be the line with Cartesian
equations
x= 5, y−4 =z−1
2.
(a) Show that the lines L1andL2intersect, and find the point of intersection.
(b) Find a Cartesian equation of the plane containing L1andL2.
2.LetPbe the plane in R3which has normal vector n= [1,−4,2] and contains the point
a= (5,1,3).
(a) Find a Cartesian equation for P.
(b) Find a parametric equation for P.22
2
Matrices and Systems
2.1 – Matrices
Letm, n∈N, and let Fbe a field. An m×nmatrix over Fis a rectangular array of
elements of Farranged in mrows and ncolumns:

a11a12··· a1n
a21a22··· a2n............
am1am2··· amn
. (2.1)
The values mandnare called the dimensions of the matrix. The scalar (i.e. element of
F) in the ith row and jth column of the matrix, aij, is known as the ij-entry . To be clear,
throughout these notes the entries aijof a matrix are always taken to be elements of some field
F, which could be the real number system R, the complex number system C, or some other field.
A 1×1 matrix [ a] is usually identified with the scalar a∈Fthat constitutes its sole entry.
Forn≥2, both n×1 and 1 ×nmatrices are called vector matrices (or simply vectors ). In
particular an n×1 matrix
x1
x2...
xn
(2.2)
is acolumn vector (orcolumn matrix ), and a 1 ×nmatrix
x1x2···xn
is arow vector (orrow matrix ). Henceforth the Euclidean vector [ x1, . . . , x n] introduced in
Chapter 1 will most of the time be represented by its corresponding column vector (2.2) so as
to take advantage of the convenient properties of matrix arithmetic.
The matrix (2.1) we typically denote more compactly by the symbol
[aij]m,n,23
which indicates that the ij-entry is the scalar aij, where i∈ {1, . . . , m }is the row number and
j∈ {1, . . . , n }is the column number. We call the sets {1, . . . , m }and{1, . . . , n }therange of
the indexes iandj, respectively. If m=nthen a square matrix results, and we define
[aij]n= [aij]n,n.
(Care should be taken with this notation: [ aij]m,ndenotes an m×nmatrix, while [ aij]mndenotes
anmn×mnsquare matrix!) If the range of the indexes iandjare known or irrelevant, we will
write (2.1) as simply [ aij]. Another word about square matrices: The diagonal entries of a
square matrix [ aij]nare the entries with matching row and column number: a11, . . . , a nn.
Very often we have no need to make any reference to the entries of a matrix, in which case
we will usually designate the matrix by a bold-faced upper-case letter such as A,B,C, and so
on. The exception is vector matrices, which are normally labeled with bold-faced lower-case
letters such as a,b,x,yand so on. If we need to make reference to the ij-entry of a matrix A,
then the symbol [ A]ijstands ready to denote it. Thus if A= [aij]m,n, then
[A]ij=aij.
The set of all m×nmatrices with entries in the field Fwill be denoted by Fm×n. That is,
Fm×n=
[aij]m,n:aij∈Ffor all 1 ≤i≤m, 1≤j≤n	
.
From this point onward we also define
Fn=Fn×1
in these notes; that is, Fnis the set of matrices consisting of nentries from Farranged in a
single column. The exception has already been encountered: throughout the first chapter (and
only the first chapter) we always took Rnto signify R1×n. In the wider world of mathematics
beyond these notes the symbol Fndenotes either row vectors (elements of F1×n) or column
vectors (elements of Fn×1), depending on an author’s whim.
Ifaij= 0 for all 1 ≤i≤mand 1 ≤j≤n, then we obtain the m×nzero matrix
Om,n= [0] m,n=
0 0··· 0
0 0··· 0
............
0 0··· 0

having mrows and ncolumns of zeros. In particular we define
On=On,n.
In any case the symbol Owill always denote a zero matrix of some kind, whereas 0will continue
to denote more specifically a zero vector (i.e. a row or column matrix consisting of zeros).
Definition 2.1. IfA,B∈Fm×nandc∈F, then we define sum A+Bandscalar multiple
cAto be the matrices in Fm×nwithij-entry
[A+B]ij= [A]ij+ [B]ijand [cA]ij=c[A]ij
for all 1≤i≤mand1≤j≤n.24
Put another way, letting A= [aij] and B= [bij], we have
A+B= [aij+bij] =
a11+b11a12+b12··· a1n+b1n
a21+b21a22+b22··· a2n+b2n............
am1+bm1am2+bm2··· amn+bmn

and
cA= [caij] =
ca11ca12··· ca1n
ca21ca22··· ca2n............
cam1cam2··· camn
.
Thus matrix addition and matrix scalar multiplications is analogous to the addition and scalar
multiplication of Euclidean vectors. Clearly matrix addition is commutative, which is to say
A+B=B+A
for any A,B∈Fm×n. We define the additive inverse ofAto be the matrix −Agiven by
−A= (−1)A= [−aij].
That
A+ (−A) =−A+A=O
is straightforward to check.
Definition 2.2. LetA∈Fm×n. The transpose ofAis the matrix A⊤∈Fn×msuch that
[A⊤]ij= [A]ji
for all 1≤i≤nand1≤j≤m.
Put another way, if A= [aij]m,n, then the transpose of Ais the matrix A⊤= [αji]n,mwith
αji=aijfor each 1 ≤j≤n, 1≤i≤m. Thus the number aijin the ith row and jth column of
Ais in the jth row and ith column of A⊤, so that
A⊤=
a11a21··· am1
a12a22··· am2............
a1na2n··· amn
. (2.3)
Comparing (2.3) with (2.1), it can be seen that the rows of Asimply become the columns of
A⊤. For example if
A=
−3 7 4
6−5 10
,
then
A⊤=
−3 6
7−5
4 10
.25
It is easy to see that ( A⊤)⊤=A. We say Aissymmetric ifA⊤=A, andskew-symmetric
ifA⊤=−A. The set of all symmetric n×nmatrices with entries in the field Fwill be denoted
by Symn(F); that is,
Symn(F) =
A∈Fn×n:A⊤=A	
.
The symbol Skw n(F) will denote the set of all skew-symmetric n×nmatrices with entries in F:
Skw n(F) =
A∈Fn×n:A⊤=−A	
.
A standard approach to proving that two matrices AandBare equal is to first confirm
that they have the same dimensions, and then show that the ij-entry of the matrices are equal
for any iandj. Thus we verify that AandBarem×nmatrices (a step that may be omitted
if it is clear), then verify that [ A]ij= [B]ijfor arbitrary 1 ≤i≤mand 1≤j≤n. The proof of
the following proposition illustrates the method.
Proposition 2.3. LetA,B∈Fm×n, and let c∈F. Then
1. (cA)⊤=cA⊤
2. (A+B)⊤=A⊤+B⊤
3. (A⊤)⊤=A.
Proof.
Proof of Part (1). Fix 1≤i≤mand 1 ≤j≤n. Then, applying Definitions 2.1 and 2.2,
[(cA)⊤]ij= [cA]ji=c[A]ji=c[A⊤]ij= [cA⊤]ij.
So we see that the ij-entry of ( cA)⊤equals the ij-entry of cA⊤, and since iandjwere arbitrary,
it follows that ( cA)⊤=cA⊤.
Proof of Part (2). We have
[(A+B)⊤]ij= [A+B]ji= [A]ji+ [B]ji= [A⊤]ij+ [B⊤]ij= [A⊤+B⊤]ij,
so the ij-entries of ( A+B)⊤andA⊤+B⊤are equal. ■
The proof of part (3) of Proposition 2.3, which can be done using the same “entrywise”
technique, is left as a problem.
Thetrace of a square matrix A= [aij]n,n, written tr(A), is the sum of the diagonal entries
ofA:
tr(A) =nX
i=1aii. (2.4)
Since A⊤= [αij]n,nsuch that αij=aji, we readily obtain
tr(A⊤) =nX
i=1αii=nX
i=1aii= tr(A).
Other properties of the trace and transpose operations will be established in future sections.
Ablock matrix is a matrix whose entries are themselves matrices. The matrices that
constitute a block matrix are called submatrices . In practice a block matrix is typically26
constructed from an ordinary matrix A∈Fm×nby partitioning the entries into two or more
smaller arrays with the placement of vertical or horizontal rules, such as

a11··· a1s a1,s+1··· a1n..................
ar1··· ars ar,s+1··· arn
ar+1,1··· ar+1,sar+1,s+1··· ar+1,n..................
am1··· ams am,s+1··· amn
, (2.5)
which partitions the matrix A= [aij]m,ninto four submatrices

a11··· a1s.........
ar1··· ars
,
a1,s+1··· a1n.........
ar,s+1··· arn
,
ar+1,1··· ar+1,s.........
am1··· ams
,
ar+1,s+1··· ar+1,n.........
am,s+1··· amn
,
where of course 1 ≤r < m and 1 ≤s < n . If we designate the above submatrices as A1,A2,
A3, and A4, respectively, then we may write (2.5) as the block matrix
A1A2
A3A4
or
A1A2
A3A4
,
with the latter representation being preferred in these notes except in certain situations. A
block matrix is also known as a partitioned matrix .
Problems
1. Prove that ( A⊤)⊤=Afor any A∈Fm×n.
2. Prove that ( A+B+C)⊤=A⊤+B⊤+C⊤for any A,B,C∈Fm×n.
3. Prove that ( aA+bB)⊤=aA⊤+bB⊤for any A,B∈Fm×nanda, b∈F.27
2.2 – Matrix Multiplication
The definition of the product of two matrices is relatively more involved than that for
addition or scalar multiplication.
Definition 2.4. LetA∈Fm×nandB∈Fn×p. Then the product ofAandBis the matrix
AB∈Fm×pwithij-entry given by
[AB]ij=nX
k=1[A]ik[B]kj
for1≤i≤mand1≤j≤p.
Letting A= [aij]m,nandB= [bij]n,p, it is immediate that AB= [cij]m,pwith ij-entry
cij=nX
k=1aikbkj.
That is,
AB= [aij]m,n[bij]n,p=hXn
k=1aikbkji
m,p, (2.6)
where it’s understood that 1 ≤i≤mis the row number and 1 ≤j≤pis the column number
of the entryPn
k=1aikbkj.
Example 2.5. If
A=
−3 0 6
2 11 −5
and B=
4 9 −6
0−1 2
−4 0 −3
,
so that Ais a 2×3 matrix and Bis a 3×3 matrix, then ABis a 2×3 matrix given by
AB=
−3 0 6
2 11 −5
4 9 −6
0−1 2
−4 0 −3

=
−3 0 6
4
0
−4
−3 0 6
9
−1
0
−3 0 6
−6
2
−3

2 11 −5
4
0
−4
2 11 −5
9
−1
0
2 11 −5
−6
2
−3


=
−36−27 0
28 7 25
.
The product BAis undefined. ■28
Vectors may be used to better see how the product ABis formed. Let
ai=ai1···ain
denote the row vectors ofAfor 1≤i≤m,


a1→a11a12··· a1n
a2→a21a22··· a2n...............
am→am1am2···amn=A(2.7)
and let
bj=
b1j...
bnj

denote the column vectors ofBfor 1≤j≤p,
B=b1b2···bp
↓ ↓ ↓

b11b12···b1p
b21b22···b2p............
bn1bn2···bnp. (2.8)
Then by definition
AB= [aibj]m,p=
a1b1a1b2···a1bp
a2b1a2b2···a2bp............
amb1amb2···ambp
,
which makes clear that the ij-entry is
[AB]ij=aibj=
ai1···ain
b1j...
bnj
=ai1b1j+ai2b2j+···+ainbnj=nX
k=1aikbkj,
in agreement with Definition 2.4. Note that ABis not defined if the number of columns in Ais
not equal to the number of rows in B!
It is common—and convenient—to denote matrices (2.7) and (2.8) by the symbols

a1
a2...
am
andb1b2···bp
,
respectively, and so we have
AB=
a1
a2...
am
b1b2···bp
=
a1b1a1b2···a1bp
a2b1a2b2···a2bp............
amb1amb2···ambp
. (2.9)29
For any j= 1, . . . , p we have bj∈Fn, which is to say bjhasnrows and so Abjcan be computed
following the pattern of (2.9):
Abj=
a1
a2...
am
bj=
a1bj
a2bj...
ambj
.
(This can be verified easily by working directly with Definition 2.4.) Comparing this result with
the right-hand side of (2.9), we see that Abjis the jth column vector of AB; that is, we have
the following.
Proposition 2.6. IfA∈Fm×nandB= [b1···bp]∈Fn×p, then
AB=A
b1···bp
=
Ab 1···Abp
.
We see how a judicious use of notation can reap significant labor-saving rewards, leading
from the unfamiliar characterization of ABgiven in Definition 2.4 to the perfectly natural
formula in Proposition 2.6.
Theorem 2.7. LetA∈Fm×n,B,C∈Fn×p,D∈Fp×q, and c∈F. Then
1.A(cB) =c(AB).
2.A(B+C) =AB+AC(thedistributive property ).
3. (AB)D=A(BD)(theassociative property ).
Proof.
Proof of Part (1). Clearly A(cB) and c(AB) are both m×pmatrices. Now, for any 1 ≤i≤m
and 1 ≤j≤p,
[A(cB)]ij=nX
k=1[A]ik[cB]kj Definition 2.4
=nX
k=1[A]ik 
c[B]kj
Definition 2.1
=cnX
k=1[A]ik[B]kj Definition 1.5(F5,6,7)
=c[AB]ij Definition 2.4 ,
= [c(AB)]ij Definition 2.1 ,
and so we see the ij-entries of A(cB) and c(AB) are equal.
Proof of Part (2). Clearly A(B+C) and AB+ACare both m×pmatrices. For 1 ≤i≤m
and 1 ≤j≤p,
[A(B+C)]ij=nX
k=1[A]ik[B+C]kj Definition 2.430
=nX
k=1[A]ik 
[B]kj+ [C]kj
Definition 2.1
=nX
k=1[A]ik[B]kj+nX
k=1[A]ik[C]kj Definition 1.5(F6)
=AB+AC. Definition 2.4 ,
which shows equality of the ijentries.
Proof of Part (3). Both matrices will be m×q. Using basic summation properties and Definition
2.4,
[(AB)D]ij=pX
k=1[AB]ik[D]kj=pX
k=1" nX
ℓ=1[A]iℓ[B]ℓk!
[D]kj#
=nX
ℓ=1pX
k=1[A]iℓ[B]ℓk[D]kj
=nX
ℓ=1 
[A]iℓpX
k=1[B]ℓk[D]kj!
=nX
ℓ=1[A]iℓ[BD]ℓj= [A(BD)]ij,
and the proof is done. ■
In light of the associative property of matrix multiplication it is not considered ambiguous
to write ABD , since whether we interpret it as meaning ( AB)DorA(BD) makes no difference.
The order of operations conventions dictate that ABD be computed in the order indicated by
(AB)D, however.
Proposition 2.8. IfA∈Fm×n,B∈Fn×p,C∈Fp×q, and D∈Fq×r, then
(AB)(CD) =A(BC)D.
Proof. LetAB=P. We have
(AB)(CD) =P(CD) = (PC)D= [(AB)C]D= [A(BC)]D, (2.10)
where the second and fourth equalities follow from Theorem 2.7(3). Next we obtain
[A(BC)]D=A(BC)D, (2.11)
since the order of operations in evaluating either expression is precisely the same: (1) execute B
times Cto obtain BC; (2) execute Atimes BCto obtain A(BC); (3) execute A(BC) times D
to obtain A(BC)D.
Combining (2.10) and (2.11) yields ( AB)(CD) =A(BC)D. ■
There is no useful way to divide matrices, but we can easily define what it means to
exponentiate a matrix by a positive integer.31
Definition 2.9. IfA∈Fn×nandm∈N, then
Am=AA···A|{z}
mfactors=mY
k=1A.
In particular A1=A.
The definition makes use of so-called product notation,
mY
k=1xk=x1x2x3···xm,
which does for products what summation notation does for sums.
TheKronecker delta is a function δij:Z×Z→ {0,1}defined as follows for integers iand
j:
δij=(
1,ifi=j
0,ifi̸=j
We use the Kronecker delta to define the n×nidentity matrix ,
In= [δij]n=
1 0··· 0
0 1··· 0
............
0 0··· 1
,
then×nmatrix with diagonal entries 1 and all other entries 0. In particular we have
I2=
1 0
0 1
and I3=
1 0 0
0 1 0
0 0 1

Definition 2.10. For any A∈Fn×nwe define A0=In.
If the dimensions of an identity matrix are known or irrelevant, then the abbreviated symbol
Imay be used. The reason Inis called the identity matrix is because, for any n×nmatrix A,
it happens that
InA=AIn=A.
Thus Inacts as an identity with respect to matrix multiplication, just as 1 is the identity with
respect to multiplication of real numbers. In fact it can be shown that Inistheidentity for
matrix multiplication, as there can be no others.
Example 2.11. Show that I2is the only matrix for which I2A=AI2=Aholds for all 2 ×2
matrices A.
Solution. Given any 2 ×2 matrix
A=
a11a12
a21a22
,32
we have
AI2=
a11a12
a21a22
1 0
0 1
=
a11(1) + a12(0)a11(0) + a12(1)
a21(1) + a22(0)a21(0) + a22(1)
=
a11a12
a21a22
=A
and
I2A=
1 0
0 1
a11a12
a21a22
=
(1)a11+ (0) a12(0)a11+ (1) a12
(1)a21+ (0) a22(0)a21+ (1) a22
=
a11a12
a21a22
=A,
so certainly I2A=AI2=Aholds for all A.
Now, let Bbe a 2 ×2 matrix such that
BA=AB=A (2.12)
for all 2 ×2 matrices A. If we set A=I2in(2.12) we obtain BI2=I2in particular, whence
B=I2. Therefore I2is the only matrix for which I2A=AI2=Aholds for all A. ■
To show more generally that Inis the only matrix for which
InA=AIn=A
for all A∈Fn×ninvolves a nearly identical argument.
Proposition 2.12. LetA∈Fn×n.
1.IfAx=xfor every n×1column vector x, then A=In.
2.IfAx=0for every n×1column vector x, then A=On.
Proof.
Proof of Part (1). Suppose that Ax=xfor all n×1 column vectors x. For each 1 ≤j≤nlet
ej= [δij]n,1=
δ1j...
δnj
,
where once again we make use of the Kronecker delta. Thus ejis the n×1 column vector with
1 in the jth row and 0 in all other rows.
Now, for each 1 ≤j≤n,Aejis an n×1 column vector with i1-entry equalling
nX
k=1aikδkj=aijδjj=aij.
for each 1 ≤i≤n. On the other hand Aej=ejby hypothesis, and so
aij=δij=(
0,ifi̸=j
1,ifi=j
for all 1 ≤i, j≤n. But this is precisely the definition for In, and therefore A=In. ■33
The proof of part (2) of the proposition is similar and left as a problem. Observe that, in
the notation established in the proof of part (1), we have
In=h
[δi1]n,1···[δin]n,1i
=
e1···en
. (2.13)
Proposition 2.13. LetA∈Fm×nandB∈Fn×p. Then
(AB)⊤=B⊤A⊤.
Proof. Note that B⊤isp×nandA⊤isn×m, so the product B⊤A⊤is defined as a p×m
matrix. Fix 1 ≤i≤pand 1 ≤j≤m. We have, using Definition 2.4 and Definition 2.2 twice
each,
[B⊤A⊤]ij=nX
k=1[B⊤]ik[A⊤]kj=nX
k=1[B]ki[A]jk=nX
k=1[A]jk[B]ki= [AB]ji=
(AB)⊤
ij.
Thus the ij-entry of B⊤A⊤is equal to the ij-entry of ( AB)⊤, soB⊤A⊤= (AB)⊤as was to be
shown. ■
Problems
1. Given that
x=
3
−1
2
,A=
1 2−3
3 0−1
−2 1 4
,C=
−4 2
1−1
0 3

compute the following.
(a)x⊤x
(b)xx⊤
(c)AC34
2.3 – Row and Column Operations
We start by establishing some necessary notation. The symbol En,lmwill denote the n×n
matrix with lm-entry 1 and all other entries 0; that is,
En,lm= [δilδmj]n
for any fixed 1 ≤l, m≤n, making use of the Kronecker delta introduced in the last section.
Put yet another way, En,lmis the n×nmatrix with ij-entry δilδmj:
[En,lm]ij=δilδmj. (2.14)
Usually the nin the symbol En,lmmay be suppressed without leading to ambiguity, so that
the more compact symbol Elmmay be used. This will usually be done except in the statement
of theorems.
Proposition 2.14. Letn∈Nand1≤l, m, p, q ≤n.
1.En,lmEn,mp=En,lp.
2.Ifm̸=p, then En,lmEn,pq=On.
Proof.
Proof of Part (1). Using Definition 2.4 and equation (2.14), the ij-entry of ElmEmpis
[ElmEmp]ij=nX
k=1[Elm]ik[Emp]kj=nX
k=1(δilδmk)(δkmδpj) = (δilδmm)(δmmδpj) =δilδpj,
where the third equality is justified since δmk= 0 for all k̸=m, and then we need only recall
thatδmm= 1. So ElmEmpis the n×nmatrix with ij-entry δilδpj, and therefore ElmEmp=Elp.
Proof of Part (2). Suppose m̸=p. Again using Definition 2.4 and equation (2.14) , the ij-entry
ofElmEmpis
[ElmEpq]ij=nX
k=1[Elm]ik[Epq]kj=nX
k=1(δilδmk)(δkpδqj) = 0 ,
where the third equality is justified since, for any 1 ≤k≤n, either k̸=mork̸=p, and so
either δmk= 0 or δkp= 0. Therefore ElmEpq=On. ■
Letn∈N. For any scalar c̸= 0 define
Mi(c) =In+ (c−1)Eii,
which is the n×nmatrix obtained by multiplying the ith row of Inbyc. Also define
Mi,j=In−Eii−Ejj+Eij+Eji
fori, j∈ {1, . . . , n }with i̸=j, which is the matrix obtained by interchanging the ith and jth
rows of In(notice that Mi,j=Mj,i). Finally, for i, j∈ {1, . . . , n }with i̸=j, and scalar c̸= 0,
define
Mi,j(c) =In+cEji,35
which is the matrix obtained by adding ctimes the ith row of Into the jth row of In. Any
matrix of the form Mi,j(c),Mi,j, orMi(c) is called an elementary matrix .
Definition 2.15. Given A∈Fm×n, anelementary row operation onAis any one of the
multiplications
Mi,j(c)A,Mi,jA,Mi(c)A.
More specifically we call left-multiplication by Mi,j(c)anR1operation, left-multiplication by
Mi,janR2operation, and left-multiplication by Mi(c)AanR3operation. A matrix A′is called
row-equivalent toAif there exist elementary matrices M1, . . . ,Mksuch that
A′=Mk···M1A.
Anelementary column operation onAis any one of the multiplications
AM⊤
i,j(c),AM⊤
i,j,orAM⊤
i(c).
More specifically we call right-multiplication by M⊤
i,j(c)aC1operation, right-multiplication by
M⊤
i,jaC2operation, and right-multiplication by M⊤
i(c)aC3operation. A matrix A′is called
column-equivalent toAif there exist elementary matrices M1, . . . ,Mksuch that
A′=AM⊤
1···M⊤
k.
It’s understood that the elementary matrices in the first part of Definition 2.15 must all be
m×mmatrices, and the elementary matrices in the second part must be n×n. Also, to be
clear, we define M⊤
i,j(c) = [Mi,j(c)]⊤andM⊤
i(c) = [Mi(c)]⊤. Finally, we define any matrix Ato
be both row-equivalent and column-equivalent to itself.
When we need to denote a collection of, say, pelementary matrices in a general way, we will
usually use symbols M1, . . . ,Mp. So for each k= 1, . . . , p the symbol Mkcould represent any
one of the three basic types of elementary matrix given in Definition 2.15.
Proposition 2.16. Suppose A∈Fm×nhas row vectors a1, . . . ,am∈Fn. Let c̸= 0, and let
1≤p, q≤mwithp̸=q.
1.Mp,q(c)Ais the matrix obtained from Aby replacing the row vector aqbyaq+cap:
Mp,q(c)
...
aq...
=
...
aq+cap...
.
2.Mp,qAis the matrix obtained from Aby interchanging apandaq:
Mp,q
...amin{p,q}...amax{p,q}...
=
...amax{p,q}...amin{p,q}...
.36
3.Mp(c)Ais the matrix obtained from Aby replacing apbycap:
Mp(c)
...
ap...
=
...
cap...
.
Proof.
Proof of Part (1). Fix 1 ≤i≤mand 1 ≤j≤n. Here Mp,q(c) must be m×m, so that
Mp,q(c) =Im+cEm,qp, since Aism×n. Then

Mp,q(c)A
ij=mX
k=1
Mp,q(c)
ik[A]kj=mX
k=1[Im+cEqp]ik[A]kj
=mX
k=1 
[Im]ik+c[Eqp]ik
[A]kj=mX
k=1[Im]ik[A]kj+cmX
k=1[Eqp]ik[A]kj
= [ImA]ij+cmX
k=1δiqδpk[A]kj= [A]ij+cδiq[A]pj,
where the last equality holds since δpk= 0 for all k̸=p.
Now, if i̸=q, then δiq= 0 and we obtain

Mp,q(c)A
ij= [A]ij
for all 1 ≤j≤n, which shows that the ith row vector of Mp,q(c)Aequals the ith row vector ai
ofAwhenever i̸=q. On the other hand if i=q, then δiq=δqq= 1 and we obtain

Mp,q(c)A
qj= [A]qj+c[A]pj
for all 1 ≤j≤n, which shows that the qth row vector of Mp,q(c)Aequals the qth row vector of
Aplusctimes the pth row vector: aq+cap.
Proof of Part (2). For 1≤i≤mand 1 ≤j≤n,
[Mp,qA]ij=mX
k=1[Mp,q]ik[A]kj=mX
k=1[Im−Epp−Eqq+Epq+Eqp]ik[A]kj
=mX
k=1 
[Im]ik−[Epp]ik−[Eqq]ik+ [Epq]ik+ [Eqp]ik
[A]kj
=mX
k=1[Im]ik[A]kj−mX
k=1[Epp]ik[A]kj−mX
k=1[Eqq]ik[A]kj+mX
k=1[Epq]ik[A]kj
+mX
k=1[Eqp]ik[A]kj
= [ImA]ij−mX
k=1δipδpk[A]kj−mX
k=1δiqδqk[A]kj+mX
k=1δipδqk[A]kj37
+mX
k=1δiqδpk[A]kj
= [A]ij−δip[A]pj−δiq[A]qj+δip[A]qj+δiq[A]pj. (2.15)
Now, if i̸=p, q, then δip=δiq= 0, and so for any 1 ≤j≤nwe find from (2.15) that
[Mp,qA]ij= [A]ij, which shows the ith row vector of Mp,qAequals the ith row vector of A.
Ifi=p, then from (2.15) we obtain
[Mp,qA]pj= [A]pj−δpp[A]pj−δpq[A]qj+δpp[A]qj+δpq[A]pj= [A]qj
for all 1 ≤j≤n, so that
[Mp,qA]p1···[Mp,qA]pn
=[A]q1···[A]qn
=aq,
and it’s seen that the pth row vector of Mp,qAis the qth row vector of A.
Finally, if i=q, then from (2.15) we obtain
[Mp,qA]qj= [A]qj−δqp[A]pj−δqq[A]qj+δqp[A]qj+δqq[A]pj= [A]pj
for all 1 ≤j≤n, so that
[Mp,qA]q1···[Mp,qA]qn
=[A]p1···[A]pn
=ap,
and it’s seen that the qth row vector of Mp,qAis the pth row vector of A.
We now see that Mp,qAis identical to Asave for a swap of the pth and qth row vectors, as
was to be shown. ■
Proposition 2.17. Suppose A∈Fm×nhas column vectors a1, . . . ,an∈Fm. Let c̸= 0, and let
1≤p, q≤nwithp̸=q.
1.AM⊤
p,q(c)is the matrix obtained from Aby replacing the column vector aqbyaq+cap:

···aq···
M⊤
p,q(c) =
···aq+cap···
.
2.AM⊤
p,qis the matrix obtained from Aby interchanging apandaq:

···amin{p,q}···amax{p,q}···
M⊤
p,q=
···amax{p,q}···amin{p,q}···
.
3.AM⊤
p(c)is the matrix obtained from Aby replacing apbycap:

···ap···
M⊤
p(c) =
···cap···
.
Proof.
Proof of Part (1). Observing that the row vectors of A⊤∈Fn×marea⊤
1, . . . ,a⊤
n, by Proposition
2.16(1) we have,
Mp,q(c)A⊤=Mp,q(c)
...
a⊤
q...
=
...
a⊤
q+ca⊤
p...
,38
and so by Proposition 2.13,
AM⊤
p,q(c) = 
Mp,q(c)A⊤⊤=
...
a⊤
q+ca⊤
p...
⊤
=
···aq+cap···
.
Proof of Part (2). By Propositions 2.13 and 2.16(2),
AM⊤
p,q= 
Mp,qA⊤⊤=
Mp,q
...
a⊤
min{p,q}...
a⊤
max{p,q}...

⊤
=
...
a⊤
max{p,q}...
a⊤
min{p,q}...
⊤
=
···amax{p,q}···amin{p,q}···
,
and we’re done. ■
The proof of part (3) of Proposition 2.17 is left as a problem.
Definition 2.18. LetA= [aij]m,n. The ith pivot ofA,pi, is the first nonzero entry (from the
left) in the ith row of A:
pi=airi,where ri= min {j:aij̸= 0}
Azero row of a matrix A, which is a row with all entries equal to 0, is said to have no
pivot.
Definition 2.19. A matrix is a row-echelon matrix (or has row-echlon form ) if the
following conditions are satisfied:
1.No zero row lies above a nonzero row.
2.Given two pivots pi1=ai1j1andpi2=ai2j2,j2> j 1whenever i2> i1.
In a row-echelon matrix, a pivot column is a column that has a pivot. An upper-
triangular matrix is a square matrix having row-echelon form. A lower-triangular matrix
is a square matrix Afor which A⊤has row-echelon form.
The first condition requires that all zero rows be at the bottom of a matrix in row-echelon
form. The second condition requires that if the first kentries of the row iare zeros, then at
least the first k+ 1 entries of row i+ 1 must be zeros. Thus, all entries that lie below a pivot in
a given column must be zero. Examples of matrices in reduced-echelon form are the following,
with pientries indicating pivots (i.e. nonzero entries) and asterisks indicating entries whose39
values may be zero or nonzero:

0p1∗ ∗ ∗ ∗ ∗ ∗ ∗
0 0 p2∗ ∗ ∗ ∗ ∗ ∗
0 0 0 0 p3∗ ∗ ∗ ∗
0 0 0 0 0 0 0 p4∗
0 0 0 0 0 0 0 0 p5
0 0 0 0 0 0 0 0 0
,
p1∗ ∗ ∗ ∗
0p2∗ ∗ ∗
0 0 p3∗ ∗
0 0 0 p4∗
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
,
p1∗ ∗ ∗ ∗
0p2∗ ∗ ∗
0 0 p3∗ ∗
0 0 0 p4∗
0 0 0 0 p5
.
The rightmost matrix is a square matrix and therefore happens to be in upper-triangular form.
Its transpose,
p10 0 0 0
∗p20 0 0
∗ ∗ p30 0
∗ ∗ ∗ p40
∗ ∗ ∗ ∗ p5
,
is an example of a matrix in lower-triangular form. The diagonal entries of a square matrix need
not be nonzero in order to have upper-triangular or lower-triangular form, however, so even

∗ ∗ ∗ ∗ ∗
0∗ ∗ ∗ ∗
0 0 ∗ ∗ ∗
0 0 0 ∗ ∗
0 0 0 0 ∗
and
∗0 0 0 0
∗ ∗ 0 0 0
∗ ∗ ∗ 0 0
∗ ∗ ∗ ∗ 0
∗ ∗ ∗ ∗ ∗

represent 5 ×5 triangular matrices regardless of what values we substitute for the asterisks.
Another way to define an upper-triangular matrix is to say it is a square matrix with all
entries below the diagonal equal to 0. Similarly, a lower-triangular matrix is a square matrix
with all entries above the diagonal equal to 0. A diagonal matrix is a square matrix [ aij]n
that is both upper-triangular and lower-triangular, so that aij= 0 whenever i̸=j. Any identity
matrix Inor square zero matrix Onis a diagonal matrix, and (trivially) so too is any 1 ×1
matrix [ a].
Proposition 2.20. Every matrix is row-equivalent to a matrix in row-echelon form. Thus if A
is a square matrix, then it is row-equivalent to an upper-triangular matrix.
Proof. We start by observing that any 1 ×nmatrix is trivially in row-echelon form for any n.
Letm∈Nbe arbitrary, and suppose that an m×nmatrix is row-equivalent to a matrix in
row-echelon form for any n. It remains to show that any ( m+ 1)×nmatrix is row-equivalent
to a matrix in row-echelon form for any n, whereupon the proof will be finished by the Principle
of Induction.
Letnbe arbitrary. Fix A= [aij]m+1,n. We may express Aas a partitioned matrix,
Ba
bbn
,40
where B= [aij]m,n−1. Observing that [ B|a] is an m×nmatrix, by our inductive hypothesis it
is row-equivalent to a matrix in row-echelon form [ R|c], and thus
Ba
bbn
∼Rr
bbn
(2.16)
Now, if
b|bn
=
b1···bn
consists of all zeros, or the pivot has column number greater than the pivot in the mth row,
then the matrix at right in (2.16) is in row-echelon form and we are done. Supposing neither is
the case, let bkbe the pivot for [ b|bn], and let row ℓbe the lowest row in [ R|r] that does not
have a pivot which lies to the right of column k. (If k= 1 then set ℓ= 0.) We now effect a
succession of R2 row operations,
A′=Mℓ+2,ℓ+1···Mm,m−1Mm+1,mRr
bbn
,
which have the effect of moving [ b|bn] to just below row ℓwithout altering the order of the
other rows. (If [ b|bn] has pivot in the first column it will become the top row since ℓ= 0.) We
now have a matrix that either is in row-echelon form, or else rows ℓandℓ+ 1 have pivots in the
same column.
Suppose the latter is the case. If ℓ= 0, then the first entries of the first and second rows
are nonzero scalars x1andx2, respectively, and performing the R1 operation M1,2(−x2/x1) of
adding −x2/x1times the first row to the second row will put a 0 at the beginning of the second
row. If ℓ >0 we need do nothing, and proceed to partition A′as follows:
c1c
0C
Now, [ 0|C] is an m×nmatrix, so by our inductive hypothesis it is row-equivalent to a matrix
[0|R′] in row-echelon form. The resultant ( m+ 1)×nmatrix,
c1c
0R′
,
is in row-echelon form, and since
A=Ba
bbn
∼Rr
bbn
∼c1c
0C
∼c1c
0R′
we conclude that Ais row-equivalent to a matrix in row-echelon form. ■
In the example to follow, and frequently throughout the remainder of the text, we will
indicate the R1 elementary row operation of left-multiplying a matrix by Mi,j(c) by writing
cri+rj→rj,
which may be read as “ ctimes row iis added to row jto yield a new row j” (see Prop-osition
2.16(1)). Similarly an R2 operation, which occurs when left-multiplying by Mi,j, will be indicated
by
ri↔rj,41
which may be read as “interchange rows iandj” (see Proposition 2.16(2)). Finally an R3
operation, which is the operation of left-multiplying by Mi(c), will be indicated by
cri→ri,
which may be read as “ ctimes row ito yield a new row i” (see Proposition 2.16(3)).
Example 2.21. Using elementary row operations, find a row-equivalent matrix for
0 1 3 −2
2 1−4 3
2 3 2 −1

that is in row-echelon form.
Solution. Call the matrix A. Then,
Ar1↔r2− − − →
2 1−4 3
0 1 3 −2
2 3 2 −1
−r1+r3→r3− − − − − − − →
2 1−4 3
0 1 3 −2
0 2 6 −4
−2r2+r3→r3− − − − − − − →
2 1−4 3
0 1 3 −2
0 0 0 0
.
In terms of elementary matrices we computed
M2,3(−2)M1,3(−1)M1,2A,
multiplying from right to left. ■
Example 2.22. Apermutation matrix is a square matrix Pwith exactly one entry equal to
1 in each row and in each column, and all other entries equal to 0. Any such matrix may be
obtained by rearranging (i.e. permuting) the rows of the identity matrix. Of course, Initself is
a permutation matrix for any n∈N, as is the n×nelementary matrix Mi,jthat results from
interchanging the ith and jth rows of In.
The matrix
P=
0 1 0
0 0 1
1 0 0

is a 3×3 permutation matrix that is obtained from I3by performing the R2 operation r1↔r2
followed by r2↔r3. By Proposition 2.16(2), P=M2,3M1,2I3, or simply P=M2,3M1,2. Thus
for any 3 ×nmatrix Awe have
PA= (M2,3M1,2)A=M2,3(M1,2A),
which shows that left-multiplication of AbyPis equivalent to performing the following
operations: first, the top and middle rows of Awill be swapped to give a new matrix A′; and
second, the middle and bottom rows of A′will be swapped to give the final product. If a1,a2,
anda3are the row vectors of A, then left-multiplication of AbyPmay be characterized as the
action of assigning new positions to the row vectors of A. Namely, PAsends a1to row 3, a2to
row 1, and a3to row 2. Note how these three placement operations correspond to the placement
of the three entries equaling 1 in P: column 1, row 3; column 2, row 1; and column 3, row 2. ■42
Problems
1. Show that, for any 1 ≤i≤n, the matrix En,iiis symmetric: E⊤
n,ii=En,ii.
2.What matrix results from right-multiplication BPof an m×3 matrix Bby the 3 ×3 matrix
Pin Example 2.22? What permutation matrix Qshould be used so that BQpermutes the
columns of Bthe same way that PApermutes the rows of a 3 ×nmatrix A?
3. Prove part (3) of Proposition 2.16.
4. Prove part (3) of Proposition 2.17.43
2.4 – The Inverse of a Matrix
Definition 2.23. Ann×nmatrix Aisinvertible if there exists a matrix Bsuch that
AB=BA=In,
in which case we call Btheinverse ofAand denote it by the symbol A−1. A matrix that is
not invertible is said to be noninvertible ornonsingular .
From the definition we see that
AA−1=A−1A=In,
provided that A−1exists. Observe that Ondoes not have an inverse since AO n=Onfor any
n×nmatrix A. Also observe that, of necessity, if Ais an n×nmatrix, then A−1must also be
n×n.
Proposition 2.24. The inverse of a matrix Ais unique.
Proof. LetAbe an invertible n×nmatrix and suppose that BandCare such that
AB=BA=Inand AC=CA=In.
From BA=Inwe obtain
(BA)C=InC=C,
and since matrix multiplication is associative by Theorem 2.7,
C= (BA)C=B(AC) =BIn=B.
That is, B=C, and so Acan have only one inverse. ■
Proposition 2.25. IfAhas0as a row or column vector, then Ais not invertible.
Proof. LetAbe an n×nmatrix with row vectors a1, . . . ,an. Suppose ai=0for some
1≤i≤n. Let
B=b1···bn
be any n×nmatrix. Since the ii-entry of ABis
ai·bi=0·bi= 0,
it is seen that AB̸=In. Since Bis arbitrary, we conclude that Ahas no inverse. That is, Ais
not invertible.
The proof that Ais not invertible if it has 0among its column vectors is similar. ■
Theorem 2.26. Letk∈N. IfA1, . . . ,Ak∈Fn×nare invertible, then A1···Akis invertible
and
(A1···Ak)−1=A−1
k···A−1
1.44
Proof. An inductive argument is suitable. The case when k= 1 is trivially true. Let k∈Nbe
arbitrary, and suppose that the invertibility of kmatrices A1, . . . ,Ak∈Fn×nimplies A1···Ak
is invertible and ( A1···Ak)−1=A−1
k···A−1
1.
Suppose that A1, . . . ,Ak+1are invertible n×nmatrices. Let
B=A2···Ak+1and C=A−1
k+1···A−1
2.
By the inductive hypothesis Bis invertible, with
B−1= (A2···Ak+1)−1=A−1
k+1···A−1
2=C,
and so by Proposition 2.8
(A1···Ak+1)(A−1
k+1···A−1
1) = (A1B)(B−1A−1
1) =A1(BB−1)A−1
1
=A1InA−1
1=A1A−1
1=In. (2.17)
(The associativity of matrix multiplication is implicitly used to justify the penultimate equality.)
Next, let
P=A1···Akand Q=A−1
k···A−1
1.
By the inductive hypothesis Qis invertible, with
P−1= (A1···Ak)−1=A−1
k···A−1
1=Q,
and so by Proposition 2.8
(A−1
k+1···A−1
1)(A1···Ak+1) = (A−1
k+1Q)(Q−1Ak+1) =A−1
k+1(QQ−1)Ak+1
=A−1
k+1InAk+1=A−1
k+1Ak+1=In. (2.18)
From (2.17) and(2.18) we conclude that A−1
k+1···A−1
1is the inverse for A1···Ak+1. That is,
A1···Ak+1is invertible and
(A1···Ak+1)−1=A−1
k+1···A−1
1.
Therefore the statement of the theorem holds for all k∈Nby the Principle of Induction. ■
We now proceed to establish some results that will help us determine whether a matrix has
an inverse, and then develop an algorithm for computing the inverse of any invertible matrix.
We start by examining elementary matrices, since the calculations involved are much simpler.
Proposition 2.27. An elementary matrix is invertible, with
M−1
i,j(c) =Mi,j(−c),M−1
i,j=Mi,j,M−1
i(c) =Mi(c−1).
Proof. Letn∈Nbe arbitrary, let c̸= 0, and let i, j∈ {1, . . . , n }with i̸=j. Using the fact
thatE2
ji=Onby Proposition 2.14(2), we have
Mi,j(−c)Mi,j(c) = (In−cEji)(In+cEji) =I2
n+cInEji−cEjiIn−c2E2
ji
=In+cEji−cEji−c2On=In,45
and
Mi,j(c)Mi,j(−c) = (In+cEji)(In−cEji) =I2
n−cInEji+cEjiIn−c2E2
ji
=In−cEji+cEji−c2On=In,
and therefore M−1
i,j(−c) is the inverse for Mi,j(c).
Next we have
M2
i,j= (In−Eii−Ejj+Eij+Eji)2
=In−Eii−Ejj+Eij+Eji−Eii+EiiEii+EiiEjj−EiiEij−EiiEji
−Ejj+EjjEii+EjjEjj−EjjEij−EjjEji+Eij−EijEii−EijEjj
+EijEij+EijEji+Eji−EjiEii−EjiEjj+EjiEij+EjiEji
=In−Eii−Ejj+Eij+Eji−Eii+Eii−Eij−Ejj+Ejj−Eji+Eij
−Eij+Eii+Eji−Eji+Ejj=In,
where the third equality owes itself to Proposition 2.14 and the understanding that i̸=j, and
so for instance EiiEjj=On,EijEij=On,EiiEii=Eii,EijEji=Eii, and so on. Therefore Mi,j
is its own inverse.
Finally we show that the inverse for Mi(c) isMi(c−1) for any fixed 1 ≤i≤nandc̸= 0.
Since E2
ii=Eiiby Proposition 2.14(1), we have
Mi(c)Mi(c−1) = 
In+ (c−1)Eii 
In+ (c−1−1)Eii
=In+ (c−1−1)Eii+ (c−1)Eii+ (c−1)(c−1−1)E2
ii
=In+ (c−1−1)Eii+ (c−1)Eii−(c−1)Eii−(c−1−1)Eii=In,
and similarly Mi(c−1)Mi(c) =In. ■
Proposition 2.28. Suppose Ais row-equivalent to B. Then Ais invertible if and only if Bis
invertible.
Proof. Since Ais row-equivalent to B, there exist elementary matrices M1, . . . ,Mksuch that
Mk···M1A=B. (2.19)
Now, suppose Ainvertible. The matrices M1, . . . ,Mkare invertible by Proposition 2.27, and
since Ais invertible by hypothesis, by Theorem 2.26 we conclude that Bis invertible.
Next, suppose Bis invertible. From (2.19) we have
A= (Mk···M1)−1B=M−1
1···M−1
kB,
where M−1
1, . . . ,M−1
kare all elementary matrices by Proposition 2.27. Thus Bis row-equivalent
toA, and since Bis invertible, the conclusion that Ais invertible follows from the first part of
the proof. ■46
Proposition 2.29. IfAis invertible, then Ais row-equivalent to an upper-triangular matrix
with nonzero diagonal elements.
Proof. LetAbe an n×nmatrix. Then Ais row-equivalent to an upper-triangular matrix
U= [uij]nby Proposition 2.20. Suppose that uii= 0 for some 1 ≤i≤n. Then ukk= 0 for
alli≤k≤n, and in particular unn= 0 so that the nth row vector of Uis0. Hence Uis not
invertible by Proposition 2.25, and since A∼Uit follows that Ais not invertible by Proposition
2.28. We have now proven that if Ais row-equivalent to an upper-triangular matrix with a
diagonal element equalling 0, then Ais not invertible. This is equivalent to the statement of
the proposition. ■
Theorem 2.30. Ann×nmatrix Ais invertible if and only if Ais row-equivalent to In.
Proof. Suppose A∈Fn×nis invertible. By Proposition 2.29 Ais row-equivalent to an upper
triangular matrix U= [uij]nwith nonzero diagonal elements. We multiply each row iofUby
u−1
ii(which of course is defined since uii̸= 0) to obtain a row-equivalent upper-triangular matrix
U1with diagonal entries all equal to 1:
M1(u−1
11)···Mn(u−1
nn)U=U1. (2.20)
In particular the first column of U′ise1as desired, recalling that In= [e1···en]. If we add
−u12times the second row of U1to the first row to obtain a row-equivalent matrix U2,
M2,1(−u12)U1=U2,
we find in particular that U2is upper-triangular with first column e1and second column e2.
Proceeding in this fashion to the jth column, we have an upper-triangular matrix
Uj−1=
e1···ej−1uj···un
on which we perform a sequence of R1 row operations to obtain a row-equivalent matrix Uj:
 j−1Y
i=1Mj,i(−uij)!
Uj−1=Mj,1(−u1j)···Mj,j−1(−uj−1,j)Uj−1=Uj, (2.21)
where
Uj=
e1···ejuj+1···un
Equation (2.21) holds for j= 2, . . . , n , and gives Unas
Un= n−1Y
i=1Mn,i(−uin)! n−2Y
i=1Mn−1,i(−ui,n−1)!
··· 1Y
i=1M2,i(−ui2)!
U1.
Observing that Un=In, and recalling (2.20), we finally obtain
In= n−1Y
i=1Mn,i(−uin)! n−2Y
i=1Mn−1,i(−ui,n−1)!
··· 1Y
i=1M2,i(−ui2)! nY
i=1Mi(u−1
ii)!
U,
which demonstrates in explicit terms that Uis row-equivalent to In. Now, A∼UandU∼In
imply that A∼Inand the first part of the proof is finished.47
The converse is much easier to prove. Suppose that Ais row-equivalent to In. Since Inis
invertible, by Proposition 2.28 we conclude that Ais invertible. ■
This theorem gives rise to a sure method for finding the inverse of any invertible matrix
A. IfA∈Fn×nis invertible, then A∼In, which is to say there exist elementary matrices
M1, . . . ,Mksuch that Mk···M1A=In. Now,
Mk···M1A=In⇔(Mk···M1A)A−1=InA−1
⇔Mk···M1(AA−1) =A−1
⇔A−1=Mk···M1In,
which demonstrates that the selfsame elementary row operations M1, . . . ,Mkthat transform
AintoInwill transform InintoA−1. In practice we set up a partitioned matrix [ A|In], and
apply identical sequences of elementary row operations to each submatrix until the submatrix
that started as Ahas become In. At that point the submatrix that started as Inwill be A−1:
[A|In]∼[M1A|M1In]∼ ··· ∼ [Mk···M1A|Mk···M1In] = [In|A−1].
The next example illustrates the procedure.
Example 2.31. Find the inverse of the matrix
2 4 3
−1 3 0
0 2 1

Solution. We employ the same sequence of elementary row operations on both AandI3, as
follows.
2 4 3 1 0 0
−1 3 0 0 1 0
0 2 1 0 0 1
r2↔r1− − − − − →
−r1→r1
1−3 0 0−1 0
2 4 3 1 0 0
0 2 1 0 0 1
−2r1+r2→r2− − − − − − − − →

1−3 0 0−1 0
0 10 3 1 2 0
0 2 1 0 0 1
r2↔r3− − − − →
1−3 0 0−1 0
0 2 1 0 0 1
0 10 3 1 2 0
−5r2+r3→r3− − − − − − − − →

1−3 0 0−1 0
0 2 1 0 0 1
0 0 −21 2 −5
1
2r2→r2− − − − − →
1−3 0 0−1 0
0 1 1 /20 0 1 /2
0 0 −21 2 −5
3r2+r1→r1− − − − − − − →

1 0 3 /20−1 3/2
0 1 1 /20 0 1 /2
0 0 −21 2 −5
1
4r3+r2→r2− − − − − − − →
3
4r3+r1→r1
1 0 0 3/4 1/2−9/4
0 1 0 1/4 1/2−3/4
0 0−2 1 2 −5
−1
2r3→r3− − − − − − →

1 0 0 3/4 1/2−9/4
0 1 0 1/4 1/2−3/4
0 0 1 −1/2−1 5 /2
.48
Therefore
A−1=
3
41
2−9
4
1
41
2−3
4
−1
2−15
2

is the inverse of A. ■
Proposition 2.32. IfA∈Fn×nis invertible, then A⊤is invertible and
(A⊤)−1= (A−1)⊤.
Proof. Suppose that A∈Fn×nis invertible, so that A−1exists. Now, by Proposition 2.13,
AA−1=In⇒(AA−1)⊤=I⊤
n⇒(A−1)⊤A⊤=In
and
A−1A=In⇒(A−1A)⊤=I⊤
n⇒A⊤(A−1)⊤=In.
Now,
(A−1)⊤A⊤=A⊤(A−1)⊤=In
shows that ( A−1)⊤is the inverse of A⊤. Therefore A⊤is invertible, and moreover ( A⊤)−1=
(A−1)⊤. ■
Example 2.33. IfP∈Fn×nis a permutation matrix (see Example 2.22), then P−1=P⊤.
To see this, first observe that Pmay be obtained by permuting the rows of In, and since
any permutation of nobjects may be accomplished by performing at most ntranspositions
(i.e. the operation of swapping two objects), we may write P=M1M2···Mm, where m≤n,
and for each 1 ≤k≤mthe matrix Mkis an elementary matrix of the form Mi,jfor some
i, j∈ {1, . . . , n }with i̸=j.
Next, we claim that any elementary matrix Mi,jis symmetric: M⊤
i,j=Mi,j. To show this,
since
Mi,j=In−Eii−Ejj+Eij+Eji,
we need to show that, generally, E⊤
pp=Epp, and E⊤
pq=Eqp. The former is a problem in §2.3, so
we’ll show the latter. Let i, j∈ {1, . . . , n }with i̸=j. By Definition 2.2 and equation (2.14),
[E⊤
pq]ij= [Epq]ji=δjpδqi,
whereas by (2.14),
[Eqp]ij=δiqδpj=δqiδjp=δjpδqi.
Hence [ E⊤
pq]ij= [Eqp]ij, and therefore E⊤
pq=Eqp. It is clear that Inis symmetric, so that I⊤
n=In.
Now, by Proposition 2.3(2) and the foregoing findings,
M⊤
i,j= (In−Eii−Ejj+Eij+Eji)⊤=I⊤
n−E⊤
ii−E⊤
jj+E⊤
ij+E⊤
ji
=In−Eii−Ejj+Eji+Eij=Mi,j.
Finally, we have
P⊤= (M1M2···Mm)⊤=M⊤
m···M⊤
2M⊤
1 (Proposition 2.13)49
=Mm···M2M1=M−1
m···M−1
2M−1
1 (Proposition 2.27)
= (M1M2···Mm)−1=P−1, (Theorem 2.26)
as was to be shown. ■50
2.5 – Systems of Linear Equations
As usual let Fdenote a field. A system overFofmlinear equations in nunknowns
x1, . . . , x nis a set of equations of the form


a11x1+a12x2+···+a1nxn=b1
a21x1+a22x2+···+a2nxn=b2............
am1x1+am2x2+···+amnxn=bm(2.22)
for which aij∈Fandbi∈Ffor all integers 1 ≤i≤mand 1 ≤j≤n. The scalars aijare the
coefficients of the system, and b1, . . . , b mare the constant terms . IfSiis the solution set of
theith equation, which is to say
Si=


x1...
xn
∈Fn:ai1x1+ai2x2+···+ainxn=bi

,
then the solution set of the system (2.22) is
S=S1∩ ··· ∩ Sm=m\
i=1Si,
or equivalently
S=


x1...
xn
∈Fn:
x1...
xn
∈Sifor all 1 ≤i≤m

.
A system is consistent if its solution set Sis nonempty (i.e. the system has at least one
solution), and inconsistent ifS=∅(i.e. the system has no solution). A consistent system is
dependent ifShas an infinite number of elements, and independent ifShas precisely one
element. As we will see later, a system of linear equations has either no solution, precisely one
solution, or an infinite number of solutions. There are no other possibilities.
If we define
A=
a11a12··· a1n
a21a22··· a2n............
am1am2··· amn
,x=
x1
x2...
xn
,and b=
b1
b2...
bm
, (2.23)
then the system (2.22) may be written as the matrix equation Ax=b,

a11a12··· a1n
a21a22··· a2n............
am1am2··· amn

x1
x2...
xn
=
b1
b2...
bm
. (2.24)51
In this representation of the system, all solutions xare expressed as column vectors
x=
x1...
xn
, (2.25)
so that the solution set Sis given as
S=


x1...
xn
∈Fn
x1...
xn
∈Sifor all 1 ≤i≤m

.
As a further notational convenience we may express the matrix equation (2.24) as an
augmented matrix featuring only the coefficients and constant terms of the system,

a11a12··· a1nb1
a21a22··· a2nb2...............
am1am2··· amnbm
. (2.26)
We see the augmented matrix is just the partitioned matrix [ A|b]. The fact that there are n
columns of coefficients (understood to be the columns to the left of the vertical line) informs us
that there are nvariables, and since an n-variable system of equations is fully determined by its
coefficients and constant terms, no information is lost in doing this.
We now consider how the system (2.22) is affected if we left-multiply the corresponding
augmented matrix [ A|b] by any one of the three elementary matrices Mi,j(c),Mi,j, orMi(c).
By Proposition 2.16 we know that
Mi,j(c)[A|b]
will effect an R1 operation, specifically adding c̸= 0 times the ith row of [ A|b] to the jth row.
What results is a new augmented matrix [ A′|b′] corresponding to a new system of equations
in which ctimes the ith equation has been added to the jth equation. But is the solution set S′
of the new system [ A′|b′] any different from the solution set Sof the original system [ A|b]?
In the system [ A|b] the ith and jth equations are
nX
k=1aikxk=biandnX
k=1ajkxk=bj, (2.27)
which have solution sets SiandSj, respectively; and in the system [ A′|b′] the ith and jth
equations are
nX
k=1aikxk=biandnX
k=1(caik+ajk)xk=cbi+bj, (2.28)
which have solution sets S′
iandS′
j, respectively. We will show that Si∩Sj=S′
i∩S′
j. To start,
we first observe that the ith equation of [ A′|b′] is the same as the ith equation of [ A|b], so
S′
i=Siand our task becomes that of showing Si∩Sj=Si∩S′
j.52
Letx∈Si∩Sjbe given by (2.25) . Thus the scalars x1, . . . , x nare such that the equations
in (2.27) are satisfied. We have, using (2.27),
nX
k=1(caik+ajk)xk=cnX
k=1aik+nX
k=1ajkxk=cbi+bj,
which shows that x1, . . . , x nsatisfy the second equation in (2.28) and so x∈S′
j. From x∈Si
andx∈S′
jwe have x∈Si∩S′
j, and therefore Si∩Sj⊆Si∩S′
j.
Now suppose x∈Si∩S′
j, so that the scalars x1, . . . , x nare assumed to satisfy the equations
in (2.28). Multiplying the first equation by cyields
nX
k=1caikxk=cbi,
so thatnX
k=1(caik+ajk)xk−nX
k=1caikxk= (cbi+bj)−cbi
obtains from the second equation in (2.28), which in turn implies that
nX
k=1ajkxk=bj
and so x∈Sj. Since x∈Sialso, we conclude that x∈Si∩Sjand therefore Si∩S′
j⊆Si∩Sj.
We have now shown that Si∩S′
j=Si∩Sj, so that
S′= (Si∩S′
j)∩ \
k̸=i,jSk!
= (Si∩Sj)∩ \
k̸=i,jSk!
=m\
k=1Sk=S.
Thus, performing an R1 operation
Mi,j(c)[A|b] = [A′|b′]
on the augmented matrix [ A|b] corresponding to a system of equations results in a new
augmented matrix [ A′|b′] that corresponds to a new system of equations that has the same
solution set as the original system. This is clearly also the case whenever performing an R2
operation Mi,j[A|b], since the outcome yields an augmented matrix corresponding to a system
of equations that is identical to the original system except that the ith and jth equations have
traded places. (Again, a system of equations is a set of equations, and sets are blind to order.)
Finally, an R3 operation Mi(c)[A|b] results in an augmented matrix corresponding to a system
of equations that is identical to the original system except that the ith equation has been
multiplied by a nonzero scalar c, which does not alter the solution set of the ith equation and
therefore does not alter the solution set of the system as a whole. We have proven the following.
Proposition 2.34. Any elementary row operation performed on the augmented matrix [A|b]
of a system of linear equations results in an augmented matrix [A′|b′]whose corresponding
system has the same solution set.
Definition 2.35. Two systems of linear equations are equivalent if their corresponding
augmented matrices are row-equivalent.53
In light of Proposition 2.34 it is immediate that equivalent systems of linear equations have
the same solution set. Thus, to solve a system of linear equations such as (2.22) , one fairly
efficient approach is to perform elementary row operations on its corresponding augmented
matrix until it is in row-echelon form, at which point it is easy to determine the system’s solution
set. The process is known as Gaussian elimination.
Example 2.36. Apply Gaussian elimination to determine the solution set of the system


3x+y+ 4z+w= 6
2x + 3z+ 4w= 13
y−2z−w= 0
x−y+z+w= 3
Solution. The corresponding augmented matrix for the system is

3 1 4 1 6
2 0 3 4 13
0 1 −2−1 0
1−1 1 1 3
.
We’ll start by interchanging the 1st and 4th rows, since it will be convenient having a 1 at the
top of the 1st column. Also we’ll interchange the 2nd and 3rd rows so as to move the 0 in the
2nd column down to a position where row-echelon form requires a 0 entry.

3 1 4 1 6
2 0 3 4 13
0 1 −2−1 0
1−1 1 1 3
r1↔r4− − − − →
r2↔r3
1−1 1 1 3
0 1 −2−1 0
2 0 3 4 13
3 1 4 1 6
−2r1+r3→r3− − − − − − − − →
−3r1+r4→r4

1−1 1 1 3
0 1 −2−1 0
0 2 1 2 7
0 4 1 −2−3
−2r2+r3→r3− − − − − − − − →
−4r2+r4→r4
1−1 1 1 3
0 1 −2−1 0
0 0 5 4 7
0 0 9 2 −3
−9
5r3+r4→r4− − − − − − − − →

1−1 1 1 3
0 1 −2−1 0
0 0 5 4 7
0 0 0 −26
5−78
5
−5
26r4→r4− − − − − − − →
1−1 1 1 3
0 1 −2−10
0 0 5 4 7
0 0 0 1 3

The fifth matrix above is in row-echelon form, so technically the last row operation is not
required. On the other hand it certainly is desirable to eliminate any fractions if there’s an easy
way to do it. We have obtained the following equivalent system of equations:


x−y+z+w= 3
y−2z−w= 0
5z+ 4w= 7
w= 354
We may now determine the solution to the system by employing so-called “backward substitution.”
Taking w= 3 from the 4th equation and substituting into the 3rd equation yields
5z+ 4(3) = 7 ⇒5z=−5⇒z=−1.
Taking w= 3 and z=−1 and substituting into the 2nd equation yields
y−2(−1)−3 = 0 ⇒y= 1.
Finally, substituting w= 3,z=−1, and y= 1 into the 1st equation yields
x−1 + (−1) + 3 = 3 ⇒x= 2.
Therefore the only solution to the system is ( x, y, z, w ) = (2 ,1,−1,3), which is to say the
solution set is {(2,1,−1,3)}. ■
Example 2.37. Apply Gaussian elimination to determine the solution set of the system


−3x−5y+ 36z= 10
−x + 7z= 5
x+y−10z=−4(2.29)
Write the solution set in terms of column vectors.
Solution. The corresponding augmented matrix for the system is

−3−5 36 10
−1 0 7 5
1 1 −10−4
.
We transform this matrix into row-echelon form:
−3−5 36 10
−1 0 7 5
1 1 −10−4
r1↔r3− − − − →
1 1 −10−4
−1 0 7 5
−3−5 36 10
r1+r2→r2− − − − − − − →
3r1+r3→r3
1 1 −10−4
0 1 −3 1
0−2 6 −2

2r2+r3→r3− − − − − − − →
1 1−10−4
0 1 −3 1
0 0 0 0
.
We have obtained the equivalent system of equations
x+y−10z=−4
y−3z= 1
From the second equation we have
y= 3z+ 1,
which, when substituted into the first equation, yields
x= 10z−y−4 = 10 z−(3z+ 1)−4 = 7 z−5.
That is, we have x= 7z−5 and y= 3z+ 1, and zis free to assume any scalar value whatsoever.55
Any ordered triple [ x, y, z ]⊤that satisfies (2.29) must be of the form
[7z−5,3z+ 1, z]⊤
for some z∈F, and therefore the solution set is
S=
[7z−5,3z+ 1, z]⊤:z∈F	
.
Since 
7z−5
3z+ 1
z
=
−5
1
0
+
7z
3z
z
=
−5
1
0
+z
7
3
1
,
we may write
S=


−5
1
0
+t
7
3
1
:t∈F

.
■
The solution set Sin Example 2.37 is called a one-parameter solution set, meaning all
elements of Smay be specified by designating a value in the field Ffor a single parameter
(namely z). The solution set of the system in Example 2.36 is a zero-parameter solution set.
In general an n-parameter set is a set Swhose elements are determined by the values of n
independent variables x1, . . . , x ncalled parameters . If the values of x1, . . . , x nderive from a
setI(sometimes called the index set ), then Shas the form
S={f(x1, . . . , x n) :xi∈Ifor each 1 ≤i≤n}.
Here fis a function that pairs each n-tuple ( x1, . . . , x n) with a single element of S.56
Problems
In Exercises 1–4 use Gaussian elimination to determine the solution set of the system of linear
equations. Write all solution sets in terms of column vectors.
1.

x+ 2y−z= 9
2x −z=−2
3x+ 5y+ 2z= 22
2.

x −z= 1
−2x+ 3y−z= 0
−6x+ 6y =−2
3.

x +z+w= 4
y−z =−4
x−2y+ 3z+w= 12
2x −2z+ 5w=−1
4.

3x−6y−z+w= 7
−x+ 2y+ 2z+ 3w= 1
4x−8y−3z−2w= 6
5. Consider the system of equations

2x+y+z= 3
x−y+ 2z= 3
x−2y+λz= 4
Determine for which values of λ, if any, the system has:
(a) No solution.
(b) A unique solution, in which case give the solution.
(c) Infinitely many solutions, in which case give the solution.
6.Find conditions on the general vector bthat would make the equation Ax=bconsistent,
where
A=
1 0 −1
−2 3 −1
3−3 0
2 0 −2
.57
2.6 – Homogeneous Systems
A system of linear equations in which all constant terms are equal to 0 is said to be
homogeneous :

a11x1+a12x2+···+a1nxn= 0
a21x1+a22x2+···+a2nxn= 0
............
am1x1+am2x2+···+amnxn= 0(2.30)
If we define Aandxas in (2.23) , which is to say A= [aij]m,nandx= [xi]n,1, then we may
write (2.30) as the matrix equation Ax=0. At a glance it is clear that setting
x1=···=xn= 0
will satisfy the system. This is called the trivial solution , and it may be represented as an
n-tuple (0 , . . . , 0), an n×1 zero vector 0, or some analogous construct.
Theorem 2.38. LetA= [aij]m,nandx= [xi]n,1. Ifn > m , then the homogeneous system
Ax=0has a nontrivial solution.
Proof. The theorem states that, for each m∈N, the system (2.30) has a nontrivial solution
whenever n > m . The proof will be accomplished using induction.
We consider the base case, when m= 1. For any n >1 the “system” consists of a single
equation
a11x1+···+a1nxn= 0. (2.31)
Now, if a1j= 0 for all 1 ≤j≤n, then anychoice of scalars for x1, . . . , x nwill satisfy this
equation, and so in particular there exists a nontrivial solution. On the other hand if a1k̸= 0 for
some 1 ≤k≤n, then we may choose any scalar values for x1, . . . , x k−1, xk+1, . . . , x n, and set
xk=−1
a1kX
j̸=kaijxj
so as to satisfy (2.31) . Since there again exists a nontrivial solution, we see that the theorem is
true in the case when m= 1.
Letm∈Nbe arbitrary and suppose that the theorem is true for this mvalue. Consider the
system

a11x1+a12x2+···+a1nxn= 0
............
am+1,1x1+am+1,2x2+···+am+1,nxn= 0(2.32)
where n > m + 1. Assume that a11̸= 0. If [ A|0] is the corresponding augmented matrix, then
the sequence of elementary row operations
M1,m+1(−am+1,1/a11)···M1,2(−a21/a11)[A|0]58
will yield a new augmented matrix [ A′|0] that has zero entries under a11in the first column,
which is to say we have attained an equivalent system of the form


a11x1+a12x2+···+a1nxn= 0
a′
22x2+···+a′
2nxn= 0
.........
a′
m+1,2x2+···+a′
m+1,nxn= 0(2.33)
Contained within this system is the system


a′
22x2+···+a′
2nxn= 0
.........
a′
m+1,2x2+···+a′
m+1,nxn= 0
which has mequations and nvariables, where n > m . By our inductive hypothesis this smaller
system has a nontrivial solution ( ˆx2, . . . , ˆxn), so that there is some 2 ≤k≤nfor which ˆxk̸= 0.
Now, if we let
ˆx1=−1
a11nX
j=2a1jˆxj,
then ( ˆx1, . . . , ˆxn) will satisfy all the equations in the system (2.33) . Since (2.32) is equivalent to
(2.33) it follows that ( ˆx1, . . . , ˆxn) is a solution to (2.32) , and moreover it is a nontrivial solution
since ˆ xk̸= 0. We conclude that the theorem is true for m+ 1 at least when a11̸= 0.
Ifa11= 0 but there exists some 2 ≤k≤nfor which a1k̸= 0, we relabel our variables thus:
y1=xk,yk=x1, and yj=xjforj̸= 1, k. We thereby obtain a system of the form


a1ky1+a12y2+···+a11yk···+a1nyn= 0
...............
am+1,ky1+am+1,2y2+···+am+1,1yk···+am+1,nyn= 0(2.34)
From this, much like before, we obtain an equivalent system in which the variable y1has been
eliminated from all equations save the first one. By our inductive hypothesis there exists a
nontrivial solution ( ˆy2, . . . , ˆyn) to the system consisting of the 2nd through ( m+ 1)st equations
of the equivalent system, whereupon setting
ˆy1=−1
a1k(a12ˆy2+···+a11ˆyk+···+a1nˆyn)
gives an n-tuple ( ˆy1, . . . , ˆyn) that is nontrivial and satisfies (2.34) . It is then a routine matter to
verify that ( ˆx1, . . . , ˆxn) with ˆx1=ˆyk,ˆxk=ˆy1, and ˆxj=ˆyjforj̸= 1, kis a nontrivial solution
to (2.32).
Ifa1j= 0 for all 1 ≤j≤n, then by our inductive hypothesis we may find a nontrivial
solution to the other mequations of (2.32) , and this solution must necessarily satisfy the first
equation.
We have now verified that the theorem is true for m+1 in all possible cases. By the Principle
of Induction, therefore, the theorem is proven. ■
A system of equations Ax=bfor which b̸=0isnonhomogeneous . The next example
shows the first of many intimate connections between a nonhomogeneous system Ax=band59
the corresponding homogeneous system Ax=0(i.e. the homogeneous system having the same
coefficient matrix A).
Example 2.39. LetAx=bbe a nonhomogeneous system of equations, and let x′be a solution.
Show that if x0is a solution to the corresponding homogeneous system Ax=0, then x′+x0is
another solution to Ax=b.
Solution. We have Ax′=bandAx0=0. Let y=x′+x0. We must show that Ay=b. But
this is immediate:
Ay=A(x′+x0) =Ax′+Ax 0=b+0=b,
using the Distributive Law of matrix multiplication established in §2.2. ■
Given any nonempty S⊆Fnand nonzero x∈Fn, we define a new set
x+S={x+y:y∈S}
called a coset ofS. We now improve on Example 2.39 with the following more general result.
Theorem 2.40. LetAx=bbe a nonhomogeneous system of linear equations with solution set
S, and let Shbe the solution set of the corresponding homogeneous system Ax=0. Ifxpis any
particular solution to Ax=b, then S=xp+Sh.
Proof. Suppose that xpis a particular solution to Ax=b. Let x′∈Sbe arbitrary. Then
A(x′−xp) =Ax′−Axp=b−b=0
shows that x′−xpis a solution to Ax=0and hence x′−xP∈Sh. Since
x′=xp+ (x′−xp)∈ {xp+xh:xh∈Sh}=xp+Sh,
we conclude that S⊆xp+Sh.
Next, suppose that x′∈xp+Sh, sox′=xp+xhfor some xh∈Sh. Since
Ax′=A(xp+xh) =Axp+Axh=b+0=b,
we conclude that x′∈Sand hence xp+Sh⊆S.
Therefore S=xp+Sh. ■
To fully determine the solution set of any nonhomogeneous system Ax=b, according to
Theorem 2.40 it suffices to find just one solution to Ax=b(called a particular solution )
along with the complete solution set of Ax=0.
We close this chapter with a final result that will later become bound up in the Inverse
Matrix Theorem, which is a theorem that will bring together over a dozen seemingly disparate
statements that all turn out to be equivalent.
Proposition 2.41. IfA∈Fn×nis invertible, then the homogeneous system Ax=0has only
the trivial solution.60
Proof. Suppose A∈Fn×nis invertible. Clearly 0is a solution to Ax=0, and it only remains
to show it is a unique solution. Suppose that x0is a solution to the system, so that Ax 0=0.
Since A−1exists, we have
Ax 0=0⇒A−1(Ax 0) =A−10⇒(A−1A)x0=0⇒Inx0=0⇒x0=0.
Thus any x0given to be a solution to the system must in fact be 0, proving uniqueness. ■61
3
Vector Spaces
3.1 – The Vector Space Axioms
LetFbe a field. In practice Fis usually either the real number system Ror the complex
number system C, but in any case it is a set of objects that obey the field axioms given in §1.1.
Definition 3.1. Avector space overFis a set Vof objects, along with operations vector
addition V×V→V(denoted by +) and scalar multiplication F×V→V(denoted by ·
or juxtaposition) subject to the following axioms:
VS1. u+v=v+ufor any u,v∈V
VS2. u+ (v+w) = (u+v) +wfor any u,v,w∈V
VS3. There exists some 0∈Vsuch that u+0=ufor any u∈V
VS4. For each u∈Vthere exists some −u∈Vsuch that u+ (−u) =0
VS5. For any a∈Fandu,v∈V,a(u+v) =au+av
VS6. For any a, b∈Fandu∈V,(a+b)u=au+bu
VS7. For any a, b∈Fandu∈V,a(bu) = (ab)u
VS8. For all u∈V,1u=u
The elements of Vare called vectors and the elements of the underlying field Fare called
scalars .
Areal vector space is a vector space over R, and a complex vector space is a vector
space over C. AEuclidean n-space over Fis specifically a vector space consisting of n-tuples
[x1, . . . , x n], where xk∈Ffor all 1 ≤k≤n. In general a Euclidean space is any Euclidean
n-space over Ffor some unspecified n∈Nand field F. IfF=R, we obtain a real Euclidean
space ; and if F=C, we obtain a complex Euclidean space .
The object 0mentioned in Axiom VS3 is called the zero vector , and the vector −u
mentioned in Axiom VS4 is the additive inverse ofu.
We have in the statement of the definition that + : V×V→V. That is, the vector addition
operation + takes any ordered pair ( u,v)∈V×Vand returns an object u+v∈V. Thus
u+vmust be an object that belong to the set V! Similarly the scalar multiplication operation
is given to be a map ·:F×V→V, which means scalar multiplication takes any ordered pair
(a,u)∈F×Vand returns an object a·u=au∈V. Thus aumust also belong to V! Some62
books state these facets of the definition of a vector space as two additional axioms:
u+v∈Vfor any u,v∈V (3.1)
and
au∈Vfor any a∈Fandu∈V. (3.2)
We call (3.1) theclosure property of scalar multiplication , and (3.2) theclosure property
of addition . When property (3.1) holds for a set V, we say that Visclosed under addition ;
and when property (3.2) holds we say Visclosed under scalar multiplication .
Remark. A set Vtogether with given operations + and ·defined on V×VandF×V,
respectively, is a vector space if and only if the eight axioms VS1–VS8 and the two closure
properties (3.1) and (3.2) are all satisfied!
Some seemingly “obvious” results actually require careful reasoning to prove their validity in
the context of vector spaces, as the next two propositions show.
Proposition 3.2. LetVbe a vector space, u∈V, and a∈F. Then the following properties
hold.
1. 0u=0
2.a0=0
3.Ifau=0, then a= 0oru=0
Proof.
Proof of Part (1). Since u∈Vand 0 ∈F, we have 0 u∈Vby the closure property (3.2). Now,
0u= 0u+0 Axiom VS3
= 0u+ [u+ (−u)] Axiom VS4
= (0u+u) + (−u) Axiom VS2
= (0u+ 1u) + (−u) Axiom VS8
= (0 + 1) u+ (−u) Axiom VS6
= 1u+ (−u) Axiom F3
=u+ (−u) Axiom VS8
=0. Axiom VS4
The proofs of parts (2) and (3) are left to the exercises. ■
Proposition 3.3. IfVis a vector space and u∈V, then (−1)u=−u.
Proof. Suppose that Vis a vector space and u∈V. Then ( −1)u∈V, and
(−1)u= (−1)u+0 Axiom VS3
= (−1)u+ [u+ (−u)] Axiom VS4
= [(−1)u+u] + (−u) Axiom VS2
= [(−1)u+ 1u] + (−u) Axiom VS863
= (−1 + 1) u+ (−u) Axiom VS6
= 0u+ (−u) Axiom F4
=0+ (−u), Proposition 3.2(1)
=−u. Axiom VS3
■
As with Euclidean vectors we define vector subtraction by
u−v=u+ (−v)
for any u,v∈V.
The objects belonging to a vector space are invariably called vectors, but they could be any
kind of mathematical entity either concrete or abstract. They often are the Euclidean vectors
encountered in Chapter 1, but they could also be matrices, polynomials, functions, or other
objects. This is part of the power of linear algebra.
Example 3.4. The set of coordinate vectors
Rn=


x1...
xn
x1, . . . , x n∈R

,
together with the definitions of vector addition and real scalar multiplication as given in §1.2, is
easily verified to be a vector space over R. Similarly, the set of coordinate vectors
Cn=


z1...
zn
z1, . . . , z n∈C

,
with vector addition and complex scalar multiplication defined in analogous fashion to Rn, is a
vector space over C. Important: the underlying fields of RnandCnare always taken to be R
andC, respectively, unless otherwise specified! ■
Example 3.5. The set Fm×nof all m×nmatrices with entries in Fis a vector space under the
standard operations of matrix addition and scalar multiplication given by Definition 2.1. In
particular the set Rm×nofm×nmatrices with real-valued entries is a real vector space, and
the set Cm×nofm×nmatrices with complex-valued entries is a complex vector space. ■
Example 3.6. Given an integer n≥0, let Pn(F) be the set of all polynomials of a single
variable xwith coefficients in Fand degree at most n; that is,
Pn(F) ={a0+a1x+···+an−1xn−1+anxn:ai∈Ffor 0≤i≤n}.
By definition the polynomial 0 has degree −∞, and so 0 ∈ P n(F) in particular. We have
P0(R) ={a:a∈R}=R,
P1(R) ={a+bx:a, b∈R},
P2(R) ={a+bx+cx2:a, b, c∈R}.64
If we define polynomial addition and scalar multiplication in the customary fashion by
(a0+a1x+···+anxn) + (b0+b1x+···+bnxn)
= (a0+b0) + (a1+b1)x+···+ (an+bn)xn,
and
c(a0+a1x+···+anxn) =ca0+ca1x+···+canxn,
then it is straightforward to verify that Pn(F) is a vector space. ■
Example 3.7. LetS⊆F, where as usual Fis some field. Let F(S,F) denote the set of all
functions S→F. Given f∈ F(S,F) and c∈F, we define scalar multiplication of cwith fas
yielding a new function cf∈ F(S,F) given by
(cf)(x) =cf(x)
for all x∈S. Iff, g∈ F(S,F), we define addition of fwith gas yielding a new function
f+g∈ F(S,F) given by
(f+g)(x) =f(x) +g(x)
for all x∈S. These operations are consonant with conventions established in elementary
algebra, and it is straightforward to verify that F(S,F) is in fact a vector space. The zero vector
is the function 0 given by 0( x) = 0 for all x∈S. The additive inverse of any f∈ F(S,F) is the
function −fgiven by ( −f)(x) =−f(x), since
(f+ (−f))(x) =f(x) + (−f)(x) =f(x) + (−f(x)) = 0 = 0( x)
for all x∈S, and hence f+ (−f) = 0.
If a set Sis not specified at the outset of an analysis involving functions f1, f2, . . . , f n, then
we take
S=n\
i=1Dom( fi) = Dom( f1)∩Dom( f2)∩ ··· ∩ Dom( fn)
and carry out the analysis in the vector space F(S,F) provided that S̸=∅. ■
Example 3.8. Show that the collection of functions4
C={f:R→R|f(2) = 0 }
is a vector space over Runder the usual operations of function addition and scalar multiplication
(see Example 3.7).
Solution. First, it’s worth noting that since F(R,R) is the set of all real-valued functions with
domain R, we have C ⊆ F (R,R).
Letf, g, h ∈ Canda, b∈R. Let x∈Rbe arbitrary. We have f+g∈ Candaf∈ Csince
(f+g)(2) = f(2) + g(2) = 0 + 0 = 0 and ( af)(2) = af(2) = a(0) = 0 ,
4Sets of functions (as well as sets of sets) are often referred to as “collections” or “families” in the mathematical
literature.65
and so there is closure under addition and scalar multiplication. In what follows we make
frequent use of the field axioms of the real number system (see §1.1).
By the Commutative Property of Addition we have
(f+g)(x) =f(x) +g(x) =g(x) +f(x) = (g+f)(x),
so that f+g=g+f. Axiom VS1 holds.
We have
f(x) + [g(x) +h(x)] = [ f(x) +g(x)] +h(x)
by the Associative Property of Addition, and thus f+ (g+h) = (f+g) +h. Axiom VS2 holds.
Letobe the zero function. That is, o(x) = 0 for all x∈R. Since o(2) = 0 we see that o∈ C.
Now,
(o+f)(x) =o(x) +f(x) = 0 + f(x) =f(x)
and
(f+o)(x) =f(x) +o(x) =f(x) + 0 = f(x),
and so o+f=f+o=f. Axiom VS3 holds.
As usual −fis the function given by ( −f)(x) =−f(x), so in particular ( −f)(2) = −f(2) = 0
implies that −f∈ C. Now,
(−f+f)(x) = (−f)(x) +f(x) =−f(x) +f(x) = 0 = o(x)
shows that −f+f=o. Similarly f+ (−f) =o. Axiom VS4 holds.
By the Distributive Property,
 
a(f+g)
(x) =a(f+g)(x) =a[f(x) +g(x)] =af(x) +ag(x)
= (af)(x) + (ag)(x) = (af+ag)(x),
which shows that a(f+g) =af+ag. Axiom VS5 holds.
Again by the Distributive Property,
 
(a+b)f
(x) = (a+b)f(x) =af(x) +bf(x) = (af)(x) + (bf)(x) = (af+bf)(x),
so (a+b)f=af+bf. Axiom VS6 holds.
By the Associative Property of Multiplication,
 
a(bf)
(x) =a(bf)(x) =a(bf(x)) = ( ab)f(x) = 
(ab)f
(x),
soa(bf) = (ab)f. Axiom VS7 holds.
Finally, since 1 ∈Ris the multiplicative identity, we have (1 f)(x) = 1 f(x) =f(x). This
shows that 1 f=f, and Axiom VS8 holds. ■66
Problems
In Exercises 1–4 a set of objects Vis given, along with definitions for operations of vector
addition and scalar multiplication. Determine whether or not Vis a vector space under the
given operations. If it is not, indicate which axioms and closure properties fail to hold.
1.V=R2, with vector addition and scalar multiplication defined by
u1
u2
+
v1
v2
=
u1+v1
u2+v2
and c
u1
u2
=
9cu1
9cu2
.
2.V=R2, with vector addition and scalar multiplication defined by
u1
u2
+
v1
v2
=
u1+v1+ 3
u2+v2+ 3
and c
u1
u2
=
cu1
cu2
.
3.Vis the set of 2 ×2 matrices of the form
a0
1b
with the standard operations of matrix addition and scalar multiplication.
4.Vis the set of real-valued one-to-one functions with domain ( −∞,∞), together with the
zero function x7→0. For any f, g∈Vandc∈R, the sum f+gand scalar product cfare
defined in the standard way.
5. Prove part (2) of Proposition 3.2.
6. Prove part (3) of Proposition 3.2.67
3.2 – Subspaces
Definition 3.9. LetVbe a vector space. If W⊆Vis a vector space under the vector addition
and scalar multiplication operations defined on V×VandF×V, respectively, then Wis a
subspace ofV.
In order for W⊆Vto be a vector space it must satisfy the statement of Definition 3.1
to the letter , except that the symbol Wis substituted for V. Straightaway this means we
must have W̸=∅since Axiom VS3 requires that 0∈W. Moreover, vector addition must
map W×W→Wand scalar multiplication must map F×W→W, which is to say for any
u,v∈Wanda∈Fwe must have u+v∈Wandau∈W. These observations prove the
forward implication in the following theorem.
Theorem 3.10. LetVbe a vector space and ∅̸=W⊆V. Then Wis a subspace of Vif and
only if au∈Wandu+v∈Wfor all a∈Fandu,v∈W.
Proof. We need only prove the reverse implication. So, suppose that for any a∈Fand
u,v∈W, it is true that au∈Wandu+v∈W. Then vector addition maps W×W→W
and scalar multiplication maps F×W→W, and it remains to confirm that Wsatisfies the
eight axioms in Definition 3.1. But it is clear that Axioms VS1, VS2, VS5, VS6, VS7, and VS8
must hold. For instance if u,v∈W, then u+v=v+usince u,v∈VandVis given to be a
vector space, and so Axiom VS1 is confirmed.
Letu∈W. Since au∈Wfor any a∈F, it follows that ( −1)u∈Win particular. Now,
(−1)u=−uby Proposition 3.3, and so −u∈W. That is, for every u∈Wwe find that
−u∈Was well, where u+ (−u) =−u+u=0. This shows that Axiom VS4 holds for W.
Finally, since au∈Wfor any a∈F, it follows that 0 u∈W. By Proposition 3.2 we have
0u=0, so0∈Wand Axiom VS3 holds for W.
We conclude that W⊆Vis a vector space under the vector addition and scalar multiplication
operations defined on V×VandF×V, respectively. Therefore Wis a subspace of Vby
Definition 3.9. ■
The following result is immediate, and provides a checklist that commonly is employed to
quickly determine whether a subset of a vector space is a subspace.
Corollary 3.11. LetVbe a vector space, and let W⊆V. Then Wis a subspace of Vif the
following conditions hold:
1.0∈W.
2.au∈Wfor all u∈Wanda∈F.
3.u+v∈Wfor all u,v∈W.
In practice, to determine whether any given subset of a vector space Vis a subspace the
first thing one usually checks is whether or not it contains the zero vector 0. IfW⊆Vdoes
not contain 0, then it is not a subspace.68
Example 3.12. Consider the set
U=


x
y
z
∈R3:xyz= 0

.
Certainly Uis a subset of R3, but is it a subspace of R3? Two vectors belonging to Uare
u1=
1
0
0
and u2=
0
1
1
,
since (1)(0)(0) = 0 and (0)(1)(1) = 0. However, the vector
u1+u2=
1
0
0
+
0
1
1
=
1
1
1

does not belong to Usince (1)(1)(1) ̸= 0. Since Uis not closed under vector addition, it is not
a subspace of R3. ■
Example 3.13. Consider the set Skw n(R) ofn×nskew-symmetric matrices with entries in R:
Skw n(R) ={A∈Rn×n:A⊤=−A}.
Clearly Skw n(R) is a subset of the vector space Rn×n, and since O⊤
n=−Onwe see that Skw n(R)
contains the “zero vector” of Rn×n. Let A,B∈Skw n(R) and c∈R. By Proposition 2.3,
(cA)⊤=cA⊤=c(−A) =−(cA)
and
(A+B)⊤=A⊤+B⊤=−A+ (−B) =−(A+B),
which shows that cA∈Skw n(R) and A+B∈Skw n(R). Therefore Skw n(R) is a subspace by
Corollary 3.11. ■
Example 3.14. As we saw in §2.5, a system of mlinear equations in nunknowns


a11x1+a12x2+···+a1nxn=b1
a21x1+a22x2+···+a2nxn=b2............
am1x1+am2x2+···+amnxn=bm(3.3)
may be written as a matrix equation Ax=b, where
A=
a11a12··· a1n
a21a22··· a2n............
am1am2··· amn
,x=
x1
x2...
xn
,b=
b1
b2...
bm
. (3.4)69
Here each vector xinFnis represented by a column matrix as in (3.4), so that
Fn=


x1...
xn
x1, . . . , x n∈F

.
As previously established, a vector
s=
s1...
sn

is asolution toAx=bif and only if substituting sforxinAx=bmakes the equation true,
and this will be the case if and only if the n-tuple ( s1, . . . , s n) is a solution to the system of
equations (3.3).
Now, if we set b=0, we obtain the matrix equation Ax=0representing the homogeneous
system in which the right-hand side of every equation in (3.3) is 0. The solution set for Ax=0
is the set
S={x∈Fn:Ax=0},
so clearly S⊆Fn. But is Sasubspace ofFn? Certainly A0=0is true, so 0∈SandS̸=∅.
To determine definitively whether Sis a subspace we use Corollary 3.11.
Lets∈Sanda∈F. Since sis inSwe have As=0, and then
A(as) =a(As) =a0=0
shows that as∈S. Next, if s,s′∈S, so that As=0andAs′=0both hold, then
A(s+s′) =As+As′=0+0=0
shows that s+s′∈Salso.
Therefore Sis a subspace of Fnby Corollary 3.11. We call Sthesolution space of the
system Ax=0. ■
Definition 3.15. Thenull space ofA∈Fm×nis the set
Nul(A) ={x∈Fn:Ax=0}.
Proposition 3.16. IfA∈Fm×n, then Nul(A)is a subspace of Fn.
Proof. This follows easily from the proceedings of Example 3.14 since the null space of a matrix
Acorresponds to the solution space of the homogeneous system of linear equations Ax=0.■
Definition 3.17. LetVbe a subspace of Rn. The orthogonal complement ofVis the set
V⊥={x∈Rn:x·v= 0 for all v∈V}.
Proposition 3.18. IfVis a subspace of Rn, then V⊥is also a subspace of Rn.70
Proof. LetVbe a subspace of Rn. Suppose x,y∈V⊥. Then for any v∈Vwe have
(x+y)·v=x·v+y·v= 0 + 0 = 0 ,
which shows that x+y∈V⊥. Moreover, for any c∈Rwe have
(cx)·v=c(x·v) =c(0) = 0
for any v∈V, which shows that cx∈V⊥. Since V⊥⊆Rnis closed under scalar multiplication
and vector addition, we conclude that it is a subspace of Rn. ■
Problems
1. Determine whether the set
W=
[x, y, z ]⊤:y= 2x−z	
is a subspace of R3. If it is, prove it; otherwise show how it fails to be a subspace.
2.Prove or disprove that the set is a subspace of the vector space R2×2of all 2 ×2 matrices
with real entries.
(a)
A∈R2×2:A⊤=A	
(b)
a b
0c
:a, b, c∈R
(c)
a0
0a2
:a∈R
(d)
a20
0b2
:a, b∈R
3.Determine whether Symn(R), the set of n×nsymmetric matrices with real entries, is a
subspace of Rn×n. If it is, prove it; otherwise show how it fails to be a subspace.
4.Prove or disprove that the set is a subspace of the vector space F(R,R) of all real-valued
functions fwith domain R.
(a){f∈ F(R,R) :f(x)≤0 for all x∈R}
(b){f∈ F(R,R) :f(0) = 0 }
(c){f∈ F(R,R) :f(0) = 9 }
(d){f∈ F(R,R) :fis a constant function }
(e){f∈ F(R,R) :f(x) =acosx+bsinxfor some a, b∈R}71
3.3 – Subspace Sums and Direct Sums
Definition 3.19. LetUandWbe subspaces of a vector space V. The sum ofUandWis the
set of vectors
U+W={v∈V:v=u+wfor some u∈Uandw∈W}.
More generally, if U1, . . . , U nare subspaces of V, then the sum ofU1, . . . , U nis the set of
vectors
nX
k=1Uk=(
v∈V:v=nX
k=1ukfor some uk∈Uk)
.
Equivalently we may write
U+W={u+w:u∈Uandw∈W}
for subspaces UandWofV, and
nX
k=1Uk=(nX
k=1uk:uk∈Uk)
for subspaces U1, . . . , U kofV.
Proposition 3.20. IfU1, . . . , U nare subspaces of a vector space VoverF, then U1+···+Un
is also a subspace of V.
Proof. Suppose U1, . . . , U nare subspaces of a vector space V, and let U=U1+···+Un. Clearly
0∈U, soU̸=∅. Let u,v∈U, so that
u=nX
k=1ukand v=nX
k=1vk
for vectors uk,vk∈Uk, 1≤k≤n. Now, uk+vk∈Uksince each Ukis closed under vector
addition, and hence
u+v=nX
k=1(uk+vk)∈nX
k=1Uk=U
and we conclude that Uis closed under vector addition. Also, for any c∈Fwe have cuk∈Uk
since each Ukis closed under scalar multiplication, and hence
cu=nX
k=1cuk∈nX
k=1Uk=U
and we conclude that Uis closed under scalar multiplication. Therefore Uis a subspace of V
by Corollary 3.11. ■72
Definition 3.21. LetUandWbe subspaces of a vector space V. We say Vis the direct sum
ofUandW, written V=U⊕W, ifV=U+WandU∩W={0}.
More generally, let U1, . . . , U nbe subspaces of V. Then Vis the direct sum ofU1, . . . , U n,
written
V=nM
k=1Uk,
ifV=Pn
k=1Ukand
Ui∩X
k̸=iUk={0} (3.5)
for each i= 1, . . . , n .
In (3.5) it’s understood that the sum is taken over all 1 ≤k≤nnot equal to i; that is,
X
k̸=iUk=U1+···+Ui−1+Ui+1+···+Un.
Thus, in particular, if U1,U2, and U3are subspaces of V, then
V=3M
k=1Uk=U1⊕U2⊕U3
if and only if
V=U1+U2+U3
and
U1∩(U2+U3) =U2∩(U1+U3) =U3∩(U1+U2) ={0}.
Proposition 3.22. LetUandWbe subspaces of V. Then V=U⊕Wif and only if for each
v∈Vthere exist unique vectors u∈Uandw∈Wsuch that v=u+w.
To say there exist unique vectors u∈Uandw∈Wsuch that v=u+wmeans, specifically,
that if u,u′∈Uandw,w′∈Ware such that v=u+wandv=u′+w′, then we must have
u=u′andw=w′. We now prove the proposition.
Proof. Suppose that V=U⊕W, and let v∈V. Since V=U+Wthere exist some u∈U
andw∈Wsuch that v=u+w. Suppose u′∈Uandw′∈Ware such that v=u′+w′. Then
0=v−v= (u+w)−(u′+w′) = (u−u′) + (w−w′),
which implies that
u−u′=w′−w
and hence u−u′,w′−w∈U∩Wsinceu−u′∈Uandw′−w∈W. However, from V=U⊕W
we have U∩W={0}, leading to
u−u′=w′−w=0
and therefore u′=uandw′=w.73
Conversely, suppose that for each v∈Vthere exists unique vectors u∈Uandw∈Wsuch
thatv=u+w. Then v∈U+W, so clearly V=U+W. Suppose that v∈U∩W. Then we
may take u∈Uto be v, and w∈Wto be 0, so that
u+w=v+0=v;
on the other hand if we let u′=0andw′=v, then u′∈Uandw′∈Ware such that
v=u′+w′. By our uniqueness hypothesis we must have u=u′andw=w′. That is, u=0
andw=0, so that v=0and we obtain v∈ {0}. From this we conclude that U∩W⊆ {0},
and since the reverse containment is obvious, we find that both U∩W={0}andV=U+W
are true. Therefore V=U⊕W. ■
Proposition 3.22 and its proof are presented largely for pedagogical reasons. The more
general result is given next, though it takes a bit more work to prove and will have limited
applicability in the next few chapters.
Theorem 3.23. LetU1, . . . , U nbe subspaces of V. Then V=U1⊕ ··· ⊕ Unif and only if for
eachv∈Vthere exist unique vectors u1∈U1, . . . ,un∈Unsuch that v=u1+···+un.
Proof. Suppose that V=U1⊕ ··· ⊕ Un. Let v∈V, so for each 1 ≤k≤nthere exists some
uk∈Uksuch that v=Pn
k=1uk. Now, suppose that v=Pn
k=1u′
k, where u′
k∈Ukfor each k.
Fix 1≤i≤n. We have u′
i−ui∈Ui, and from
nX
k=1(uk−u′
k) =nX
k=1uk−nX
k=1u′
k=v−v=0
we obtain
u′
i−ui=X
k̸=i(uk−u′
k)∈X
k̸=iUk.
That is,
u′
i−ui∈Ui∩X
k̸=iUk={0},
so that u′
i−ui=0and hence u′
i=ui. Since 1 ≤i≤nis arbitrary we conclude that
u′
1=u1, . . . ,u′
n=un, and therefore the vectors u1∈U1, . . . ,un∈Unfor which v=Pn
k=1uk
are unique.
Next, suppose that for each v∈Vthere exist unique vectors u1∈U1, . . . ,un∈Unsuch that
v=Pn
k=1uk. Then it is clear that
V=nX
k=1Uk. (3.6)
Fix 1≤i≤n, and suppose that
v∈Ui∩X
k̸=iUk.
Thus v∈Uiimplies we have u=Pn
k=1uk, where uk∈Ukis0fork̸=i, and ui=v∈Ui. On
the other hand v∈P
k̸=iUkimplies that, for each k̸=ithere exists some u′
k∈Uksuch that74
v=P
k̸=iu′
k, and so if we let u′
i∈Uibe0, we obtain v=Pn
k=1u′
k. Now, by our uniqueness
hypothesis it must be that uk=u′
kfor each 1 ≤k≤n. In particular,
v=ui=u′
i=0,
and so v∈ {0}. This shows that Ui∩P
k̸=iUk⊆ {0}, and since the reverse containment is
obvious, we conclude that
Ui∩X
k̸=iUk={0}. (3.7)
Now, the equations (3.6) and (3.7) imply that V=U1⊕ ··· ⊕ Un. ■75
3.4 – Linear Combinations and Spans
Definition 3.24. A vector vis called a linear combination of the vectors v1, . . . ,vnif there
exist scalars c1, . . . , c nsuch that
v=c1v1+···+cnvn=nX
i=1civi.
Example 3.25. Define u,v,w∈R3by
u=
2
−3
5
,v=
0
7
−1
,and w=
4
1
9
.
Show that wis a linear combination of uandv.
Solution. We must find scalars aandbsuch that
w=au+bv=
2a
−3a
5a
+
0
7b
−b
=
2a
−3a+ 7b
5a−b
.
That is, we need aandbto satisfy

2a
−3a+ 7b
5a−b
=
4
1
9
,
which is the system of equations
(2a = 4
−3a+ 7b= 1
5a−b= 9
From the first equation we have a= 2. Substituting this into the second equation yields
−6 + 7 b= 1, or b= 1. Now we must determine whether ( a, b) = (2 ,1) satisfies the third
equation, in which general is unlikely but in this case works:
5a−b= 9⇒5(2)−1 = 9 ⇒9 = 9 .
Sow= 2u+v, and therefore wis a linear combination of uandv. ■
Example 3.26. Define u,v,w∈R3by
u=
2
−3
5
,v=
0
7
−1
,and w=
4
−13
9
.
Show that wis not a linear combination of uandv.76
Solution. We must show that there exist no scalars aandbsuch that
4
−13
9
=w=au+bv=
2a
−3a+ 7b
5a−b
,
which sets up the system of equations
(2a = 4
−3a+ 7b=−13
5a−b= 9
The first equation gives a= 2. Substituting this into the second equation yields −6 + 7b=−13,
orb=−1. However, putting ( a, b) = (2 ,−1) into the third equation yields a contradiction:
5a−b= 9⇒5(2)−(−1) = 9 ⇒11 = 9 .
Hence the system of equations has no solution, which is to say there are no scalars aandbfor
which w=au+bv. ■
Definition 3.27. LetVbe a vector space and v1, . . . ,vn∈V. We say vectors v1, . . . ,vnspan
V, orVisspanned by the set {v1, . . . ,vn}, if for every v∈Vthere exist scalars c1, . . . , c n
such that v=c1v1+···+cnvn.5
Thus vectors v1, . . . ,vnspan Vif and only if every vector in Vis expressible as a linear
combination of v1, . . . ,vn. Define the span ofv1, . . . ,vnto be the set
Span{v1, . . . ,vn}=(nX
i=1civi:c1, . . . , c n∈F)
,
which is to say Span{v1, . . . ,vn}is the set of allpossible linear combinations of the vectors
v1, . . . ,vn∈V. It is easy to see in light of the closure properties (3.1) and(3.2) that Vis
spanned by {v1, . . . ,vn}if and only if
V= Span {v1, . . . ,vn}.
IfSis an arbitrary subset of a vector space VoverF, then Span (S) is defined to be the set
of all linear combinations of finitely many vectors belonging to S. Precisely put,
Span( S) =(nX
k=1ckvk:n∈N,v1, . . . ,vn∈S, and c1, . . . , c n∈F)
.
This definition allows us to speak meaningfully of the span of an infinite set, in particular.
Example 3.28. Determine whether the vectors
v1=
1
1
1
,v2=
2
2
0
,v3=
3
0
0

spanR3.
5Some books say v1, . . . , vngenerate V, orVisgenerated by the set {v1, . . . , vn}.77
Solution. Let
x=
x
y
z
∈R3.
We attempt to find scalars c1, c2, c3∈Rsuch that c1v1+c2v2+c3v3=x; that is,
c1
1
1
1
+c2
2
2
0
+c3
3
0
0
=
x
y
z
.
This yields the system(c1+ 2c2+ 3c3=x
c1+ 2c2 =y
c1 =z
which indeed has a solution:
(c1, c2, c3) =
z,y−z
2,x−y
3
.
Thus every vector in R3is expressible as a linear combination of v1,v2, and v3, which shows
that the set {v1,v2,v3}spans R3. ■
Example 3.29. Determine whether the vectors
v1=
2
−1
3
,v2=
4
1
2
,v3=
8
−1
8

spanR3.
Solution. Let
x=
x
y
z
∈R3.
We attempt to find scalars c1, c2, c3∈Rsuch that c1v1+c2v2+c3v3=x. This yields the system
(2c1+ 4c2+ 8c3=x
−c1+c2−c3=y
3c1+ 2c2+ 8c3=z
This can be cast as an augmented matrix and manipulated using elementary row operations:

2 4 8 x
−1 1−1y
3 2 8 z
∼
−1 1−1y
2 4 8 x
3 2 8 z
∼
−1 1−1 y
0 6 6 2y+x
0 5 5 3y+z

∼
1−1 1 −y
0 1 12y+x
6
0 5 5 3y+z
∼
1−1 1 −y
0 1 12y+x
6
0 0 0 3y+z−5 2y+x
6
78
We see that in order for xto be expressed as a linear combination of v1,v2, and v3, we need x,
y, and zsuch that
3y+z−52y+x
6
= 0,
or
5x−8y−6z= 0.
This leads to 1 = 0 if we choose x= 0, y= 0, and z= 1, for instance. That is, we cannot
express
0
0
1

as a linear combination of v1,v2, and v3. We conclude that {v1,v2,v3}does not span R3.■
Proposition 3.30. LetVbe a vector space. If v1, . . . ,vn∈V, then W=Span{v1, . . . ,vn}is
a subspace of V.
Proof. Suppose v1, . . . ,vn∈V. First we observe that
0= 0v1+···+ 0vn∈W.
Now, let a∈F, and let u∈Wso that there exist c1, . . . , c n∈Fsuch that
u=c1v1+···+cnvn.
Since
au=a(c1v1+···+cnvn) =ac1v1+···+acnvn
forac1, . . . , ac n∈F, it follows that au∈Walso.
Next, let u,v∈W. Then there exist c1, . . . , c n, d1, . . . , d n∈Fsuch that
u=c1v1+···+cnvnand v=d1v1+···+dnvn,
and then
u+v= (c1+d1)v1+···+ (cn+dn)vn
forc1+d1, . . . , c n+dn∈Fshows that u+v∈Walso.
Therefore Wis a subspace of Vby Corollary 3.11. ■
Proposition 3.31. LetVbe a vector space, and let S={v1, . . . ,vn} ⊆V. Ifa∈Fis nonzero,
then
Span( S) = Span 
(S\ {vi})∪ {vi+avj}
for any i, j∈ {1, . . . , n }withi̸=j.
Proof. Suppose a∈F\{0}, and let i, j∈ {1, . . . , n }with i̸=j. Let T= (S\{vi})∪{vi+avj},
and note that Tis the set obtained from Sby replacing viwithvi+avj. Suppose v∈Span (S),
so that v=Pn
k=1ckvkfor some c1, . . . , c n∈F. Now,
v=civi+cjvj+X
k̸=i,jckvk=ci(vi+avj) + (cj−aci)vj+X
k̸=i,jckvk,79
which shows that vis a linear combination of the elements of Tand hence v∈Span( T).
Next, suppose that v∈Span( T), so there exists c1, . . . , c n∈Fsuch that
v=ci(vi+avj) +X
k̸=ickvk,
and hence
v=nX
k=1c′
kvk
with c′
k=ckfork̸=jandc′
j=aci+cj, which shows that vis a linear combination of elements
ofSand hence v∈Span( S).
Since Span (S)⊆Span (T) and Span (T)⊆Span (S), we conclude that Span (S) =Span (T)
as was to be shown. ■80
Problems
1. Let
u1=
−1
3
and u2=
2
−6
.
Prove or disprove that Span {u1,u2}=R2.
2. Determine which of the following are linear combinations of vectors
u=
1
−1
3
and v=
2
4
0
.
(a)w= [−1,−11,9]⊤
(b)w= [3,7,−2]⊤
3. Express each polynomial as linear combinations of
p1= 2 + x+ 4x2, p 2= 1−x+ 3x2,and p3= 3 + 2 x+ 5x2.
(a) 6
(b) 2 + 6 x2
(c) 5 + 9 x+ 5x2
4. Determine whether the given vectors span R3.
(a)v1= [3,3,3]⊤,v2= [−2,−2,0]⊤,v3= [1,0,0]⊤
(b)v1= [1,−1,3]⊤,v2= [4,0,2]⊤,v3= [6,−1,6]⊤
(c)v1= [3,1,4]⊤,v2= [2,−3,5]⊤,v3= [5,−2,9]⊤,v4= [1,4,−1]⊤
(d)v1= [1,3,3]⊤,v2= [1,3,4]⊤,v3= [1,4,3]⊤,v4= [6,2,1]⊤
5. Determine whether the polynomials
p1= 1 + 2 x−x2p2= 3 + x2
p3= 5 + 4 x−x2p4=−2 + 2 x−2x2
span the vector space P2(R) ={a+bx+cx2:a, b, c∈R}.81
3.5 – Linear Independence and Bases
Definition 3.32. LetVbe a vector space and A={v1, . . . ,vn} ⊆Vbe nonempty. If the
equation
c1v1+···+cnvn=0 (3.8)
admits only the trivial solution c1=···=cn= 0, then we call Aalinearly independent set
andv1, . . . ,vnarelinearly independent vectors . Otherwise we call Aalinearly dependent
setandv1, . . . ,vnarelinearly dependent vectors .
An arbitrary set S⊆Vislinearly independent if every finite subset of Sis linearly
independent. Otherwise Sislinearly dependent .
It is straightforward to show that the definition for linear independence of an arbitrary set S
is equivalent to the definition for linear independence of A={v1, . . . ,vn} ̸=∅in the case when
Sis a nonempty finite set. Thus, the second paragraph of Definition 3.32 is the more general
definition of linear independence.
A careful reading of Definition 3.32 should make clear that vectors v1, . . . ,vn∈Vare
linearly dependent if and only if there exist scalars c1, . . . , c nnot all zero such that (3.8) is
satisfied. Also, an arbitrary set Sis linearly dependent if and only if there exists some finite set
{v1, . . . ,vn} ⊆Sfor which (3.8) has a nontrivial solution.
Theorem 3.33. LetAbe a row-echelon matrix. Then the nonzero row vectors of Aare linearly
independent, and the column vectors of Athat contain a pivot are linearly independent.
Proof. We shall prove the second statement concerning the column vectors using induction,
and leave the proof of the first statement (which is quite similar) as a problem.
Letm∈Nbe arbitrary. It is clear that if A∈Fmis a row-echelon matrix with a pivot, then
its single column vector constitutes a linearly independent set. Let n∈Nbe arbitrary, and
suppose that the pivot columns of any row-echelon matrix A∈Fm×nare linearly independent.
LetA∈Fm×(n+1)be a row-echelon matrix. Then the matrix B∈Fm×nthat results from
deleting column n+ 1 from Ais also a row-echelon matrix, and so its pivot columns p1, . . . ,pr
are linearly independent by inductive hypothesis. Now, if column n+ 1 of Ais not a pivot
column, then the pivot columns of Aare precisely p1, . . . ,pr, and we conclude that the pivot
columns of Aare linearly independent.
Suppose rather that column n+ 1 of Ais a pivot column. Then the pivot columns of A
are precisely p1, . . . ,prandq, where q= [q1···qm]⊤denotes column n+ 1 of A. For each
1≤j≤rlet
pj=
p1j...
pmj
.
Since qis a pivot column, there exists some 1 ≤ℓ≤msuch that qℓis a pivot of A, and then
by the definition of a pivot we have pℓj= 0 for all 1 ≤j≤r. Suppose c1, . . . , c r, a∈Fare such
that
c1p1+···+crpr+aq=0. (3.9)82
This yields
c1pℓ1+···+crpℓr+aqℓ= 0,
which reduces to aqℓ= 0, and since qℓ̸= 0 on account of being a pivot, we finally obtain a= 0.
Hence
c1p1+···+crpr=0,
and since p1, . . . ,prare linearly independent, it follows that cj= 0 for 1 ≤j≤r. This shows
that(3.9) only admits the trivial solution, and therefore {p1, . . . ,pr,q}is a linearly independent
set. That is, the pivot columns of A∈Fm×(n+1)are linearly independent, and we conclude by
induction that the pivot columns of any row-echelon matrix are linearly independent. ■
Recall the vector space F(S,F) of functions S→Fintroduced in Example 3.7. A linear
combination of f1, f2, . . . , f n∈ F(S,F) is an expression of the form
c1f1+c2f2+···+cnfn
for some choice of constants c1, c2, . . . , c n∈F, which of course is itself a function in F(S,F)
given by
(c1f1+c2f2+···+cnfn)(x) =c1f1(x) +c2f2(x) +···+cnfn(x)
for all x∈S. To write
c1f1+c2f2+···+cnfn= 0 (3.10)
means
(c1f1+c2f2+···+cnfn)(x) = 0
for all x∈S; that is, c1f1+c2f2+···+cnfnis the zero function 0 : S→ {0}.
We say f1, f2, . . . , f n∈ F(S,F) arelinearly independent on Sif (3.10) implies that
c1=c2=···=cn= 0.
Functions that are not linearly independent on Sare said to be linearly dependent on S.
Thus, f1, f2, . . . , f nare linearly dependent on Sif there can be found constants c1, c2, . . . , c n∈F,
not all zero , such that ( c1f1+c2f2+···+cnfn)(x) = 0 for all (and it must beall)x∈S.
Example 3.34. Consider the functions f, g:R→Rgiven by
f(t) =eatand g(t) =ebt
fora, b̸= 0 such that a̸=b. To show that fandg(as vectors in the vector space RR) are
linearly independent on R, we start by supposing that c1, c2∈Rare such that
c1f+c2g= 0.
That is, the constants c1andc2are such that
c1eat+c2ebt=c1f(t) +c2g(t) = (c1f+c2g)(t) = 0
for all t∈R. Thus, by choosing t= 0 and t= 1, we have in particular
c1+c2= 0 and c1ea+c2eb= 0.83
From the first equation we have
c2=−c1, (3.11)
which, when put into the second equation, yields
c1ea−c1eb= 0,
and thus
c1(ea−eb) = 0 . (3.12)
From a̸=bwe have
ea= exp( a)̸= exp( b) =eb
since the exponential function is one-to-one as established in §7.2 of the Calculus Notes , so
ea−eb̸= 0 and from equations (3.12) and(3.11) we conclude that c1=c2= 0. Therefore the
functions fandg, which is to say eatandebt, are linearly independent on Rfor any distinct
nonzero real numbers aandb. ■
Example 3.35. Show that the functions 1, x, and x2are linearly independent on any open
interval I⊆(0,∞).
Solution. LetIbe an interval in (0 ,∞), so that I= (a, b) for some 0 < a < b ≤ ∞. From
analysis we know there can be found some ρ >1 such that a < ρa < 2ρa < 3ρa < b . To show
that the functions 1, x, and x2(as vectors in the space RI) are linearly independent on I, we
suppose that c1, c2, c3∈Rare such that
c1+c2x+c3x2= 0. (3.13)
for all x∈I. Substituting ρa, 2ρa, and 3 ρaforxin (3.13) yields the system


c1+ (ρa)c2+ (ρa)2c3= 0
c1+ (2ρa)c2+ (2ρa)2c3= 0
c1+ (3ρa)c2+ (3ρa)2c3= 0
We can employ Gaussian Elimination to help solve this system for c1,c2, and c3:
1ρa (ρa)20
1 2ρa4(ρa)20
1 3ρa9(ρa)20
−r1+r2→r2
−r1+r3→r3− − − − − − →
1ρa (ρa)20
0ρa3(ρa)20
0 2ρa8(ρa)20
−2r2+r3→r3− − − − − − − →

1ρa (ρa)20
0ρa3(ρa)20
0 0 2( ρa)20
r2÷ρa→r2
r3÷2(ρa)2→r3− − − − − − − − →
1ρa(ρa)20
0 1 3 ρa0
0 0 1 0
.
Thus we now have the system


c1+ (ρa)c2+ (ρa)2c3= 0
c2+ (3ρa)c3= 0
c3= 0
from which it easily follows that c1=c2=c3= 0. This shows that the set {1, x, x2}is a linearly
independent set of functions in RIfor any open interval I⊆(0,∞). ■84
Remark. The basic approach exhibited in Example 3.35 can, with minor modifications, be
used to show that
{1, x, x2, . . . , xn}
is linearly independent in RIforanyinterval I⊆Rand integer n≥0.
Example 3.36. Consider the functions
x7→cos 2x, x 7→cos2x, x 7→sin2x
with domain R. Suppose c1, c2, c3∈Rare such that
c1cos 2x+c2cos2x+c3sin2x= 0 (3.14)
for all x∈R. The functions cos2x,cos2x, and sin2xare linearly independent on Rif and only
if the only way to satisfy (3.14) for all x∈Ris to have c1=c2=c3= 0. However, it is true
that
cos 2x= cos2x−sin2x
onR, and hence (3.14) is equivalent to the equation
c1(cos2x−sin2x) +c2cos2x+c3sin2x= 0.
Now notice that this equation, and subsequently (3.14) , is satisfied for all x∈Rif we let c1= 1,
c2=−1, and c3= 1. So (3.14) has a nontrivial solution on R, and therefore the functions
cos 2x, cos2x, and sin2xare linearly dependent on R. ■
Proposition 3.37. LetVbe a vector space.
1.The set {0} ⊆Vis linearly dependent.
2.The empty set ∅is linearly independent.
Proof.
Proof of Part (1). The equation c0=0is satisfied by letting c= 1. Since this is a nontrivial
solution, it follows that {0}is linearly dependent.
Proof of Part (2). From Definition 3.32 an arbitrary set Sis linearly independent if and only if
the following statement (P) is true: “If v1, . . . ,vn∈S, then v1, . . . ,vnare linearly independent.”
However if S=∅, then the statement “ v1, . . . ,vn∈S” is necessarily false, and therefore (P) is
vacuously true. We conclude that ∅is linearly independent. ■
Proposition 3.38. LetVbe a vector space. If v1, . . . ,vn∈Vare linearly independent and
x1v1+···+xnvn=y1v1+···+ynvn
for scalars x1, . . . , x nandy1, . . . , y n, then xi=yifor all 1≤i≤n.85
Proof. Suppose that v1, . . . ,vn∈Vare linearly independent and
nX
i=1xivi=nX
i=1yivi
for scalars xiandyi. Then
nX
i=1(xi−yi)vi=0,
and since the vectors viare linearly independent, it follows that xi−yi= 0 for 1 ≤i≤n. That
is,xi=yifor 1≤i≤n. ■
Proposition 3.39. Suppose Vis a vector space, and S={v1, . . . ,vn}is a linearly independent
set in V. Given w∈V, the set S∪ {w}is linearly dependent if and only if w∈Span( S).
Proof. Suppose that S∪ {w}is linearly dependent. Then the equation
x1v1+···+xnvn+xn+1w= 0
has a nontrivial solution, which is to say at least one of the coefficients x1, . . . , x n+1is nonzero.
Ifxn+1= 0, then xk̸= 0 for some 1 ≤k≤n, in which case
x1v1+···+xnvn= 0
has a nontrivial solution and we conclude that v1, . . . ,vnare linearly dependent—a contradiction.
Hence xn+1̸= 0, and we may write
w=nX
k=1−xk
xn+1vk,
which shows that w∈Span( S).
Conversely, suppose that w∈Span( S), so that
w=a1v1+···+anvn
for some a1, . . . , a n∈F. If we choose xk=−akfor each 1 ≤k≤n, and let xn+1= 1, then
x1v1+···+xnvn+xn+1w=−a1v1− ··· − anvn+w
=−(a1v1+···+anvn) + (a1v1+···+anvn)
=0,
and hence
x1v1+···+xnvn+xn+1w=0.
has a nontrivial solution. Therefore S∪ {w}is a linearly dependent set. ■
Definition 3.40. Abasis for a vector space Vis a linearly independent set B ⊆Vsuch that
Span (B) =V. In the case of the trivial vector space {0}we take the basis to be ∅, the empty
set.86
A basis Bis frequently indexed ; that is, there exists an index set Iof positive integers
together with a function I→ B that pairs each element of Bwith a unique k∈I. Typically I
is either {1, . . . , n }for some n∈N, or else I=N. In this fashion the vectors in Bareordered
according to the integers to which they are paired, with a symbol such as vkbeing used to
denote the vector that is paired with the integer k∈I. IfBis an indexed set containing n
vectors that we wish to list explicitly, then the list is most properly presented as an n-tuple,
B= (v1, . . . ,vn),
rather than as a set B={v1, . . . ,vn}. We will adhere to this practice in all situations in which
the order of the vectors in Bis important.
Theorem 3.41. If{v1, . . . ,vn}is a basis for V, then for any v∈Vthere exist unique scalars
x1, . . . , x nfor which v=x1v1+···+xnvn.
Proof. Suppose that {v1, . . . ,vn}is a basis for V, and let v∈V. Since v1, . . . ,vnspan V,
there exist scalars x1, . . . , x nsuch that
v=x1v1+···+xnvn.
Now, suppose
v=y1v1+···+ynvn
for scalars y1, . . . , y n, so that
x1v1+···+xnvn=y1v1+···+ynvn.
Then since v1, . . . ,vnare linearly independent we must have yi=xifor all 1 ≤i≤n
by Proposition 3.38. Therefore the scalars x1, . . . , x nfor which v=x1v1+···+xnvnare
unique. ■
The following proposition pertaining to R2will be verified using only the most basic algebra.
A more general result applying to Rnfor all n≥2 must wait until later, when more sophisticated
machinery will have been built to allow for a far more elegant proof.
Proposition 3.42. Let[a, b]⊤,[c, d]⊤∈R2.
1. [a, b]⊤and[c, d]⊤are linearly dependent if and only if ad−bc= 0.
2.If[a, b]⊤and[c, d]⊤are linearly independent, then they form a basis for R2.
Proof.
Proof of Part (1). Suppose that [ a, b]⊤and [ c, d]⊤are linearly dependent. Then there exist
scalars rands, not both zero, such that
r
a
b
+s
c
d
=
0
0
.
This vector equation gives rise to the system
ar+cs= 0,(ϵ1)
br+ds= 0,(ϵ2)87
Ifr̸= 0, then d(ϵ1)−c(ϵ2) (i.e. dtimes equation ( ϵ1) minus ctimes equation ( ϵ2)) yields
adr−bcr= 0, or ( ad−bc)r= 0. Since r̸= 0, we conclude that ad−bc= 0.
Ifs̸= 0, then −b(ϵ1) +a(ϵ2) yields −bcs+ads= 0, or ( ad−bc)s= 0. Since s̸= 0, we
conclude that ad−bc= 0 once more.
Now, we have that either r̸= 0 or s̸= 0, both of which lead to the conclusion that ad−bc= 0
and so the forward implication of part (1) is proven.
Suppose next that ad−bc= 0. We must find scalars xandy, not both 0, such that
x
a
b
+y
c
d
=
0
0
. (3.15)
This vector equation gives rise to the system

ax+cy= 0,(ϵ3)
bx+dy= 0,(ϵ4)
Assume first that a̸= 0. Then from ( ϵ3) we have x=−cy/a, and from −b(ϵ3) +a(ϵ4) we obtain
−bcy+ady= 0 and then ( ad−bc)y= 0. Since ad−bc= 0, we may satisfy ( ad−bc)y= 0 by
letting y=a, and then x=−cy/a =−c. It’s easy to check that x=−candy=a̸= 0 will
satisfy (3.15), and thus [ a, b]⊤and [ c, d]⊤are linearly dependent.
Now assume that a= 0. Then ad−bc= 0 implies that bc= 0, and so either b= 0 or c= 0.
Butb= 0 leads us to [ a, b]⊤= [0,0]⊤, in which case [ a, b]⊤and [ c, d]⊤are linearly dependent.
Suppose that c= 0 and b̸= 0. Then equation ( ϵ3) in the system above vanishes, and only ( ϵ4)
remains to give x=−dy/b. If we let y=b, then x=−dy/b =−d. It’s easy to check that
x=−dandy=b̸= 0, together with our assumptions that a= 0 and c= 0, will satisfy (3.15) .
Since either a= 0 or a̸= 0 must be the case, and both lead to the conclusion that xand
ymay be chosen such that both aren’t zero and (3.15) is satisfied, it follows that [ a, b]⊤and
[c, d]⊤must be linearly dependent. The reverse implication of part (1) is proven.
Proof of Part (2). Suppose that [ a, b]⊤and [ c, d]⊤are linearly independent. To show that the
vectors form a basis for R2, we need only verify that
R2= Span
a
b
,
c
d
.
Let [x1, x2]⊤∈R2. Scalars s1ands2must be found so that

x1
x2
=s1
a
b
+s2
c
d
. (3.16)
This gives rise to the system
as1+cs2=x1,(ϵ5)
bs1+ds2=x2,(ϵ6)
in which s1ands2are the unknowns. From −b(ϵ5) +a(ϵ6) comes ( ad−bc)s2=ax2−bx1, and
since by part (1) the linear independence of [ a, b] and [ c, d]⊤implies that ad−bc̸= 0, we obtain
s2=ax2−bx1
ad−bc88
Putting this into ( ϵ5) and solving for s1yields
s1=1
a
x1−ax2−bx1
ad−bcc
if we assume that a̸= 0, which shows that there exist scalars s1ands2that satisfy (3.16).
Ifa= 0, then ad−bc̸= 0 becomes bc̸= 0 and thus b, c̸= 0. Since ( ϵ5) is now just cs2=x1
andc̸= 0, we obtain s2=x1/c. Putting this into ( ϵ6) gives
bs1+dx1
c=x2⇒s1=1
b
x2−dx1
c
,
since b̸= 0. Once again there exist scalars satisfying (3.16).
Therefore [ a, b]⊤and [ c, d]⊤spanR2, and we conclude that the set {[a, b]⊤,[c, d]⊤}forms a
basis for R2. This proves part (2). ■
The two parts of Proposition 3.42, when combined, provide an easy test to determine whether
two given vectors in R2are linearly independent.
Example 3.43. Show that [1 ,−3]⊤and [5 ,6]⊤form a basis for R2.
Solution. Here we have [ a, b]⊤= [1,−3]⊤and [ c, d]⊤= [5,6]⊤, and since
ad−bc= (1)(6) −(−3)(5) = 21 ̸= 0
we conclude by part (1) of Proposition 3.42 that the vectors are linearly independent. Then, by
part (2), it follows that the vectors do indeed form a basis for R2. ■
Problems
1. Let
u1=
2
0
−1
,u2=
3
1
0
,u3=
−2
3
2
.
(a) Show that {u1,u2,u3}is a linearly independent set.
(b) The ordered set B= (u1,u2,u3) is a basis for R3. Given
v=
−6
−10
−5
,
find [v]B, the coordinates of vwith respect to the basis B.
2. Write down a basis for the yz-plane in R3.
3. The plane Pgiven by x+ 2y−3z= 0 is a subspace of R3. Find a basis for it.89
3.6 – Dimension
The first proposition we consider is useful mainly for proving more momentous results in
this section.
Proposition 3.44. LetVbe a vector space such that V=Span{v1, . . . ,vm}. Ifu1, . . . ,un∈V
for some n > m , then the vectors u1, . . . ,unare linearly dependent.
Proof. Letu1, . . . ,un∈Vfor some n > m . Since the vectors v1, . . . ,vmspan V, there exist
scalars aijsuch that
uj=mX
i=1aijvi=a1jv1+a2jv2+···+amjvm (3.17)
for each 1 ≤j≤n.
Now, by Theorem 2.38 the homogeneous system of equations


a11x1+a12x2+···+a1nxn= 0
a21x1+a22x2+···+a2nxn= 0
............
am1x1+am2x2+···+amnxn= 0
has a nontrivial solution since n(the number of variables) is greater than m(the number of
equations). That is, there exists a solution ( x1, . . . , x n) = ( c1, . . . , c n) such that not all the
scalars cjare equal to 0.
We now have
nX
j=1aijcj=ai1c1+···+aincn= 0
for each 1 ≤i≤m, which implies that
mX
i=1nX
j=1aijcjvi=nX
j=1a1jcjv1+···+nX
j=1amjcjvm= 0v1+···+ 0vm=0. (3.18)
But, recalling (3.17), we may also write
mX
i=1nX
j=1aijcjvi=nX
j=1mX
i=1aijcjvi=nX
j=1 
cjmX
i=1aijvi!
=nX
j=1cjuj. (3.19)
Combining (3.18) and (3.19), we find that
nX
j=1cjuj=c1u1+···+cnun=0
forc1, . . . , c nnot all equal to 0.
Therefore u1, . . . ,unare linearly dependent. ■90
Theorem 3.57 at the end of this section states that every vector space Vhas a basis, but
it leaves open two mutually-exclusive possibilities: either Vhas a finite basis (i.e. a basis
containing a finite number of vectors), or it does not. If Vhas a finite basis, then it is called a
finite-dimensional vector space; otherwise it is an infinite-dimensional vector space. Note
that the trivial vector space {0}, which has basis ∅by definition, is finite-dimensional.
Remark. If a vector space Vis finite-dimensional, so that it has a finite basis B={v1, . . . ,vm},
then it is an immediate consequence of Proposition 3.44 and the fact that V=Span{v1, . . . ,vm}
thatVcannot possess any basis that is infinite. Indeed no set of more than mvectors can even
be linearly independent!
While it is usually the case that many different sets of vectors can serve as a basis for a
finite-dimensional vector space V(the trivial vector space being the sole exception), it turns
out that every basis for a finite-dimensional vector space must contain the same number of
vectors. In what follows we let |S|denote the number of elements of a set S, also known as the
cardinality ofS.
Theorem 3.45. IfB1andB2are two bases for a finite-dimensional vector space V, then
|B1|=|B2|.
Proof. The remark made above makes clear that B1andB2must both be finite sets, so
B1={v1, . . . ,vm}andB2={u1, . . . ,un}for integers mandn, and we have |B1|=mand
|B2|=n.
Since Span (B1) =V, ifn > m thenu1, . . . ,unmust be linearly dependent by Proposition
3.44, which contradicts the hypothesis that B2is a basis for V. Hence n≤m.
Since Span (B2) =V, ifn < m thenv1, . . . ,vmmust be linearly dependent by Proposition
3.44, which contradicts the hypothesis that B1is a basis for V. Hence n≥m.
Therefore m=n, which is to say |B1|=|B2|. ■
Throughout these notes, if a vector space is not said to be finite-dimensional, then it can
be assumed to be either finite- or infinite-dimensional. It is the fact that the cardinality of all
the bases of a given finite-dimensional vector space is a constant that allows us to make the
following definition.
Definition 3.46. Thedimension of a finite-dimensional vector space V,dim(V), is the
number of elements in any basis for V. That is, if Bis a basis for V, then dim(V) =|B|.
Remark. Since the basis for the trivial vector space {0}is∅by Definition 3.40, it follows
that the dimension of {0}is|∅|= 0. If a vector space Vis infinite-dimensional then we might
be tempted to write dim(V) =∞, but there is little use in doing this since there are in fact
different “sizes” of infinity. We will not make a study of such matters in these notes, for it is
more properly the domain of a book on the subject of functional analysis.
Example 3.47. A basis for the vector space R2isE2={e1,e2}, where
e1=
1
0
and e2=
0
1
.91
Since there are two elements in the set we conclude that dim( R2) = 2.
More generally, as we have seen, a basis for Rnis provided by the set
En={e1, . . . ,en}
of standard unit vectors. Since |En|=n, we see that dim( Rn) =n.
Example 3.48. The vector space Rm×nofm×nmatrices with real-valued entries has as a
basis the set
Emn={Eij: 1≤i≤m,1≤j≤n},
where Eijis the m×nmatrix with ij-entry 1 and all other entries 0. There are mnelements in
Emn, and thus dim( Rm×n) =mn.
Example 3.49. Example 3.13 showed that Skw n(R) is a subspace of Rn×n, and thus is a vector
space over Rin its own right. The goal now is to find the dimension of Skw n(R). The first thing
to notice is that the diagonal entries of any skew-symmetric matrix A= [aij] must all be zero:
A⊤=−A⇒aii=−aii⇒aii= 0.
So, in the case when n= 2, we must have
A=
0a
−a0
for some a∈R, which is to say
Skw 2(R) =
0a
−a0
:a∈R
=
a
0 1
−1 0
:a∈R
= Span
0 1
−1 0
.
Thus we see that the set
B2=
0 1
−1 0
={E2,12−E2,21}
spans Skw 2(R), where the definitions of the matrices E2,12andE2,21are given by Equation
(2.14) . Since B2is a linearly independent set it follows that B2is a basis for Skw 2(R), and
therefore dim(Skw 2(R)) =|B2|= 1.
When n= 3 we find that
Skw 3(R) =


0a b
−a0c
−b−c0
:a, b, c∈R


=

a
0 1 0
−1 0 0
0 0 0
+b
0 0 1
0 0 0
−1 0 0
+c
0 0 0
0 0 1
0−1 0
:a, b, c∈R


= Span

0 1 0
−1 0 0
0 0 0
,
0 0 1
0 0 0
−1 0 0
,
0 0 0
0 0 1
0−1 0


= Span 
{E3,12−E3,21,E3,13−E3,31,E3,23−E3,32}
.92
The set
B3={E3,12−E3,21,E3,13−E3,31,E3,23−E3,32}
is linearly independent, and so dim(Skw 3(R)) = 3.
More generally, for arbitrary n∈N, we find A= [aij]nis such that aii= 0 for 1 ≤i≤n, and
aij=−ajiwhenever i̸=j. Thus the entries of Aare fully determined by just the entries above
the main diagonal, since each entry below the diagonal must be the negative of the corresponding
entry above the diagonal. The entries above the diagonal are aijfor 1≤i < j≤n, and it is
straightforward to check that
Bn={En,ij−En,ji: 1≤i < j≤n}
is a linearly independent set such that
Skw n(R) = Span 
{En,ij−En,ji: 1≤i < j≤n}
,
and so
dim(Skw n(R)) =|Bn|= (n−1) + ( n−2) +···+ 1 =n−1X
k=1k=n(n−1)
2.
This is just the number of entries in an n×nmatrix that are above the main diagonal. ■
Definition 3.50. LetVbe a vector space and A⊆Va nonempty set. We call B⊆Aa
maximal subset of linearly independent vectors if the following are true:
1.Bis a linearly independent set.
2.For all S⊆Awith|S|>|B|,Sis a linearly dependent set.
Thus if B⊆Ais a maximal subset of linearly independent vectors and |B|=r, then there
exist rlinearly independent vectors in A, but there cannot be found r+ 1 linearly independent
vectors in A. It may be that only one combination of rvectors in Acan be used to construct
the set B, or there may be many different possible combinations.
Theorem 3.51. LetVbe a vector space, and let A={v1, . . . ,vn} ⊆Vbe such that V=
Span( A). Then
1.The dimension of Vis at most n:dim(V)≤n.
2.IfB⊆Ais a maximal subset of linearly independent vectors, then Bis a basis for V.
Proof.
Proof of Part (1). By Proposition 3.44 any set containing more than nvectors in Vmust be
linearly dependent, so if Bis a basis for V, then we must have dim( V) =|B| ≤ n.
Proof of Part (2). Suppose that B⊆Ais a maximal subset of linearly independent vectors.
Reindexing the elements of Aif necessary, we may assume that B={v1, . . . ,vr}. Ifr=n,
then B=A, and so Bspans Vand we straightaway conclude that Bis a basis for Vand we’re
done. Suppose, then, that 1 ≤r < n . For each 1 ≤i≤n−rlet
Bi=B∪ {vr+i}={v1, . . . ,vr,vr+i}.93
The set Biis linearly dependent since |Bi|>|B|, and so there exist scalars ai1, . . . , a ir, bi, not
all zero, such that
ai1v1+···+airvr+bivr+i=0. (3.20)
We must have bi̸= 0, since otherwise (3.20) becomes
ai1v1+···+airvr=0,
whereupon the linear independence of v1, . . . ,vrwould imply that ai1=···=air= 0 and
so contradict the established fact that not all the scalars ai1, . . . , a ir, biare zero! From the
knowledge that bi̸= 0 we may write (3.20) as
vr+i=−ai1
biv1− ··· −air
bivr=rX
j=1aij
−bivj=rX
j=1dijvj, (3.21)
where we define dij=−aij/bifor each 1 ≤i≤n−rand 1 ≤j≤r. Hence the vectors
vr+1, . . . ,vnare each expressible as a linear combination of v1, . . . ,vr.
Letu∈Vbe arbitrary. Since v1, . . . ,vnspan Vthere exist scalars c1, . . . , c nsuch that
u=c1v1+···+cnvn,
and then from (3.21) we have
u=c1v1+···+crvr+n−rX
i=1cr+ivr+i=rX
j=1cjvj+n−rX
i=1
cr+irX
j=1dijvj
=rX
j=1cjvj+n−rX
i=1rX
j=1cr+idijvj=rX
j=1cjvj+rX
j=1n−rX
i=1cr+idijvj
=rX
j=1
cjvj+n−rX
i=1cr+idijvj
=rX
j=1
cj+n−rX
i=1cr+idij
vj.
Setting
ˆcj=cj+n−rX
i=1cr+idij
for each 1 ≤j≤r, we finally obtain
u= ˆc1v1+···+ ˆcrvr
and so conclude that u∈Span{v1, . . . ,vr}= Span( B).
Therefore V= Span( B), and so Bis a basis for V. ■
Closely related to the concept of a maximal subset of linearly independent vectors is the
following.
Definition 3.52. LetVbe a vector space. A set B⊆Vis amaximal set of linearly
independent vectors inVif the following are true:
1.Bis a linearly independent set.
2.For all w∈Vsuch that w/∈B, the set B∪ {w}is linearly dependent.94
Theorem 3.53. IfVis a vector space and Sa maximal set of linearly independent vectors in
V, then Sis a basis for V.
Proof. Suppose that Vis a vector space and S={v1, . . . ,vn}is a maximal set of linearly
independent vectors. Let u∈V. Then the set {v1, . . . ,vn,u}is linearly dependent, and so
there exist scalars c0, . . . , c nnot all zero such that
c0u+c1v1+···+cnvn=0. (3.22)
Now, if c0were 0 we would obtain c1v1+···+cnvn=0, whereupon the linear independence of
Swould imply that c1=···=cn= 0 and so contradict the established fact that not all the
scalars c0, . . . , c nare zero. Hence we must have c0̸= 0, and (3.22) gives
u=−c1
c0v1− ··· −cn
c0vn.
That is, every vector in Vis expressible as a linear combination of vectors in S, so that
Span( S) =Vand we conclude that Sis a basis for V. ■
Theorem 3.54. LetVbe a finite-dimensional vector space, and let S⊆Vwith|S|=dim(V).
1.IfSis a linearly independent set, then Sis a basis for V.
2.IfSpan( S) =V, then Sis a basis for V.
Proof.
Proof of Part (1). Setting n=dim(V), suppose S={v1, . . . ,vn} ⊆Vis a linearly independent
set. Any basis for Vwill span Vand have nvectors, so by Proposition 3.44 the set S∪ {w}
must be linearly dependent for every w∈Vsuch that w/∈S. Hence Sis a maximal set of
linearly independent vectors, and therefore Sis a basis for Vby Theorem 3.53.
Proof of Part (2). Again set n=dim(V), and suppose S={v1, . . . ,vn}is such that Span (S) =
V. Assume Sis not a basis for V. Then Smust not be a linearly independent set. Let B⊆S
be a maximal subset of linearly independent vectors. Then Bcannot contain all of the vectors
inS, so|B|<|S|=n. By Theorem 3.51(2) it follows that Bis a basis for V, and so
dim(V) =|B|< n.
Since this is a contradiction, we conclude that Smust be a linearly independent set and therefore
Sis a basis for V. ■
Theorem 3.55. LetVbe a vector space with dim(V) =n >0. Ifv1, . . . ,vr∈Vare linearly
independent vectors for some r < n , then vectors vr+1, . . . ,vn∈Vmay be found such that
{v1, . . . ,vn}is a basis for V.
Proof. Suppose that v1, . . . ,vr∈Vare linearly independent vectors, where r < n . The set
Sr={v1, . . . ,vr}cannot be a basis for Vsince by Definition 3.46 any basis for Vmust contain
nvectors. Hence Srcannot be a maximal set of linearly independent vectors by Theorem 3.53,
and so there must exist some vector vr+1∈Vsuch that the set
Sr+1=Sr∪ {vr+1}={v1, . . . ,vr+1}95
is linearly independent. Now, if r+ 1 = n, then Theorem 3.54 implies that Sr+1is a basis for
Vand the proof is done. If r+ 1< n, then we repeat the arguments made above to obtain
successive sets of linearly independent vectors
Sr+i=Sr+i−1∪ {vr+i}={v1, . . . ,vr+i}
until such time that r+i=n, at which point the linearly independent set
Sn=Sn−1∪ {vn}={v1, . . . ,vr,vr+1, . . . ,vn}
will be a basis for V. ■
Theorem 3.56. LetVbe a finite-dimensional vector space, and let Wbe a subspace of V.
Then
1.Wis finite-dimensional.
2. dim( W)≤dim(V).
3.Ifdim(W) = dim( V), then W=V.
Proof. IfW={0}, then all three conclusions of the theorem follow trivially. Thus, we will
henceforth assume W̸={0}, so that dim( V) =n≥1.
Proof of Part (1). Suppose Wis infinite-dimensional. Let w1be a nonzero vector in W. The set
{w1}cannot be a maximal set of linearly independent vectors in Wsince otherwise Theorem
3.53 would imply that {w1}is a basis for Wand hence dim(W) = 1, a contradiction. Thus for
some k≥2 additional vectors w2, . . . ,wk∈Wmay be found such that Sk={w1, . . . ,wk}is a
linearly independent set of vectors in W. However, for no k∈NcanSkbe a maximal set of
linearly independent vectors in W, since otherwise Theorem 3.53 would imply that dim(W) =k.
It follows that there exists, in particular, a linearly independent set
{w1, . . . ,wn+1} ⊆W⊆V,
which is impossible since by Proposition 3.44 there can be no linearly independent set in V
containing more than nvectors. Therefore Wmust be finite-dimensional.
Proof of Part (2). By Part (1) it is known that Wis finite-dimensional, so there exists a basis
B={w1, . . . ,wm}forW, where m∈N. Since Bis a linearly independent set in V, and by
Proposition 3.44 there can be no linearly independent set in Vcontaining more than dim(V) =n
vectors, it follows that dim( W) =m≤n= dim( V).
Proof of Part (3). Suppose that dim(W) =dim(V) =n, where nis some integer since Vis
given to be finite-dimensional. Let B={w1, . . . ,wn}be a basis for W, so that W=Span (B).
Since dim(V) =nandw1, . . . ,wn∈Vare linearly independent, Bis a basis for Vby Theorem
3.54. Thus V= Span( B), and we have V=W. ■
Given a matrix A∈Fm×n, recall from §3.1 that the set of all x∈Fnfor which Ax=0
is true is a subspace of Fncalled the null space ofA, denoted by Nul(A). Later on we will
frequently be concerned with determining the dimension of Nul(A), which we will often refer to
as the nullity ofA. That is,
nullity( A) = dim(Nul( A)).96
Theorem 3.57. Every vector space has a basis.
Proof. LetVbe a vector space over a field F. By definition ∅is the basis for {0}, so assume
thatVis nontrivial. Let Sbe the collection of all linearly independent subsets of V:
S={A⊆V:Ais a linearly independent set }.
(Note that Scontains at least one singleton {v}withv̸=0since Vis nontrivial.) Then Sis
a nonempty partially ordered set under the inclusion relation ⊆. LetC ⊆ S be a chain in S.
We have C={Ci:i∈I}for some index set I, and for every A, B∈ Ceither A⊆BorB⊆A.
Claim:
U=[
i∈ICi
is an upper bound for the chain Csuch that U∈ S. It is clear that Ci⊆Ufor all i∈I. Suppose
that U /∈ S, which is to say Uis not a linearly independent set in V. This implies that, for
some n∈N, there exist u1, . . . ,un∈Usuch that {u1, . . . ,un}is linearly dependent, which in
turn implies that for each 1 ≤k≤nthere is some ik∈Iwithuk∈Cik. For convenience we
may assume the vectors u1, . . . ,unare indexed such that
Ci1⊆Ci2⊆ ··· ⊆ Cin,
recalling that each Cikis an element of the totally ordered set C. Thus u1, . . . ,un∈Cin, which
shows that Cinis not a linearly independent set and hence Cin/∈ S—a contradiction. We
conclude that Umust be a linearly independent set, and hence Uis an upper bound for Cwith
U∈ S. Since every chain in Shas an upper bound in S, Zorn’s Lemma implies that Shas a
maximal element M.
Letv∈Vbe arbitrary. Suppose, for all n∈N(or 1 ≤n≤ |M|ifMis finite) and
v1, . . . ,vn∈M, the only r1, . . . , r n, r∈Fthat satisfy the equation
nX
k=1rkvk+rv=0 (3.23)
arer1=···=rn=r= 0. Then M∪ {v}is a linearly independent set, which implies
that M∪ {v} ∈ S . Since M⊆M∪ {v}andMis a maximal element of S, we must have
M=M∪ {v}and therefore v∈M. In particular we see that v∈Span( M).
Suppose, in contrast, that for some n∈Nandv1, . . . ,vn∈Mthe equation (3.23) admits a
nontrivial solution. Since v1, . . . ,vnare linearly independent this means we must have r̸= 0
(otherwise we are forced to embrace the trivial solution). Since Fis a field there exists some
r−1∈Fsuch that r−1r= 1. Hence
rv=−nX
k=1rkvk⇒v=r−1nX
k=1rkvk=nX
k=1(r−1rk)vk,
and we see that v∈Span (M) once more. Thus V=Span (M), and since Mis a linearly
independent set we conclude that Mis a basis for V. ■97
Problems
1.Find the dimension of P3(R), the vector space over Rof polynomials in xof degree at most
3 with real coefficients.
2. Recall that Symn(R) denotes the vector space of n×nsymmetric matrices over R.
(a) Find a basis for Sym2(R). What is the dimension of Sym2(R)?
(b) Find a basis for Sym3(R). What is the dimension of Sym3(R)?
(c) Find a basis for Sym4(R). What is the dimension of Sym4(R)?
(d) Find a basis for Symn(R). What is the dimension of Symn(R)?98
3.7 – Product Spaces
Definition 3.58. LetUandVbe vector spaces over F. The product ofUandVis the set
U×V={(u,v) :u∈U,v∈V}.
More generally, let V1, . . . , V nbe vector spaces over F. The product ofV1, . . . , V nis the set
nY
k=1Vk={(v1, . . . ,vn) :vk∈Vkfor each 1≤k≤n}
We see that the product of two or more vector spaces amounts to nothing more than the
Cartesian product of the sets of objects contained within the vector spaces. Let
u,v∈nY
k=1Vk
be the n-tuples
u= (u1, . . . ,un) and v= (v1, . . . ,vn),
and let c∈F. If we define the sum ofuandvby
u+v= (u1+v1, . . . ,un+vn),
and the scalar product ofcwith vby
cv= (cv1, . . . , c vn),
then it is a routine matter to verify thatQn
k=1Vkbecomes a vector space in its own right, called
theproduct space ofV1, . . . , V n.99
3.8 – The Rank of a Matrix
LetA∈Fm×n, so that
A=
a11a12··· a1n
a21a22··· a2n............
am1am2··· amn
. (3.24)
Denote the column vectors of Aby
cj=
a1j...
amj

for 1≤j≤n, and denote the row vectors of Aby
ri=ai1. . . a in
for 1≤i≤m. The column space ofAis defined to be the set
Col(A) = Span {c1, . . . ,cn},
and the row space ofAis defined to be the set
Row(A) = Span {r⊤
1, . . . ,r⊤
m}.
Proposition 3.30 implies that Col(A) is a subspace of FmandRow(A) is a subspace of Fn. The
column rank ofAis the dimension of the column space of A:
col-rank( A) = dim[Col( A)].
Therow rank ofAis the dimension of the row space:
row-rank( A) = dim[Row( A)].
Proposition 3.59. LetA∈Fm×n, with c1, . . . ,cn∈Fmthe column vectors of Aand
r1, . . . ,rm∈Fnthe row vectors of A.
1.IfS⊆ {c1, . . . ,cn}is a maximal subset of linearly independent vectors, then
col-rank( A) =|S|.
2.IfS⊆ {r1, . . . ,rm}is a maximal subset of linearly independent vectors, then
row-rank( A) =|S|.
Proof.
Proof of Part (1). Suppose S⊆ {c1, . . . ,cn}is a maximal subset of linearly independent
vectors. Let col-rank (A) =k. Since Col(A) is a vector space, Col(A) =Span{c1, . . . ,cn}, and
S⊆ {c1, . . . ,cn}is a maximal subset of linearly independent vectors, it follows by Theorem
3.51 that Sis a basis for Col(A). Now, because the dimension of Col(A) isk, we must have
|S|=k= col-rank( A) as was to be shown.
Proof of Part (2). Done similarly, and so left as a problem. ■100
In the proof of Proposition 3.59(1), since |S|=col-rank (A) =k, we can conclude that S
consists of kelements of the set {c1, . . . ,cn}, and so we may write S={cn1, . . . ,cnk}for some
n1, . . . , n k∈ {1, . . . , n }. That is, {cn1, . . . ,cnk}is a maximal subset of linearly independent
vectors, which is to say the maximum number of linearly independent column vectors of Ais
col-rank( A).
What we ultimately want to show is that the row and column ranks of a matrix are always
equal. It is not an obvious fact, and so a few more results will need to be developed before we
are in a position to prove it.
Lemma 3.60. LetVandWbe vector spaces, with
SV={v1, . . . ,vn} ⊆Vand SW={w1, . . . ,wn} ⊆W.
If
nX
k=1xkvk=0⇔nX
k=1xkwk=0
for all x1, . . . , x n∈F, then dim(Span SV) = dim(Span SW).
Proof. Suppose that, for all x1, . . . , x n∈F,Pn
i=1xivi=0if and only ifPn
i=1xiwi=0. We
shall refer to this hypothesis as (H). Let
RV={vi1, . . . ,vir} ⊆SV
be a maximal subset of linearly independent vectors for SV, which means any subset of SVwith
more than relements must be linearly dependent. By Theorem 3.51 RVis a basis for Span (SV),
and so dim(Span SV) =|RV|=r.
LetRW={wi1, . . . ,wir} ⊆SW. Suppose that
xi1wi1+···+xirwir=0.
Then by (H) we have
xi1vi1+···+xirvir=0
as well, and since vi1, . . . ,virare linearly independent we conclude that xi1=···=xir= 0.
That is,Pr
k=1xikwik=0necessarily implies that xik= 0 for all 1 ≤k≤r, and so RWis itself
a linearly independent set of vectors.
Next, assume B={wj1, . . . ,wjt} ⊆SWis such that |B|=t > r . Set
xj1vj1+···+xjtvjt=0. (3.25)
Since any subset of SVcontaining more than relements must be linearly dependent, it follows
thatvj1, . . . ,vjtmust be linearly dependent and there exist scalars xj1, . . . , x jt,not all equal to
zero, which satisfy (3.25). By (H) these same scalars must satisfy
xj1wj1+···+xjtwjt=0,
which shows that wj1, . . . ,wjtmust also be linearly dependent. Hence there does not exist any
linearly independent set B⊆SWfor which |B|> r.
We conclude that RW⊆SWis a maximal subset of linearly independent vectors. By
Theorem 3.51 RWis a basis for Span( SW), and so dim(Span SW) =|RW|=r.101
Therefore dim(Span SV) =r= dim(Span SW). ■
Lemma 3.61. Suppose A∈Fm×mis invertible, and let B∈Fm×n. Then col-rank (AB) =
col-rank( B).
Proof. Letb1, . . . ,bn∈Fmbe the column vectors of B, so that
B=b1···bn
,
and thus by Proposition 2.6
AB=Ab 1···Abn
,
where Ab 1, . . . ,Abn∈Fm. Let x1, . . . , x n∈F. IfPn
j=1xjbj=0, then by Theorem 2.7 we have
nX
j=1xj(Abj) =nX
j=1A(xjbj) =AnX
j=1xjbj=A0=0;
and ifPn
j=1xj(Abj) =0, then since Ais invertible we have
nX
j=1xjbj=nX
j=1xj(A−1Abj) =A−1nX
j=1xj(Abj) =A−10=0.
Therefore
col-rank( AB) = dim 
Span{Ab 1, . . . ,Abn}
= dim 
Span{b1, . . . ,bn}
= col-rank( B)
by Lemma 3.60. ■
Proposition 3.62. LetA∈Fm×n.
1.IfA′row-equivalent to A, then
Row(A) = Row( A′)and col-rank( A) = col-rank( A′).
2.IfA′column-equivalent to A, then
Col(A) = Col( A′)and row-rank( A) = row-rank( A′)
Thus both col-rank (A)androw-rank (A)are invariant under arbitrary finite sequences of ele-
mentary row and column operations applied to A.
Proof.
Proof of Part (1). Suppose that A′is row-equivalent to A. This means there exists a finite
sequence of elementary matrices M1, . . . ,Mk∈Fm×msuch that
A′=Mk···M1A.
By Proposition 2.27 each matrix Mjis invertible, and hence M=Mk···M1is invertible by
Theorem 2.26. Therefore
col-rank( A) = col-rank( MA) = col-rank( A′)
by Lemma 3.61.
To show that Row(A) =Row(A′), it is sufficient to show that Row(A) is invariant under
each one of the three elementary row operations. By Proposition 2.16(1) an R1 operation102
Mi,j(c)Areplaces the row vector ajofAbyaj+cai, and thus the row space of the resultant
(row-equivalent) matrix is equal to Row(A) by Proposition 3.31. By Proposition 2.16(2) an R2
operation Mi,jAmerely interchanges two row vectors of A, which clearly does not alter the row
space. Finally by Proposition 2.16(3) an R3 operation Mi(c)Amultiplies the row vector aiofA
by the nonzero scalar c, and the straightforward formal verification that the row space of the
resultant matrix equals Row( A) is left as a problem.
Proof of Part (2). Suppose A′is column-equivalent to A, so there are elementary matrices
M1, . . . ,Mk∈Fn×nsuch that
A′=AM⊤
1···M⊤
k,
and hence (taking the transpose of both sides and applying Proposition 2.13) we have
(A′)⊤=Mk···M1A⊤.
Again M=Mk···M1is invertible, so Lemma 3.61 implies that
col-rank( A⊤) = col-rank( MA⊤) = col-rank(( A′)⊤).
Since the column spaces of A⊤and (A′)⊤are the row spaces of AandA′, respectively, we
finally obtain row-rank( A) = row-rank( A′).
The proof that Col(A) =Col(A′) is nearly identical to the proof that Row(A) =Row(A′)
in part (1), only Proposition 2.17 is employed instead of Proposition 2.16. ■
In brief, elementary row operations do not change the row space of a matrix, and elementary
column operations do not change the column space. On the other hand elementary row (resp.
column) operations may change the column (resp. row) space of a matrix, but the dimension of
the column (resp. row) space will remain the same. That is, any elementary row operation may
change the span of the column vectors, and any elementary column operation may change the
span of the row vectors.
Example 3.63. Find a basis for the column space of the matrix
A=
2 0 3 4 1
0 1 1 −1 3
3 1 0 2 −6
1 0−4 2 1

Solution. One way to proceed is to use elementary column operations to put the matrix into
row-echelon form.

2 0 3 4 1
0 1 1 −1 3
3 1 0 2 −6
1 0−4 2 1
1
2c4+c1→c1− − − − − − − →
2c4+c3→c3
4 0 11 4 1
−1
21−1−1 3
2 1 4 2 −6
0 0 0 2 1
−1
2c3+c1→c1− − − − − − − − →
c2↔c3103

−3
211 0 4 1
0−1 1−1 3
0 4 1 2 −6
0 0 0 2 1
−4c3+c2→c2− − − − − − − − →
−2c1
3 11 0 4 1
0−5 1−1 3
0 0 1 2 −6
0 0 0 2 1
−2c5+c4→c4− − − − − − − − →

3 11 0 2 1
0−5 1−7 3
0 0 1 14 −6
0 0 0 0 1
=A′
The first, second, third, and fifth column vectors of the row-echelon matrix A′,
c1=
3
0
0
0
,c2=
11
−5
0
0
,c3=
0
1
1
0
,c5=
1
3
−6
1
,
contain pivots, and so are linearly independent by Theorem 3.33. Since dim(R4) = 4 and
c1,c2,c3,c5∈R4are linearly independent, by Theorem 3.54(1) the set
S={c1,c2,c3,c5}
is a basis for R4, and so Span (S) =R4. By Proposition 3.44 any subset of R4containing more
vectors than S(i.e. more than four vectors) must be linearly dependent, and therefore Smust
be a maximal set of linearly independent vectors in Col(A′) since any vector in Col(A′) is
necessarily a vector in R4. By Theorem 3.53 we conclude that Sis a basis for Col(A′). Now,
because A′is column-equivalent to Awe have Col(A′) =Col(A) by Proposition 3.62. Therefore
Sis a basis for Col( A) and we are done. ■
The next theorem is momentous. It tells us that the column rank of a matrix Aalways equals
the row rank, so that we may simply refer to the rank ofA,rank(A), without discriminating
between the column and row spaces. That is,
rank(A) = dim(Col( A)) = dim(Row( A)).
Also the theorem provides a definitive strategy for determining rank( A).
Theorem 3.64. IfA∈Fm×nis such that row-rank (A) =r, then Ais equivalent via elementary
row and column operations to the m×nmatrixIrO
OO
. (3.26)
Hence col-rank( A) = row-rank( A).
Proof. Suppose A∈Fm×nwith row-rank (A) =r. By Proposition 2.20, Ais row-equivalent to
a matrix A′in row-echelon form. Since the nonzero row vectors of A′are linearly independent
androw-rank (A′) =rby Proposition 3.62, it follows that the top rrows of A′must be nonzero
row vectors while the bottom m−rrows must consist solely of zero entries.
Now, the pivots p1, . . . , p rin the top rrows of A′are nonzero entries having only zero
entries to the left of them. Each nonzero entry xto the right of p1we may “eliminate” by104
performing a C1 operation: namely, if p1is in column iandxis in column j > i , then add
−x/p 1times column ito column j. Since all entries below p1are zero, this affects no other
entries in the matrix beyond replacing the 1 j-entry xwith 0. In the end we obtain a matrix in
which p1is the only nonzero entry in its row and column, and we repeat the process for p2,p3,
and finally pr. The resultant matrix will have only p1, . . . , p ras nonzero entries, still in their
original row-echelon formation. Now we perform C2 operations to make pitheii-entry for each
1≤i≤r. Finally we perform C3 operations: we multiply column iby 1/piso that the ii-entry
is 1 for each 1 ≤i≤r, thereby securing the desired matrix (3.26).
The matrix (3.26) clearly has row rank and column rank both equal to r, and since the matrix
was obtained from Aby applying a finite sequence of elementary row and column operations,
Proposition 3.62 implies that the row rank and column rank of Aare likewise both equal to r.
This finishes the proof. ■
Example 3.65. Apply a sequence of elementary row and column operations to
A=
1 1 2 1
1 0 1 2
2 1 3 4

to obtain an equivalent matrix of the form (3.26) . Show that the row vectors of Aare linearly
independent, and that row-rank( A) = col-rank( A). State the rank of A.
Solution. First we will get a matrix in row-echelon form using strictly elementary row operations:
1 1 2 1
1 0 1 2
2 1 3 4
−r1+r2→r2
−2r1+r3→r3− − − − − − − →
1 1 2 1
0−1−1 1
0−1−1 2
−r2+r3→r3− − − − − − →
1 1 2 1
0−1−1 1
0 0 0 1

Now elementary column operations will be used to first put zeros to the right of the ii-entries,
and then to obtain a diagonal of 1’s:
1 1 2 1
0−1−1 1
0 0 0 1
−c1+c2→c2
−2c1+c3→c3− − − − − − − →
1 0 0 1
0−1−1 1
0 0 0 1
−c1+c4→c4− − − − − − →
1 0 0 0
0−1−1 1
0 0 0 1
c3↔c4− − − →

1 0 0 0
0−1 1−1
0 0 1 0
c2+c3→c3
−c2+c4→c4− − − − − − →
1 0 0 0
0−1 0 0
0 0 1 0
−c2− − →
1 0 0 0
0 1 0 0
0 0 1 0
.
The row vectors of the final matrix are [1 ,0,0,0], [0,1,0,0], and [0 ,0,1,0], which are linearly
independent, and so the row rank is 3. By Proposition 3.62 it follows that row-rank (A) = 3
as well, and therefore the row vectors of Amust be linearly independent by Theorem 3.54(2).
The nonzero column vectors of the final matrix are [1 ,0,0]⊤, [0,1,0]⊤, and [0 ,0,1]⊤, which are
linearly independent, and so col-rank( A) = 3 by Proposition 3.62. Thus we have
row-rank( A) = col-rank( A) = 3 ,
and therefore rank( A) = 3. ■
In Example 3.65 it should be noted that rank(A) could have been determined rather easily
early on, right after performing the R1 row operation −r2+r3→r3. The row vectors at that105
stage were [1 ,1,2,1], [0,−1,−1,1], and [0 ,0,0,1], which can be seen to be linearly independent
on account of the placement of the zeros. Thus rank( A) = row-rank( A) = 3.
With our definition of rank in hand, the findings of Proposition 3.62 and Theorem 3.64
combine to yield the following result.
Theorem 3.66. IfAis row-equivalent or column-equivalent to A′, then rank(A) = rank( A′).
The next example makes use of a variety of results developed throughout this chapter. What
once may have required much tedious calculation now can be accomplished quickly and elegantly.
Example 3.67. Let
v1=
−1
1
1
and v2=
1
2
1
.
Show that B={v1,v2}is a basis for the vector space W⊆R3given by
W=


x
y
z
x−2y+ 3z= 0

.
Solution. Define the matrix
B=v1v2
=
−1 1
1 2
1 1
,
and consider the first two row vectors [ a, b] = [−1,1] and [ c, d] = [1 ,2]. Since
ad−bc= (−1)(2)−(1)(1) = −3̸= 0,
these row vectors of Bare linearly independent by Proposition 3.42, and so row-rank (B)≥2.
On the other hand Bhas only two columns, so col-rank( B)≤2. Hence, by Theorem 3.64,
2≤row-rank( B) = rank( B) = col-rank( B)≤2,
which implies that rank(B) = 2. Since v1andv2are the column vectors of B, it follows that v1
andv2are linearly independent.
It is easily verified that v1,v2∈W, so that S=Span (B) is a subspace of Wand thus
dim(S)≤dim(W) by Theorem 3.56(2). Since Bis a basis for S, we have dim(S) = 2; and since
1
0
0
/∈W,
so that Wis a subspace of R3that does not equal R3, it follows by Theorem 3.56 that
dim(W)<dim(R3) = 3 .
That is,
2 = dim( S)≤dim(W)≤2,
which shows that dim( W) = 2, and therefore Bis a basis for Wby Theorem 3.54(1). ■106
Problems
1. For any matrix Ashow that Col( A) = Row( A⊤) and Row( A) = Col( A⊤).
2. Show that rank( A) = rank( A⊤) for any matrix A.
3. Let A∈Fm×nandB∈Fn×p.
(a) Show that rank( AB)≤rank(A).
(b) Show that rank( AB)≤rank(B).107
4
Linear Mappings
4.1 – Linear Mappings
Amapping (ortransformation ) is nothing more than a function, but usually a function
between sets that have some additional structure such as a vector space. We have encountered
mappings already in the definition of a vector space V: namely the scalar multiplication and
vector addition functions, whose ranges both consist of elements of V. As with functions in
general, to say a mapping Tmaps a set Xinto a set Y, written T:X→Y, means that T
maps each object x∈Xto a unique object y∈Y. We denote this by writing T(x) =y, or
sometimes Tx=y, and call Xthedomain ofTandYthecodomain . A little more formally
a mapping Tis a set of ordered pairs ( x, y)∈X×Ywith the property that
∀x∈X
∃y∈Y 
((x, y)∈T)∧(ˆy̸=y→(x,ˆy)/∈T)
.
We call T(x) the value of Tatx. Given any set A⊆X, we define the image of Aunder T
to be the set
T(A) ={T(x) :x∈A} ⊆Y,
with T(X) in particular being called the image ofT(also known as the range ofT) and
denoted by Img( T).
A common practice is to write x7→yto indicate a mapping. For instance x7→3√xmay be
written to denote a mapping T:R→Rfor which T(x) =3√xfor all x∈R. The symbol →is
placed between sets, while 7→is placed between elements of sets.
Definition 4.1. A mapping T:X→Yisinjective (orone-to-one ) if
T(x1) =T(x2)⇒x1=x2.
for all x1, x2∈X. Thus if x1̸=x2, then T(x1)̸=T(x2).
A mapping T:X→Yissurjective (oronto ) if for each y∈Ythere exists some x∈X
such that T(x) =y. Thus we have T(X) =Y.
If a mapping is both injective and surjective, then it is called a bijection .
A large part of linear algebra is occupied with the study of a special kind of mapping known
as a linear mapping.108
Definition 4.2. LetVandWbe vector spaces over F. A mapping L:V→Wis called a
linear mapping if the following properties hold.
LT1. L(u+v) =L(u) +L(v)for all u,v∈V
LT2. L(cu) =cL(u)for all c∈Fandu∈V.
Whenever L:V→Wis given to be a linear mapping, it is understood that VandWmust
be vector spaces. A linear operator is a linear mapping L:V→V, which may be more
specifically referred to as a linear operator on Vwhenever the occasion warrants.
Proposition 4.3. IfL:V→Wis a linear mapping, then
1.L(0) =0
2.L(−v) =−L(v)for any v∈V.
3.For any c1, . . . , c n∈F,v1, . . . ,vn∈V,
L nX
k=1ckvk!
=nX
k=1ckL(vk).
Proof.
Proof of Part (1). Using the linearity property LT1, we have
L(0) =L(0+0) =L(0) +L(0).
Subtracting L(0) from the leftmost and rightmost sides then gives
L(0)−L(0) = [L(0) +L(0)]−L(0),
and thus 0=L(0).
Proof of Part (2). Letv∈Vbe arbitrary. Using property LT1 and part (1), we have
L(v) +L(−v) =L(v+ (−v)) =L(0) =0.
This shows that L(−v) is the additive inverse of L(v). That is, L(−v) =−L(v).
Proof of Part (3). We have L(c1v1) =c1L(v1) by property LT2. Let n∈Nand suppose that
L(c1v1+···+cnvn) =c1L(v1) +···+cnL(vn) (4.1)
for any c1, . . . , c n∈F,v1, . . . ,vn∈V. Let c1, . . . , c n+1∈Fandv1, . . . ,vn+1∈Vbe arbitrary.
Then
LXn+1
i=1civi
=L 
(c1v1+···+cnvn) +cn+1vn+1
=L(c1v1+···+cnvn) +L(cn+1vn+1) Property LT1
=c1L(v1) +···+cnL(vn) +L(cn+1vn+1) Hypothesis (4.1)
=c1L(v1) +···+cnL(vn) +cn+1L(vn+1) Property LT2
=Xn+1
i=1ciL(vi)
The proof is complete by the Principle of Induction. ■109
In part (1) of Proposition 4.3 the vector 0on the left side of L(0) =0is the zero vector in
V, and the 0on the right side is the zero vector in W. Occasionally there may arise a need
to distinguish between these two zero vectors, in which case we will denote 0∈Vby0Vand
0∈Wby0W.
Example 4.4. LetVandWbe vectors spaces. The mapping V→Wgiven by v7→0Wfor all
v∈Vis called the zero mapping and denoted by O. Thus we may write O:V→Wsuch
thatO(v) =0for all v∈V, where the symbol 0on the right side is understood to be the zero
vector in W. It is easy to verify that Ois a linear mapping. ■
Example 4.5. Given a vector space V, the mapping IV:V→Vgiven by IV(v) =vfor all
v∈Vis called the identity mapping . It is a linear mapping as well, and may be denoted by
Iif the vector space it is acting on is not in question. ■
Example 4.6. Given a vector space Vanda∈V, a mapping Ta:V→Vgiven by Ta(v) =v+a
for all v∈Vis atranslation by a . Note that this mapping is not linear unless a=0, in
which case it is simply an identity mapping. One geometric interpretation is to regard vas a
“point” in V, and v+ais a new “point” obtained by translating vbya.
For example, fixing a nonzero vector
a=
a
b
∈R2,
we may define Ta:R2→R2by
Ta(x) =x+a=
x
y
+
a
b
=
x+a
y+b
(4.2)
for each x= [x, y]⊤∈R2.
Very often a mapping L:Rn→Rnis taken to be a change in coordinates, for instance
in order to effect a change of variables in a double or triple integral in vector calculus. In
the case of Ta:R2→R2we may regard the mapping as taking the coordinates of a point
(x, y) inxy-coordinates and converting them to uv-coordinates ( u, v) by setting u=x+aand
v=y+b. Thus, if we let the symbol R2
xyrepresent R2inxy-coordinates, and let R2
uvrepresent
xy
ax0
Ta(x0)
x0y0
x0+ay0+b
Figure 9. Taas a translation by ainR2.110
xy
x0
x0y0Ta
uv
u0
x0+ax0+b
Figure 10. Taas a change in coordinates R2
xy→R2
uv.
R2inuv-coordinates, then we may define the mapping Tadefined by (4.2) to be the mapping
Ta:R2
xy→R2
uvgiven by
Ta: (x, y)7→(u, v) = (x+a, y+b).
In vector notation we may still write Ta:x7→x+a, since it makes no difference, mathematically,
whether we talk of points (x, y) and ( u, v), or vectors

x
y
and
u
v
.
Thus, translation by ainR2corresponds to a change in coordinates from the xy-system R2
xyto
theuv-system R2
uv. Figure 9 shows the translation by ainR2interpretation of Tain the case
when a >0 and b <0, letting
x0
y0
=x0;
and Figure 10 shows the change in coordinates interpretation of Taletting

u0
v0
=u0=x0+a.
■
Example 4.7. LetA= [aij] be an m×nmatrix and define L:Rn→RmbyL(x) =Ax; that
is,
L(x) =
a11··· a1n
a21··· a2n.........
am1··· amn

x1
x2...
xn

for each x∈Rn. The mapping Lis easily shown to be linear using properties of matrix
arithmetic established in Chapter 2: for each c∈Randx∈Rnwe have
L(cx) =A(cx) =c(Ax) =cL(x),111
and for each x,y∈Rnwe have
L(x+y) =A(x+y) =Ax+Ay=L(x) +L(y).
This verifies properties LT1 and LT2. ■
Definition 4.8. Given linear mappings L1, L2:V→W, we define the mapping L1+L2:V→
Wby
(L1+L2)(v) =L1(v) +L2(v)
for each v∈V.
Given linear mapping L:V→Wandc∈F, we define cL:V→Wby
(cL)(v) =cL(v)
for each v∈V. In particular we define −L= (−1)L.
Given vector spaces VandWoverF, the symbol L(V, W ) will be used to denote the set of
all linear mappings V→W; that is,
L(V, W ) ={L:V→W|Lis a linear mapping }.
As it turns out, L(V, W ) is a vector space in its own right.
Proposition 4.9. IfVandWare vector spaces over F, then L(V, W )is a vector space under
the operations of vector addition and scalar multiplication given in Definition 4.8.
Proof. LetL1, L2∈ L(V, W ). For any u,v∈V,
(L1+L2)(u+v) =L1(u+v) +L2(u+v) Definition 4.8
=L1(u) +L1(v) +L2(u) +L2(v) Property LT1
= [L1(u) +L2(u)] + [L1(v) +L2(v)] Axioms VS1 and VS2
= (L1+L2)(u) + (L1+L2)(v). Definition 4.8
For any c∈F,
(L1+L2)(cv) =L1(cv) +L2(cv) Definition 4.8
=cL1(v) +cL2(v) Property LT2
=c[L1(v) +L2(v)] Axioms VS5
=c(L1+L2)(v). Definition 4.8
Thus L1+L2:V→Wsatisfies properties LT1 and LT2, implying that L1+L2∈ L(V, W ) and
therefore L(V, W ) is closed under vector addition. The proof that L(V, W ) is also closed under
scalar multiplication is left as a problem. It remains to verify the eight axioms VS1–VS8 given
in Definition 3.1.
For any v∈Vwe have
(L1+L2)(v) =L1(v) +L2(v) =L2(v) +L1(v) = (L2+L1)(v),112
where the middle equality follows from VS1 for W. Thus L1+L2=L2+L1, verifying VS1 for
L(V, W ).
LetL3∈ L(V, W ). For any v∈V,
(L1+ (L2+L3))(v) =L1(v) + (L2+L3)(v) =L1(v) + (L2(v) +L3(v))
= (L1(v) +L2(v)) +L3(v) = (L1+L2)(v) +L3(v)
= ((L1+L2) +L3)(v),
where the middle equality follows from VS2 for W. Thus
L1+ (L2+L3) = (L1+L2) +L3,
verifying VS2 for L(V, W ).
The zero mapping O:V→Wis a linear mapping, as mentioned in Example 4.4, and thus
O∈ L(V, W ). It is straightforward to verify that O+L=L+O=Lfor any L∈ L(V, W ),
and thus L(V, W ) satisfies VS3.
For any L∈ L(V, W ) we have −L∈ L(V, W ) also, since −L= (−1)Lby Definition 4.8, and
it has been already verified that L(V, W ) is closed under scalar multiplication. Now, for any
v∈V,
(L+ (−L))(v) =L(v) + (−L)(v) =L(v) + ((−1)L)(v)
=L(v) + (−1)L(v) =L(v) + (−L(v)) =0,
where the first three equalities follow from Definition 4.8, the fourth equality from Proposition
3.3, and the fifth equality from VS4 for W. Thus L+ (−L) =O, verifying VS4 for L(V, W ).
Leta∈F. For any v∈V,
(a(L1+L2))(v) =a(L1+L2)(v) =a(L1(v) +L2(v))
=aL1(v) +aL2(v) = (aL1)(v) + (aL2)(v)
= (aL1+aL2)(v),
where the middle equality follows from VS5 for W. Thus a(L1+L2) =aL1+aL2, verifying
VS5 for L(V, W ).
The verification of Axiom VS6 is left as a problem, as is the verification of VS7.
Finally, for any L∈ L(V, W ) and v∈Vwe have
(1L)(v) = 1 L(v) =L(v),
by application of Definition 4.8 and VS8 for W. Thus 1 L=L, verifying VS8 for L(V, W ).■
Definition 4.10. A bijective linear mapping is called an isomorphism .
IfVandWare vector spaces and there exists a linear mapping L:V→Wthat is an
isomorphism, then VandWare said to be isomorphic and we write V∼=W.
Isomorphic vector spaces are truly identical in all respects save for the symbols used to
represent their elements. In fact any vector space Vof dimension ncan be shown to be113
isomorphic to Rn. To see this, let B= (v1, . . . ,vn) be an ordered basis for Vand observe that
the operation of taking a vector
v=x1v1+···+xnvn
inVand giving its B-coordinates,
[v]B=
x1...
xn
,
is actually a mapping v7→[v]Bfrom VtoRncalled the B-coordinate map (or the coordinate
map determined by B) and is denoted by φB. Thus, by definition,
φB(v) = [v]B
for all v∈V. The mapping φBis a well-defined function: given v∈V, by Theorem 3.41 there
exist unique scalars x1, . . . , x nfor which v=x1v1+···+xnvn, and therefore
φB(v) =
x1...
xn

is the only possible definition for φB. The mapping φBis, in fact, linear, injective, and surjective,
which is to say it is an isomorphism.
Theorem 4.11. LetB= (v1, . . . ,vn)be an ordered basis for a vector space VoverF. Then
the coordinate map φB:V→Fnis an isomorphism.
Proof. Suppose u,v∈Vare such that
φB(u) =
a1...
an
=
b1...
bn
=φB(v).
Then u=Pn
i=1aiviandv=Pn
i=1bivisuch that ai=bifori= 1, . . . , n , whence
u−v=nX
i=1aivi−nX
i=1bivi=nX
i=1(ai−bi)vi=nX
i=10vi=0,
and so u=v. Thus φBis injective.
Next, let
x1...
xn
∈Fn
be arbitrary. Defining v∈Vbyv=Pn
i=1xivi, we observe that
φB(v) =
x1...
xn
,
and thus φBis surjective.114
Finally, for any u=Pn
i=1aiviandv=Pn
i=1biviinVwe have u+v=Pn
i=1(ai+bi)vi, so
φB(u+v) =
a1+b1...
an+bn
=
a1...
an
+
b1...
bn
=φB(u) +φB(v)
by the definition of vector addition in Fn. Also for any c∈Fwe have cv=Pn
i=1caivi, so
φB(cu) =
ca1...
can
=c
a1...
an
=cφB(v)
by the definition of scalar multiplication in Fn. Hence φBis a linear mapping.
Therefore φBis an isomorphism. ■
Example 4.12. Consider the vector space W⊆R3given by
W=


x
y
z
x−2y+ 3z= 0

.
Two ordered bases for Ware
B=

−1
1
1
,
1
2
1

 and C=

2
1
0
,
−3
0
1

.
Given
v=
5
7
3
∈W,
find [v]Band [v]C.
Solution. Since ( x, y, z ) = (5 ,7,3) is a solution to the equation x−2y+ 3z= 0, it is clear that
v∈W. To find the B-coordinates of v, we find a, b∈Rsuch that
a
−1
1
1
+b
1
2
1
=
5
7
3
,
which is to say we solve the system

−a+b= 5
a+ 2b= 7
a+b= 3
The only solution is ( a, b) = (−1,4), and therefore
[v]B=
−1
4
.115
To find the C-coordinates of v, we find a, b∈Rsuch that
a
2
1
0
+b
−3
0
1
=
5
7
3
,
giving the system

2a−3b= 5
a+ 0b= 7
0a+b= 3
which immediately yields the unique solution ( a, b) = (7 ,3), and therefore
[v]C=
7
3
.
■116
4.2 – Images and Null Spaces
The image (or range) of a mapping was already defined in §4.1, but for convenience we give
the definition again in a slightly different guise. We also narrow the focus to linear mappings in
particular.
Definition 4.13. LetL:V→Wbe a linear mapping. The image ofLis the set
Img(L) ={w∈W:L(v) =wfor some v∈V},
and the null space (orkernel ) ofLis the set
Nul(L) ={v∈V:L(v) =0}.
Note that for L:V→Wwe have Img(L) =L(V). Another term for the null space of Lis
thekernel ofL, denoted by Ker( L) in many books.
Proposition 4.14. LetL:V→Wbe a linear mapping. Then the following hold.
1. Img( L)is a subspace of W.
2. Nul( L)is a subspace of V.
Proof.
Proof of Part (1). As we have shown in the previous section L(0) =0, and so 0∈Img(L).
Suppose that w1,w2∈Img(L). Then there exist vectors v1,v2∈Vsuch that L(v1) =w1
andL(v2) =w2. Now, since v1+v2∈Vand
L(v1+v2) =L(v1) +L(v2) =w1+w2,
we conclude that w1+w2∈Img(L). Hence Img( L) is closed under vector addition.
Finally, let c∈Rand suppose w∈Img(L). Then there exists some v∈Vsuch that
L(v) =w, and since cv∈Vand
L(cv) =cL(v) =cw,
we conclude that cw∈Img(L). Hence Img( L) is closed under scalar multiplication.
Therefore Img( L)⊆Wis a subspace.
Proof of Part (2). Since L(0) =0we immediately obtain 0∈Nul(L).
Suppose that v1,v2∈Nul(L). Then L(v1) =L(v2) =0, and since
L(v1+v2) =L(v1) +L(v2) =0+0=0,
it follows that v1+v2∈Nul(L) and so Nul( L) is closed under vector addition.
Finally, let c∈Rand suppose v∈Nul(L). Then L(v) =0, and since
L(cv) =cL(v) =c0=0
we conclude that cv∈Nul(L) and so Nul( L) is closed under scalar multiplication.
Therefore Nul( L)⊆Vis a subspace. ■117
Proposition 4.15. LetL:V→Wbe a linear mapping. Then Lis injective if and only if
Nul(L) ={0}.
Proof. Suppose that L:V→Wis injective. Let v∈Nul(L), so that L(v) =0. By Proposition
4.3 we have L(0) =0also, and since Lis injective it follows that v=0. Hence Nul(L)⊆ {0},
andL(0) =0shows that {0} ⊆Nul(L). Therefore Nul( L) ={0}.
Next, suppose that Nul(L) ={0}. Suppose that L(v1) =L(v2), soL(v1)−L(v2) =0. Then
L(v1−v2) =L(v1)−L(v2) =0
shows that v1−v2∈Nul(L) ={0}and thus v1−v2=0. Therefore v1=v2and we conclude
thatLis injective. ■
Proposition 4.16. LetL:V→Wbe an injective linear mapping. If v1, . . . ,vn∈Vare
linearly independent, then L(v1), . . . , L (vn)∈Ware linearly independent.
Proof. Suppose v1, . . . ,vnare linearly independent vectors in V. Let a1, . . . , a n∈Fbe such
that
a1L(v1) +···+anL(vn) =0.
From this we obtain
L(a1v1+···+anvn) =0,
and since Nul( L) ={0}it follows that
a1v1+···+anvn=0.
Now, since v1, . . . ,vnare linearly independent, it follows that a1=···=an= 0.
Therefore the vectors L(v1), . . . , L (vn) inWare linearly independent. ■
Example 4.17. Define the mapping T:Fn×n→Fn×nby
T(A) =A−A⊤
2.
(a) Show that Tis a linear mapping.
(b) Find the null space of T, and give its dimension.
(c) Find the image of T, and give its dimension.
Solution.
(a) Let A,B∈Fn×nandc∈F. Recalling Proposition 2.3, we have
T(cA) =(cA)−(cA)⊤
2=cA−cA⊤
2=cA−A⊤
2
=cT(A),
and
T(A+B) =(A+B)−(A+B)⊤
2=(A+B)−(A⊤+B⊤)
2
=A+B−A⊤−B⊤
2=A−A⊤
2+B−B⊤
2118
=T(A) +T(B),
and therefore Tis linear.
(b) By definition we have
Nul(T) ={A∈Fn×n:T(A) =On},
where Onis the n×nzero matrix. Now,
T(A) =On⇔A−A⊤
2=On⇔A−A⊤=On⇔A=A⊤,
and therefore
Nul(T) ={A∈Fn×n:A⊤=A}= Symn(F).
That is, the null space of Tconsists of the set of all n×nsymmetric matrices. In a problem at
the end of §3.6 it is found that dim(Symn(F)) =n(n+ 1)/2, and therefore
dim(Nul( T)) =n(n+ 1)
2
as well.
(c) By definition we have
Img(T) ={T(A) :A∈Fn×n}=A−A⊤
2:A∈Fn×n
.
Now, appealing to Proposition 2.3 once more, we find that
A−A⊤
2⊤
=1
2(A−A⊤)⊤=1
2[A⊤−(A⊤)⊤] =1
2(A⊤−A) =−A−A⊤
2,
and so the elements
A−A⊤
2
in the image of Tare skew-symmetric. Let Skw n(F) denote the set of n×nskew-matrices with
entries in F:
Skw n(F) ={A∈Fn×n:A⊤=−A}.
We have just shown that Img(T)⊆Skw n(F). Suppose B∈Skw n(F), so that B⊤=−B. Now,
it happens that
T(B) =B−B⊤
2=B−(−B)
2=2B
2=B,
and since there exists some matrix Afor which T(A) =B(namely we can let AbeBitself),
it follows that B∈Img(T) and hence Skw n(F)⊆Img(T). Therefore Img(T) =Skw n(F). In
Example 3.49 we found that dim(Skw n(F)) =n(n−1)/2, and therefore
dim(Img( T)) =n(n−1)
2
as well. ■119
Example 4.18. LetVbe a vector space over Fwith dim(V) = nand basis B, and let
φB:V→Fnbe the B-coordinate map. Now, suppose Wis a subspace of Vwith dim(W) =m,
and consider the restriction φB|W:W→Fn. By Proposition 4.14, Img(φB|W) is a subspace of
Fn. For brevity we define
[W]B= Img( φB|W) =φB(W).
What is the dimension of [ W]B? Let ( wi)m
i=1be any ordered basis for W. We wish to show
thatC= ([wi]B)m
i=1is a basis for [ W]B. Since φBis injective on Vby Theorem 4.11, it is also
injective on W, and thus Cis a linearly independent set by Proposition 4.16.
Ifx∈[W]B, then x= [w]Bfor some w∈W, where w=c1w1+···+cmwmfor some
c1, . . . , c m∈F. Now, using the linearity properties of φB,
x= [c1w1+···+cmwm]B=c1[w1]B+···+cm[wm]B∈Span(C)
Conversely, if x∈Span(C), so that
x=mX
i=1ci[wi]B
for some choice of constants c1, . . . , c m∈F, then the vector w∈Wgiven by
w=mX
i=1ciwi
is such that φB(w) = [w]B=x, and thus x∈φB(W) = [W]B. Hence Span (C) = [W]B, and we
conclude that Cis a basis for [ W]B. It follows that dim([ W]C) =|C|=m, and therefore
dim([ W]C) = dim( W)
for any choice of basis CforW. ■
Problems
1.LetAx=bbe a nonhomogeneous system of linear equations, where Ais an m×nmatrix.
Define L:Rn→RmbyL(x) =Ax. Without using Theorem 2.40, prove that if x0is a
solution to the system then the system’s solution set is
x0+ Nul( L) ={x0+y:y∈Nul(L)}.120
4.3 – Matrix Representations of Mappings
We begin with the case of Euclidean vector spaces. Let L:Rn→Rmbe an arbitrary linear
mapping. For each 1 ≤j≤nletejbe the jth standard unit vector of Rn, represented as an
n×1 column vector. Thus, as ever, ej= [δij]n×1such that
δij=(
0,ifi̸=j
1,ifi=j
Also, for each 1 ≤i≤mletϵibe the ith standard unit vector of Rm, represented as an m×1
column vector. Choosing En={ej: 1≤j≤n}andEm={ϵi: 1≤i≤m}to be the bases
forRnandRm, respectively, we view the elements of both Euclidean spaces as being column
vectors in what follows.
For each 1 ≤j≤nwe have L(ej)∈Rmso that
L(ej) =mX
i=1aijϵi
for some scalars a1j, . . . , a mj, and so the Em-coordinates of L(ej) are
L(ej) =
a1j...
amj
.
(We could write [ L(ej)]Em, but since both L(ej) and [ L(ej)]Emare elements of Rmis would be
overly fastidious.) Now,
L(ej) =a1jϵ1+···+amjϵm=a1j
1
0
...
0
+···+amj
0
0
...
1
=
a1j...
amj
.
Now, for any x∈Rnthere exist scalars x1, . . . , x nsuch that
x=nX
j=1xjej,
and so the En-coordinates of xare
x=
x1...
xn
.
By the linearity of Lwe have
L(x) =L nX
j=1xjej!
=nX
j=1xjL(ej) =nX
j=1xj
a1j...
amj
,121
and hence, defining A= [aij]m,n,
L(x) =
Pn
j=1xja1j
...Pn
j=1xjamj
=
a11··· a1n.........
am1···amn

x1...
xn
=Ax.
That is, the linear mapping Lhas a corresponding matrix A, called the matrix corresponding
toL:Rn→Rmwith respect to EnandEm. Since Lis arbitrary we have shown that every
linear mapping between Euclidean spaces has a corresponding matrix, and moreover we have
devised a means for determining the entries of the matrix.
Example 4.19. LetL:R3→R2be given by
L

x1
x2
x3

=
3x1+ 2x2−7x3
4x1−6x2+ 5x3
Find the matrix corresponding to Lwith respect to the standard bases for R2andR3.
Solution. We must find some matrix A= [aij]2×3such that L(x) =Axfor all x∈R3; that is,

a11a12a13
a21a22a23
x1
x2
x3
=
3x1+ 2x2−7x3
4x1−6x2+ 5x3
.
This straightaway yields

a11x1+a12x2+a13x3
a21x1+a22x2+a23x3
=
3x1+ 2x2−7x3
4x1−6x2+ 5x3
,
from which we immediately obtain
A=
a11a12a13
a21a22a23
=
3 2 −7
4−6 5
and we’re done. ■
Now that we have examined the lay of the land in the case of real Euclidean vector spaces,
it is time to turn our attention to abstract vector spaces. Recall that once an ordered basis B
for any finite-dimensional vector space Vover a field Fhas been chosen, every vector v∈V
can be represented by coordinates with respect to Busing the coordinate map φB, where
φB(v) = [v]B
as discussed in §4.1. Depending on whatever is most convenient in a given situation, we may
write [ v]Bas a column or row matrix,

x1...
xn
,x1···xn
,
or more compactly as [ x1, . . . , x n].122
LetL:V→Wbe a linear mapping, and let B= (v1, . . . ,vn) be an ordered basis for Vand
C= (w1, . . . ,wm) an ordered basis for W. For each 1 ≤j≤nwe have L(vj)∈W, and since
w1, . . . ,wmspan Wit follows that
L(vj) =a1jw1+···+amjwm (4.3)
for some scalars aij∈F, 1≤i≤m. In terms of coordinates with respect to the bases BandC
we may write (4.3) for each 1 ≤j≤nas

L(vj)
C=
a1j...
amj
.
(Recall that [ vj]B, written as a column matrix, will have 1 in the jth row and 0 in all other
rows.) Now, given any v∈V, there exist scalars x1, . . . , x nsuch that v=x1v1+···+xnvn,
and so
[v]B=
x1...
xn
.
Now, by the linearity of LandφC,

L(v)
C=
L(x1v1+···+xnvn)
C=x1
L(v1)
C+···+xn
L(vn)
C
=x1
a11...
am1
+···+xn
a1n...
amn
=
Pn
j=1xja1j
...Pn
j=1xjamj
. (4.4)
If we define the m×nmatrix
[L]BC=h
L(v1)
C···
L(vn)
Ci
=
a11··· a1n.........
am1···amn
,
then we see from (4.4) that

L(v)
C=
a11··· a1n.........
am1···amn

x1...
xn
,
or equivalently

L(v)
C= [L]BC[v]B (4.5)
for all v∈V. The matrix [ L]BCis the matrix corresponding to L∈ L(V, W )with respect
toBandC, also called the BC-matrix ofL. We may write (4.5) simply as L(v) = [L]BCvif it
is understood that BandCare the bases for VandW, respectively. In any case [ L]BC[v]Bis
seen to give the C-coordinates (as a column matrix) of the vector L(v)∈W. We formalize the
foregoing findings as a theorem for later use.123
Theorem 4.20. LetL:V→Wbe a linear mapping, with B= (v1, . . . ,vn)an ordered basis
forVandC= (w1, . . . ,wm)an ordered basis for W. The BC-matrix of Lis
[L]BC=h
L(v1)
C···
L(vn)
Ci
, (4.6)
and
[L(v)]C= [L]BC[v]B.
for all v∈V.
The situation simplifies somewhat in the commonly encountered case when Lis a linear
operator on a vector space Vfor which we consider only a single ordered basis B= (v1, . . . ,vn).
To begin with, the BB-matrix of L, [L]BB, is denoted by the more compact symbol [ L]B, and
referred to as either the matrix corresponding to Lwith respect to Bor the B-matrix of
L. The following corollary is immediate.
Corollary 4.21. IfL∈ L(V)andB= (v1, . . . ,vn)is an ordered basis for V, then the B-matrix
ofLis
[L]B=h
L(v1)
B···
L(vn)
Bi
, (4.7)
and
[L(v)]B= [L]B[v]B.
for all v∈V.
Example 4.22. LetL:R2→R3be a linear mapping for which
L
1
1
=
1
1
1
and L
1
−1
=
1
−1
−1
. (4.8)
Find the matrix corresponding to Lwith respect to the standard bases for R2andR3, and then
find an expression for L([x, y]⊤).
Solution. The vectors [1 ,1]⊤and [1 ,−1]⊤are linearly independent and hence form a basis
forR2, so that (4.8) in fact uniquely determines L. Let [ L] denote the matrix corresponding
toLwith respect to the standard bases for R2andR3, which we’ll denote by {e1,e2}andE
respectively. Theorem 4.20 informs us that
[L] =h
L(e1)
E
L(e2)
Ei
=L(e1)L(e2)
,
where the last equality is simply a recognition of the fact that, for any x∈R2, the vector
L(x)∈R3is already in E-coordinates. The problem is we don’t know the values of L(e1) and
L(e2). These could be figured out with a little clever tinkering using the linearity properties of
L, but the tack we’ll take is one which will work in general.
Setting
B=
1 1
1−1
,124
by Proposition 2.6 and the definition of [ L] we have
[L]B="
[L]
1
1
[L]
1
−1#
=
1 1
1−1
1−1
. (4.9)
By the methods of §2.4 we find that Bis invertible, with
B−1="
1
21
2
1
2−1
2#
.
Right-multiplying through (4.9) by B−1at once gives us [ L]:
[L] =
1 1
1−1
1−1
B−1=
1 1
1−1
1−1
"
1
21
2
1
2−1
2#
=
1 0
0 1
0 1
.
Now for any [ x, y]⊤∈R2we have
L
x
y
= [L]
x
y
=
1 0
0 1
0 1

x
y
=
x
y
y
.
The image of Lis easily verified to be Col([ L]), which is the plane y=zinR3. ■
Example 4.23. LetL∈ L(R2×2) be given by L(A) =A⊤, and let E=E22, the standard
ordered basis for R2×2. Find [ L]E, theE-matrix of L.
Solution. We have E= (E11,E12,E21,E22), where
E11=
1 0
0 0
,E12=
0 1
0 0
,E21=
0 0
1 0
,E22=
0 0
0 1
.
Since
E11= 1E11+ 0E12+ 0E21+ 0E22,E12= 0E11+ 1E12+ 0E21+ 0E22,
and so on, the E-coordinates of the elements of Eare
[E11]E=
1
0
0
0
,[E12]E=
0
1
0
0
,[E21]E=
0
0
1
0
,[E22]E=
0
0
0
1
.
Now, in general,
L
a b
c d
=
a c
b d
,
so that
L(E12) =L
0 1
0 0
=
0 0
1 0
=E21
and
L(E21) =L
0 0
1 0
=
0 1
0 0
=E12,125
while L(E11) =E11andL(E22) =E22. By Corollary 4.21,
[L]E=h
L(E11)
E
L(E12)
E
L(E21)
E
L(E22)
Ei
=
[E11]E[E21]E[E12]E[E22]E
,
and therefore
[L]E=
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

is the E-matrix of L. ■
Theorem 4.24. IfVandWare vector spaces over Fwithdim(V) =nanddim(W) =m, then
L(V, W )∼=Fm×n.
Proof. LetB= (v1, . . . ,vn) andC= (w1, . . . ,wm) be ordered bases for VandW, respectively.
Proposition 4.9 established that L(V, W ) is a vector space, so define Φ : L(V, W )→Fm×nby
Φ(L) = [L]BC. By Theorem 4.20,
Φ(L) =h
φC 
L(v1)
···φC 
L(vn)i
,
which shows that Φ is a well-defined function since the C-coordinate map φC:W→Fmis a
well-defined function by the discussion preceding Theorem 4.11. We will show that Φ is an
isomorphism.
LetL1, L2∈ L(V, W ). Then by Theorem 4.20,
Φ(L1+L2) = [L1+L2]BC=h
(L1+L2)(v1)
C···
(L1+L2)(vn)
Ci
;
that is, Φ( L1+L2) is the matrix with jth column vector [( L1+L2)(vn)]Cfor 1≤j≤n. Now,
since φC:W→Fmis linear by Theorem 4.11,

(L1+L2)(vj)
C=
L1(vj) +L2(vj)
C=φC 
L1(vj) +L2(vj)
=φC 
L1(vj)
+φC 
L2(vj)
=
L1(vj)
C+
L2(vj)
C,
and so by the definition of matrix addition,
Φ(L1+L2) =h
L1(v1)
C+
L2(v1)
C···
L1(vn)
C+
L2(vn)
Ci
=h
L1(v1)
C···
L1(vn)
Ci
+h
L2(v1)
C···
L2(vn)
Ci
= Φ(L1) + Φ( L2).
Next, for any c∈FandL∈ L(V, W ), again recalling that φCis linear,
Φ(cL) =h
···
(cL)(vj)
C···i
=h
···
cL(vj)
C···i
=h
···c
L(vj)
C···i
=ch
···
L(vj)
C···i
=cΦ(L),
where the fourth equality follows from the definition of scalar multiplication of a matrix. We
see that Φ satisfies properties LT1 and LT2, and hence is a linear mapping.126
LetL∈Nul(Φ), so that
Φ(L) = [L]BC=h
L(v1)
C···
L(vn)
Ci
=Om,n.
Thus, for each 1 ≤j≤n,
φC(L(vj)) =
L(vj)
C= [0] m,1,
which shows that L(vj)∈Nul(φC). However φCis injective, so Nul(φC) ={0}by Proposition
4.15, and hence L(vj) =0. Now, for any v∈Vthere exist c1, . . . , c k∈Fsuch that
v=nX
j=1cjvj,
and then
L(v) =L nX
j=1cjvj!
=nX
j=1cjL(vj) =nX
j=1cj0=0.
Thus L(v) =0for all v∈V, which is to say L=O, the zero mapping. It follows that
Nul(Φ)⊆ {O}, and since the reverse containment obtains from Proposition 4.3(1), we have
Nul(Φ) = {O}. Hence Φ is injective by Proposition 4.15.
Next, let A∈Fm×n, so
A=a1···an
with
aj=
a1j...
amj
∈Fm
for each 1 ≤j≤n. Let L∈ L(V, W ) be such that
L(vj) =a1jw1+···+amjwm
for each 1 ≤j≤n, so that
[L(vj)]C=aj.
Then
Φ(L) =h
L(v1)
C···
L(vn)
Ci
=a1···an
=A,
which shows that Φ is surjective.
Since Φ : L(V, W )→Fm×nis linear, injective, and surjective, we conclude that it is an
isomorphism. Therefore L(V, W )∼=Fm×n. ■
Corollary 4.25. LetVandWbe finite-dimensional vector spaces over Fwith ordered bases B
andC, respectively. For every mapping L∈ L(V, W )there is a unique matrix A∈Fm×nsuch
thatA= [L]BC. For every matrix A∈Fm×nthere is a unique mapping L∈ L(V, W )such that
[L]BC=A.
Proof. In the proof of Theorem 4.24 it was found that Φ : L(V, W )→Fm×ngiven by Φ( L) =
[L]BCis an isomorphism. The first statement of the corollary follows from the fact that Φ is
a well-defined function on L(V, W ), and the second statement follows from the fact that Φ is
surjective and injective. ■127
Example 4.26. Another way to argue that, in particular, there is a unique matrix Acorre-
sponding to a linear mapping L:V→Wwith respect to bases BandCis as follows. Suppose
that [ L]BC,[L]′
BC∈Fm×nare two matrices corresponding to Lwith respect to BandC. Then
[L(v)]C= [L]BC[v]Band [ L(v)]C= [L]′
BC[v]B,
and thus 
[L]BC−[L]′
BC
[v]B=0
for all v∈V. Now, setting B= [L]BC−[L]′
BCand observing that [ vj]B= [δij]n×1, we have
B[vj]B=0for each 1 ≤j≤n, or
B[vj]B=
a11··· a1n.........
am1···amn

δ1j...
δnj
=
Pn
k=1b1kδkj...Pn
k=1bmkδkj
=
b1j...
bmj
=
0
...
0
.
Thus the jth column vector of Bis0, and since 1 ≤j≤nis arbitrary we conclude that all the
columns of Bconsist of zeros and so B=Om,n. Therefore [ L]BC−[L]′
BC=Om,n, and it follows
that [ L]BC= [L]′
BC. ■
Though there cannot be two distinct matrices corresponding to the same linear mapping
L:V→Wwith respect to the same bases BandC, a different choice of basis for either Vor
Wwill result in a different corresponding matrix for L. This turns the discussion toward the
idea of changing from one basis Bof a vector space Vto another basis B′, the subject of the
next section.
Problems
1. Suppose that L:R2→R3is the linear transformation given by
L
x1
x2
=
x2
−5x1+ 13x2
−7x1+ 16x2
.
Find [ L]BC, the matrix corresponding to Lwith respect to the ordered bases
B=
3
1
,
5
2
and C=

1
0
−1
,
−1
2
2
,
0
1
2

.128
4.4 – Change of Basis
LetVbe an n-dimensional vector space over F, where n≥1. Let
B={v1, . . . ,vn}and B′={v′
1, . . . ,v′
n}
be two distinct bases for V. We would like to devise a ready means of expressing any vector
v∈Vgiven in B-coordinates in terms of B′-coordinates instead. In other words we seek a
mapping Fn→Fngiven by [ v]B7→[v]B′for all v∈V. How to find the mapping? Consider
the identity mapping IV:V→V, which of course is linear. By Theorem 4.20 the matrix
corresponding to IVwith respect to BandB′is
[IV]BB′=h
IV(v1)
B′···
IV(vn)
B′i
=h
[v1]B′···[vn]B′i
, (4.10)
and for all v∈V
[IV]BB′[v]B= [IV(v)]B′= [v]B′.
This is it! To convert any v∈VfromB-coordinates to B′-coordinates we simply multiply the
column vector [ v]Bby the matrix [ IV]BB′, which happens to be the matrix corresponding to the
identity matrix IVwith respect to BandB′, but we will call it the change of basis matrix
from BtoB′(or the coordinate transformation matrix from BtoB′) and denote it by
IBB′. We have proven the following.
Theorem 4.27. LetB= (v1, . . . ,vn)andB′= (v′
1, . . . ,v′
n)be two ordered bases for V, and
define IBB′∈Fn×nby
IBB′=h
[v1]B′···[vn]B′i
.
Then
IBB′[v]B= [v]B′
for all v∈V.
Clearly to find IBB′we must determine the B′-coordinates of the vectors in B. For each
1≤j≤nthere exist scalars a1j, . . . , a njsuch that
vj=a1jv′
1+···+anjv′
n,
and so
[vj]B′=
a1j...
anj
.
As the following example illustrates, the task of determining IBB′in practice amounts to solving
a system of equations that has a unique solution.
Example 4.28. LetVbe a vector space with ordered basis B= (v1,v2).
(a)Show that B′= (v′
1,v′
2) with v′
1=−v1+ 2v2andv′
2= 3v1+v2is another ordered basis
for the vector space V.
(b) Determine the change of basis matrix from BtoB′,IBB′.
(c) Determine the change of basis matrix from B′toB,IB′B.129
Solution.
(a) We see that dim(V) =|B|= 2, and so by Theorem 3.54(1) to show that B′is a basis for V
is suffices to show that v′
1andv′
2are linearly independent. Suppose that c1, c2∈Fare such that
c1v′
1+c2v′
2=0. (4.11)
This implies that
c1(−v1+ 2v2) +c2(3v1+v2) =0,
or equivalently
(−c1+ 3c2)v1+ (2c1+c2)v2=0
Since v1andv2are linearly independent we must have
−c1+ 3c2= 0
2c1+c2= 0
This system readily informs us that c1=c2= 0, and so (4.11) admits only the trivial solution.
Therefore v′
1andv′
2must be linearly independent.
(b) By Theorem 4.27 we have
IBB′=h
[v1]B′[v2]B′i
.
We set
[v1]B′=
x1
x2
and [ v2]B′=
y1
y2
,
which is to say
x1v′
1+x2v′
2=v1and y1v′
1+y2v′
2=v2
Using the fact that the coordinate map φBis a linear mapping, we obtain
1
0
= [v1]B=φB(v1) =φB(x1v′
1+x2v′
2) =x1φB(v′
1) +x2φB(v′
2)
=x1[v′
1]B+x2[v′
2]B=x1
−1
2
+x2
3
1
=
−1 3
2 1
x1
x2
, (4.12)
and similarly
0
1
= [v2]B=y1[v′
1]B+y2[v′
2]B=y1
−1
2
+y2
3
1
=
−1 3
2 1
y1
y2
. (4.13)
From (4.12) and (4.13) we obtain the systems
−x1+ 3x2= 1
2x1+x2= 0and
−y1+ 3y2= 0
2y1+y2= 1
Solving these systems yields x1=−1/7,x2= 2/7,y1= 3/7, and y2= 1/7. Therefore we have
IBB′=h
[v1]B′[v2]B′i
=
x1y1
x2y2
=
−1/7 3/7
2/7 1/7
.
(c) As for the change of basis matrix from B′toB, that’s relatively straightforward since the
vectors in B′were given in terms of the vectors in B:
IB′B=h
[v′
1]B[v′
2]Bi
=
−1 3
2 1130
Observe that
IB′BIBB′=IBB′IB′B=I2,
so that IB′BandIBB′are in fact inverses of one another. ■
Example 4.29. Consider the vector space W⊆R3given by
W=


x
y
z
x−2y+ 3z= 0

.
Two ordered bases for Ware
B= (u1,u2) =

−1
1
1
,
1
2
1

 and C= (v1,v2) =

2
1
0
,
−3
0
1

.
Find the change of basis matrix from BtoC, and use it to find the C-coordinates of v∈W
given that [ v]B= [−1 4]⊤
Solution. By Theorem 4.27 we have
IBC=h
[u1]C[u2]Ci
,
and so we must find the C-coordinates of u1andu2. Starting with u1, we find a, b∈Rsuch
thatav1+bv2=u1; that is,
a
2
1
0
+b
−3
0
1
=
−1
1
1
,
which has ( a, b) = (1 ,1) as the only solution, and hence
[u1]C=
1
1
.
Next, we find a, b∈Rsuch that av1+bv2=u2; that is,
a
2
1
0
+b
−3
0
1
=
1
2
1
,
which has ( a, b) = (2 ,1) as the only solution, and hence
[u2]C=
2
1
.
Therefore
IBC=
1 2
1 1
is the change of basis matrix from BtoC. Now,
[v]C=IBC[v]B=
1 2
1 1
−1
4
=
7
3
,
which agrees with the results of Example 4.12. ■131
Example 4.30. Two ordered bases for the vector space
P2(R) ={a0+a1x+a2x2:a0, a1, a2∈R}
are
B= (1, x, x2) and D= (1,1 +x,1 +x+x2).
(a) Find the change of basis matrix from BtoD.
(b) Find the change of basis matrix from DtoB.
Solution.
(a) We have B= (v1,v2,v3) with v1= 1,v2=x, and v3=x2, and D= (v′
1,v′
2,v′
3) with
v′
1= 1,v′
2= 1 + x, and v′
3= 1 + x+x2. By Theorem 4.27,
IBD=h
[1]D[x]D[x2]Di
.
Setting
[1]D=
a1
a2
a3
,[x]D=
b1
b2
b3
,[x2]D=
c1
c2
c3
,
three equations arise:
a1(1) + a2(1 +x) +a3(1 +x+x2) = 1 ,
b1(1) + b2(1 +x) +b3(1 +x+x2) =x,
c1(1) + c2(1 +x) +c3(1 +x+x2) =x2.
Rewriting these equations as
(a1+a2+a3) + (a2+a3)x+a3x2= 1,
(b1+b2+b3) + (b2+b3)x+b3x2=x,
(c1+c2+c3) + (c2+c3)x+c3x2=x2,
we obtain the systems
(a1+a2+a3= 1
a2+a3= 0
a3= 0(b1+b2+b3= 0
b2+b3= 1
b3= 0(c1+c2+c3= 0
c2+c3= 0
c3= 1
which have solutions
a1
a2
a3
=
1
0
0
,
b1
b2
b3
=
−1
1
0
,
c1
c2
c3
=
0
−1
1
.
Therefore
IBD=
1−1 0
0 1 −1
0 0 1
.132
(b) By Theorem 4.27
IDB=h
[1]B[1 +x]B[1 +x+x2]Bi
.
Clearly
[1]B=
1
0
0
,[1 +x]B=
1
1
0
,[1 +x+x2]B=
1
1
1
,
and therefore
IDB=
1 1 1
0 1 1
0 0 1
.
■
Proposition 4.31. LetBandB′be ordered bases for a finite-dimensional vector space V. Then
the change of basis matrix IBB′is invertible, with
I−1
BB′=IB′B.
Proof. By Theorem 4.27
IBB′[v]B= [v]B′and IB′B[v]B′= [v]B,
for all v∈V, and so
IB′BIBB′[v]B=IB′B[v]B′= [v]B (4.14)
and
IBB′IB′B[v]B′=IBB′[v]B= [v]B′ (4.15)
for all v∈V.
Letn=dim(V), and fix x∈Fn. By Theorem 4.11 the coordinate maps φB, φB′:V→Fn
are isomorphisms, and so there exist unique vectors u,u′∈Vsuch that
φB(u) = [u]B=xand φB′(u′) = [u′]B′=x,
whereupon equations (4.14) and (4.15) give
IB′BIBB′x=xand IBB′IB′Bx=x,
respectively. Since x∈Fnis arbitrary, we conclude that
(IB′BIBB′)x=xand ( IBB′IB′B)x=x
for all x∈Fn. It follows by Proposition 2.12(1) that
IB′BIBB′=Inand IBB′IB′B=In,
and therefore IBB′is invertible with I−1
BB′=IB′B. ■133
Now suppose that Lis a linear operator on a vector space V, which is to say Lis a linear
mapping V→V. Let B= (v1, . . . ,vn) be an ordered basis for V. For any v∈Vwe have
L(v)∈Vgiven by
[L(v)]B= [L]B[v]B,
where
[L]B=h
L(v1)
B···
L(vn)
Bi
by Corollary 4.21. If B′= (v′
1, . . . ,v′
n) is another ordered basis for V, then another corresponding
matrix [ L]B′is obtained for the operator L:
[L]B′=h
L(v′
1)
B′···
L(v′
n)
B′i
,
where for any v∈Vwe have L(v)∈Vgiven by
[L(v)]B′= [L]B′[v]B′.
We would like to determine the relationship between [ L]Band [ L]B′.
Recall that if ABis defined and B= [b1···bn], then by Proposition 2.6
AB=Ab1···bn
=Ab 1···Abn
.
We therefore have
IBB′[L]B=IBB′h
L(v1)
B···
L(vn)
Bi
=h
IBB′
L(v1)
B···IBB′
L(vn)
Bi
=h
L(v1)
B′···
L(vn)
B′i
= [L]BB′, (4.16)
the last equality a direct consequence of Theorem 4.20. On the other hand,
[L]B′IBB′= [L]B′h
[v1]B′···[vn]B′i
=h
[L]B′[v1]B′···[L]B′[vn]B′i
=h
L(v1)
B′···
L(vn)
B′i
= [L]BB′ (4.17)
Comparing (4.16) and (4.17), we have proven the following.
Proposition 4.32. Suppose Vis a finite-dimensional vector space and L∈ L(V). IfBandB′
are ordered bases for V, then
IBB′[L]B= [L]B′IBB′= [L]BB′.
Note that [ L]BB′is the matrix corresponding to the operator L:V→Vin the case when
each input vforLis given in B-coordinates but the output L(v) is given in B′-coordinates.
That is, the Vcomprising the domain of Lhas basis Band the Vcomprising the codomain of
Lhas basis B′!
Corollary 4.33. Suppose Vis a finite-dimensional vector space and L∈ L(V). IfBandB′
are ordered bases for V, then
[L]B′=IBB′[L]BI−1
BB′.134
Proof. Suppose that BandB′are ordered bases for V. Then IBB′[L]B= [L]B′IBB′by Proposition
4.32, and thus
IBB′[L]B= [L]B′IBB′⇒IBB′[L]BI−1
BB′= [L]B′IBB′I−1
BB′⇒[L]B′=IBB′[L]BI−1
BB′
since the matrix IBB′is invertible by Proposition 4.31. ■
Problems
1. The ordered sets
E=
1
0
,
0
1
and B=
1
2
,
−2
1
are bases for R2(the former being the standard basis).
(a) Find the change of basis matrix IEBfor changing from the basis Eto the basis B.
(b) Use IEBto find the B-coordinates of x= [2,−5]⊤.
(c) Find IBEusing Proposition 4.31.
2. The ordered sets
B=
7
5
,
−3
−1
and C=
1
−5
,
−2
2
are bases for R2. Find the change of basis matrices IBCandICB.135
4.5 – The Rank-Nullity Theorem
Given a matrix A∈Fn×n, recall from §3.5 that the nullity of Ais defined to be nullity (A) =
dim(Nul(A)), and also recall from §3.6 that the rank of Amay be characterized as rank(A) =
dim(Col( A)). We now attribute similar terminology to linear mappings.
Definition 4.34. LetL:V→Wbe a linear mapping. The rank ofLis the dimension of the
image of L,
rank( L) = dim(Img( L)),
and the nullity ofLis the dimension of the null space of L,
nullity( L) = dim(Nul( L)).
From here onward we will use the new notation rank(L) and nullity (L) interchangeably with
the old notation dim(Img(L)) and dim(Nul(L)), since both are used extensively in the literature.
The motivation behind Definition 4.34 will become more apparent presently.
In the statement of the next proposition we take
[Img( L)]C=φC 
Img(L)
={φC(w) :w∈Img(L)}={[w]C:w∈Img(L)}.
Proposition 4.35. LetVandWbe finite-dimensional vector spaces with ordered bases Band
C, respectively. If L:V→Wis a linear mapping, then
[Img( L)]C= Col([ L]BC).
Proof. LetB= (b1, . . . ,bn), and suppose L:V→Wis a linear mapping. By Theorem 4.20
we have
[L]BC=h
L(b1)
C···
L(bn)
Ci
.
Now fix y∈[Img(L)]C. Then y= [w]Cfor some w∈Img(L), and so there exists some
v∈V, where
[v]B=
v1...
vn
,
such that w=L(v). Since v=v1b1+···+vnbn, and both LandφCare linear mappings, we
have
y= [w]C= [L(v1b1+···+vnbn)]C=v1[L(b1)]C+···+vn[L(bn)]C∈Col([L]BC),
and therefore [Img( L)]C⊆Col([L]BC).
Conversely, y∈Col([L]BC) implies that
y=x1[L(b1)]C+···+xn[L(bn)]C
for some x1, . . . , x n∈F, and then
y= [L(x1b1+···+xnbn)]C
forL(x1b1+···+xnbn)∈Img(L) shows y∈[Img( L)]C. Hence Col([ L]BC)⊆[Img( L)]C.■136
As a consequence of Proposition 4.35 we have
rank([ L]BC) = dim(Col([ L]BC)) = dim([Img( L)]C) = dim(Img( L)) = rank( L),
where the third equality follows from Example 4.18. Thus the rank of a linear mapping
L:V→Wequals the rank of its corresponding matrix with respect to any choice of ordered
bases for VandW, and so the thrust behind Definition 4.34 should now be clear. We have
proven the following.
Corollary 4.36. IfVandWare finite-dimensional and L∈ L(V, W ), then
rank( L) = rank([ L]),
where [L]is the matrix corresponding to Lwith respect to any choice of ordered bases for Vand
W.
Theorem 4.37 (Rank-Nullity Theorem for Mappings ).LetVbe a finite-dimensional
vector space. If L:V→Wis a linear mapping, then
rank( L) + nullity( L) = dim( V).
Proof. Letn=dim(V),p=nullity (L), and q=rank(L). We must demonstrate that n=p+q.
Ifnullity (L) =n, then Nul(L) =Vby Theorem 3.56(3); that is, L(v) =0for all v∈V, so
Img(L) ={0}and therefore
nullity( L) + rank( L) = dim( V) + dim( {0}) =n+ 0 = n= dim( V)
as desired.
Ifnullity (L) = 0, then Nul(L) ={0}. Let {v1, . . . ,vn}be a basis for V. The set
{L(v1), . . . , L (vn)} ⊆Img(L) is linearly independent by Proposition 4.16. Now,
S= Span {L(v1), . . . , L (vn)} ⊆Img(L)
since Img(L) is a subspace of W. Let w∈Img(L), so that w=L(v) for some v∈V. There
exist scalars c1, . . . , c nsuch that v=c1v1+···+cnvn, and then
w=L(v) =L nX
i=1civi!
=nX
i=1ciL(vi)∈Span{L(v1), . . . , L (vn)}=S
shows that Img(L)⊆S. Hence S=Img(L) and we’ve shown that Sis a basis for Img(L).
Therefore
nullity( L) + rank( L) = 0 + |S|= 0 + n= dim( V)
once again.
Finally, assume that 0 <nullity (L)< n, so that Nul(L) is neither {0}norV. Since
Nul(L)̸=Vthere exists some v∈Vsuch that L(v)̸=0, which implies that Img(L)̸={0}
and hence rank(L) =q≥1. Also Nul(L)̸={0}implies that nullity (L) =p≥1. Thus
Img(L) has some basis {w1, . . . ,wq} ̸=∅, and Nul(L) has some basis {u1, . . . ,up} ̸=∅. Since
{w1, . . . ,wq} ⊆Img(L), for each 1 ≤i≤qthere exists some vi∈Vsuch that L(vi) =wi. The
claim is that
B={u1, . . . ,up,v1, . . . ,vq} (4.18)137
is a basis for V.
Letv∈V. Then L(v) =wfor some w∈W, and since w∈Img(L) there exist scalars
b1, . . . , b qsuch that
w=b1w1+···+bqwq.
Hence, by the linearity of L,
L(v) =w=b1L(v1) +···+bqL(vq) =L(b1v1+···+bqvq),
and so
L(v−(b1v1+···+bqvq)) =L(v)−L(b1v1+···+bqvq) =0.
So we have v−(b1v1+···+bqvq)∈Nul(L), and since {u1, . . . ,up}is a basis for Nul(L) there
exist scalars a1, . . . , a psuch that
v−(b1v1+···+bqvq) =a1u1+···+apup.
From this we obtain
v=a1u1+···+apup+b1v1+···+bqvq∈Span{u1, . . . ,up,v1, . . . ,vq},
and therefore
V= Span {u1, . . . ,up,v1, . . . ,vq}.
It remains to shows that u1, . . . ,up,v1, . . . ,vqare linearly independent. Suppose that
a1u1+···+apup+b1v1+···+bqvq=0. (4.19)
Then
0=L(a1u1+···+apup+b1v1+···+bqvq)
=L(a1u1+···+apup) +L(b1v1+···+bqvq)
=a1L(u1) +···+apL(up) +b1L(v1) +···+bqL(vq)
=a10+···+ap0+b1w1+···+bqwq
=b1w1+···+bqwq,
and since w1, . . . ,wqare linearly independent we obtain b1=···=bq= 0. Now (4.19) becomes
a1u1+···+apup= 0, but since u1, . . . ,upare linearly independent we obtain a1=···=ap= 0.
Hence all coefficients in (4.19) are zero and we conclude that the set Bin(4.18) is a linearly
independent set.
We have now shown that Bis a basis for V, from which is follows that
dim(V) =|B|=p+q= nullity( L) + rank( L)
and the proof is done. ■
Notice that the rank-nullity theorem we have just proved holds even in the case when Wis
an infinite-dimensional vector space!138
Example 4.38. Recall the mapping T:Fn×n→Fn×ngiven by
T(A) =A−A⊤
2.
in Example 4.17. We found that Nul(T) =Symn(F) in part (b) of the example, and so by
Theorem 4.37 we obtain
dim(Img( T)) = dim( Fn×n)−dim(Nul( T)) =n2−n(n+ 1)
2=n(n−1)
2,
recalling from Example 3.48 that dim(Fn×n) =n2. We see that, with Theorem 4.37 in hand,
the determination of dim(Img(T)) does not depend on knowing that Img(T) =Skw n(F). Once
the dimensions of a linear mapping’s domain and null space are known, the dimension of the
image follows immediately. ■
Example 4.39. Determine the dimension of the subspace UofRngiven by
U={x∈Rn:a·x= 0},
where n≥1 and a̸=0.
Solution. Define the mapping L:Rn→Rby
L(x) =a·x,
which is easily verified to be linear using properties of the Euclidean dot product established in
§1.4: for any x= [x1, . . . , x n] and y= [y1, . . . , y n] inRnandc∈Rwe have
L(x+y) =a·(x+y) =a·x+a·y=L(x) +L(y)
and
L(cx) =a·(cx) =c(a·x) =cL(x).
Moreover,
Nul(L) ={x∈Rn:L(x) = 0}={x∈Rn:a·x= 0}=U.
Now, Img(L) is a subspace of Rby Proposition 4.14. Since dim(R) = 1, by Theorem 3.56(2)
dim(Img(L)) is either 0 or 1. But dim(Img(L)) = 0 if and only if Img(L) ={0}, which cannot
be the case since a̸=0implies that
L(a) =a·a=∥a∥2̸= 0,
and therefore dim(Img(L)) = 1. (By Theorem 3.56(3) it further follows that Img(L) =R
since dim(Img(L)) = dim(R), but we do not need this fact.) Recalling that dim(Rn) =nand
Nul(L) =U, by Theorem 4.37 we have
n= dim( Rn) = dim(Nul( L)) + dim(Img( L)) = dim( U) + 1,
and hence dim( U) =n−1. That is, Uis a hyperplane in Rn. ■
Theorem 4.40 (Rank-Nullity Theorem for Matrices ).IfA∈Fm×n, then
rank(A) + nullity( A) =n.139
Proof. Suppose that A∈Fm×n. Let L:Fn→Fmbe given by L(x) =Ax. Then Lis a linear
mapping such that
Nul(L) ={x∈Fn:L(x) =0}={x∈Fn:Ax=0}= Nul( A).
Also by Proposition 4.35 we have
Img(L) = Col( A).
with respect to the standard bases. Now by the Rank-Nullity Theorem for Mappings we have
n= dim( Fn) = rank( L) + nullity( L) = dim(Img( L)) + dim(Nul( L))
= dim(Col( A)) + dim(Nul( A)) = rank( A) + nullity( A).
That is, rank( A) + nullity( A) =n, as desired. ■
Example 4.41. Find the dimension of the solution space Sfor the system of equations
4x1+ 7x2−πx3= 0
2x1−x2+x3= 0
and also find a basis for S.
Solution. Letting
x=
x1
x2
x3
and A=
4 7 −π
2−1 1
,
we find that Sis the set of all x∈R3that satisfy the matrix equation Ax=0, and so
S= Nul( A). By the Rank-Nullity Theorem for Matrices we have
dim(S) = nullity( A) = dim( R3)−rank(A) = 3−rank(A).
Since
A=
4 7 −π
2−1 1
−2r2+r1→r1− − − − − − − − →
0 9 −2−π
2−1 1
and the row rank of the matrix on the right is clearly 2, it follows that rank(A) = 2 and so
dim(S) = 3−2 = 1.
Next we set to the task of finding a basis for S. From the second equation in the system we
have
x2= 2x1+x3. (4.20)
Putting this into the first equation then yields
4x1+ 7(2 x1+x3)−πx3= 0,
and thus
x1=π−7
18x3. (4.21)
Substituting this into (4.20), we get
x2= 2π−7
18x3
+x3=π+ 2
9x3. (4.22)140
From (4.21) and (4.22), replacing x3with t, we find that
S=(
tπ−7
18,π+ 2
9,1⊤
:t∈R)
, (4.23)
which shows that
B=(π−7
18,π+ 2
9,1⊤)
would qualify as a basis for S. This is not the only possibility, however, since any nonzero element
ofSwill span S. For instance, if we set t= 18 we find from (4.23) that [ π−7,2π+ 4,18]⊤is in
S, and so
B=
[π−7,2π+ 4,18]⊤	
is a basis for S. ■
Example 4.42. Find the dimension of the subspace of R7consisting of all vectors that are
orthogonal to the vectors
r1= [1,1,−2,3,4,5,6]⊤and r2= [0,0,2,1,0,7,0]⊤,
Solution. The subspace of R7in question consists of the set of vectors
S={x∈R7:r1·x= 0 and r2·x= 0}.
Indeed, if we define A∈Fm×nby
A=
1 1−2 3 4 5 6
0 0 2 1 0 7 0
then we find that
S={x∈R7:Ax=0}= Nul( A).
By Theorem 4.40 we have
dim(S) = dim(Nul( A)) = 7 −rank(A).
Now, Ais already in row-echelon form, and so it should be clear that the row vectors of A,
which are r⊤
1andr⊤
2, are linearly independent. Thus rank(A) =row-rank (A) = 2, and therefore
dim(S) = 7−2 = 5. ■141
4.6 – Dimension and Rank Formulas
Proposition 4.43. IfUandWare subspaces of a vector space V, then
dim(U+W) = dim( U) + dim( W)−dim(U∩W).
Proof. Suppose that UandWare subspaces of a vector space V. The product space U×W
defined in section 3.1 is a vector space, and so we define a mapping L:U×W→Vby
L(u, w) =u−w. For any ( u,w),(u′,w′)∈U×Wandc∈Rwe have
L((u,w) + (u′,w′)) =L(u+u′,w+w′) = (u+u′)−(w+w′)
= (u−w) + (u′−w′) =L(u,w) +L(u′,w′)
and
L(c(u,w)) =L(cu, cw) =cu−cw=c(u−w) =cL(u,w),
soLis a linear mapping.
Ifv∈Img(L), then there exists some ( u,w)∈U×Wsuch that
L(u,w) =u−w=v,
sov=u+ (−w)∈U+Wand we have Img(L)⊆U+W. Ifv∈U+W, then v=u+wfor
some u∈Uandw∈W, and then
L(u,−w) =u−(−w) =u+w=v
shows v∈Img(L) and thus U+W⊆Img(L). Therefore Img( L) =U+W.
Letu∈Uandw∈W, and suppose ( u,w)∈Nul(L). Then L(u,w) =u−w=0, which
implies that w=uand thus ( u,w) = (u,u) with u∈U∩W. From this we conclude that
Nul(L)⊆ {(v,v) :v∈U∩W}, and since the reverse containment is easy to verify we obtain
Nul(L) ={(v,v) :v∈U∩W}. (4.24)
Let{v1, . . . ,vr}be a basis for U∩W. We wish to show the set
B={(vi,vi) : 1≤i≤r}
is a basis for Nul(L). Let ( u,w)∈Nul(L). By (4.24) , (u,w) = (v,v) for some v∈U∩W, and
since there exist scalars c1, . . . , c rsuch that v=c1v1+···+crvr, we find that
(u,v) = rX
i=1civi,rX
i=1civi!
=rX
i=1(civi, civi) =rX
i=1ci(vi,vi)
and thus
(u,v)∈Span{(vi,vi) : 1≤i≤r}= Span( B). (4.25)
On the other hand if we suppose that (4.25) is true, so that ( u,w) =Pr
i=1ci(vi,vi) for some
scalars c1, . . . , c r, then
L(u,w) =L rX
i=1ci(vi,vi)!
=rX
i=1ciL(vi,vi) =rX
i=1ci(vi−vi) =0142
demonstrates that ( u,w)∈Nul(L) and so
Nul(L) = Span {(vi,vi) : 1≤i≤r}= Span( B).
Next, set
rX
i=1ci(vi,vi) = (0,0).
Then
(0,0) =rX
i=1(civi, civi) = rX
i=1civi,rX
i=1civi!
,
which gives
rX
i=1civi=0
and hence c1=···=cr= 0 since v1, . . . ,vrare linearly independent. Therefore Bis a linearly
independent set and Span( B) = Nul( L), which shows that Bis a basis for Nul( L) and then
dim(Nul( L)) =|B|=r= dim( U∩W).
Because L:U×W→Vis a linear mapping,
dim(U×W) = dim(Nul( L)) + dim(Img( L))
by Theorem 4.37. But Img( L) =U+Wand dim(Nul( L)) = dim( U∩W), so that
dim(U×W) = dim( U∩W) + dim( U+W).
In§3.5 we established that dim( U×W) = dim( U) + dim( W), and thus
dim(U) + dim( W) = dim( U∩W) + dim( U+W)
obtains and the proof is done. ■
Recall the concept of a direct sum introduced in section §3.3. The dimension formula
furnished by Proposition 4.43 becomes especially nice if a vector space Vhappens to be the
direct sum of two subspaces UandW.
Proposition 4.44. LetVbe a vector space. If UandWare subspaces such that V=U⊕W,
then dim(V) = dim( U) + dim( W).
Proof. From U∩W={0}we have dim( U∩W) = 0, so that
dim(U+W) = dim( U) + dim( W)
by Proposition 4.43. The conclusion follows from U+W=V. ■
Theorem 4.45. LetVbe a vector space, and let U1, . . . , U nbe subspaces of V. Then
V=nM
k=1Uk⇒dim(V) =nX
k=1dim(Uk).143
Proof. The statement of the proposition is trivially true when n= 1. Let n∈Nbe arbitrary,
and suppose the proposition is true for n. Let U1, . . . , U n+1be subspaces of a vector space V
such that
V=n+1M
k=1Uk.
Define U=U1+···+UnandW=Un+1, so that V=U+W. Note that Uis a subspace of V
by Proposition 3.20. By Definition 3.21 it is immediate that
U∩W=Un+1∩nX
k=1Uk={0},
and so in fact V=U⊕W.
Letv∈U, so that for 1 ≤k≤nthere exist vectors uk∈Uksuch that
nX
k=1uk=v.
Suppose that for 1 ≤k≤nthe vectors u′
k∈Ukare such that
nX
k=1u′
k=v
also. Setting un+1=u′
n+1=0, we obtain
v=n+1X
k=1uk=n+1X
k=1u′
k∈V=n+1M
k=1Uk,
and so by Theorem 3.23 we must have uk=u′
kfor all 1 ≤k≤n+ 1. Since v∈Uis arbitrary,
we conclude that for each v∈Uthere exist unique vectors u1∈U1, . . . ,un∈Unsuch that
v=u1+···+un, and therefore
U=nM
k=1Uk
by Theorem 3.23. Now, by Proposition 4.44 and our inductive hypothesis,
dim(V) = dim( U) + dim( W) =nX
k=1dim(Uk) + dim( Un+1) =n+1X
k=1dim(Uk)
as desired. ■
Proposition 4.46. IfVis a subspace of Rn, then dim(V) + dim( V⊥) =n.
Proof. Suppose that Vis a subspace of Rn. Setting r= dim( V), so that r≤n, let
BV={b1, . . . ,br}
be a basis for V, where
bi=
bi1...
bin
144
for each 1 ≤i≤r. Let Abe the n×nmatrix given by
A=
b⊤
1...
b⊤
r
0
...
0
=
b11··· b1n.........
br1··· brn
0··· 0
.........
0··· 0

and observe that
Row(A) = Span {b1, . . . ,br,0}= Span {b1, . . . ,br}=V.
Now, define L:Rn→Rnto be the linear mapping given by L(x) =Axfor all x∈Rn. Since
Img(L) = Col( A) by Proposition 4.35, we have
dim(Img( L)) = dim(Col( A)) = rank( A) = dim(Row( A)) = dim( V).
Suppose x∈Nul(L), so that Ax=0and we obtain b⊤
ix= 0 for all 1 ≤i≤r. Let v∈V.
Then v=a1b1+···+arbrfor some a1, . . . , a r∈R, and since
x·v=v⊤x= (a1b⊤
1+···+arb⊤
r)x=a1b⊤
1x+···+arb⊤
rx=a1(0) +···+an(0) = 0
we conclude that x∈V⊥and so Nul( L)⊆V⊥.
Now suppose that x∈V⊥. Then x·v= 0 for all v∈V, and in particular x·bi= 0 for
each 1 ≤i≤r. Thus
L(x) =Ax=
b⊤
1x
...
b⊤
rx
0
...
0
=
x·b1...
x·br
0
...
0
=
0
...
0
=0,
which shows that x∈Nul(L) and so V⊥⊆Nul(L).
We now have Nul( L) =V⊥, and so of course dim(Nul( L)) = dim( V⊥). By Theorem 4.37
dim(Rn) = dim(Nul( L)) + dim(Img( L)),
and from this we obtain n= dim( V⊥) + dim( V). ■
For the remainder of this section we develop a few formulas involving the ranks of matrices
that will be useful later on.
Theorem 4.47.
1.IfA∈Fn×nis invertible, then rank(A) = rank( A−1).
2.IfA∈Fm×mis invertible and B∈Fm×n, then rank(AB) = rank( B).
3.IfB∈Fn×nis invertible and A∈Fm×n, then rank(AB) = rank( A).
4.IfA,C∈Fn×nare invertible and B∈Fn×n, then rank(ABC ) = rank( B).145
Proof.
Proof of Part (1). Suppose A∈Fn×nis invertible. Since A−1is also invertible, both AandA−1
are row-equivalent to Inby Theorem 2.30, and then by Theorem 3.66 we have rank(A) =rank(In)
and rank( A−1) = rank( In). Therefore rank( A) = rank( A−1) =n.
Proof of Part (2). Suppose A∈Fm×mis invertible and B∈Fm×n. By the Rank-Nullity
Theorem for Matrices,
rank(AB) + nullity( AB) =nand rank( B) + nullity( B) =n,
and hence
rank(AB) + nullity( AB) = rank( B) + nullity( B). (4.26)
Now, since Ais invertible,
A(Bx) =0⇒A−1[A(Bx)] =A−10⇒Bx=0,
and so
x∈Nul(B)⇔Bx=0⇔A(Bx) =0⇔(AB)x=0⇔x∈Nul(AB),
Hence Nul(B) =Nul(AB), so that nullity (B) =nullity (AB), and then (4.26) gives rank(AB) =
rank(B).
Proof of Part (3). Suppose B∈Fn×nis invertible and A∈Fm×n. Since B⊤is invertible by
Proposition 2.32, we use Problem 3.8.2 and Part (2) to obtain
rank(AB) = rank 
(AB)⊤
= rank( B⊤A⊤) = rank( A⊤) = rank( A).
Proof of Part (4). Suppose A,C∈Fn×nare invertible and B∈Fn×n. We have
rank(ABC ) = rank 
A(BC)
= rank( BC) = rank( B),
where the second equality follows from Part (2), and the third equality follows from Part (3). ■146
4.7 – Compositions of Mappings
Definition 4.48. Given mappings S:X→YandT:Y→Z, thecomposition ofTwithS
is the mapping T◦S:X→Zgiven by
(T◦S)(x) =T(S(x))
for all x∈X.
The composition operation ◦is not commutative in general (i.e. T◦Sis generally not the
same function as S◦T), but it does have associative and distributive properties as the next two
theorems establish.
Theorem 4.49. LetX1,X2,X3,X4be sets. If T1:X1→X2,T2:X2→X3, and T3:X3→X4
are mappings, then
T3◦(T2◦T1) = (T3◦T2)◦T1.
Proof. For any x∈X1,
(T3◦(T2◦T1))(x) =T3((T2◦T1)(x)) =T3(T2(T1(x)))
= (T3◦T2)(T1(x)) = (( T3◦T2)◦T1)(x).
Therefore T3◦(T2◦T1) = (T3◦T2)◦T1. ■
Given mappings
T1:X1→X2, T 2:X2→X3, T 3:X3→X4,
it is routine to write the composition as simply T3◦T2◦T1without fear of ambiguity. Whether
we interpret T3◦T2◦T1as signifying T3◦(T2◦T1) or (T3◦T2)◦T1makes no difference according to
Theorem 4.49. This idea extends naturally to the composition of any finite number of mappings.
Theorem 4.50. LetV1,V2,V3be vector spaces over F. Let S1, S2:V1→V2andT1, T2:V2→V3
be mappings, and let c∈F. Then
1. (T1±T2)◦S1=T1◦S1±T2◦S1
2.T1◦(S1±S2) =T1◦S1±T1◦S2ifT1is linear.
3. (cT1)◦S1=c(T1◦S1)
4.T1◦(cS1) =c(T1◦S1)ifT1is linear.
Proof.
Proof of Part (1). For any u∈V1
((T1+T2)◦S1)(u) = (T1+T2)(S1(u)) =T1(S1(u)) +T2(S1(u))
= (T1◦S1)(u) + (T2◦S1)(u) = (T1◦S1+T2◦S1)(u),
and therefore ( T1+T2)◦S1=T1◦S1+T2◦S1. The proof that ( T1−T2)◦S1=T1◦S1−T2◦S1
is similar.147
Proof of Part (2). For any u∈V1
(T1◦(S1+S2))(u) =T1((S1+S2)(u)) =T1(S1(u)) +S2(u))
=T1(S1(u)) +T1(S2(u)) = ( T1◦S1)(u) + (T1◦S2)(u)
= (T1◦S1+T1◦S2)(u),
where the third equality obtains from the linearity of T1. Therefore
T1◦(S1+S2) =T1◦S1+T1◦S2
ifT1is linear. The proof that T1◦(S1−S2) =T1◦S1−T1◦S2ifT1is linear is similar.
Proof of Part (3). For any u∈V1
((cT1)◦S1)(u) = (cT1)(S1(u)) =cT1(S1(u)) =c(T1◦S1)(u),
and therefore ( cT1)◦S1=c(T1◦S1).
Proof of Part (4). Suppose that T1is a linear mapping. For any u∈V1
(T1◦(cS1))(u) =T1((cS1)(u)) =T1(cS1(u)) =cT1(S1(u)) =c(T1◦S1)(u),
where the third equality obtains from the linearity of T1. Therefore T1◦(cS1) =c(T1◦S1) ifT1
is linear. ■
Proposition 4.51. LetV1,V2,V3be vector spaces over F. IfL1:V1→V2andL2:V2→V3
are linear mappings, then the composition L2◦L1:V1→V3is linear.
Proof. For any u,v∈V1we have
(L2◦L1)(u+v) =L2(L1(u+v)) =L2(L1(u) +L1(v))
=L2(L1(u)) +L2(L1(v)) = ( L2◦L1)(u) + (L2◦L1)(v),
and for any c∈Fandu∈V1we have
(L2◦L1)(cu) =L2(L1(cu)) =L2(cL1(u)) =cL2(L1(u)) =c(L2◦L1)(u).
Therefore L2◦L1is linear. ■
IfL:V→Vis a linear operator on a vector space V, then L◦Lis likewise a linear operator
onV, as is L◦L◦Land so on. A useful notation is to let L2denote L◦L,L3denote L◦L◦L,
and in general
Ln=L◦L◦ ··· ◦ L|{z }
n L’s
for any n∈N. We also define L0=IV, the identity operator on V.
A linear operator Π : V→Vfor which Π2= Π is called a projection and is of special
theoretical importance. We have
Π(Π(v)) = (Π ◦Π)(v) = Π2(v) = Π( v)148
for any v∈V.
Example 4.52. LetVbe a vector space, and let Π : V→Vbe a projection.
(a) Show that V= Nul(Π) + Img(Π).
(b) Show that Nul(Π) ∩Img(Π) = {0}.
Therefore V= Nul(Π) ⊕Img(Π).
Solution.
(a)Letv∈V, and let IV:V→Vbe the identity operator on Vso that IV(v) =v. By
Theorem 4.50(2) we have
Π(v−Π(v)) = Π( IV(v)−Π(v)) = Π(( IV−Π)(v)) = (Π ◦(IV−Π))(v)
= (Π◦IV−Π◦Π)(v) = (Π ◦IV)(v)−(Π◦Π)(v)
= Π( IV(v))−Π2(v) = Π( v)−Π(v) =0,
and so v−Π(v)∈Nul(Π). Noting that Π( v)∈Img(Π), we readily obtain
v= (v−Π(v)) + Π( v)∈Nul(Π) + Img(Π) .
Thus V⊆Nul(Π) + Img(Π), and since the reverse containment follows from the closure
properties of a vector space, we conclude that V= Nul(Π) + Img(Π).
(b)Letv∈Nul(Π)∩Img(Π). Then Π( v) =0and there exists some u∈Vsuch that Π( u) =v.
With these results and the hypothesis Π2= Π, we have
0= Π(v) = Π(Π( u)) = Π2(u) = Π( u) =v,
implying v∈ {0}and so Nul(Π)∩Img(Π)⊆ {0}. The reverse containment holds
since Nul(Π) and Img(Π) are subspaces of Vand so must both contain 0. Therefore
Nul(Π) ∩Img(Π) = {0}. ■
We found in §4.4 (Theorem 4.24) that every linear mapping L:V→Whas a unique
corresponding matrix [ L]BCwith respect to chosen bases BandCfor the vector spaces Vand
W, respectively. Let U,V, and Wbe vector spaces with bases A,B, andC, respectively. Let
L1:U→Vhave corresponding matrix [ L1]ABwith respect to AandB, and let L2:V→W
have corresponding matrix [ L2]BCwith respect to BandC, so that
[L1(u)]B= [L1]AB[u]Aand [ L2(v)]C= [L2]BC[v]B.
Thus for any u∈Uwe have
[(L2◦L1)(u)]C= [L2(L1(u))]C= [L2]BC[L1(u)]B= [L2]BC[L1]AB[u]A
Thus we see that the matrix Acorresponding to L2◦L1:U→Wwith respect to AandCis
given by A= [L2]BC[L1]AB. That is,
[L2◦L1]AC= [L2]BC[L1]AB
and we have proven the following.149
Proposition 4.53. LetL1:V1→V2andL2:V2→V3be linear mappings, and let Bibe a basis
forVi. Then
[L2◦L1]B1B3= [L2]B2B3[L1]B1B2.150
4.8 – The Inverse of a Mapping
Definition 4.54. LetT:X→Ybe a mapping. We say Tisinvertible if there exists a
mapping S:Y→Xsuch that S◦T=IXandT◦S=IY, in which case Sis called the inverse
ofTand we write S=T−1.
Proposition 4.55. IfT:X→Yis an invertible mapping, then
Img(T−1) = Dom( T) =X and Dom( T−1) = Img( T) =Y,
and for all x∈X,y∈Y,
T(x) =y⇔T−1(y) =x.
Proof. Suppose that T:X→Yis invertible, so that there is a mapping T−1:Y→Xsuch
thatT−1◦T=IXandT◦T−1=IY. From this it is immediate that
Img(T−1)⊆X= Dom( T) and Img( T)⊆Y= Dom( T−1).
Letx∈X, so that T(x) =yfor some y∈Y. Then
T−1(y) =T−1(T(x)) = ( T−1◦T)(x) =IX(x) =x
shows that x∈Img(T−1), and so Img( T−1) =Xand
T(x) =y⇒T−1(y) =x
for all x∈X.
Next, for any y∈Ywe have T−1(y) =xfor some x∈X, whence
T(x) =T(T−1(y)) = ( T◦T−1)(y) =IY(y) =y
shows that y∈Img(T), and so Img( T) =Yand
T−1(y) =x⇒T(x) =y
for all y∈Y. ■
Proposition 4.56. IfS:X→YandT:Y→Zare invertible mappings, then
(T◦S)−1=S−1◦T−1.
Proof. Suppose that S:X→YandT:Y→Zare invertible mappings. Then SandTare
bijective, from which it follows that T◦Sis likewise bijective and so ( T◦S)−1:Z→Xexists.
That is, T◦Sis invertible.
Letz∈Z. Then ( T◦S)−1(z) =xfor some x∈X, and by repeated use of Proposition 4.55
we obtain
(T◦S)−1(z) =x⇔(T◦S)(x) =z⇔T(S(x)) =z
⇔S(x) =T−1(z)⇔x=S−1(T−1(z)).
⇔(S−1◦T−1)(z) =x151
Hence
(T◦S)−1(z) = (S−1◦T−1)(z)
for all z∈Z, and we conclude that ( T◦S)−1=S−1◦T−1. ■
Proposition 4.57. LetVandWbe vector spaces over F. IfL:V→Wis an invertible linear
mapping, then its inverse L−1:W→Vis also linear.
Proof. Suppose that L:V→Wis an invertible linear mapping, and let L−1:W→Vbe its
inverse. Let w1,w2∈W. Then L−1(w1) and L−1(w2) are vectors in V, and by the linearity of
Lwe obtain
L(L−1(w1) +L−1(w2)) =L(L−1(w1)) +L(L−1(w2))
= (L◦L−1)(w1) + (L◦L−1)(w2)
=IW(w1) +IW(w2) =w1+w2,
and hence
L−1(w1+w2) =L−1(w1) +L−1(w2)
by Proposition 4.55.
Next, let w∈Wandc∈F. Then cL−1(w) is a vector in V, and from
L(cL−1(w)) =cL(L−1(w)) =c(L◦L−1)(w) =cIW(w) =cw
we obtain
L−1(cw) =cL−1(w)
by Proposition 4.55. ■
There is a close connection between the idea of an invertible linear mapping and that of an
invertible matrix which the following theorem makes clear.
Theorem 4.58. LetVandWbe vector spaces with ordered bases BandC, respectively, and
suppose that dim(V) =dim(W) =nandL∈ L(V, W ). Then Lis invertible if and only if [L]BC
is invertible, in which case
[L]−1
BC= [L−1]CB.
Proof. Suppose that Lis invertible. Then there exists a mapping L−1:W→Vsuch that
L−1◦L=IVandL◦L−1=IW, and since L−1is linear by Proposition 4.57 it has a corresponding
matrix [ L−1]CB∈Fn×nwith respect to the bases CandB. For all v∈Vwe have
[L(v)]C= [L]BC[v]B,
and for all w∈W
[L−1(w)]B= [L−1]CB[w]C.
Now, for all w∈W,
 
[L]BC[L−1]CB
[w]C= [L]BC 
[L−1]CB[w]C
= [L]BC[L−1(w)]B
= [L(L−1(w))]C= [(L◦L−1)(w)]C= [IW(w)]C= [w]C,152
which shows that [ L]BC[L−1]CB=Inby Proposition 2.12(1). Similarly, for all v∈V,
 
[L−1]CB[L]BC
[v]B= [L−1]CB 
[L]BC[v]B
= [L−1]CB[L(v)]C
= [L−1(L(v))]B= [(L−1◦L)(v)]B= [IV(v)]B= [v]B,
and so [ L−1]CB[L]BC=In. Thus [ L−1]CBis the inverse for [ L]BC, which is to say [ L]BCis invertible
and
[L]−1
BC= [L−1]CB.
For the converse, suppose that [ L]BCis invertible. Then there exists a matrix [ L]−1
BC∈Fn×n
such that
[L]BC[L]−1
BC= [L]−1
BC[L]BC=In.
Let Λ : W→Vbe the linear mapping with corresponding matrix [ L]−1
BCwith respect to Cand
B, so that
[Λ(w)]B= [L]−1
BC[w]C
for each w∈W. For each w∈Wwe have
[(L◦Λ)(w)]C= [L(Λ(w))]C= [L]BC[Λ(w)]B= [L]BC[L]−1
BC[w]C= [w]C,
and since the coordinate map w7→[w]Cis an isomorphism—and hence injective—by Theorem
4.11, it follows that ( L◦Λ)(w) =w. Next, for each v∈Vwe have
[(Λ◦L)(v)]B= [Λ( L(v))]B= [L]−1
BC[L(v)]C= [L]−1
BC[L]BC[v]B= [v]B,
and since the coordinate map v7→[v]Bis an isomorphism it follows that (Λ ◦L)(v) =v. Since
L◦Λ =IWand Λ ◦L=IV, we conclude that Λ is the inverse of L, and therefore Lis invertible.
Finally, since Λ = L−1and [Λ] CB= [L]−1
BC, we find that
[L]−1
BC= [L−1]CB
once again. ■
The result [ L−1]CB= [L]−1
BCgiven in the theorem reduces the task of finding the inverse of
an invertible linear mapping L∈ L(V, W ) to an exercise in finding the inverse of the matrix
corresponding to Lwith respect to BandC. Indeed, once a linear mapping’s corresponding
matrix is known, the mapping itself is effectively known.
Corollary 4.59. LetVbe a vector space with ordered basis B, and let L∈ L(V). Then Lis
invertible if and only if [L]Bis invertible, in which case
[L]−1
B= [L−1]B.
Theorem 4.60. A mapping T:X→Yis invertible if and only if it is a bijection.
Theorem 4.61. LetVandWbe finite-dimensional vector spaces such that dim(V) =dim(W),
and let L:V→Wbe a linear mapping.
1.IfLis injective, then Lis invertible.
2.IfLis surjective, then Lis invertible.153
Proof.
Proof of Part (1). Suppose that Lis injective. By Proposition 4.15 Nul( L) ={0}, and so
dim(W) = dim( V) = dim(Nul( L)) + dim(Img( L)) = 0 + dim(Img( L)) = dim(Img( L))
by the Rank-Nullity Theorem for Mappings. Now, since Img(L) is a subspace of Wand
dim(Img(L)) = dim(W), by Theorem 3.56(3) Img(L) =Wand so Lis surjective. Since Lis
injective and surjective, it follows by Theorem 4.60 that Lis invertible.
Proof of Part(2). Suppose that Lis surjective, so that Img(L) =W. By the Rank-Nullity
Theorem for Mappings
dim(V) = dim(Nul( L)) + dim(Img( L)) = dim(Nul( L)) + dim( W) = dim(Nul( L)) + dim( V),
whence dim(Nul(L)) = 0 and so Nul(L) ={0}. Now, by Proposition 4.15 we conclude that Lis
injective, and therefore Lis invertible by Theorem 4.60. ■
Proposition 4.62. Leta1,a2, . . . ,an∈Fn. The n×nmatrix
A=a1a2···an
is invertible if and only if a1,a2, . . . ,anare linearly independent.
Proof. Suppose that Ais invertible. Let L:Fn→Fnbe the linear mapping with associated
matrix A, so that L(x) =Axfor all x∈Fn. Then Lis invertible by Theorem 4.58, and so by
Theorem 4.60 Lis bijective and we have Img(L) =Fn. But by Proposition 4.35 we also have
Img(L) = Col( A) = Span {a1, . . . ,an}, whence
dim(Span {a1, . . . ,an}) = dim(Img( L)) = dim( Fn).
Since Span{a1, . . . ,an}is a subspace of Fnwith dimension equal to dim(Fn), by Theorem 3.56(3)
we conclude that Span{a1, . . . ,an}=Fn, and thus {a1, . . . ,an}is a basis for Fnby Theorem
3.54(2). That is, the vectors a1, . . . ,anare linearly independent.
Next, suppose that a1, . . . ,anare linearly independent. Then {a1, . . . ,an}is a basis for Fn
by Theorem 3.54(1), so that Span{a1, . . . ,an}=Fn. Let L:Fn→Fnbe the linear mapping
given by L(x) =Axfor all x∈Fn. By Proposition 4.35
Img(L) = Col( A) = Span {a1, . . . ,an}=Fn,
and thus Lis surjective and it follows by Theorem 4.61(2) that Lis invertible. Therefore Ais
invertible by Theorem 4.58. ■
We can employ Proposition 4.62 to show that a change of basis matrix is always invertible—a
fact already established in §4.5 by quite different means. Let B= (v1, . . . ,vn) andB′be ordered
bases for a vector space V. By Theorem 4.27 the change of basis matrix IBB′is given by
IBB′=h
[v1]B′···[vn]B′i
=h
φB′(v1)···φB′(vn)i
.
The coordinate map φB′:V→Fnis an isomorphism by Theorem 4.11, and so in particular
is an injective linear mapping. Thus Nul(φB′) ={0}by Proposition 4.15, and since the basis
vectors v1, . . . ,vnare linearly independent, it follows by Proposition 4.16 that the column154
vectors φB′(v1), . . . , φ B′(vn) ofIBB′are likewise linearly independent. Therefore IBB′is invertible
by Proposition 4.62.
We finish this section with a theorem that establishes that, in a certain sense, there is only
“one kind” of vector space for each dimension value n≥0.
Theorem 4.63. LetVandWbe finite-dimensional vector spaces. Then V∼=Wif and only if
dim(V) = dim( W).
Proof. Suppose that V∼=W, so there exists an isomorphism L:V→W. Since Lis injective,
Nul(L) ={0}by Proposition 4.15, and then
nullity( L) = dim(Nul( L)) = 0 .
Since Lis surjective, Img( L) =W, and then
rank( L) = dim(Img( L)) = dim( W).
Now, by the Rank-Nullity Theorem for Mappings,
dim(V) = rank( L) + nullity( L) = dim( W) + 0 = dim( W)
as desired.
Now suppose that dim(V) =dim(W) =n. LetBbe a basis for VandCa basis for W. By
Theorem 4.11 the coordinate maps φB:V→FnandφC:W→Fnare isomorphisms. Since φC
is a bijection, by Theorem 4.60 it is invertible, with the inverse φ−1
C:Fn→Wbeing a linear
mapping by Proposition 4.57. Of course φ−1
Cis itself invertible with inverse φC, so that Theorem
4.60 implies that φ−1
Cis bijective and hence an isomorphism. Now, by Proposition 4.51 the
composition φ−1
C◦φB:V→Wis a linear mapping that is easily verified to be an isomorphism,
and therefore V∼=W. ■
Example 4.64. Given vector spaces VandWoverF, with dim(V) =nanddim(W) =m, by
Theorem 4.24 we found that L(V, W )∼=Fm×n. Therefore
dim 
L(V, W )
= dim 
Fm×n
=mn
by Theorem 4.63. ■155
4.9 – Properties of Invertible Operators and Matrices
Linear operators play a central role in the more advanced developments of linear algebra,
and so it will be convenient to collect some of their most important general properties into a
single theorem.
Theorem 4.65 (Invertible Operator Theorem ).LetVbe a finite-dimensional vector space,
and let L∈ L(V). Then the following statements are equivalent.
1.Lis invertible.
2.Lis an isomorphism.
3.Lis injective.
4.Lis surjective.
5. Nul( L) ={0}.
6. [L]Bis invertible for any basis B.
7. [L]Bis invertible for some basis B.
Proof.
(1)⇒(2):IfLis invertible, then Lis bijective by Theorem 4.60, and hence Lis an isomorphism
by Definition 4.10.
(2)⇒(3):IfLis an isomorphism, then of course it must be injective.
(3)⇒(4):IfL:V→Vis injective, then Lis invertible by Theorem 4.61(1). By Theorem 4.60
it follows that Lis bijective, and therefore Lis surjective.
(4)⇒(5):IfL:V→Vis surjective, then Lis invertible by Theorem 4.61(2). By Theorem 4.60
it follows that Lis bijective, which implies that Lis injective. We conclude that Nul(L) ={0}
by Proposition 4.15.
(5)⇒(6): Suppose that Nul(L) ={0}, and let Bbe any basis for V. Now, Lis injective by
Proposition 4.15, and hence must be invertible by Theorem 4.61(1). The invertibility of [ L]B
now follows from Corollary 4.59.
(6)⇒(7):This is trivial.
(7)⇒(1):If [L]Bis invertible for some basis B, then Lis invertible by Corollary 4.59. ■
The following proposition will be improved on in the next chapter, at which point it will be
promoted to a theorem.
Proposition 4.66 (Invertible Matrix Proposition ).LetA∈Fn×n, and let LAbe the linear
operator on Fnhaving corresponding matrix Awith respect to the standard basis EofFn. Then
the following statements are equivalent.
1.Ais invertible.
2.A⊤is invertible.156
3.Ais row-equivalent to In.
4.The row vectors of Aare linearly independent.
5.Ais column-equivalent to In.
6.The column vectors of Aare linearly independent.
7. col-rank( A) =n.
8. row-rank( A) =n.
9. rank( A) =n.
10.The system Ax=bhas a unique solution for each b∈Fn.
11.The system Ax=0has only the trivial solution.
12. Nul( A) ={0}.
13.LA∈ L(Fn)is invertible.
Proof.
(1)⇒(2):This follows immediately from Proposition 2.32.
(2)⇒(3): Suppose A⊤is invertible. Then by Proposition 2.32 ( A⊤)⊤is invertible, where of
course ( A⊤)⊤=A. Now, by Theorem 2.30 the invertibility of Aimplies that Ais row-equivalent
toIn.
(3)⇒(4): Suppose that Ais row-equivalent to In. Then Ais invertible by Theorem 2.30, so
by Proposition 2.32 A⊤is invertible, and then by Proposition 4.62 the column vectors of A⊤
are linearly independent. Since the row vectors of Aare the column vectors of A⊤, we conclude
that the row vectors of Aare linearly independent.
(4)⇒(5): Suppose the row vectors of Aare linearly independent. Then the column vectors
ofA⊤are linearly independent, whereupon Proposition 4.62 implies that A⊤is invertible.
By Theorem 2.30 A⊤is row-equivalent to In, which is to say there exist elementary matrices
M1, . . . ,Mksuch that
Mk···M2M1A⊤=In,
where each left-multiplication by Miis an elementary row operation by Definition 2.15. Taking
the transpose of each side then yields
AM⊤
1M⊤
2···M⊤
k=In,
where each right-multiplication by M⊤
iis an elementary column operation by Definition 2.15.
Therefore Ais column-equivalent to In.
(5)⇒(6):Suppose Ais column-equivalent to In. Then
col-rank( A) = rank( A) = rank( In) =n
by the definition of rank and Theorem 3.66, which implies that the ncolumn vectors of Aare
linearly independent.
(6)⇒(7): Suppose the column vectors of Aare linearly independent. There are ncolumn
vectors, so col-rank( A) =n.157
(7)⇒(8):Suppose col-rank( A) =n. Then row-rank( A) =nby Theorem 3.64.
(8)⇒(9):Suppose row-rank( A) =n. By definition rank( A) = row-rank( A) =n.
(9)⇒(10): Suppose that rank(A) =n. Then col-rank (A) =n, which is to say the dimension
of the span of the column vectors of Aisn. Since Ahasncolumn vectors in all, it follows that
the column vectors of Aare linearly independent, and so by Proposition 4.62 Ais invertible.
Thus A−1exists. Let b∈Fnbe arbitrary. Then A−1bis a solution to the system, for when we
substitute A−1bforxin the equation Ax=b, we obtain
A(A−1b) = (AA−1)b=Inb=b.
This proves the existence of a solution. As for uniqueness, suppose x1andx2are solutions to
the system, so that Ax 1=bandAx 2=b. Now, for i∈ {1,2},
Axi=b⇒A−1(Axi) =A−1b⇒(A−1A)xi=A−1b⇒xi=A−1b.
That is, x1=x2=A−1b, which proves the uniqueness of a solution.
(10)⇒(11): Suppose that the system Ax=bhas a unique solution for each b∈Fn. Then if
we choose b=0, it follows that the system Ax=0has a unique solution, and clearly that
solution must be the trivial solution 0.
(11)⇒(12): IfAx=0admits only the trivial solution, then
Nul(A) ={x∈Fn:Ax=0}={0}
obtains immediately.
(12)⇒(13): Suppose Nul( A) ={0}, and suppose x∈Fnis such that LA(x) =0. Since
LA(x) =0⇒Ax=0⇒x∈Nul(A)⇒x=0,
it follows that Nul(LA) ={0}. Therefore LAmust be invertible by the Invertible Operator
Theorem.
(13)⇒(1):Suppose that LA∈ L(Fn) is invertible. Then [ LA]Eis invertible by Corollary 4.59,
and since [ LA]E=Awe conclude that Ais invertible. ■
With the help of the Invertible Matrix Proposition we now prove that any square matrix
with either a left-inverse or a right-inverse must be invertible,
Proposition 4.67. LetA∈Fn×n. Then the following statements are equivalent:
1.Ais invertible.
2.There exists some D∈Fn×nsuch that AD=In.
3.There exists some C∈Fn×nsuch that CA=In.158
Proof.
(1)⇒(2): Suppose that Ais invertible. Then by definition there exists some D∈Fn×nsuch
thatAD=DA=In.
(2)⇒(1):Suppose that
D=d1···dn
∈Fn×n
is such that AD=In. Ifa1, . . . ,an∈Fnare such that a⊤
1, . . . ,a⊤
nare the row vectors for A,
then we have
a⊤
1...
a⊤
n
d1···dn
=In,
and thus
a⊤
idj=(
1,ifi=j
0,ifi̸=j(4.27)
Now, let
b=
b1...
bn
∈Fn
be arbitrary and consider the system Ax=b. Choose
x=nX
i=1bidi.
Then we obtain
Ax=
a⊤
1...
a⊤
n
x=
a⊤
1x
...
a⊤
nx
=
a⊤
1(Pn
i=1bidi)
...
a⊤
n(Pn
i=1bidi)
=
Pn
i=1bi(a⊤
1di)
...Pn
i=1bi(a⊤
ndi)
=
b1...
bn
=b,
where the penultimate equality follows from (4.27) . This shows that Ax=bhas a solution for
anyb∈Fn.
LetLA∈ L(Fn) be the linear operator with corresponding matrix Awith respect to the
standard basis E. For each b∈Fnthere exists some x∈Fnsuch that Ax=b, and hence
LA(x) =b. This shows that LAis surjective, so LAis invertible by the Invertible Operator
Theorem, and hence Ais invertible by the Invertible Matrix Proposition.
(1)⇒(3): Suppose that Ais invertible. Then by definition there exists some C∈Fn×nsuch
thatCA=AC=In.
(3)⇒(1):Suppose there exists some C∈Fn×nsuch that CA=In. Then Ais a right-inverse
forC, and by the equivalency of parts (1) and (2) it follows that Cis invertible. Thus C−1
exists (and is invertible), and since
CA=In⇒C−1(CA) =C−1In⇒(C−1C)A=C−1⇒A=C−1,159
we conclude that Ais invertible. ■
An immediate application of Proposition 4.67 provides something of a converse to Theorem
2.26.
Proposition 4.68. LetA,B∈Fn×n. IfABis invertible, then AandBare invertible.
Proof. Suppose that ABis invertible. Then there exists some D∈Fn×nsuch that ( AB)D=In,
and so by associativity of matrix multiplication we obtain A(BD) =In. Therefore Ais invertible
by Proposition 4.67.
Now, the invertibility of Ameans that A−1exists, and since A−1andABare invertible, by
Theorem 2.26 A−1(AB) is invertible. But
A−1(AB) = (A−1A)B=InB=B,
and therefore Bis invertible. ■
The following proposition (and its corollary) could have been proved at the end of the
previous chapter and has wide application in the calculus of manifolds, among other fields.
Proposition 4.69. ForA∈Fm×nlet1≤k < min{m, n}. Then there is an invertible
(k+ 1)×(k+ 1) submatrix of Aif and only if rank(A)≥k+ 1.
Proof. Suppose A= [a1···an] has an invertible ( k+ 1)×(k+ 1) submatrix. If the submatrix
is formed by the entries that are in rows i1, . . . , i k+1and columns j1, . . . , j k+1ofA, and we
designate the ordered index sets α= (i1, . . . , i k+1) and β= (j1, . . . , j k+1), then we may denote
the submatrix by A[α, β]. Let A[·, β] denote the m×(k+ 1) submatrix formed by the entries
in rows 1 , . . . , m (i.e. all the rows) and columns j1, . . . , j k+1, which is to say
A[·, β] =aj1···ajk+1
.
Then A[α, β] is a submatrix of A[·, β], and in particular the k+ 1 row vectors of A[α, β] are
row vectors of A[·, β]. Now, since A[α, β] is invertible, by the Invertible Matrix Proposition
we have rank(A[α, β]) =k+ 1. Since rank(A[α, β]) equals the dimension of the row space of
A[α, β], it follows that the k+ 1 row vectors of A[α, β] are linearly independent, and therefore
at least k+ 1 row vectors of A[·, β] are linearly independent. That is, the dimension of the row
space of A[·, β] is at least k+ 1, and then we find that
col-rank 
A[·, β]
= row-rank 
A[·, β]
≥k+ 1.
In fact, since A[·, β] has precisely k+ 1 column vectors we must have
col-rank 
A[·, β]
=k+ 1,
which is to say the k+ 1 column vectors of A[·, β] are linearly independent. However, the
column vectors of A[·, β] are also column vectors of Aitself, and so now we have
rank(A) = col-rank( A)≥k+ 1> k.160
For the converse, suppose that rank(A)> k. Then at least k+1 column vectors aj1, . . . ,ajk+1
ofAare linearly independent, and with βdefined as before we construct the n×(k+1) submatrix
A[·, β]. Since col-rank (A[·, β]) =k+ 1, it follows that row-rank (A[·, β]) =k+ 1 also. Thus
there are k+ 1 linearly independent row vectors in A[·, β], which we number i1, . . . , i k+1. With
αdefined as before, we obtain the ( k+ 1)×(k+ 1) submatrix A[α, β] that has k+ 1 linearly
independent row vectors. Now,
rank 
A[α, β]
= row-rank 
A[α, β]
=k+ 1,
and the Invertible Matrix Proposition implies that A[α, β] is invertible. ■
Applying Proposition 4.69 in the case when m=min{m, n}andk=m−1, then we conclude
that rank(A)≥miff some m×msubmatrix of Ais invertible, and thus (since the rank of a
matrix cannot exceed its smaller dimension) rank(A) =miff some m×msubmatrix of Ais
invertible. A similar conclusion obtains if n=min{m, n}. Defining a matrix A∈Fm×nto have
full rank ifrank(A) =min{m, n}(i.e.Ahas the greatest possible rank), we have proved the
following.
Corollary 4.70. ForA∈Fm×nletk=min{m, n}. Then Ahas full rank if and only if Ahas
an invertible k×ksubmatrix.
A good exercise is to prove Corollary 4.70 from established principles, and then use it to
prove Proposition 4.68. Is the argument any easier than that above?
Proposition 4.71. LetVandWbe finite-dimensional vector spaces over Fwith bases Band
C, letL∈ L(V, W )be a linear mapping, and let [L]be itsBC-matrix.
1.IfLis injective, then [L]has full rank.
2.If[L]has full rank and dim(V)≤dim(W), then Lis injective.
Proof.
Proof of Part (1). Setn=dim(V) and m=dim(W), so that [ L]∈Fm×n. Suppose that Lis
injective. Proposition 4.15 implies that Nul(L) ={0}, and thus Nul([L]) ={0}as well. This
gives nullity ([L]) = 0, and so rank([L]) =nby the Rank-Nullity Theorem for Matrices. Since n
is a dimension of [ L], it must in fact be the smaller dimension (see remark below) and so we
conclude that [ L] has full rank.
Proof of Part (2). For the converse, suppose that Lis not injective. Then Nul(L)̸={0}
implies Nul([L])̸={0}, so that nullity ([L])>0 and therefore rank([L])< nby the Rank-Nullity
Theorem for Matrices. If n=dim(V)≤dim(W) =m, then it follows that [ L] does not have
full rank and we are done. ■
Remark. In the proof of the first part of Proposition 4.71, note that L:V→L(V) is an
isomorphism, which is to say V∼=L(V), and so dim(L(V)) = dim(V) =nby Theorem 4.63.
ButL(V) is a vector subspace of W, and so n= dim( L(V))≤dim(W) =mby Theorem 3.56.
In short, if L∈ L(V, W ) is injective, then dim(V)≤dim(W). A similar truth, left as a problem,
states that if L∈ L(V, W ) is surjective then dim( V)≥dim(W).161
5
Determinants
5.1 – Determinants of Low Order
Definition 5.1. The1×1determinant function det1:F1×1→Fis given by
det1([a]) =a
for each [a]∈F1×1.
The2×2determinant function det2:F2×2→Fis given by
det2
a b
c d
=ad−bc.
Generally the scalar det n(A) is called the determinant of the matrix A, and may also be
denoted more simply by det( A) or|A|.
The 1 ×1 determinant function has little practical value and tends to arise only in inductive
arguments as in the proof of Theorem 5.4. The 2 ×2 determinant function, on the other
hand, is highly important, and so it will be the focus of study for the remainder of this section.
Henceforth we will denote det 2(A) simply as det( A).162
5.2 – Determinants of Arbitrary Order
The general definition we will give here for the determinant of an n×nmatrix is recursive
in nature. That is, for n≥2, the determinant of an n×nmatrix will be defined in terms of
determinants of ( n−1)×(n−1) matrices. Thus determinants of n×nmatrices are ultimately
defined in terms of determinants of 1 ×1 matrices, and since the determinant of a 1 ×1 matrix
is defined to equal the sole scalar entry of the matrix, we can see that the definition rests on a
firm foundation.
Before stating the definition a bit of notation needs to be established. If A= [aij] is an
n×nmatrix, then we define Aijto be the submatrix that results when the ith row and jth
column of Aare deleted. That is,
Aij=
a11··· a1(j−1) a1(j+1) ··· a1n..................
a(i−1)1··· a(i−1)(j−1)a(i−1)(j+1)··· a(i−1)n
a(i+1)1··· a(i+1)(j−1)a(i+1)(j+1)··· a(i+1)n..................
an1··· an(j−1) an(j+1) ··· ann
.
We now have what we need to give the general definition for the determinant function.
Definition 5.2. Letn≥2. The n×ndeterminant function detn:Fn×n→Fis given by
detn(A) =nX
j=1(−1)1+ja1jdetn−1(A1j) (5.1)
for each n×nmatrix Awith entries in F. The scalar detn(A)is called an n×ndeterminant .
As is our custom we will take the field Fto beRunless otherwise indicated. Often we will
write det n(A) as simply det( A). Other symbols for the determinant of Aare
|A|,det

a11···a1n.........
an1···ann

,anda11···a1n.........
an1···ann.
Example 5.3. Given that
A=
−2 3 −1
0 2 5
0−6 4
,
evaluate det( A).
Solution. We have
det(A) = (−1)1+1(−2)2 5
−6 4+ (−1)1+2(3)0 5
0 4+ (−1)1+3(−1)0 2
0−6
=−22 5
−6 4−30 5
0 4−0 2
0−6163
=−2[(2)(4) −(5)(−6)]−3[(0)(4) −(5)(0)] −[(0)(−6)−(2)(0)]
=−76,
using Definitions 5.2 and 5.1. ■
It is frequently convenient to regard detn:Fn×n→Fas being a function of the column
vectors of a matrix A∈Fn×n. Thus, if
A=a1···an
,
where aj∈Fnis a column vector for each 1 ≤j≤n, then we define
detn(a1, . . . ,an) = det n a1···an
so that in fact we have detn:Qn
j=1Fn→F. This leads to no ambiguity since there is a natural
isomorphism between the vector spaces
Fn×n=x1···xn
:xk∈Fnfor 1≤k≤n	
and
nY
j=1Fn={(x1, . . . ,xn) :xk∈Fnfor 1≤k≤n}
that enables us to identify, in particular, the column vectors of any matrix A= [aij]ninFn×n
with a unique n-tuple ( a1, . . . ,an) of vectors in Fn. We use this natural identification to express
certain properties of determinants.
Theorem 5.4. For all n∈N, the determinant function detn:Fn×n→Fhas the following
properties, where all vectors represent column vectors.
DP1. Multilinearity. For any 1≤j≤n, ifaj=u+vthen
detn(a1, . . . ,u+v, . . . ,an) = det n(a1, . . . ,u, . . . ,an) + det n(a1, . . . ,v, . . . ,an),
and if aj=xuthen
detn(a1, . . . , x u, . . . ,an) =xdetn(a1, . . . ,u, . . . ,an).
DP2. Alternating. For any 1≤j < k≤n,
detn(a1, . . . ,aj, . . . ,ak, . . . ,an) =−detn(a1, . . . ,ak
j, . . . ,aj
k, . . . ,an).
DP3. Normalization.
detn(In) = 1 .
DP4. IfA= [a1···an]withaj=akfor some j̸=k, then detn(A) = 0 .
DP5. For any x∈Fandj̸=k,
detn(a1, . . . ,aj, . . . ,an) = det n(a1, . . . ,aj+xak, . . . ,an).
DP6. For any 1≤j≤n, ifaj=0then
detn(a1, . . . ,0, . . . ,an) = 0 .164
Proof.
Proof of DP1. Given any u, v∈F, we have [ u+v]∈F1×1with
det([u+v]) =u+v= det([ u]) + det([ v])
by Definition 5.1. Thus DP1 holds in the case when n= 1. Suppose that DP1 holds for some
arbitrary n∈N. Let
A= [aij] = [a1···an+1]∈F(n+1)×(n+1),
fixk∈ {1, . . . , n + 1}, and suppose ak=u+v. For each 1 ≤j≤n+ 1 define
a′
j=
a2j...
a(n+1)j
∈Fn,
and also
u′=
u2...
un+1
and v′=
v2...
vn+1
.
By Definition 5.2
det(A) =n+1X
j=1(−1)1+ja1jdet(A1j) =n+1X
j=1(−1)1+ja1jdet(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,an+1),(5.2)
where it’s understood that
det(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,an+1) = det( a′
2, . . . ,an+1)
ifj= 1, and
det(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,an+1) = det( a′
1, . . . ,an)
ifj=n+ 1.
Now, if j < k , then
det(Aij) = det( a′
1, . . . ,a′
j−1,a′
j+1, . . . ,a′
k, . . . ,a′
n+1)
= det( a′
1, . . . ,a′
j−1,a′
j+1, . . . ,u′+v′, . . . ,a′
n+1)
= det( . . . ,a′
j−1,a′
j+1, . . . ,u′, . . .) + det( . . . ,a′
j−1,a′
j+1, . . . ,v′, . . .)
by the inductive hypothesis, since Aijis an n×nmatrix. Similarly, if j > k then
det(Aij) = det( a′
1, . . . ,a′
k, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1)
= det( a′
1, . . . ,u′+v′, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1)
= det( . . . ,u′, . . . ,a′
j−1,a′
j+1, . . .) + det( . . . ,v′, . . . ,a′
j−1,a′
j+1, . . .)
These results, together with equation (5.2), yields
det(A) =k−1X
j=1(−1)1+ja1jdet(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,u′+v′, . . . ,a′
n+1)165
+ (−1)1+ka1kdet(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1)
+n+1X
j=k+1(−1)1+ja1jdet(a′
1, . . . ,u′+v′, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1)
=k−1X
j=1(−1)1+ja1j[det(. . . ,a′
j−1,a′
j+1, . . . ,u′, . . .) + det( . . . ,a′
j−1,a′
j+1, . . . ,v′, . . .)]
+ (−1)1+k(u1+v1) det(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1)
+n+1X
j=k+1(−1)1+ja1j[det(. . . ,u′, . . . ,a′
j−1,a′
j+1, . . .) + det( . . . ,v′, . . . ,a′
j−1,a′
j+1, . . .)],
where we use the fact that a1k=u1+v1. Observing that
det(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1) = det( A1k),
we finally obtain
det(A) ="k−1X
j=1(−1)1+ja1jdet(. . . ,a′
j−1,a′
j+1, . . . ,u′, . . .) + (−1)1+ku1det(A1k)
+n+1X
j=k+1(−1)1+ja1jdet(. . . ,u′, . . . ,a′
j−1,a′
j+1, . . .)#
+"k−1X
j=1(−1)1+ja1jdet(. . . ,a′
j−1,a′
j+1, . . . ,v′, . . .) + (−1)1+kv1det(A1k)
+n+1X
j=k+1(−1)1+ja1jdet(. . . ,v′, . . . ,a′
j−1,a′
j+1, . . .)#
= det( a1, . . . ,u, . . . ,an+1) + det( a1, . . . ,v, . . . ,an+1)
That is,
det(a1, . . . ,u+v, . . . ,an+1) = det( a1, . . . ,u, . . . ,an+1) + det( a1, . . . ,v, . . . ,an+1),
and so the first multilinearity property holds for all n≥1 by induction.
We now prove the second multilinearity property. We have det([xa]) =xa=xdet([a]) for
anyx∈Fand [ a]∈F1×1, so the property holds in the case when n= 1. Suppose it holds for
some arbitrary n∈N. For A∈F(n+1)×(n+1),k∈ {1, . . . , n + 1}andx∈Fwe have
det(a1, . . . , x ak, . . . ,an+1) =k−1X
j=1(−1)1+ja1jdet(a′
1, . . . ,a′
j−1,a′
j+1, . . . , x a′
k, . . . ,a′
n+1)
+ (−1)1+kxa1kdet(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1)
+n+1X
j=k+1(−1)1+ja1jdet(a′
1, . . . , x a′
k, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1).166
Since the determinants in the summations are n×n, we use the inductive hypothesis to obtain
det(a1, . . . , x ak, . . . ,an+1) =k−1X
j=1(−1)1+jxa1jdet(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,a′
k, . . . ,a′
n+1)
+ (−1)1+kxa1kdet(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1)
+n+1X
j=k+1(−1)1+jxa1jdet(a′
1, . . . ,a′
k, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1),
and hence
det(a1, . . . , x ak, . . . ,an+1) =x"k−1X
j=1(−1)1+ja1jdet(a′
1, . . . ,a′
j−1,a′
j+1, . . . ,a′
k, . . . ,a′
n+1)
+ (−1)1+ka1kdet(a′
1, . . . ,a′
k−1,a′
k+1, . . . ,a′
n+1)
+n+1X
j=k+1(−1)1+ja1jdet(a′
1, . . . ,a′
k, . . . ,a′
j−1,a′
j+1, . . . ,a′
n+1)#
=xdet(a1, . . . ,an+1).
Therefore the second multilinearity property holds for all n≥1 by induction.
Proof of DP2. This is done using induction and careful bookkeeping much as with the proofs of
the previous two properties, and so is left as a problem.
Proof of DP3. Certainly det([1]) = 1, so normalization holds when n= 1. Suppose it holds for
some n∈N. Let I=In+1, with ij-entry denoted by eij. We have e11= 1 and e1j= 0 for all
2≤j≤n+ 1, and so
det(I) =n+1X
j=1(−1)1+je1jdet(I1j) = det( I11) = det( In) = 1 .
Therefore the normalization property holds for all n∈Nby induction.
Proof of DP4. LetA∈Fn×n, and fix 1 ≤j < k≤n. By the alternating property DP2,
det(A) = det( a1, . . . ,aj, . . . ,ak, . . . ,an) =−det(a1, . . . ,ak, . . . ,aj, . . . ,an),
and so if aj=akwe obtain
det(A) =−det(a1, . . . ,aj, . . . ,ak, . . . ,an) =−det(A).
That is, 2 det( A) = 0, and therefore det( A) = 0.
Proof of DP5. LetA∈Fn×n, and fix 1 ≤j, k≤nwith j̸=k. For any x∈Fwe have by DP1,
det(a1, . . . ,aj+xak
j, . . . ,an) = det( a1, . . . ,aj, . . . ,an) + det( a1, . . . , x ak
j, . . . ,an)167
= det( a1, . . . ,aj, . . . ,an) +xdet(a1, . . . ,ak
j, . . . ,an)
The matrix
a1···ak
j···an
hasjth and kth column both equal to ak, so that
det(a1, . . . ,ak
j, . . . ,an) = 0
by DP4, and we obtain
det(a1, . . . ,aj+xak, . . . ,an) = det( a1, . . . ,aj, . . . ,an)
as desired.
Proof of DP6. LetA= [a1···0···an], soaj=0for some 1 ≤j≤n. By DP1,
det(A) = det( a1, . . . ,0+0, . . . ,an)
= det( a1, . . . ,0, . . . ,an) + det( a1, . . . ,0, . . . ,an)
= det( A) + det( A),
which immediately implies that det( A) = 0. ■
Proposition 5.5. IfA∈Fn×nis an upper-triangular or lower-triangular matrix, then
det(A) =nY
i=1aii.
Proof. The statement of the proposition is vacuously true in the case when n= 1. Let n∈N
be arbitrary and suppose whenever A= [aij]nis an upper-triangular or lower-triangular matrix,
then det( A) =a11a22···ann.
Suppose that A∈Fn×nis an upper-triangular matrix, so that A= [aij] such that aij= 0
whenever i > j . Now, for all 2 ≤j≤n+ 1 the matrix A1jhas0in its first column, so that
det(A1j) = 0 by DP6 and we obtain
det(A) =n+1X
j=1(−1)1+ja1jdet(A1j) =a11det(A11). (5.3)
Now, A11is an n×nupper-triangular matrix,
A11=
a22··· a2(n+1).........
0···a(n+1)(n+1)
,
and so by the inductive hypothesis det(A11) =a22···a(n+1)(n+1). Then from (5.3) we conclude
that
det(A) =a11a22···a(n+1)(n+1).168
Next, suppose that Ais a lower-triangular matrix, so that aij= 0 whenever i < j . Then
a1j= 0 for all 2 ≤j≤n, and since A11is an n×nlower-triangular matrix, we once again we
obtain
det(A) =a11det(A11) =a11a22···a(n+1)(n+1)
as desired. ■
Lemma 5.6. Define the function det′
n:Fn×n→Fby
det′
n(A) =nX
i=1(−1)i+1ai1det′
n−1(Ai1), (5.4)
with det′
1([a]) =ain particular. Then det′
n(A) = det n(A)for all n∈NandA∈Fn×n.
Proof. First, it can be shown via analogous arguments that the function det′
npossesses the
same six properties listed in Theorem 5.4 that detnpossesses. Also Proposition 5.5 applies to
det′
n, with the proof being symmetric to the one given for det n.
Fixn∈Nand let A∈Fn×n. Recall the elementary row and column operations R1,R2,
C1, and C2 from Definition 2.15. If A′is obtained from Aby an application of C1, then by
Proposition 2.17(1) and DP5 we have det(A) =det(A′); and if A′is obtained from Aby an
application of C2, then det(A) =−det(A′) by Proposition 2.14(2) and DP2. By Proposition
2.20 and the particulars of its proof, row operations R1 and R2 may be applied to A⊤to obtain
an upper-triangular matrix U, which corresponds to employing a succession of C1 and C2
operations to Ato obtain a lower-triangular matrix
L=
ℓ11··· 0
.........
ℓn1···ℓnn
=U⊤;
that is, L= [ℓij]nwith ℓij= 0 for i < j . If a total of kC2 operations are performed in doing
this, then det( A) = (−1)kdet(L). Now
det(A) = (−1)kdet(L) = (−1)kℓ11ℓ22···ℓnn
by Proposition 5.5.
On the other hand, because Theorem 5.4 applies to det′, we have det′(A) = (−1)kdet′(L).
And then because Proposition 5.5 also applies to det′, we easily obtain
det′(A) = (−1)kℓ11ℓ22···ℓnn= det( A)
as claimed. ■
Theorem 5.7. For any A∈Fn×n,detn(A) = det n(A⊤).
Proof. LetA∈Fn×n. Let a⊤
ijdenote the ij-entry for A⊤. Since a⊤
1j=aj1and (A⊤)1j= (Aj1)⊤
for all j,
detn(A⊤) =nX
j=1(−1)1+ja⊤
1jdetn−1[(A⊤)1j] =nX
j=1(−1)1+jaj1detn−1[(Aj1)⊤].169
Now, by Lemma 5.6 we have det n−1[(Aj1)⊤] = det′
n−1[(Aj1)⊤] so that
detn(A⊤) =nX
j=1(−1)1+jaj1det′
n−1[(Aj1)⊤] = det′
n(A),
and therefore
detn(A⊤) = det n(A)
by another application of Lemma 5.6. ■
Lemma 5.8. For all n∈Nand1≤j≤n, define det′
n,j:Fn×n→Fby
det′
n,j(A) =nX
i=1(−1)i+jaijdet′
n−1,j(Aij),
withdet′
1,1([a]) =ain particular. Then, for every n∈N,det′
n,j(A) =det′
n(A)for all 1≤j≤n
andA∈Fn×n.
Proof. The conclusion is trivially true in the case when n= 1, so suppose the conclusion is
true for some n∈N. Since det′
n+1,1= det′
n+1by definition, consider det′
n+1,jfor some j≥2.
LetA= [a1···an+1]∈F(n+1)×(n+1), and let
B=aj···aj−1a1aj+1···an+1
.
Since Theorem 5.4—and in particular DP2—applies to det′
n+1, we have
det′
n+1(A) =−det′
n+1(B) =−n+1X
i=1(−1)i+1aijdet′
n(Bi1), (5.5)
where
Bi1=a′
2···a′
j−1a′
1a′
j+1···a′
n+1
,
eacha′
krepresenting akwith its ith component deleted. A succession of j−2 transpositions
of the column vectors of Bi1will bring a′
1to the position of the column without altering the
relative positions of the other vectors:
a′
1···a′
j−1a′
j+1···a′
n+1
.
This matrix is precisely Aij, and since Aijobtains from Bi1viaj−2 column transpositions, by
DP2 and the inductive hypothesis we have
det′
n(Bi1) = (−1)j−2det′
n(Aij) = (−1)j−2det′
n,j(Aij).
Substituting this result into (5.5) yields
det′
n+1(A) =−n+1X
i=1(−1)i+1(−1)j−2aijdet′
n,j(Aij) =n+1X
i=1(−1)i+jaijdet′
n,j(Aij) = det′
n+1,j(A)
as desired. Therefore det′
n+1,j= det′
n+1for all 1 ≤j≤n+ 1. ■170
Lemma 5.9. For all n∈Nand1≤i≤n, define detn,i:Fn×n→Fby
detn,i(A) =nX
j=1(−1)i+jaijdetn−1,i(Aij),
with det1,1([a]) =ain particular. Then, for every n∈N,detn,i(A) =detn(A)for all 1≤i≤n
andA∈Fn×n.
Proof. The conclusion is trivially true in the case when n= 1, so suppose the conclusion is
true for some n∈N. Since detn+1,1= det n+1by definition, consider detn+1,ifor some i≥2. Let
A∈F(n+1)×(n+1). We have
detn+1(A) = det n+1(A⊤) = det′
n+1(A⊤) = det′
n+1,i(A⊤) (5.6)
by Theorem 5.7, Lemma 5.6, and Lemma 5.8, respectively. Letting A⊤=B= [bjk]n, where
bjk=akj, we have
det′
n+1,i(A⊤) = det′
n+1,i(B) =n+1X
j=1(−1)j+ibjidet′
n,i(Bji). (5.7)
However, since Bji= (A⊤)ji= (Aij)⊤, it follows that
det′
n,i(Bji) = det′
n,i 
(Aij)⊤
= det′
n 
(Aij)⊤
= det n 
(Aij)⊤
= det n(Aij) = det n,i(Aij),
making use of Lemma 5.8, Lemma 5.6, Theorem 5.7, and the inductive hypothesis, in turn.
This result, along with bji=aijand (5.6), turns (5.7) into
detn+1(A) =n+1X
j=1(−1)i+jaijdetn,i(Aij),
and therefore det n+1(A) = det n+1,i(A) as desired. ■
All of the functions detn,ianddet′
n,jare rightly called determinant functions; however
Lemmas 5.6, 5.8 and 5.9, taken together, show that
detn,i= det n= det′
n= det′
n,j
for any n∈Nand 1≤i, j≤n. That is, all of the determinant functions defined thus far in this
section turn out to be the same function, even though they are given by different formulas! For
each i, the formula given for detn,i(A) is called “expansion of the determinant of Aalong the
ith row”; and for each j, the formula given for det′
n,j(A)is called “expansion of the determinant
ofAalong the jth column.” Since all of the functions detn,ianddet′
n,jare the same, and since
in practice it is not generally necessary or desirable to specify which way the determinant of a
square matrix is being expanded, from now on we shall denote all expansions of the determinant
ofAby the symbol det n(A) or det( A). We summarize as follows.171
Definition 5.10. Given A∈Fn×n, the sum
detn(A) =nX
j=1(−1)i+jaijdetn−1(Aij)
is called the expansion of the determinant of Aalong the ith row , and the sum
detn(A) =nX
i=1(−1)i+jaijdetn−1(Aij)
is called the expansion of the determinant of Aalong the jth column .
Given column vectors a1, . . . ,an, we define
detn(a⊤
1, . . . ,a⊤
n) = det n

a⊤
1...
a⊤
n

;
that is, we take detn(a⊤
1, . . . ,a⊤
n) to be the determinant of the matrix with row vectors
a⊤
1, . . . ,a⊤
n. (It is important to bear in mind that, notational conventions aside, detnis by
definition strictly a function with domain Fn×n—which is to say the allowed “inputs” are n×n
matrices, and not n-tuples of vectors in Fn.) In light of Theorem 5.7 we readily obtain the
following result.
Proposition 5.11. The properties DP1 – DP6 given in Theorem 5.4 remain valid if a1, . . . ,an
represent the row vectors of a matrix A∈Fn×ninstead of the column vectors.
Proof. The proof for DP1 should suffice to convey the general strategy. Given row vectors
a1, . . . ,u+v, . . . ,an, we have
detn(a1, . . . ,u+v, . . . ,an) = det n

a1...
u+v
...
an

= det n

a1...
u+v
...
an
⊤

= det n a⊤
1···u⊤+v⊤···a⊤
n
= det n a⊤
1···u⊤···a⊤
n
+ det n a⊤
1···v⊤···a⊤
n
= det na⊤
1···u⊤···a⊤
n⊤
+ det na⊤
1···v⊤···a⊤
n⊤
= det n

a1...
u
...
an

+ det n

a1...
v
...
an


= det n(a1, . . . ,u, . . . ,an) + det n(a1, . . . ,v, . . . ,an)172
by our notational convention and repeated use of Theorem 5.7. ■
Example 5.12. Evaluate the determinant
3 0−6
−2 4 7
1 0 10
Solution. Since the second column of the determinant has two zero entries, our labors will be
lessened if we expand the determinant along the second column:
3 0−6
−2 4 7
1 0 10= (−1)1+2(0)−2 7
1 10+ (−1)2+2(4)3−6
1 10+ (−1)3+2(0)3−6
−2 7
= 43−6
1 10= 4
(3)(10) −(−6)(1)
= 144 .
Expanding along any other column or row will yield the same result. ■
Example 5.13. Given that
A=
3 1 −5 9
−6 4 10 −18
0−2 8 −7
5 1 −1 3
,
evaluate det( A).
Solution. Applying DP5 together with Proposition 5.11, we add twice the first row of the
determinant to the second row, obtaining a new determinant having the same value as the old
one:
det(A) =3 1 −5 9
−6 4 10 −18
0−2 8 −7
5 1 −1 32r1+r2→r2= = = = = = = =3 1 −5 9
0 6 0 0
0−2 8 −7
5 1 −1 3
Now we find it convenient to expand the determinant of Aalong the second row, since that row
contains three zero entries:
det(A) = (−1)2+2(6) =3−5 9
0 8 −7
5−1 3= 63−5 9
0 8 −7
5−1 3
Expanding the 3 ×3 determinant along the first column, we finally obtain
det(A) = 6
38−7
−1 3+ 5−5 9
8−7
= 6
3(17) + 5( −37)
=−804
and we’re done. ■173
Example 5.14. Then×nVandermonde determinant is
Vn= det 
[xj−1
i]n×n
=1x1··· xn−1
1
1x2··· xn−1
2............
1xn··· xn−1
n
The claim is that
Vn+1=Y
1≤i<j≤n+1(xj−xi) (5.8)
for all n≥1. This clearly holds when n= 1 and n= 2:
V2=1x1
1x2=x2−x1and V3=1x1x2
1
1x2x2
2
1x3x3
3= (x2−x1)(x3−x1)(x3−x2).
Letn≥1 be arbitrary, and suppose that (5.8) is true. Now, by DP5,
Vn+2=1x1··· xn+1
1
1x2··· xn+1
2............
1xn+2··· xn+1
n+2−x1cj+cj+1→cj+1= = = = = = = = = = = = =
forj=n+ 1, . . . , 11 0 ··· 0
1x2−x1··· xn+1
2−x1xn
2............
1xn+2−x1··· xn+1
n+2−x1xn
n+2.
Expanding the determinant along the first row and then employing Proposition 5.11 to DP1
yields
Vn+2=x2−x1 x2
2−x1x2··· xn+1
2−x1xn
2............
xn+2−x1x2
n+2−x1xn+2··· xn+1
n+2−x1xn
n+2
= (x2−x1)···(xn+2−x1)1x2··· xn
2............
1xn+2··· xn
n+2
The last determinant is an ( n+ 1)×(n+ 1) Vandermonde determinant, and so by (5.8) we have
1x2··· xn
2............
1xn+2··· xn
n+2=Y
2≤i<j≤n+2(xj−xi).
Hence
Vn+2= (x2−x1)···(xn+2−x1)Y
2≤i<j≤n+2(xj−xi) =Y
1≤i<j≤n+2(xj−xi),
and so by the principle of induction we conclude that (5.8) holds for all n≥1. ■174
5.3 – Applications of Determinants
As a first application, we establish a few results that will enable us to significantly extend
the Invertible Matrix Proposition of §4.9.
Proposition 5.15. LetA= [a1···an]∈Fn×n. The vectors a1, . . . ,anare linearly dependent
if and only if det(A) = 0 .
Proof. Suppose that a1, . . . ,anare linearly dependent, so there exist c1, . . . , c n∈Fsuch that
nX
j=1cjaj=0,
andck̸= 0 for some 1 ≤k≤n. Now
ckak+X
j̸=kcjaj=0⇒ak=−X
j̸=kcj
ckaj,
and so
det(A) = det( a1, . . . ,ak, . . . ,an) = det 
a1, . . . ,−X
j̸=kcj
ckaj, . . . ,an!
=−X
j̸=kcj
ckdet(a1, . . . ,aj, . . . ,an) (5.9)
by the multilinearity properties of the determinant function. By DP4 we have
det(a1, . . . , aj|{z}
kth col., . . . ,an) = 0
for each 1 ≤j≤nsuch that j̸=k, and so from (5.9) we obtain det( A) = 0.
For the converse, suppose that a1, . . . ,anare linearly independent, so col-rank (A) =n.
Recall the elementary row and column operations R1,R2,C1, and C2 from Definition 2.15. The
proof of Theorem 3.64 shows that Ais equivalent via the operations R1,R2,C1, and C2 to a
diagonal matrix
B=
b11··· 0
.........
0···bnn
,
and since by Theorem 3.66
col-rank( B) = col-rank( A) =n,
it follows that bjj̸= 0 for all 1 ≤j≤n.
Now, if pis the number of R2 and C2 operations performed (which by Propositions 2.16(2)
and 2.17(2) correspond to swapping rows and columns) in passing from AtoB, then by DP2
and 5.4(5), together with Proposition 5.11, we have
det(A) = (−1)pdet(B).175
Of course, Bis an upper-triangular matrix, and so
det(A) = (−1)pb11b22···bnn̸= 0
by Proposition 5.5. ■
Proposition 5.16. A ∈Fn×nis invertible if and only if detn(A)̸= 0.
Proof. By the Invertible Matrix Proposition (Proposition 4.66),
A=a1···an
is invertible if and only if a1, . . . ,anare linearly independent, and by Proposition 5.15 the
vectors a1, . . . ,anare linearly independent if and only if detn(A)̸= 0. The conclusion is now
self-evident. ■
We now improve on the Invertible Matrix Proposition given in §4.9 to obtain what we shall
call the Invertible Matrix Theorem, incorporating also the results of Proposition 4.67 as well as
observing that nullity( A) = 0 is equivalent to Nul( A) ={0}.
Theorem 5.17 (Invertible Matrix Theorem ).LetA∈Fn×n, and let LAbe the linear
operator on Fnhaving corresponding matrix Awith respect to the standard basis EofFn. Then
the following statements are equivalent.
1.Ais invertible.
2.A⊤is invertible.
3.Ais row-equivalent to In.
4.The row vectors of Aare linearly independent.
5.Ais column-equivalent to In.
6.The column vectors of Aare linearly independent.
7. col-rank( A) =n.
8. row-rank( A) =n.
9. rank( A) =n.
10.The system Ax=bhas a unique solution for each b∈Fn.
11.The system Ax=0has only the trivial solution.
12. Nul( A) ={0}.
13. nullity( A) = 0 .
14.LA∈ L(Fn)is invertible.
15.There exists some D∈Fn×nsuch that AD=In.
16.There exists some C∈Fn×nsuch that CA=In.
17. det n(A)̸= 0.
Determinants can be applied to find the solution to a nonhomogeneous system of nequations
with nunknowns, provided that a unique solution exists.
Theorem 5.18 (Cramer’s Rule ).Leta1, . . . ,an∈Fnsuch that detn(a1, . . . ,an)̸= 0. If
b∈Fnandx1, . . . , x nare scalars such that
x1a1+···+xnan=b, (5.10)176
then
xj=detn(a1, . . . ,aj−1,b,aj+1, . . . ,an)
detn(a1, . . . ,an)
for each 1≤j≤n.
Proof. Suppose that b∈Fn. Since detn(a1, . . . ,an)̸= 0 it follows from the Invertible Matrix
Theorem that there exist unique scalars x1, . . . , x nsuch that equation (5.10) holds. Fix 1 ≤j≤n.
Letting
detn(a1, . . . ,b, . . . ,an) = det n(a1, . . . ,aj−1,b,aj+1, . . . ,an)
for brevity, we obtain
detn(a1, . . . ,b, . . . ,an) = det n 
a1, . . . ,nX
k=1xkak, . . . ,an!
=nX
k=1xkdetn(a1, . . . ,ak, . . . ,an) (5.11)
by DP1. Now, for each k̸=jwe have detn(a1, . . . ,ak, . . . ,an) = 0 by DP4, since both the jth
andkth column of the matrix
a1···ak|{z}
jth col.···an
is equal to aj. Hence from (5.11) comes
detn(a1, . . . ,b, . . . ,an) =xjdetn(a1, . . . , aj|{z}
jth col., . . . ,an) =xjdetn(a1, . . . ,an),
and therefore
xj=detn(a1, . . . ,b, . . . ,an)
detn(a1, . . . ,an)
as desired. ■
If we let
A=a1a2···an
and
x=
x1...
xn
,
then Cramer’s Rule may be given as
Ax=b⇒xj=detn(a1, . . . ,aj−1,b,aj+1, . . . ,an)
detn(A)
for each 1 ≤j≤n, so long as det( A)̸= 0.177
Example 5.19. Solve the system


2x−y+z= 1
x+ 3y−2z= 0
4x−3y+z= 2
using Cramer’s Rule.
Solution. HereAx=bwith
A=
2−1 1
1 3 −2
4−3 1
,x=
x
y
z
,b=
1
0
2
.
We have
det(A) = 23−2
−3 1−−1 1
−3 1+ 4−1 1
3−2= 2(−3)−2 + 4(−1) =−12,
so det( A)̸= 0 and by Cramer’s Rule
x=1
det(A)1−1 1
0 3 −2
2−3 1=−1
12(−5) =5
12
y=1
det(A)2 1 1
1 0−2
4 2 1=−1
12(1) =−1
12
z=1
det(A)2−1 1
1 3 0
4−3 2=−1
12(−1) =1
12
Therefore the solution to the system, which is unique, is (5 /12,−1/12,1/12). ■
Next, we construct a method for finding the inverse of a square matrix using determinants,
provided the matrix is invertible.
Theorem 5.20. LetA= [aij]n. Ifdetn(A)̸= 0, then X= [xij]ngiven by
xij=(−1)i+jdetn−1(Aji)
detn(A)
for all 1≤i, j≤nis the inverse for A.
Proof. Suppose that det n(A)̸= 0. For any j∈ {1, . . . , n }, let
xj=
x1j...
xnj
,
and recall the jth standard unit vector ejofFn. By Cramer’s Rule the system of equations
corresponding to the matrix equation
Axj=ej178
has a unique solution given by
xij=detn(a1, . . . ,ai−1,ej,ai+1, . . . ,an)
detn(A)
for each 1 ≤i≤n. Since the jth coordinate of ejis 1 and all other coordinates are 0, we obtain
detn(a1, . . . ,ai−1,ej,ai+1, . . . ,an) = (−1)i+jdetn−1(Aji)
by expanding the determinant along the ith column. Therefore
xij=(−1)i+jdetn−1(Aji)
detn(A)
for each 1 ≤i≤nand 1 ≤j≤n, and if we define X= [xij]n, then we readily obtain
AX=In. (5.12)
It remains to show that XA=In. Since detn(A⊤) =detn(A)̸= 0, we can find a matrix Y
such that A⊤Y=In, and then
A⊤Y=In⇒(A⊤Y)⊤=I⊤
n⇒Y⊤A=In. (5.13)
Now, using (5.12) we obtain
Y⊤A=In⇒(Y⊤A)X=InX⇒Y⊤(AX) =X⇒Y⊤In=X⇒X=Y⊤,
and hence
XA=In
by the rightmost equation in (5.13).
Since XA=AX=In, we conclude that
X=(−1)i+jdetn−1(Aji)
detn(A)
n
is the inverse for A. ■
Put another way, Theorem 5.20 states that if detn(A)̸= 0 then Ais invertible, and the
inverse A−1is given by
A−1=(−1)i+jdetn−1(Aji)
detn(A)
n. (5.14)
Example 5.21. Show that if D∈Fn×nis given as a block matrix by
D=
A B
O C
,
where A= [aij]ℓandC= [cij]mare square matrices, then
detn(D) = det ℓ(A) det m(C).179
Solution. We must show that, for all ℓ, m∈N,
detℓ+m(D) = det ℓ+m[aij]ℓB
O [cij]m
= det ℓ([aij]ℓ) det m([cij]m), (5.15)
where of course O= [0] m×ℓandB= [bij]ℓ×m.
First consider the case when ℓ= 1 and m∈Nis arbitrary. Letting D= [dij]m+1denote the
block matrix and expanding along the first column, we have
detm+1aB
O[cij]m
=m+1X
i=1(−1)i+1di1detm(Di1).
Since D11= [cij]m,d11=aanddi1= 0 for i >1, let a11=ato obtain
detm+1aB
O[cij]m
= (−1)1+1d11detm(D11) =adetm([cij]m)
= det 1([aij]1) det m([cij]m).
This establishes the base case of an inductive argument on ℓ.
Next, fix ℓ∈N, and assume that (5.15) is true for ℓand all m∈N. We must show that
(5.15) is true for ℓ+ 1 and all m. Let m∈Nbe arbitrary, and define
D= [dij]ℓ+m+1=[aij]ℓ+1 B
O [cij]m
.
Letting Bidenote Bwith ith row deleted, and also setting A= [aij]ℓ+1, we have
detℓ+m+1(D) = det ℓ+m+1[aij]ℓ+1 B
O [cij]m
=ℓ+m+1X
i=1(−1)i+1di1detℓ+m(Di1)
=ℓ+1X
i=1(−1)i+1ai1detℓ+m(Di1).
Since Ai1is an ℓ×ℓmatrix for each 1 ≤i≤ℓ+ 1, by the inductive hypothesis we find that
detℓ+m(Di1) = det ℓ+m
Ai1Bi
O C
= det ℓ(Ai1) det m(C)
for each 1 ≤i≤ℓ+ 1, and hence
detℓ+m+1(D) =ℓ+1X
i=1(−1)i+1ai1detℓ(Ai1) det m(C) = det ℓ+1(A) det m(C).
By induction we conclude that (5.15) holds for ℓ, m∈N, and therefore
det
A B
O C
= det( A) det(C)
for any square matrices AandC. ■
For the next example we define a minor of a matrix A∈Fm×nto be the determinant of
any square submatrix of A. We have encountered minors already: each Aijthat appears in
Definition 5.10 is an ( n−1)×(n−1) minor of the n×nmatrix A.180
Example 5.22. ForA∈Fm×nlet 1≤k <min{m, n}. Show that rank(A)≤kif and only if
every ( k+ 1)×(k+ 1) minor of Aequals 0.
Solution. By Proposition 4.69, rank(A)≤kif and only if every ( k+ 1)×(k+ 1) submatrix of
Ais noninvertible. By the Invertible Matrix Theorem a ( k+ 1)×(k+ 1) submatrix of Ais
noninvertible if and only if the determinant of the submatrix equals 0. Therefore rank(A)≤k
if and only if every ( k+ 1)×(k+ 1) minor of Aequals 0. ■
Problems
1. Solve the system

x+y+ 2z= 1
2x + 4z= 2
3y+z= 3
using Cramer’s Rule.181
5.4 – Determinant Formulas
Recall the elementary matrices Mi,j(c) and Mi,jdefined in section 2.3. Given a scalar xand
ann×nmatrix Awith row vectors a1, . . . ,an, by Proposition 2.16 we have
Mi,j(x)A=
a1...
aj+xai...
an
	
jth row,
and so by Proposition 5.11 (recalling DP5 in Theorem 5.4) we find that
detn(Mi,j(x)A) = det n

a1...
aj+xai...
an

= det n

a1...
aj...
an

= det n(A). (5.16)
By Proposition 2.16 the matrix Mi,jAis obtained from Aby interchanging the ith and jth
rows, and so by Proposition 5.11 (recalling DP2 in Theorem 5.4) we find that
detn(Mi,jA) =−detn(A). (5.17)
We use these facts to prove the following.
Theorem 5.23. For any A,B∈Fn×n,
detn(AB) = det n(A) det n(B).
Proof. IfAis not invertible, then ABis not invertible by Proposition 4.68 and we obtain
detn(A) det n(B) = 0·detn(B) = 0 = det n(AB)
by the Invertible Matrix Theorem. If Bis not invertible we obtain a similar result since
detn(AB) = 0 and det n(B) = 0.
Suppose that AandBare both invertible, so that ABis also invertible by Theorem 2.26.
By the proof of Theorem 2.30 the matrix Ais row-equivalent via R1 and R2 operations to a
diagonal matrix
D=
d1 0
...
0 dn
=
d1e1...
dnen

that is, there exists a sequence of elementary matrices M1, . . . ,Mk, of which ℓare of the R2
variety and the rest of the R1 variety, such that
A=Mk···M1D.182
Now, if b1, . . . ,bnare the row vectors of B, then
DB=
d1b1...
dnbn

and so, recalling (5.16) and (5.17) as well as Theorem 5.7,
detn(AB) = det n 
(Mk···M1D)B
= det n 
Mk···M1(DB)
= (−1)ℓdetn(DB)
= (−1)ℓdetn 
(DB)⊤
= (−1)ℓdetn 
d1b⊤
1, . . . , d nb⊤
n
= (−1)ℓd1···dndetn 
b⊤
1, . . . ,b⊤
n
= (−1)ℓd1···dndetn 
B⊤
= (−1)ℓd1···dndetn(B) = (−1)ℓdetn(D) det n(B)
= det n(Mk···M1D) det n(B) = det n(A) det n(B).
Here we use the fact that Dis an upper-triangular matrix and so by Proposition 5.5 has
determinant equal to the product of its diagonal entries. ■
Theorem 5.24. IfA∈Fn×nis invertible, then
detn(A−1) =1
detn(A).
Proof. Suppose that A∈Fn×nis invertible. Then there exists some A−1∈Fn×nsuch that
AA−1=In, and thus
detn(A) det n(A−1) = det n(AA−1) = det n(In) = 1 . (5.18)
by Theorems 5.23 and 5.4(7) . Now, the invertibility of Aimplies that detn(A)̸= 0 by the
Invertible Matrix Theorem, and so from (5.18) we readily obtain
detn(A−1) =1
detn(A).
as desired. ■
Another way to write the statement of Theorem 5.24 that is particularly elegant is:
detn(A−1) = det n(A)−1
ifA∈Fn×nis invertible.
Recall Corollary 4.33: given a linear operator L:V→V, bases BandB′forV, and
corresponding matrices [ L]Band [ L]B′, we have
[L]B′=IBB′[L]BI−1
BB′, (5.19)
From this matrix equation we obtain an interesting result involving determinants.183
Theorem 5.25. Letdim(V) =n, letLbe a linear operator on V, and let BandB′be bases
forV. If [L]Bis the matrix corresponding to Lwith respect to Band[L]B′is the matrix
corresponding to Lwith respect to B′, then
detn([L]B′) = det n([L]B). (5.20)
Proof. From equation (5.19) we obtain
detn([L]B′) = det n(IBB′[L]BI−1
BB′).
Now, by Theorems 5.23 and 5.24,
detn([L]B′) = det n(IBB′) det n([L]B) det n(I−1
BB′)
= det n(IBB′) det n([L]B)1
detn(IBB′)
= det n([L]B),
which affirms (5.20) and finishes the proof. ■
Thus the determinant of the matrix corresponding to a linear operator on Vis invariant in
value under change of bases, so that we can meaningfully speak of the “determinant” of a linear
operator.
Definition 5.26. Letdim(V) =n, and let Lbe a linear operator on V. The determinant of
Lis defined to be
detn(L) = det n([L]),
where [L]is the matrix corresponding to Lwith respect to any basis for V.184
5.5 – Permutations and the Symmetric Group
Definition 5.27. Letn∈N, and let In={1,2, . . . , n }. The symmetric group Snis the
group consisting of all bijections
σ:In→In
under the operation of function composition ◦. Each σ∈ S nis called a permutation .
By definition every group must have an identity element. We denote by εtheidentity
permutation in Snthat is given by ε(k) =kfor each k∈In.
A special matrix notation, known as the two-line notation , is often used to define a
permutation σ∈ S nexplicitly. We write
σ=
1 2 ··· n
σ(1)σ(2)···σ(n)
to indicate that σmaps 1 to the value σ(1), 2 to the value σ(2), and so on. Thus the first row
of the matrix lists the “inputs” for the function σ, and the second row lists the corresponding
“outputs.”
Since σ∈ S nis a bijection, it has an inverse which we denote (as usual) by σ−1, and it is
easy to see that σ−1∈ S nalso. We also define σ0=ε,σ1=σ,σ2=σ◦σ, and so on.
Example 5.28. One permutation belonging to the group S5isσ:I5→I5given by
σ(1) = 4 , σ(2) = 2 , σ(3) = 1 , σ(4) = 5 , σ(5) = 3 ,
which we denote by
1 2 3 4 5
4 2 1 5 3
in the two-line notation. ■
Example 5.29. Just as there are 6 possible permutations (i.e. ordered arrangements) of a set
of 3 distinct objects {a, b, c}, namely
(a, b, c ),(a, c, b ),(b, a, c ),(b, c, a ),(c, a, b ),(c, b, a ),
so too are there six permutations in the group S3. These are
1 2 3
1 2 3
,
1 2 3
1 3 2
,
1 2 3
2 1 3
,
1 2 3
2 3 1
,
1 2 3
3 1 2
,
1 2 3
3 2 1
.
The first permutation in the list is the identity permutation ε. ■
Ifσ, τ∈ S n, then τ◦σ∈ S nis given by
(τ◦σ)(i) =τ(σ(i))
for each i∈Inin the usual manner of function composition. Thus
τ◦σ=
1 2 ··· n
τ(1)τ(2)···τ(n)
◦
1 2 ··· n
σ(1)σ(2)···σ(n)185
=
1 2 ··· n
τ(σ(1)) τ(σ(2))···τ(σ(n))
.
Example 5.30. InS3we have
1 2 3
1 3 2
◦
1 2 3
3 2 1
=
1 2 3
2 3 1
, (5.21)
Note that the matrix immediately to the right of the symbol ◦in(5.21) takes the input first, so
1→
1 2 3
3 2 1
→3→
1 2 3
1 3 2
→2,
which gives the first column of the matrix to the right of the = symbol in (5.21). ■
Assuming n≥2, atransposition is a permutation τ∈ S nfor which there exist k, ℓ∈In
with k̸=ℓsuch that
τ(i) =

i,ifi∈In\ {k, ℓ}
k,ifi=ℓ
ℓ,ifi=k.
Thus a transposition interchanges precisely two distinct elements of Inwhile leaving all other
elements fixed. The classic example in S2is
1 2
2 1
,
and an example in S4is
1 2 3 4
1 4 3 2
.
Any permutation σ∈ S nis uniquely determined by the arrangement of the elements of Inin
the second row of its corresponding matrix. Since the nelements in Inhave n! possible distinct
arrangements, it follows that Snitself has n! elements. This proves the following.
Proposition 5.31. |Sn|=n!for all n∈N.
We now introduce another notation for elements of Sncalled cycle notation . For m≤n
letJ={j1, j2, j3, . . . , j m}be a set of distinct elements of In. Then the symbol
(j1, j2, j3, . . . , j m), (5.22)
denotes a permutation in Snthat performs the mappings
j17→j27→j37→ ··· 7→ jm−17→jm7→j1,
and also i7→ifor any i∈In\J. Using function notation, if σ∈ S nis such that
σ= (j1, j2, j3, . . . , j m),
then
σ(j1) =j2, σ(j2) =j3, . . . , σ (jm−1) =jm, σ(jm) =j1,
with σ(i) =ifor any i∈In\J.186
Any permutation expressible in the form (5.22) is called a cycle . The entries in (5.22) are
ideally envisioned as being written in a circular arrangement, like the numbers on a clock, so
that the “last” entry jmis naturally seen to be followed by j1. In this way
(jm, j1, j2, . . . , j m−1)
is easily recognized as being the same permutation as that given by (5.22).
Example 5.32. InS3the cycle (1 ,3,2) is the permutation

1 2 3
3 1 2
.
InS5the cycle (1 ,3,2) is the permutation

1 2 3 4 5
3 1 2 4 5
.
Since (1 ,3,2)∈ S 5does not feature 4 or 5 among its entries, we see that (1 ,3,2) maps 4 7→4
and 5 7→5.
InSnfor any n≥3 we have
(1,3,2) = (2 ,1,3) = (3 ,2,1).
That is, moving the last entry in a cycle to the first position does not change the corresponding
permutation. ■
As with permutations in general, two cycles σandτinSnmay be composed. If
σ= (j1, j2, . . . , j m) and τ= (i1, i2, . . . , i ℓ), (5.23)
then
(j1, j2, . . . , j m)◦(i1, i2, . . . , i ℓ)
is the permutation σ◦τ. Typically the symbol ◦is omitted in the cycle notation, and we write
σ◦τ= (j1, j2, . . . , j m)(i1, i2, . . . , i ℓ).
Thelength of a cycle is simply the number of entries it contains. For instance the cycles σ
andτin(5.23) have lengths mandℓ, respectively. We will say a cycle is an m-cycle if it has
length m. We now gather a few facts about transpositions.
Proposition 5.33. Letn≥2.
1.S1has no transpositions.
2.τ∈ S nis a transposition if and only if τis a 2-cycle.
3.Ifτ∈ S nis a transposition, then τ◦τ=ε.
4.Ifτ1, τ2∈ S nare transpositions, then τ1◦τ2=τ2◦τ1.187
Proof.
Proof of (3). Suppose τ∈ S nis a transposition, so τ= (a, b) for some a, b∈Inwith a̸=bby
part (2). Then
(τ◦τ)(a) =τ(τ(a)) =τ(b) =aand ( τ◦τ)(b) =τ(τ(b)) =τ(a) =b,
and furthermore
(τ◦τ)(i) =τ(τ(i)) =τ(i) =i
for any i∈In\ {a, b}. Therefore τ◦τ=ε. ■
Proofs of the other parts of Proposition 5.33 are left as exercises.
Two cycles ( j1, . . . , j m) and ( i1, . . . , i ℓ) inSnaredisjoint if
{j1, . . . , j m} ∩ {i1, . . . , i ℓ}=∅,
which is to say the cycles have no entries in common. Thus (1 ,6,3) and (4 ,2,5,8) are disjoint since
{1,6,3}∩{4,2,5,8}=∅, but (5 ,2,1) and (3 ,1,9,2) are not disjoint since {5,2,1}∩{3,1,9,2}=
{1,2}.
Proposition 5.34. If(j1, . . . , j m)and(i1, . . . , i ℓ)are disjoint cycles in Sn, then
(j1, . . . , j m)(i1, . . . , i ℓ) = (i1, . . . , i ℓ)(j1, . . . , j m).
The proof of Proposition 5.34 is left as an exercise. Another way to state Proposition 5.34 is to
say that disjoint cycles commute. Parts (2) and (4) of Proposition 5.33 imply that commutativity
always holds in the special case of 2-cycles, even if the 2-cycles under consideration are not
disjoint.
The process of expressing a permutation as a composition of two or more cycles is known
ascycle decomposition . Even a permutation that is itself a cycle we may be interested in
expressing anew as a composition of two cycles of lesser length. Indeed, of particular importance
to us along our path to a new formulation for determinants in the next section is the process of
decomposing a permutation into 2-cycles (i.e. transpositions).
Example 5.35. Consider the permutation
σ=
1 2 3 4 5 6 7
3 6 1 2 5 7 4
inS7. We see that σmaps 1 to 3, and also 3 back to 1. We may write this as 1 7→37→1. We
also have the chain of mappings
27→67→77→47→2.
The only mapping left is 5 7→5. Thus σhas the cycle decomposition
(1,3)(2,6,7,4),
or equivalently (2 ,6,7,4)(1,3). Recall that if a value is absent from a cycle’s list of entries, then
the cycle returns that value unchanged. Thus
5→(1,3)→5→(2,6,7,4)→5,188
whereas
1→(1,3)→3→(2,6,7,4)→3.
To decompose σinto transpositions it is only necessary to decompose (2 ,6,7,4) into trans-
positions. In fact we have
(2,6,7,4) = (2 ,6)(2,7)(2,4),
where the three transpositions on the right-hand side may be written in any order, and so
σ= (1,3)(2,6,7,4) = (1 ,3)(2,6)(2,7)(2,4),
where again any order is permissible. ■
It was not mere luck that the permutation σin Example 5.35 was able to be decomposed
into transpositions. As the next proposition makes clear, this is true of any permutation in Sn
forn≥2.
Proposition 5.36. Letn≥2. Ifσ∈ S n, then for some k∈Nthere exist transpositions
τ1, . . . , τ k∈ S nsuch that
σ=τ1◦ ··· ◦ τk.
Proof. The proof will employ induction, so we start by showing the n= 2 case is true. The
symmetric group S2has only two elements: εand (1 ,2). Since (1 ,2) is already a transposition,
we need only show that
ε=
1 2
1 2
can be expressed as a composition of transpositions. But by Proposition 5.33(3) we immediately
have ε= (1,2)(1,2), and so we’re done.
Now let n≥2 be arbitrary, and suppose the statement of the proposition holds for this value
n. Let σ∈ S n+1, so that
σ=
1 2 ···n+ 1
i1i2··· in+1
.
Since σ:In+1→In+1is a bijection there exists some m∈In+1such that σ(m) =n+ 1. There
are two cases to consider: either m=n+ 1 or m < n + 1.
Ifm=n+ 1, so that σ(n+ 1) = n+ 1, then σ(i)∈Infor each i∈In. If we define
ˆσ:In→Inbyˆσ(i) =σ(i) for each i∈In, then ˆσ∈ S n, and by our inductive hypothesis there
exist transpositions τ1, . . . , τ k∈ S nsuch that ˆσ=τ1◦ ··· ◦ τk. By Proposition 5.33(2) each
transposition τjis a 2-cycle ( aj, bj), and since aj, bj∈InandIn⊆In+1, it follows that ( aj, bj)
also defines a 2-cycle in Sn+1. Taking τj= (aj, bj) to be in Sn+1for each 1 ≤j≤k, we find
thatσ=τ1◦ ··· ◦ τk, and so σis expressible as a composition of transpositions.
Suppose next that m < n + 1, so σ(m) =n+ 1 for m∈In. Defining σ0∈ S n+1by
σ0=σ◦(m, n+ 1), Proposition 5.33(3) and the known associativity of the function composition
operation imply that
σ=σ◦ε=σ◦((m, n + 1)◦(m, n + 1))
= (σ◦(m, n + 1))◦(m, n + 1) = σ0◦(m, n + 1). (5.24)189
Now, since
σ0(n+ 1) = ( σ◦(m, n + 1))( n+ 1) = σ(m) =n+ 1,
we see that σ0has the property treated in the m=n+ 1 case, and so by the same argument
used in that case there exist transpositions τ1, . . . , τ k∈ S n+1such that σ0=τ1◦ ··· ◦ τk. Then
by (5.24) we find that
σ=τ1◦ ··· ◦ τk◦(m, n + 1),
which shows that σis again expressible as a composition of transpositions. ■
What Proposition 5.36 does not say is that the cycle decomposition of a permutation into
transpositions is necessarily unique, and that’s because it never is. Even for (1 ,2)∈ S 2we have
(1,2) = (1 ,2)(1,2)(1,2) = (1 ,2)(1,2)(1,2)(1,2)(1,2),
and in general (1 ,2) = (1 ,2)2k−1for any k∈N.
Is there anything more that can be said about the decomposition of a permutation into
transpositions, beyond its mere existence? Recall that any integer has a parity, which is to
say the integer is either even (divisible by 2) or odd (not divisible by 2). Now we define the
parity of a particular decomposition of σ∈ S ninto transpositions τ1, . . . , τ kas being odd ifk
is odd, and even ifkis even. The next proposition states that no one permutation can have
two decompositions of opposite parity.
Proposition 5.37. Letn≥2. Ifσ∈ S n, then the decompositions of σinto transpositions are
either all odd or all even.
Proof. The proof will employ induction, so we start by showing the n= 2 case is true. The
symmetric group S2has only two elements, εand (1 ,2), with (1 ,2) in particular being the only
transposition available. Now, for any k≥0 Proposition 5.33(3) implies that
(1,2)2k= [(1 ,2)(1,2)]k=εk=ε,
and
(1,2)2k+1= (1,2)(1,2)2k= (1,2)◦ε= (1,2).
Thus all the possible even decompositions equal ε, and all the possible odd decompositions equal
(1,2). It follows that εhas only even decompositions, and (1 ,2) has only odd decompositions.
Now let n≥2 be arbitrary, and suppose the statement of the proposition holds for this value
n. The remainder of the proof we leave as an exercise. ■
It is because of Proposition 5.37 that the following definition is meaningful.
Definition 5.38. Letn≥2. A permutation σ∈ S niseven if it can be expressed as a
composition of an even number of transpositions, and oddif it can be expressed as a composition
of an odd number of transpositions. By definition ε∈ S 1we take to be even.
Thesign function on Snis the function sgn :Sn→ {− 1,1}given by
sgn(σ) =(
1,ifσis even
−1,ifσis odd.190
The only element of S1is
ε=
1
1
,
which cannot be expressed as a composition of transpositions since there are no transpositions
inS1. Nonetheless it will be convenient to define ε∈ S 1to be even, and therefore sgn( ε) = 1.
Remark. Since ( −1)mis 1 if mis even and −1 ifmis odd, we see from Definition 5.38 that if
a permutation σcan be expressed as a composition of mtranspositions, then sgn( σ) = (−1)m.
It is straightforward to check that the composition of two even permutations is again even,
and so if Anis the set of all even permutations in Sn, then Anis in fact a subgroup of Sncalled
theantisymmetric group . In contrast the composition of two odd permutations is even, and
so the set Sn\ Anof all odd permutations in Snis not a group since it is not closed under the
operation ◦of function composition.191
5.6 – The Leibniz Formula
In§5.2 we found that, for each n∈N, the functions detn,ianddet′
n,jwere equal for all
1≤i, j≤n; that is,
detn,1=···= det n,n= det′
n,1=···= det′
n,n.
That all these functions are the same ultimately derives from the fact that they all possess the
six properties given in Theorem 5.4. A close look at these properties, however, reveals that not
all of them are fundamental. That is, some of the properties are an immediate consequence of
one or more of the others. In particular, analyzing the details of the theorem’s proof, it can be
seen that properties DP1, DP2, and DP3 are independent (i.e. no two can be used to derive the
third), and yet taken together they readily imply DP4, DP5, and DP6.
While all the “different” determinants defined in §5.2 turned out to be the same, it is
reasonable to wonder whether there is some way to define the determinant of a square matrix A
so that it possesses the properties in Theorem 5.4 and yet is genuinely different. Put another
way, if the minimum qualifications that a function must satisfy in order for it to be called a
“determinant” are that it possess the multilinearity, alternating, and normalization properties in
Theorem 5.4, does that uniquely characterize the function? The answer is yes.
Theorem 5.39 (Uniqueness of the Determinant ).Forn∈Nsuppose D:Fn×n→Fhas
the following properties:
DP1. Multilinearity. For any 1≤j≤nandx∈F,
D(. . . ,aj, . . .) +D(. . . ,bj, . . .) =D(. . . ,aj+bj, . . .),
and
D(. . . , xaj, . . .) =xD(. . . ,aj, . . .).
DP2. Alternating. For any 1≤j < k≤n,
D(. . . ,aj, . . . ,ak, . . .) =−D(. . . ,ak
j, . . . ,aj
k, . . .).
DP3. Normalization.
D(In) = 1 .
Then D= det n.
Proof. Applying DP2 in the case when aj=ak=ugives
D(. . . ,u, . . . ,u, . . .) =−D(. . . ,u, . . . ,u, . . .),
and hence
D(. . . ,u, . . . ,u, . . .) = 0 .
That is, D(A) = 0 whenever A∈Fn×nhas two identical columns.
LetA= [a1···an]∈Fn×nbe arbitrary. By DP1,
D(A) =D(a1, . . . ,an) =D nX
i1=1ai11ei1, . . . ,nX
in=1ainnein!192
=nX
i1=1···nX
in=1D 
ai11ei1, . . . , a innein
=nX
i1=1···nX
in=1ai11···ainnD(ei1, . . . ,ein). (5.25)
It remains to evaluate D(ei1, . . . ,ein) in(5.25) . In fact we have D(ei1, . . . ,ein) = 0 whenever
ik=iℓfor some k̸=ℓ, since the matrix [ ei1···ein] then has two identical columns, and it
follows that only those terms in the sum (5.25) for which the list of values i1, . . . , i nrepresents a
permutation σ∈ S nare all that’s left. In particular, for each such term we take σto be given by
σ(k) =ikfor 1≤k≤n, and since there is a one-to-one correspondence between the remaining
terms in (5.25) and the elements of Sn, we obtain
D(A) =X
σ∈Snaσ(1),1···aσ(n),nD(eσ(1), . . . ,eσ(n)). (5.26)
Now, for any σ∈ S nthere exist, by Proposition 5.36, transpositions τ1, . . . , τ msuch that
σ=τm◦ ··· ◦ τ1. By DP2,
D(e1, . . . ,en) =−D(eτ1(1), . . . ,eτ1(n)) = (−1)2D(eτ2(τ1(1)), . . . ,eτ2(τ1(n)))
= (−1)3D(eτ3(τ2(τ1(1))), . . . ,eτ3(τ2(τ1(n))))
...
= (−1)mD(e(τm◦···◦τ1)(1), . . . ,e(τm◦···◦τ1)(n))
= sgn( σ)D(eσ(1), . . . ,eσ(n)),
and so by DP3, noting that 1 /sgn(σ) = sgn( σ),
D(eσ(1), . . . ,eσ(n)) = sgn( σ)D(e1, . . . ,en) = sgn( σ)D(In) = sgn( σ).
Putting this result into (5.26) gives
D(A) =X
σ∈Snaσ(1),1···aσ(n),nsgn(σ). (5.27)
The expression at right in (5.27) is entirely independent of D. Indeed, if we assume ˆDis
another function on Fn×nthat satisfies the properties DP1, DP2, and DP3, then an identical
argument will lead to ˆD(A) equalling the same expression, and hence ˆD(A) =D(A). Since
detnhas the properties DP1, DP2, and DP3, we conclude that det n(A) =D(A). ■
The proof of Theorem 5.39 immediately gives the following result, which is a formula for the
determinant function that is explicit rather than recursive.
Theorem 5.40 (Leibniz Formula ).For any n∈NandA∈Fn×n,
detn(A) =X
σ∈Snsgn(σ)aσ(1),1aσ(2),2···aσ(n),n.193
Proposition 5.41. Letn≥2. For any 1≤k < ℓ≤n,
X
σ∈Snsgn(σ) 
aσ(k),kaσ(ℓ),kY
i∈In\{k,ℓ}aσ(i),i!
= 0. (5.28)
Proof. Fix 1≤k < ℓ≤n, and let Σ σdenote the sum in (5.28). Then
Σσ=X
π∈An 
aπ(k),kaπ(ℓ),kY
i∈In\{k,ℓ}aπ(i),i!
−X
ν∈Sn\An 
aν(k),kaν(ℓ),kY
i∈In\{k,ℓ}aν(i),i!
. (5.29)
Fixπ0∈ A n. Then ν0=π0◦(k, ℓ)∈ S n\ Anis given by
ν0(i) =

π0(i),ifi∈In\ {k, n}
π0(ℓ),ifi=k
π0(k),ifi=ℓ,
so that
aν0(ℓ),kaν0(k),kY
i∈In\{k,ℓ}aν0(i),i=aπ0(k),kaπ0(ℓ),kY
i∈In\{k,ℓ}aπ0(i),i.
This shows that the term in the sumP
π∈Anthat corresponds to π0is canceled by the term inP
ν∈Sn\Anthat corresponds to ν0at right in (5.29) . In a similar way, for any ν1∈ S n\ Anwe
have π1=ν1◦(k, ℓ)∈ A n, and the terms inP
π∈AnandP
ν∈Sn\Ancorresponding to π1andν1
will cancel in (5.29). Therefore the sumP
σmust equal zero. ■194
6
Eigen Theory
6.1 – Eigenvectors and Eigenvalues
Throughout this chapter we assume that all vector spaces are finite-dimensional with
dimension at least 1 unless otherwise specified.
Definition 6.1. LetVbe a vector space over FandL:V→Va linear operator. An
eigenvector ofLis a nonzero vector v∈Vsuch that
L(v) =λv
for some λ∈F. The scalar λis an eigenvalue ofL, and vis said to be an eigenvector
corresponding to λ. The set
EL(λ) ={v∈V:L(v) =λv}
is the eigenspace ofLcorresponding to λ.
The symbol σ(L) will occasionally be used to denote the set of eigenvalues possessed by a
linear operator L, so that |σ(L)|denotes the number of distinct eigenvalues of L.
A careful examination of Definition 6.1 should make it clear that, while the zero vector
0∈Vcannot be an eigenvector, the zero scalar 0 ∈Fcan be an eigenvalue. Despite not being
an eigenvector, however, it is always true that 0is an element of EL(λ) since
L(0) =0=λ0
holds for any linear operator L
Proposition 6.2. LetVbe a vector space. If L:V→Vis a linear operator with eigenvalue λ,
then EL(λ)is a subspace of V.
Proof. Suppose L:V→Vis linear with eigenvalue λ. It has already been established that
0∈EL(λ). Given u,v∈EL(λ) and scalar cwe have
L(u+v) =L(u) +L(v) =λu+λv=λ(u+v)195
and
L(cv) =cL(v) =c(λv) =λ(cv),
which shows that u+v∈EL(λ) and cv∈EL(λ). ■
Example 6.3. LetVbe a vector space and consider the identity operator IV:V→Vgiven
byIV(v) =vfor all v∈V. It is clear that λ= 1 is the only eigenvalue of IV, and all nonzero
vectors in Vare corresponding eigenvectors. Indeed,
EIV(1) ={v∈V:IV(v) =v}=V
is the corresponding eigenspace. ■
Example 6.4. LetVbe a vector space and consider the zero operator OV:V→Vgiven by
OV(v) =0for all v∈V. For any v̸=0, then, we have
OV(v) =0= 0v,
which shows that 0 is an eigenvalue of OV. Moreover
EOV(0) ={v∈V:OV(v) = 0v}=V
is the corresponding eigenspace. There are no other eigenvalues. ■
In addition to eigenvectors, eigenvalues, and eigenspaces of linear mappings, there are related
notions for square matrices.
Definition 6.5. LetA∈Fn×n. Aneigenvector ofAis a nonzero vector x∈Fnsuch that
Ax=λx
for some λ∈F. The scalar λis an eigenvalue ofA, and xis said to be an eigenvector
corresponding to λ. The set
EA(λ) ={x∈Fn:Ax=λx}
is the eigenspace ofAcorresponding to λ.
The symbol σ(A) will occasionally be used to denote the set of eigenvalues possessed by a
square matrix A, so that |σ(A)|denotes the number of distinct eigenvalues of A.
Remark. A careful reading of Definition 6.5 should make it clear that any eigenvector corre-
sponding to an eigenvalue of A∈Fn×nmust be an element of Fn. Thus, if we are given that
A∈Rn×n, then we would discount any z∈Cn\Rnfor which Az=λzfor some λ∈R.
Proposition 6.6. Ifλis an eigenvalue of A∈Fn×n, then EA(λ)is a subspace of Fn.196
Proof. Suppose that λis an eigenvalue of A. By Definitions 6.5 and 3.15,
x∈EA(λ)⇔Ax=λx⇔Ax−λx=0⇔Ax−λInx=0
⇔(A−λIn)x=0⇔x∈Nul(A−λIn).
That is,
EA(λ) = Nul( A−λIn), (6.1)
the null space of A−λIn. By Proposition 3.16 Nul(A−λIn) is a subspace of Fn, and hence so
too is EA(λ). ■
Proposition 6.7. LetVbe a vector space over F, and suppose L∈ L(V)has eigenvalues λ1, λ2
with corresponding eigenvectors v1,v2, respectively.
1.Ifλ1̸=λ2, then v1̸=v2.
2.EL(λ1)∩EL(λ2) ={0}if and only if λ1̸=λ2.
Proof.
Proof of Part (1). We will prove the contrapositive: “If v1=v2, then λ1=λ2.” Suppose that
v1=v2=v, so
λ1v=λ1v1=L(v1) =L(v2) =λ2v2=λ2v,
and then
(λ1−λ2)v=λ1v−λ2v=0.
By Proposition 3.2(3) either λ1−λ2= 0 or v=0. But v̸=0since an eigenvector is nonzero
by definition, and so it must be that λ1−λ2. Therefore λ1=λ2.
Proof of Part (2). Suppose λ1=λ2=λ, so that λis an eigenvalue of Lwith corresponding
eigenvectors v1andv2. In particular
v1∈EL(λ) =EL(λ1) =EL(λ2),
and thus
v1∈EL(λ1)∩EL(λ2).
Since v1̸=0, it follows that EL(λ1)∩EL(λ2)̸={0}.
For the converse, suppose λ1̸=λ2. Let v∈EL(λ1)∩EL(λ2). Then L(v) =λ1vand
L(v) =λ2v, and thus λ1v=λ2v. Now,
λ1v=λ2v⇒(λ1−λ2)v=0,
and since λ1−λ2̸= 0, Proposition 3.2(3) implies that v=0. Therefore EL(λ1)∩EL(λ2) =
{0}. ■
The converse of Proposition 6.7(1) is not true in general; that is, if L∈ L(V) has eigenvalues
λ1, λ2with corresponding eigenvectors v1,v2, then v1̸=v2does not necessarily imply that
λ1̸=λ2. Consider for example v2= 2v1: certainly v1̸=v2since we know v1̸=0, but
L(v2) =L(2v1) = 2 L(v1) = 2( λ1v1) =λ1(2v1) =λ1v2
shows that λ1=λ2.197
Theorem 6.8. LetVbe a vector space over F, and let L∈ L(V)have distinct eigenvalues
λ1, . . . , λ n∈F. Ifv1, . . . ,vnare eigenvectors corresponding to λ1, . . . , λ n, respectively, then the
set{v1, . . . ,vn}is linearly independent.
Proof. An eigenvector is nonzero by definition, so if n= 1 then certainly the set {v1}is linearly
independent. This establishes the base case of an inductive argument.
Suppose the theorem is true when n=m, where mis some arbitrary positive integer (this
is our “inductive hypothesis”). Let Lbe a linear operator on Vwith distinct eigenvalues
λ1, . . . , λ m+1and corresponding eigenvectors v1, . . . ,vm+1, so that L(vk) = λkvkfor each
1≤k≤m+ 1. Suppose c1, . . . , c m+1∈Fare such that
m+1X
k=1ckvk=0. (6.2)
Since the eigenvalues λ1, . . . , λ m+1are distinct, there exists some 1 ≤k0≤m+ 1 such that
λk0̸= 0. Since the eigenvalues may be indexed in any convenient way, we can assume k0=m+ 1
so that λm+1̸= 0. Multiplying (6.2) by λm+1gives
m+1X
k=1ckλm+1vk=0, (6.3)
and we also have
L m+1X
k=1ckvk!
=L(0)⇒m+1X
k=1ckL(vk) =0⇒m+1X
k=1ckλkvk=0. (6.4)
Subtracting (6.3) from the rightmost equation in (6.4), we obtain
m+1X
k=1ckλkvk−m+1X
k=1ckλm+1vk=0,
so that
m+1X
k=1ck(λk−λm+1)vk=0. (6.5)
Of course
ck(λk−λm+1)vk=0
ifk=m+ 1, and so (6.5) becomes
mX
k=1ck(λk−λm+1)vk=0. (6.6)
Now, v1, . . . ,vmare the eigenvectors corresponding to the distinct eigenvalues λ1, . . . , λ m, and
so by our inductive hypothesis the set {v1, . . . ,vm}is linearly independent. From (6.6) it follows
that
ck(λk−λm+1) = 0198
for 1≤k≤m, which in turn implies that ck= 0 for 1 ≤k≤msince λ1, . . . , λ mdo not equal
λm+1. Now (6.2) becomes cm+1vm+1=0, which immediately yields cm+1= 0. Since (6.2) results
only in the trivial solution
c1=···=cm+1= 0
we conclude that v1, . . . ,vm+1are linearly independent.
We see now that the theorem holds for n=m+ 1 when we assume that it holds for n=m,
and therefore by induction it holds for all n∈N. ■
Corollary 6.9. IfVis a finite-dimensional vector space and L∈ L(V), then Lhas at most
dim(V)distinct eigenvalues.
Proof. Suppose that Vis an n-dimensional vector space and L∈ L(V). Suppose λ1, . . . , λ n+1
are distinct eigenvalues of Lwith corresponding eigenvectors v1, . . . ,vn+1. Then {v1, . . . ,vn+1}
is a basis for Vby Theorem 6.8 and we are led to conclude that the dimension of Visn+ 1,
which is a contradiction. Therefore Lhas at most ndistinct eigenvalues. ■
Example 6.10. LetA∈Fn×nbe the diagonal matrix
A=
λ1··· 0
.........
0···λn

having distinct diagonal entries λ1, . . . , λ n(i.e.λi̸=λjwhenever i̸=j). Ife1, . . . ,enare the
standard basis vectors for Fn, so that
e1=
1
0
...
0
,e2=
0
1
...
0
, . . . , en=
0
0
...
1
,
then for each 1 ≤k≤nwe find that Aek=λkek, and so λkis an eigenvalue of A.
IfLis the linear operator on Fnhaving Aas its corresponding matrix with respect to
the standard basis E={e1, . . . ,en}, then clearly λ1, . . . , λ nare distinct eigenvalues of L, with
e1, . . . ,enbeing corresponding eigenvectors:
L(ek) =Aek=λkek.
Of course the eigenvectors e1, . . . ,enare linearly independent as predicted by Theorem 6.8. ■
Proposition 6.11. An operator L∈ L(V)is not invertible if and only if 0is an eigenvalue of
L. A matrix A∈Fn×nis not invertible if and only if 0is an eigenvalue of A.
Proof. By the Invertible Operator Theorem (Theorem 4.65), Lis not invertible if and only if
Nul(L)̸={0}, and Nul(L)̸={0}if and only if there exists some v̸=0such that L(v) =0,
which is to say 0 is an eigenvalue of Lsince 0= 0v.
By the Invertible Matrix Theorem (Theorem 5.17), Ais not invertible if and only if
Nul(A)̸={0}, and Nul(A)̸={0}if and only if there exists some x̸=0such that Ax=0,
which is to say 0 is an eigenvalue of Asince 0= 0x. ■199
Proposition 6.12. LetL∈ L(V)andA∈Fn×n.
1.Letn∈N. Ifλis an eigenvalue of L(resp. A) with corresponding eigenvector v, then λnis
an eigenvalue of Ln(resp. An) with eigenvector v.
2.Suppose LandAare invertible. If λis an eigenvalue of L(resp. A) with corresponding
eigenvector v, then λ−1is an eigenvalue of L−1(resp. A−1) with eigenvector v.
The proof will consider only the statements about an operator L:V→V, since the
arguments are much the same for a square matrix A.
Proof.
Proof of Part (1): Then= 1 case is trivially true. Suppose the statement of Part (1) is true for
some arbitrary n∈N. Let λbe an eigenvalue of Lwith corresponding eigenvector v. Then
L(v) =λv, and by our inductive hypothesis Ln(v) =λnv. Now,
Ln+1(v) =Ln(L(v)) =Ln(λv) =λLn(v) =λ(λnv) =λn+1v,
and Part (1) is proven for all n∈Nby the principle of induction.
Proof of Part (2): Suppose that λis an eigenvalue of Lwith corresponding eigenvector v, so
thatL(v) =λv. By Proposition 4.55 we obtain L−1(λv) =v, and since λ̸= 0 by Proposition
6.11, it follows that
L−1(λv) =v⇒λL−1(v) =v⇒L−1(v) =λ−1v.
Hence λ−1is an eigenvalue of L−1. ■200
6.2 – The Characteristic Polynomial
Definition 6.13. LetA∈Fn×n. The characteristic polynomial ofAis the polynomial
function PA:F→Fgiven by
PA(t) = det n(A−tIn).
Some books define the characteristic polynomial of Ato be det(tIn−A) instead of
det(A−tIn), but whichever way it is done will have no impact on either the theory of
characteristic polynomials or any application involving them. This is because only the zeros of
the characteristic polynomial will be of any concern. Setting QA(t) =det(tIn−A), observe
thatPA=QAifnis even, and PA=−QAifnis odd. In either case PAandQAwill have the
same zeros.
Proposition 6.14. LetVbe a vector space over Fwith dim(V) =n, and let L∈ L(V). Then
the following statements are equivalent.
1.λis an eigenvalue of Lwith corresponding eigenvector u.
2.There exists some basis BforVsuch that λis an eigenvalue of [L]Bwith corresponding
eigenvector [u]B.
3.For all bases BforV,λis an eigenvalue of [L]Bwith corresponding eigenvector [u]B.
Proof.
(1)⇒(3):Suppose that λis an eigenvalue of L, so there exists some u̸=0such that L(u) =λu.
LetBbe any basis for V, and let [ L]B∈Fn×nbe the matrix corresponding to Lwith respect to
B, so that
[L]B[v]B= [L(v)]B
for all v∈V. Recall that by Theorem 4.11 the coordinate map V→Fngiven by v7→[v]Bis
an isomorphism, so [ u]B∈Fnis not the zero vector since Nul([ ·]B) ={0}. Thus, from
[L]B[u]B= [L(u)]B= [λu]B=λ[u]B
we conclude that λis an eigenvalue of [ L]Bwith corresponding eigenvector [ u]B.
(3)⇒(2):This is obvious.
(2)⇒(1): Suppose there exists some basis BforVsuch that λis an eigenvalue of [ L]Bwith
corresponding eigenvector [ u]B. Again, the coordinate map [·]B:V→Fnis an isomorphism, so
[L]B[u]B=λ[u]B⇒[L(u)]B= [λu]B⇒L(u) =λu,
where the last implication follows from the fact that [·]B:V→Fnis injective. Therefore λis
an eigenvalue of Lwith corresponding eigenvector u. ■
We see from the proposition that if we consider two different bases BandB′forV, then
the matrices corresponding to Lwith respect to these bases, [ L]Band [ L]B′, have the same
eigenvalues as L. The only thing that changes is the corresponding eigenvector: [ v]Bis an201
eigenvector of [ L]Bcorresponding to eigenvalue λif and only if [ v]B′is an eigenvector of [ L]B′
corresponding to eigenvalue λ.
But there’s something more: the characteristic polynomials of [ L]Band [ L]B′will be found
to be the same! To see this, recall IBB′, the change of basis matrix from BtoB′. By Corollary
4.33 we have
[L]B′=IBB′[L]BI−1
BB′.
Now, noting that In=IBB′InI−1
BB′and
det(IBB′) det(I−1
BB′) = det( IBB′)·1
det(IBB′)= 1
by Theorem 5.24, for any t∈Fwe have
P[L]B′(t) = det([ L]B′−tIn) = det 
IBB′[L]BI−1
BB′−t(IBB′InI−1
BB′)
= det 
(IBB′[L]B−t(IBB′In))I−1
BB′
= det 
(IBB′[L]B−IBB′(tIn))I−1
BB′
= det 
IBB′([L]B−tIn)I−1
BB′
= det( IBB′) det([ L]B−tIn) det(I−1
BB′)
= det([ L]B−tIn) =P[L]B(t)
by Theorem 5.23. That is, P[L]B′=P[L]B, which is to say that the characteristic polynomial of
a linear operator’s associated matrix is invariant under change of basis. We have proven the
following.
Proposition 6.15. LetL:V→Vbe a linear operator, and let BandB′be bases for V. If
[L]Band[L]B′are the matrices corresponding to Lwith respect to BandB′, then P[L]B=P[L]B′.
Because of Proposition 6.15, it makes sense to speak of the “characteristic polynomial” of a
linear operator on Vwithout reference to any specific basis for V.
Definition 6.16. LetLbe a linear operator on V. The characteristic polynomial of Lis
the polynomial function PL:F→Fgiven by
PL(t) =P[L](t),
where [L]is the matrix corresponding to Lwith respect to any basis for V.
While the idea of an eigenvalue is simple, it can be quite difficult to find eigenvalues of either
a linear operator or a matrix by direct means. To help find the eigenvalue of a linear operator
we have the following.
Theorem 6.17. LetL:V→Vbe a linear operator. Then λis an eigenvalue of Lif and only
ifL−λIVis not invertible.
Proof. Suppose that λis an eigenvalue of L. Then there exists some v∈Vsuch that v̸=0
andL(v) =λv. Now,
(L−λIV)(v) =L(v)−(λIV)(v) =λv−λIV(v) =λv−λv=0,202
which shows that v∈Nul(L−λIV) and so Nul(L−λIV)̸={0}. Hence L−λIVis not invertible
by the Invertible Operator Theorem.
For the converse, suppose that L−λIVis not invertible. Then Nul(L−λIV)̸={0}
by the Invertible Operator Theorem, and it follows that there exists some v̸=0such that
(L−λIV)(v) =0. Now,
(L−λIV)(v) =0⇔L(v)−λIV(v) =0⇔L(v)−λv=0⇔L(v) =λv,
and therefore λis an eigenvalue of L. ■
The next theorem plainly reduces the problem of finding eigenvalues of an n×nmatrix to
that of finding the zeros of an nth-degree polynomial function. We begin to see the utility of
characteristic polynomials at this point.
Theorem 6.18. LetA∈Fn×n. Then λis an eigenvalue of Aif and only if PA(λ) = 0 .
Proof. Suppose that λis an eigenvalue of A, so that Ax0=λx0for some x0∈Fn. Define the
linear mapping L:Fn→FnbyL(x) =Ax. Then
L(x0) =Ax 0=λx0
shows that λis an eigenvalue of L, and so by Theorem 6.17 the mapping
L−λIFn:Fn→Fn
is not invertible. Let I=IFn, and observe that the matrix associated with L−λIisA−λIn:
(L−λI)(x) =L(x)−λI(x) =Ax−λx=Ax−λInx= (A−λIn)x
for all x∈Fn. Thus, since the operator L−λIis not invertible, by Corollary 4.59 its associated
square matrix A−λInis also not invertible, and so det(A−λIn) = 0 by the Invertible Matrix
Theorem. That is, PA(λ) = 0.
Conversely, suppose that PA(λ) = 0. Then det(A−λIn) = 0, so by the Invertible Matrix
Theorem A−λInis not invertible. Define L:Fn→FnbyL(x) =Ax. Then A−λInis the
matrix corresponding to the linear operator L−λI:Fn→Fn, and by Corollary 4.59 L−λIis
not invertible. So λis an eigenvalue of Lby Theorem 6.17, which is to say there exists some
nonzero x0∈Fnsuch that L(x0) =λx0. Hence Ax 0=λx0and we conclude that λis an
eigenvalue of A. ■
Example 6.19. Find the characteristic polynomial PA:R→Rof
A=
1−3 3
3−5 3
6−6 4
,
find the eigenvalues of A, and find a basis for each eigenspace as a subspace of R3.203
Solution. We have
PA(t) = det( A−tI3) = det

1−3 3
3−5 3
6−6 4
−
t0 0
0t0
0 0 t

=1−t−3 3
3−5−t3
6 −6 4 −t
c1+c2→c2= = = = = = =1−t−2−t3
3−2−t3
6 0 4 −t−r1+r2→r2= = = = = = = =1−t−2−t3
t+ 2 0 0
6 0 4 −t.
Expanding the determinant according to the 2nd row then gives
PA(t) = (−1)2+1(t+ 2)−2−t3
0 4 −t= (t+ 2)2(t−4),
and so we see that PA(t) = 0 for t=−2,4. Thus by Theorem 6.18 the eigenvalues of Aare
λ=−2,4.
By (6.1) the eigenspace of Acorresponding to λ=−2 is
EA(−2) = Nul( A+ 2I3) ={x∈R3: (A+ 2I3)x=0}
=


x1
x2
x3
∈R3:
3−3 3
3−3 3
6−6 6

x1
x2
x3
=
0
0
0


.
Writing the matrix equation—which is a homogeneous system of equations—as an augmented
matrix, we have
3−3 3 0
3−3 3 0
6−6 6 0
−r1+r2→r2− − − − − − − − →
−2r1+r3→r3
3−3 3 0
0 0 0 0
0 0 0 0
1
3r1→r1− − − − − →
1−1 1 0
0 0 0 0
0 0 0 0
. (6.7)
Hence x1−x2+x3= 0, which implies that x3=x2−x1and so
EA(−2) =


x1
x2
x3
∈R3:x1−x2+x3= 0

=


x1
x2
x2−x1
:x1, x2∈R

.
Observing that
x1
x2
x2−x1
=
x1
0
−x1
+
0
x2
x2
=
1
0
−1
x1+
0
1
1
x2,
we have
EA(−2) =


1
0
−1
x1+
0
1
1
x2:x1, x2∈R


and so it is clear that
B−2=


1
0
−1
,
0
1
1



is a linearly independent set of vectors that spans EA(−2) and therefore must be a basis for
EA(−2). Notice that the elements of B−2are in fact eigenvectors of Acorresponding to the
eigenvalue −2, as are all the vectors belonging to EA(−2).204
Next, the eigenspace of Acorresponding to λ= 4 is
EA(4) = Nul( A−4I3) ={x∈R3: (A−4I3)x=0}
=


x1
x2
x3
∈R3:
−3−3 3
3−9 3
6−6 0

x1
x2
x3
=
0
0
0


.
Applying Gaussian Elimination to the corresponding augmented matrix yields

−3−3 3 0
3−9 3 0
6−6 0 0
r1+r2→r2− − − − − − − →
2r1+r3→r3
−3−3 3 0
0−12 6 0
0−12 6 0
−r2+r3→r3− − − − − − − − →
−1
2r2+r1→r1
−3 3 0 0
0−12 6 0
0 0 0 0
.
From the top row we obtain x2=x1, and from the middle row we obtain x3= 2x2and thus
x3= 2x1. Now,
EA(4) =


x1
x1
2x1
:x1∈R

=


1
1
2
x1:x1∈R

.
Clearly
B4=


1
1
2



is a linearly independent set that spans EA(4) and so qualifies as a basis for EA(4). The vector
belonging to B4is an eigenvector of Acorresponding to the eigenvalue 4, as is any real scalar
multiple of the vector. ■
In Example 6.19 we found in (6.7) that A+ 2I3is row-equivalent to
B=
1−1 1
0 0 0
0 0 0
,
which clearly has rank 1, and so
rank(A+ 2I3) = rank( B) = 1
by Theorem 3.66. Then by the Rank-Nullity Theorem for Matrices we have
dim(EA(−2)) = nullity( A+ 2I3) = dim( R3)−rank(A+ 2I3) = 3−1 = 2 . (6.8)
Then, employing the equation x1−x2+x3= 0 obtained at right in (6.7), we could have easily
obtained the two solutions 
1
1
0
and
0
1
1
.
Since these two vectors in EA(−2) are linearly independent and we know from (6.8) thatEA(−2)
has dimension 2, we can conclude by Theorem 3.54(1) that the two vectors must be a basis for
EA(−2). We mention this here in order to suggest an alternative means of finding a basis for
an eigenspace which makes use of earlier theoretical developments.205
In the next example, for variety’s sake, eigenspaces will be found using Definition 6.5 directly,
rather than equation (6.1).
Example 6.20. Find the eigenvalues of the matrix
A=
−1 4−2
−3 4 0
−3 1 3
,
and also find a basis for each eigenspace as a subspace of R3.
Solution. Expanding the determinant according to the second row, we have
PA(t) = det( A−tI3) =−1−t4−2
−3 4 −t0
−3 1 3 −t
= (−1)2+1(−3)4−2
1 3−t+ (−1)2+2(4−t)−1−t−2
−3 3 −t
=−t3+ 6t2−11t+ 6,
and so
PA(t) = 0 ⇔t3−6t2+ 11t−6 = 0 .
By the Rational Zeros Theorem of algebra, the only rational numbers that may be zeros of PA
are±1,±2,±3 and ±6. It’s an easy matter to verify that 1 is in fact a zero, and so by the
Factor Theorem of algebra t−1 must be a factor of PA(t). Now,
t3−6t2+ 11t−6
t−1=t2−5t+ 6,
whence we obtain
t3−6t2+ 11t−6 = 0 ⇔(t−1)(t2−5t+ 6) = 0 ⇔(t−1)(t−2)(t−3) = 0 ,
and therefore PA(t) = 0 if and only if t= 1,2,3. By Theorem 6.18 the eigenvalues of Aare
λ= 1,2,3.
The eigenspace of Acorresponding to λ= 1 is
EA(1) ={x∈R3:Ax=x}=


x
y
z
∈R3:
−1 4−2
−3 4 0
−3 1 3

x
y
z
=
x
y
z


.
The matrix equation yields the system of equations


−x+ 4y−2z=x
−3x+ 4y =y
−3x+y+ 3z=z206
or equivalently


−x+ 2y−1z= 0
−x+y = 0
−3x+y+ 2z= 0
Apply Gaussian elimination on the corresponding augmented matrix:

−1 2−10
−1 1 0 0
−3 1 2 0
−r1+r2→r2− − − − − − − − →
−3r1+r3→r3
−1 2 −10
0−1 1 0
0−5 5 0
−5r2+r3→r3− − − − − − − − →
−1 2 −10
0−1 1 0
0 0 0 0
,
so from the second row we have y=z, and from the first row we have x= 2y−z= 2z−z=z.
Replacing zwith t, so that x=y=z=t, we have
EA(1) =


t
t
t
:t∈R

=


1
1
1
t:t∈R

.
From this we see that the set
B1=


1
1
1



is a basis for EA(1).
The eigenspace of Acorresponding to λ= 2 is
EA(2) ={x∈R3:Ax= 2x}=


x
y
z
∈R3:
−1 4−2
−3 4 0
−3 1 3

x
y
z
=
2x
2y
2z


.
The matrix equation yields the system of equations


−3x+ 4y−2z= 0
−3x+ 2y = 0
−3x+y+z= 0
Apply Gaussian elimination on the corresponding augmented matrix:

−3 4−20
−3 2 0 0
−3 1 1 0
−r1+r2→r2− − − − − − − →
−r1+r3→r3
−3 4 −20
0−2 2 0
0−3 3 0
−3
2r2+r3→r3− − − − − − − − →
−3 4 −20
0−2 2 0
0 0 0 0
,
so from the second row we have y=z, and from the first row we have
x=4
3y−2
3z=4
3z−2
3z=2
3z.
Hence
EA(2) =


2z/3
z
z
:z∈R

=


2/3
1
1
z:z∈R

.207
If we replace zwith 3 t, we obtain an equivalent rendition of E2that features no fractions:
EA(2) =


2
3
3
t:t∈R

.
The set
B2=


2
3
3



is a basis for E2.
Finally, the eigenspace of Acorresponding to λ= 3 is
EA(3) ={x∈R3:Ax= 3x}=


x
y
z
∈R3:
−1 4−2
−3 4 0
−3 1 3

x
y
z
=
3x
3y
3z


.
The matrix equation yields the system of equations


−4x+ 4y−2z= 0
−3x+y = 0
−3x+y = 0
Once more we apply Gaussian elimination to the augmented matrix:

−4 4−20
−3 1 0 0
−3 1 0 0
−r2+r3→r3− − − − − − − →
1
2r1→r1
−2 2−10
−3 1 0 0
0 0 0 0
,
soy= 3xand−2x+ 2y−z= 0, where
−2x+ 2y−z= 0⇒z=−2x+ 2y⇒z= 4x.
Therefore, replacing xwith tso that y= 3tandz= 4t, we have
EA(3) =


1
3
4
t:t∈R

.
The set
B3=


1
3
4



is a basis for EA(3). ■
Example 6.21. Find the eigenvalues of the matrix
A=
2 3
−1 4
,
and also find a basis for each eigenspace as a subspace of C2.208
Solution. We have
PA(t) = det( A−tI2) =2−t3
−1 4−t=t2−6t+ 11,
and so
PA(t) = 0 ⇔t2−6t+ 11 = 0 ⇔t= 3±i√
2.
That is, Ahas two complex-valued eigenvalues. Let λ= 3−i√
2. The eigenspace corresponding
toλis
EA(λ) ={z∈C2:Az=λz}=
z1
z2
∈C2:
2 3
−1 4
z1
z2
=
λz1
λz2
.
Now,
2 3
−1 4
z1
z2
=
λz1
λz2
corresponds to the system of equations
(2−λ)z1+ 3 z2= 0
−z1+ (4−λ)z2= 0
We apply Gaussian elimination to the augmented matrix,
2−λ 3 0
−1 4−λ0
r1↔r2− − − − →
−1 4−λ0
2−λ 3 0
(2−λ)r1+r2→r2− − − − − − − − − − →
−1 4−λ0
0 0 0
,
observing that
(2−λ)(4−λ) + 3 = ( λ2−6λ+ 8) + 3 = 
3−i√
22−6 
3−i√
2
+ 11 = 0 .
Thus
z1= 
−1−i√
2
z2,
and so we obtain
EA(λ) = 
−1−i√
2
z2
z2
:z2∈C
=
−1−i√
2
1
z:z∈C
,
where for simplicity we replace z2with zin the end. Hence the eigenvector
−1−i√
2
1
corresponding to the eigenvalue 3 −i√
2 constitutes a basis for the eigenspace EA(λ).
The analysis of the other eigenvalue 3 + i√
2is quite similar (with eigenspace also of
dimension 1) and so is left as a problem. ■
Example 6.22. We will show that, for all n∈N, ifA∈Fis given by
A=
0 0 0 ··· 0−a0
1 0 0 ··· 0−a1
0 1 0 ··· 0−a2..................
0 0 0 ··· 0−an−2
0 0 0 ··· 1−an−1
, (6.9)209
then
PA(t) = (−1)n(a0+a1t+···+an−1tn−1+tn). (6.10)
In the case when n= 1 we take A= [−a0], whereupon we obtain
PA(t) = det 1(A−tI1) = det 1([−a0−t]) =−a0−t= (−1)(a0+t).
This establishes the base case of an inductive argument. Fix n∈N, and suppose any matrix of
the form (6.9) has characteristic polynomial (6.10); that is, det n(A−tIn) is given by
−t0 0 ··· 0−a0
1−t0··· 0−a1
0 1 −t··· 0−a2..................
0 0 0 ··· − t−an−2
0 0 0 ··· 1−an−1−t= (−1)n(a0+a1t+···+an−1tn−1+tn).
Now, define A∈F(n+1)×(n+1)by
A=
0 0 0 ··· 0−a0
1 0 0 ··· 0−a1
0 1 0 ··· 0−a2..................
0 0 0 ··· 0−an−1
0 0 0 ··· 1−an
,
so
A−tIn+1=
−t0 0 ··· 0−a0
1−t0··· 0−a1
0 1 −t··· 0−a2..................
0 0 0 ··· − t−an−1
0 0 0 ··· 1−an−t
.
Letting B=A−tIn+1,
PA(t) = det n+1(B) =n+1X
j=1(−1)1+ja1jdetn(B1j)
=−tdetn(B11) + (−1)n+2(−a0) det n(B1(n+1)),
where
B11=
−t0 0 ··· 0−a1
1−t0··· 0−a2
0 1 −t··· 0−a3..................
0 0 0 ··· − t−an−1
0 0 0 ··· 1−an−t
and B1(n+1)=
1−t0··· 0 0
0 1 −t··· 0 0
0 0 1 ··· 0 0
..................
0 0 0 ··· 1−t
0 0 0 ··· 0 1
.210
Clearly det n(B1(n+1)) = 1, and by the inductive hypothesis we have
detn(B11) = (−1)n(a1+a2t+···+antn−1+tn),
so that
PA(t) =−t(−1)n(a1+a2t+···+antn−1+tn)−(−1)na0
= (−1)n+1(a1t+a2t2+···+antn+tn+1) + (−1)n+1a0
= (−1)n+1(a0+a1t+···+antn+tn+1),
as desired. ■
Problems
1. For each of the 2 ×2 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)
3 0
8−1
(b)
0 3
4 0
(c)
−2−7
1 2
(d)
0 0
0 0
2. For each of the 3 ×3 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)
4 0 1
−2 1 0
−2 0 1
 (b)
3 0 −5
1
5−1 0
1 1 −2
 (c)
5 6 2
0−1−8
1 0 −2

3. For each of the 4 ×4 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)
0 0 2 0
1 0 1 0
0 1−2 0
0 0 0 1
(b)
10−9 0 0
4−2 0 0
0 0 −2−7
0 0 1 2
211
6.3 – Applications of the Characteristic Polynomial
Recall that Pn(F) denotes the set of polynomials of degree nwith coefficients in F. That is,
forn∈W,
Pn(F) =(nX
k=0akxk:a0, . . . , a n∈Fandan̸= 0)
.
We regard 0 to be the polynomial of degree −1 and define P−1(F) ={0}. IfFis an infinite field
such as RorC, it is common to treat a polynomial as a function f:F→Fgiven by
f(x) =nX
k=0akxk
for all x∈F, in which case it is called a polynomial function. For n≥ −1, a polynomial function
pis said to have degree niff(x)∈ P n(F), in which case we write deg(p) =n. Thus, we may
just as well regard Pn(F) as the set of all polynomial functions of degree n, so that it makes as
much sense to write f∈ P n(F) asf(x)∈ P n(F). Finally, we define
P(F) =∞[
n=0Pn−1(F).
In what follows we will have need of the following theorem, which is proven in §5.1 of the
Complex Analysis Notes.
Theorem 6.23 (Fundamental Theorem of Algebra ).If
p(z) =anzn+an−1zn−1+···+a1z+a0
is a polynomial function of degree n≥1with coefficients a0, . . . , a n∈C, then there exists some
z0∈Csuch that p(z0) = 0 .
Theorem 6.24 (Division Algorithm for Polynomials ).Letf∈ P n(F), and let g∈ P m(F)
for some m≥0. Then there exist unique polynomial functions qandrsuch that
f(x) =q(x)g(x) +r(x)
for all x∈F, where deg(r)≤m.
Theorem 6.25 (Factor Theorem ).Letf∈ P n(F)for some n≥1, and let c∈F. Then
f(c) = 0 if and only if x−cis a factor of f(x).
Lemma 6.26. Suppose P(t) = [pij(t)]n∈Fn×nis such that pijis a polynomial function for all
1≤i, j≤n. If
deg(pij)≤(
0, i̸=j
1, i=j
then deg(det n(P(t)))≤n.212
Proof. The statement of the lemma is clearly true in the case when n= 1. Suppose that it is
true for some arbitrary n∈N. Let P(t) = [pij(t)]n+1. Then, expanding along the first row, we
have
detn+1(P(t)) =n+1X
j=1(−1)1+jp1j(t) det n(P1j(t)).
For each 1 ≤j≤n+ 1 we find that the n×nsubmatrix P1jis such that all non-diagonal
entries are degree 0 polynomial functions (i.e. constants), and all diagonal entries are polynomial
functions of either degree 0 or degree 1. Thus deg(detn(P1j))≤nby our inductive hypothesis,
and since p1j(t) is a constant for 2 ≤j≤n+ 1, it follows that
deg 
(−1)1+jp1j(t) det n(P1j(t))
≤n
for 2≤j≤n+ 1. In the case when j= 1 we have
(−1)1+jp1j(t) det n(P1j(t)) =p11(t) det n(P11(t)),
where p11(t) has degree at most 1, and det n(P11(t)) has degree at most n. Hence
deg 
p11(t) det n(P11(t))
≤n+ 1,
and therefore deg(detn(P(t)))≤n+ 1 since detn+1(P(t)) is the sum of polynomials of degree at
most n+ 1. Thus the lemma holds true in the n+ 1 case, and so it must hold for all n∈Nby
induction. ■
Proposition 6.27. IfA∈Fn×n, then deg(PA) =nand the lead coefficient of PAis(−1)n.
Proof. In the case when n= 1 we have A= [a], so that
PA(t) = det 1([a]−[t]) = det 1([a−t]) =a−t=−t+a,
and we clearly that deg( PA) = 1 and the lead coefficient of PAis (−1)1.
Suppose the proposition is true for some n∈N. Let A= [aij]n+1∈F(n+1)×(n+1), and define
P(t) =A−tIn+1so that P(t) = [pij(t)]n+1with
deg(pij) =(
0, i̸=j
1, i=j
Now,
PA(t) = det n+1(P(t)) =n+1X
k=1(−1)1+kp1k(t) det n(P1k(t)),
where for each kwe have P1k(t) = [pk,ij(t)]nsuch that
deg(pk,ij)≤(
0, i̸=j
1, i=j
and so deg(detn(P1k(t)))≤nby Lemma 6.26. Since p1k(t) is a constant for 2 ≤k≤n+ 1, it
follows that
deg 
(−1)1+kp1k(t) det n(P1k(t))
≤n213
for 2≤k≤n+ 1. In the case when k= 1 we have
(−1)1+kp1k(t) det n(P1k(t)) =p11(t) det n(P11(t)),
where
detn(P11(t)) = det n(A11−tIn) =PA11(t)
has degree nand lead coefficient ( −1)nby our inductive hypothesis. That is,
detn(P11(t)) = (−1)ntn+bn−1tn−1+···+b1t+b0
for some bn−1, . . . , b 0∈F, and since p11(t) =a11−twe obtain
p11(t) det n(P11(t)) = (−t+a11) 
(−1)ntn+bn−1tn−1+···+b1t+b0
= (−1)n+1tn+1+cntn+···+c1t+c0
for some cn, . . . , c 0∈F. Hence Q(t) =p11(t)detn(P11(t)) has degree n+ 1 with lead coefficient
(−1)n+1. Since PA(t) =detn+1(P(t)) is the sum of Q(t) with other polynomials of degree at
most n, it follows that PAlikewise has degree n+ 1 with lead coefficient ( −1)n+1.
We conclude by the principle of induction that the proposition holds for all n∈N, which
finishes the proof. ■
Corollary 6.28. IfVis a nontrivial finite-dimensional vector space over FandL∈ L(V), then
deg(PL) = dim( V)and the lead coefficient of PLis(−1)dim(V).
Proof. Suppose Vis a nontrivial finite-dimensional vector space over FandL∈ L(V). Let Bbe
any basis for V. Since [ L]B∈Fdim(V)×dim(V), by Proposition 6.27 we have deg(P[L]B) =dim(V)
and the lead coefficient of P[L]Bis (−1)n. Now, PL=P[L]Bby Definition 6.16, and so the proof
is done. ■
Proposition 6.29. Letn∈N.
1.IfA∈Cn×n, then 1≤ |σ(A)| ≤n.
2.LetVbe an n-dimensional vector space over C. IfL∈ L(V), then 1≤ |σ(L)| ≤n.
Proof.
Proof of Part (1): LetA∈Cn×n. By Proposition 6.27 the polynomial function PAis of degree
n∈N, so by the Fundamental Theorem of Algebra PAhas at least one zero in C, and by the
Factor Theorem PAhas at most nzeros in C. Since, by Theorem 6.18, λis an eigenvalue of
Aif and only if PA(λ) = 0, it follows that Apossesses at least one and at most ndistinct
eigenvalues. That is, 1 ≤ |σ(A)| ≤n.
Proof of Part (2): Suppose L∈ L(V), and let Bbe an ordered basis for V. Then [ L]B∈Cn×n,
and by Part (1) we have 1 ≤ |σ([L]B)| ≤n. Now, because λis an eigenvalue of Lif and only if
it is an eigenvalue of [ L]Bby Proposition 6.14, we conclude that 1 ≤ |σ(L)| ≤n. ■214
Definition 6.30. IfA∈Fn×n, then the algebraic multiplicity αA(λ)of an eigenvalue
λ∈σ(A)is given by
αA(λ) = max {j: (t−λ)jis a factor of PA(t)}. (6.11)
Thegeometric multiplicity ofλisγA(λ) = dim( EA(λ)).
IfL∈ L(V), then the algebraic multiplicity αL(λ)of an eigenvalue λ∈σ(L)is given by
αL(λ) =α[L](λ),
where [L]denotes the matrix corresponding to Lwith respect to any basis for V. The geometric
multiplicity ofλisγL(λ) = dim( EL(λ)).
Proposition 6.15 ensures that the algebraic multiplicity of any eigenvalue λof an operator
L∈ L(V) is independent of the choice of basis for V. That is, αL(λ) is invariant under change
of bases.
It must be stressed that if a matrix Ais regarded as being an element of Fn×n, then in
general we consider only eigenvalues that are elements of F. Thus, if A∈Rn×n, then σ(A)⊆R,
and we discount any value in C\Ras being an eigenvalue. A similar convention is observed in
the case when L∈ L(V), where Vis given to be a vector space over the field F; that is, we take
σ(L)⊆F.
An easy consequence of the Factor Theorem is that the multiplicities of the distinct complex
zeros of an nth-degree polynomial function must sum to n. Thus, since the characteristic
polynomial of a matrix A∈Cn×nhas degree nby Proposition 6.27, it readily follows from
Theorem 6.18 that the sum of the algebraic multiplicities of the distinct complex eigenvalues
λ1, . . . , λ mofAmust be n:
mX
k=1αA(λk) =n. (6.12)
It is in this sense (i.e. counting multiplicities) that it can be said that an n×nmatrix Awith
complex-valued entries has “ neigenvalues,” which we may sometimes denote by λ1, . . . , λ n. The
same applies to any linear operator Lon an n-dimensional vector space over C.
Theorem 6.31. IfA∈Cn×nhas distinct complex eigenvalues λ1, . . . , λ m, then
detn(A) =mY
k=1λαA(λk)
k
Proof. Suppose A∈Cn×nhas distinct complex eigenvalues λ1, . . . , λ m. Then λ1, . . . , λ mare
precisely the zeros of PAby Theorem 6.18, and so
PA(t) = (−1)n(t−λ1)αA(λ1)···(t−λm)αA(λm)
by the Factor Theorem and (6.11) , along with Proposition 6.27 which tells us that the lead
coefficient of PAis (−1)n. Now, since
detn(A) = det n(A−0In) =PA(0),
from (6.12) we obtain
detn(A) = (−1)n(−λ1)αA(λ1)···(−λm)αA(λm)215
= (−1)n(−1)αA(λ1)+···+αA(λm)λαA(λ1)
1···λαA(λm)
m
= (−1)n(−1)nλαA(λ1)
1···λαA(λm)
m
=λαA(λ1)
1···λαA(λm)
m
as desired. ■
Proposition 6.32. LetA∈Fn×n. Ifλis an eigenvalue of A, then it is also an eigenvalue of
A⊤
Proof. Suppose that λ∈Fis an eigenvalue of A. Then PA(λ) = 0 by Theorem 6.18, and thus
detn(A−λIn) = 0 .
Now, by Theorem 5.7
detn 
(A−λIn)⊤
= det n(A−λIn),
and since
(A−λIn)⊤=A⊤−λI⊤
n=A⊤−λIn,
it follows that
detn(A⊤−λIn) = 0 .
That is, PA⊤(λ) = 0, and so by Theorem 6.18 we conclude that λis an eigenvalue of A⊤.■216
6.4 – Similar Matrices
Definition 6.33. LetA,B∈Fn×n. We say Aissimilar toB, written As∼B, if there exists
an invertible matrix Q∈Fn×nsuch that B=QAQ−1.
Theorem 6.34. The similarity relations∼is an equivalence relation on the class of square
matrices over F.
Proof. For any A∈Fn×nwe have A=InAI−1
n, so that As∼Aand hences∼is reflexive.
Suppose that As∼B. Then B=QAQ−1for some invertible matrix Q, and since
B=QAQ−1⇒A=Q−1BQ⇒A=Q−1B(Q−1)−1,
it follows that Bs∼Aand therefores∼is symmetric.
Suppose As∼BandBs∼C, so that
B=QAQ−1and C=PBP−1
for some invertible matrices QandP. Then by the associativity of matrix multiplication and
Theorem 2.26 we obtain
C=P(QAQ−1)P−1= (PQ)A(Q−1P−1) = (PQ)A(PQ)−1,
which shows that As∼Cand therefores∼is transitive. ■
Remark. Because the relations∼is symmetric, when two matrices AandBare said to be
similar it does not matter whether we take that to mean As∼B(i.e.B=QAQ−1) orBs∼A
(i.e.A=QBQ−1).
Proposition 6.35. Suppose that AandBare similar matrices.
1.Ais invertible if and only if Bis invertible.
2. det( A) = det( B).
3.PA=PB.
4.σ(A) =σ(B).
5. rank( A) = rank( B).
Proof.
Proof of Part (1): IfAis invertible, then there exists an invertible matrix Qsuch that
B=QAQ−1, and therefore Bis invertible by Theorem 2.26. The converse follows from the
symmetric property ofs∼.
Proof of Part (2): There exists an invertible matrix Qsuch that B=QAQ−1. Now,
det(B) = det( QAQ−1) = det( Q) det(A) det(Q−1) = det( Q) det(A)1
det(Q)= det( A)
by Theorems 5.23 and 5.24.217
Proof of Part (3): There exists an invertible matrix Qsuch that B=QAQ−1, and so
PB(t) = det( B−tI) = det( QAQ−1−tQIQ−1) = det 
Q(A−tI)Q−1
= det( Q) det(A−tI) det(Q−1) = det( Q) det(A−tI) det(Q)−1
= det( A−tI) =PA(t)
for any t∈F. Therefore PA=PB.
Proof of Part (4): Applying Theorem 6.18 and Part (3), we have
λ∈σ(A)⇔PA(λ) = 0 ⇔PB(λ) = 0 ⇔λ∈σ(B),
and therefore σ(A) =σ(B).
Proof of Part (5): This is an immediate consequence of Theorem 4.47(4). ■
The following proposition is a direct consequence of Corollary 4.33 and will prove useful
later on.
Proposition 6.36. Suppose Vis a finite-dimensional vector space and L∈ L(V). IfBandB′
are ordered bases for V, then [L]Band[L]B′are similar matrices.
Proposition 6.37. Suppose that Vis a finite-dimensional vector space, L∈ L(V), and
A∈Fn×n. If there is an ordered basis BforVsuch that [L]Bs∼A, then there exists a basis B′
such that [L]B′=A.
Proof. Suppose B= (v1, . . . ,vn) is an ordered basis for Vsuch that [ L]Bs∼A. Thus there
exists an invertible matrix
Q= [qij]n=q1···qn
such that [ L]B=QAQ−1. LetB′={v′
1, . . . ,v′
n}be the set of vectors for which
v′
k=q1kv1+···+qnkvn
for each 1 ≤k≤n, so that
[v′
k]B=
q1k...
qnk
=qk.
Since Qis invertible, by the Invertible Matrix Theorem the column vectors q1, . . . ,qnofQ
are linearly independent, which is to say [ v′
1]B, . . . , [v′
n]Bare linearly independent vectors in Fn.
Thus, since the mapping φ−1
B:Fn→V(the inverse of the B-coordinate map) is an isomorphism
and
φ−1
B 
[v′
k]B
=v′
k
for 1≤k≤n, it follows by Proposition 4.16 that v′
1, . . . ,v′
nare linearly independent and
therefore B′is a basis for V. We give it the natural order: B′= (v′
1, . . . ,v′
n).218
Now, by Theorem 4.27,
IB′B=h
[v′
1]B···[v′
n]Bi
=q1···qn
=Q,
and so
[L]B=QAQ−1=IB′BAI−1
B′B=I−1
BB′AIBB′
by Proposition 4.31. Finally, by Corollary 4.33 we obtain
A=IBB′[L]BI−1
BB′= [L]B′
as desired. ■
We will often have need to raise matrix expressions of the form QAQ−1andQ−1AQto an
arbitrary positive integer power, for which the following proposition will prove invaluable.
Proposition 6.38. IfA,Q∈Fn×nandQis invertible, then
(Q−1AQ)k=Q−1AkQ (6.13)
and
(QAQ−1)k=QAkQ−1(6.14)
for all k∈N.
Proof. First we prove that (6.13) holds for all k≥1. Certainly the equation holds when
k= 1. Suppose it holds for some arbitrary k≥1. Then, exploiting the associativity of matrix
multiplication, we obtain
(Q−1AQ)k+1= (Q−1AQ)k(Q−1AQ) = (Q−1AkQ)(Q−1AQ)
=Q−1Ak(QQ−1)AQ=Q−1Ak(Ik)AQ=Q−1AkAQ
=Q−1(AkA)Q=Q−1Ak+1Q,
which shows the equation holds for k+ 1. By the Principle of Induction we conclude that (6.13)
holds for all k∈N.
Equation (6.14) is a symmetrical result that is easily derived from (6.13) merely by replacing
QwithQ−1. ■219
6.5 – The Theory of Diagonalization
Definition 6.39. Suppose Vis a nontrivial finite-dimensional vector space over F, and let
L∈ L(V). An ordered basis for Vconsisting of the eigenvectors of Lis called a spectral basis
forL. We say Lisdiagonalizable if there exists a spectral basis for L. Any procedure that
finds a spectral basis for Lis called diagonalization .
A matrix A∈Fn×nisdiagonalizable inFif it is similar to a diagonal matrix
D∈Fn×n.
Theorem 6.40. Suppose Vis a finite-dimensional vector space over F,L∈ L(V), and
λ1, . . . , λ mare the distinct eigenvalues of L. Then the following statements are equivalent.
1.Lis diagonalizable.
2.There exists some ordered basis BforVsuch that [L]Bis a diagonal matrix.
3.There exists some ordered basis BforVsuch that [L]Bis diagonalizable in F.
4.Vdecomposes as
V=EL(λ1)⊕ ··· ⊕ EL(λm).
5.The dimension of Vis
dim(V) = dim( EL(λ1)) +···+ dim( EL(λm)).
Proof.
(1)⇒(2):Suppose Lis diagonalizable. Then there exists some ordered basis B= (v1, . . . ,vn)
consisting of eigenvectors of L, so that L(vk) =λkvkfor each 1 ≤k≤n. By Corollary 4.21 the
matrix corresponding to Lwith respect to Bis
[L]B=h
L(v1)
B···
L(vn)
Bi
=h
λ1v1
B···
λnvn
Bi
=h
λ1
v1
B···λn
vn
Bi
=
λ1
1
0
...
0
··· λn
0
0
...
1

=
λ1 0
...
0 λn
,
and so we see that [ L]Bis a diagonal matrix as desired.
(2)⇒(1):Suppose there exists some ordered basis B= (v1, . . . ,vn) such that [ L]B∈Fn×nis a
diagonal matrix:
[L]B=
d1 0
...
0 dn
.
Since [ vk]B= [δik]n×1for each 1 ≤k≤n, we have
[L]B[vk]B=dk[vk]B,
and so dkis an eigenvalue of [ L]Bwith corresponding eigenvector [ vk]B. By Proposition 6.14 we
conclude that, for each 1 ≤k≤n,dkis an eigenvalue of Lwith corresponding eigenvector vk,
and therefore Bis an ordered basis for Vconsisting of eigenvectors of L.220
(2)⇒(3):This is trivial since equal matrices are similar matrices.
(3)⇒(2):If there is an ordered basis Bsuch that [ L]Bis similar to a diagonal matrix D, then
by Proposition 6.37 there is an ordered basis B′such that [ L]B′=D.
(1)⇒(4):Suppose Lis diagonalizable, so there is an ordered basis B= (v1, . . . ,vn) consisting
of eigenvectors of L. We may take the order to be such that v1, . . . ,vmhave the distinct
eigenvalues λ1, . . . , λ m. Let λm+1, . . . , λ nbe the eigenvalues corresponding to vm+1, . . . ,vn. For
anyu∈Vthere exist c1, . . . , c n∈Fsuch that u=c1v1+···+cnvn, and so
L(u) =nX
k=1ckL(vk) =nX
k=1ckλkvk. (6.15)
Now,
EL(λk) ={v∈V:L(v) =λkv}
is the eigenspace of Lcorresponding to λk, and since
λm+1, . . . , λ n∈ {λ1, . . . , λ m},
it is clear that we may recast (6.15) as
L(u) =mX
k=1c′
kλkv′
k
by combining terms with matching eigenvalues. For each 1 ≤k≤mwe have
L(c′
kv′
k) =c′
kL(v′
k) =c′
kλkv′
k=λk(c′
kv′
k),
so that c′
kv′
k∈EL(λk), and thus
u=nX
k=1ckvk=mX
k=1c′
kv′
k∈mX
k=1EL(λk).
This establishes that V=EL(λ1) +···+EL(λm).
Next, suppose that
mX
k=1uk=uandmX
k=1u′
k=u
foruk,u′
k∈EL(λk). Then
mX
k=1(uk−u′
k) =0, (6.16)
where uk−u′
k∈EL(λk) for each 1 ≤k≤m. Suppose that ukj−u′
kj̸=0for some values
1≤k1< k 2<···< k ℓ≤m,221
withuk−u′
k=0for all k /∈ {k1, . . . , k ℓ}. Then (6.16) becomes
ℓX
j=1(ukj−u′
kj) =0. (6.17)
However, each ukj−u′
kj(being nonzero) is an eigenvalue of Lwith corresponding eigenvalue
λkj, and since the eigenvalues λk1, . . . , λ kℓare distinct, it follows by Theorem 6.8 that the set

uk1−u′
k1, . . . ,ukℓ−u′
kℓ	
is linearly independent. Now (6.17) forces us to conclude that ukj−u′
kj=0for some 1 ≤j≤ℓ,
which is a contradiction. We must conclude that uk−u′
k=0for all 1 ≤k≤m, or equivalently
u1=u′
1, . . .um=u′
m.
Hence any u∈Vhas a unique representation u1+···+umsuch that each ukis an element of
EL(λk), and therefore V=EL(λ1)⊕ ··· ⊕ EL(λm).
(4)⇒(5):That
V=mM
k=1EL(λk)⇒dim(V) =mX
k=1dim(EL(λk))
is an immediate consequence of Theorem 4.45.
(5)⇒(1):Suppose that
dim(V) = dim( EL(λ1)) +···+ dim( EL(λm)),
with dim( EL(λi)) =nifor each 1 ≤i≤m. Let
Bi={vi1, . . . ,vini}
be a basis for EL(λi). Suppose
mX
i=1niX
j=1aijvij=n1X
j=1a1jv1j+···+nmX
j=1amjvmj=0, (6.18)
where
vi=niX
j=1aijvij∈EL(λi)
and so
v1+···+vm=0. (6.19)
For each 1 ≤i≤mthe nonzero elements of EL(λi) are eigenvectors of Lwith corresponding
eigenvalue λi, and since λ1, . . . , λ mare distinct we conclude by Theorem 6.8 that if v1, . . . ,vm̸=0,
thenv1, . . . ,vmare linearly independent. However, (6.19) implies that v1, . . . ,vmare not linearly
independent, and so at least one of the vectors must be the zero vector. In fact, if we suppose
that
vk1, . . . ,vkℓ∈ {v1, . . . ,vm}222
are the nonzero vectors, then (6.19) becomes
vk1+···+vkℓ=0
and we are compelled to conclude—just as before—that at least one term on the left-hand side
must be 0! HenceniX
j=1aijvij=vi=0
for all 1 ≤i≤m, and since vi1, . . . ,viniare linearly independent it follows that
ai1= 0, . . . , a ini= 0
for all 1 ≤i≤m. It is now clear that (6.18) admits only the trivial solution, so that the set
B=m[
i=1Bi
of eigenvectors of Lis linearly independent; and because
|B|=mX
i=1|Bi|=mX
i=1ni=mX
i=1dim(EL(λi)) = dim( V)
(Proposition 6.7 ensures that Bi∩ B j=∅for any i̸=j), Theorem 3.51(1) implies that Bmust
in fact be a basis for Vconsisting of eigenvectors of L. Assigning any order to Bthat we wish,
we conclude that Lis diagonalizable. ■
From the details of the proof of Theorem 6.40 (specifically that the first statement implies
the second statement) we immediately obtain the following result.
Corollary 6.41. Suppose Vis a finite-dimensional vector space over F. If L∈ L(V)is
diagonalizable, B= (v1, . . . ,vn)is a spectral basis for L, and λkis the eigenvalue corresponding
to eigenvector vk, then [L]B∈Fn×nis a diagonal matrix with kk-entry λkfor1≤k≤n. That
is,[L]B= diag
λ1, . . . , λ n
.
Definition 6.42. A polynomial function p∈ P n(F)splits over Fif there exist c, a1, . . . , a n∈F
such that
p(t) =cnY
k=1(t−ak)
for all t∈F.
Proposition 6.43. Suppose Vis a finite-dimensional vector space over F. IfL∈ L(V)is
diagonalizable, then PLsplits over F.
Proof. Suppose L∈ L(V) is diagonalizable, with dim(V) =n. LetBbe a spectral basis for L,
so that [ L]Bis a diagonal matrix
[L]B=
d1 0
...
0 dn
223
for some d1, . . . , d n∈Fby Theorem 6.40. Now,
PL(t) =P[L]B(t) = det n([L]B−tIn) =d1−t 0
...
0 dn−t= (−1)nnY
k=1(t−dk),
and therefore PLsplits over F. ■
The first part of the following theorem tells us that the algebraic multiplicity of an eigenvalue
of a diagonalizable linear operator on a finite-dimensional vector space is always equal to its
geometric multiplicity.
The following theorem will, in the next section, show itself to be the workhorse that yields a
practical method for diagonalizing linear operators and square matrices alike.
Theorem 6.44. Suppose Vis a finite-dimensional vector space over F,L∈ L(V), and
λ1, . . . , λ mare the distinct eigenvalues of L. Assuming that PLsplits over F, then:
1.Lis diagonalizable if and only if αL(λk) =γL(λk)for all 1≤k≤m.
2.IfLis diagonalizable and Bkis a basis for EL(λk)for each 1≤k≤m, thenSm
k=1Bkis a
spectral basis for L.
Proof.
Proof of Part (1). Letn= dim( V). Suppose that
max{j: (t−λk)jis a factor of PL(t)}=αL(λk) =γL(λk) = dim( EL(λk))
for each 1 ≤k≤m. Then
PL(t) =p(t)mY
k=1(t−λk)dim(EL(λk))
for some polynomial function pfor which λ1, . . . , λ mare not zeros. However, PLsplits over F
by hypothesis, and so deg(p) is either 0 or 1. If deg(p) = 1, so that p(t) =c(t−λ) for some
λ, c∈F, then PL(λ) = 0 and we conclude that λ̸=λ1, . . . , λ mmust be an eigenvalue of L. This
is a contradiction since λ1, . . . , λ mrepresent all the distinct eigenvalues of L. Hence deg(p) = 0,
which is to say p(t) =cfor some c∈Fand we have
PL(t) =cmY
k=1(t−λk)dim(EL(λk)). (6.20)
Now, since deg( PL) =nby Corollary 6.28, it follows from (6.20) that
mX
k=1dim(EL(λk)) =n= dim( V),
and therefore Lis diagonalizable by Theorem 6.40.
Suppose that Lis diagonalizable, and let B= (v1, . . . ,vn) be a spectral basis for Lsuch
that L(vk) =λkvkfor each 1 ≤k≤n. By Corollary 6.41, [ L]B∈Fn×nis a diagonal matrix224
with kk-entry λk:
[L]B=
λ1 0
...
0 λn
.
Letrk=αL(λk) for each 1 ≤k≤m; that is,
rk= max {i: (t−λk)iis a factor of PL(t)},
and so
PL(t) = det n([L]B−tIn) =λ1−t 0
...
0 λn−t
=nY
k=1(λk−t) = (−1)nnY
k=1(t−λk) = (−1)nmY
k=1(t−λk)rk. (6.21)
The last equality holds since λk∈ {λ1, . . . , λ m}for all 1 ≤k≤n, so there can be no factor of
PL(t) of the form t−λsuch that λ̸=λ1, . . . , λ m. Corollary 6.28 and (6.21) now imply that
dim(V) = deg( PL) =mX
k=1rk. (6.22)
From (6.21) we also see that, for each 1 ≤k≤m, the scalar λkmust occur precisely rktimes
on the diagonal of [ L]B; that is, for each 1 ≤k≤mthere exist
1≤i1< i2<···< irk≤n
such that
λi1=λi2=···=λirk=λk,
and therefore
S={vi1,vi2, . . . ,virk} ⊆EL(λk).
Now Theorem 3.56(2) implies that
dim(EL(λk))≥rk (6.23)
since Span( S) is a subspace of EL(λk) of dimension rk.
Since Lis diagonalizable,
dim(V) =mX
k=1dim(EL(λk)) (6.24)
by Theorem 6.40. If we suppose that dim(Eλj(L))> rjfor some 1 ≤j≤m, then by equations
(6.24), (6.23), and (6.22), in turn, we obtain
dim(V) =mX
k=1dim(EL(λk))>mX
k=1rk= dim( V),
which is an egregious contradiction. Hence dim(EL(λk))≤rkfor all 1 ≤k≤m, which together
with (6.23) leads to the conclusion that
αL(λk) =rk= dim( EL(λk)) =γL(λk)225
for all 1 ≤k≤m.
Proof of Part (2). Suppose that Lis diagonalizable and Bkis a basis for EL(λk) for each
1≤k≤m. Statement (5) of Theorem 6.40 is true, and in the proof that statement (5) implies
statement (1) we immediately see that B=Sm
k=1Bkis a basis for Vconsisting of eigenvectors
ofL. That is, Bis a spectral basis for L. ■226
6.6 – Diagonalization Methods and Applications
In general, if a square matrix Ais given to be in Fn×n, then to say Ais “diagonalizable”
means in particular “diagonalizable in F.”
Theorem 6.45 (Matrix Diagonalization Procedure ).LetA∈Fn×nhave distinct eigen-
values λ1, . . . , λ m, with Bka basis for EA(λk)for each 1≤k≤m. IfPAsplits over Fand
αA(λk) =γA(λk)for each k, then Ais diagonalizable in Fwith diagonal matrix D∈Fn×ngiven
by
D=IEBAI−1
EB,
where Eis the standard basis for FnandB= (v1, . . . ,vn)is an ordered basis formed from the
elements ofSm
k=1Bk. Therefore
A=v1···vn
diag
µ1, . . . , µ nv1···vn−1, (6.25)
where µkis an eigenvalue corresponding to vkfor each 1≤k≤n.
Proof. Suppose PAsplits over F, and αA(λk) =γA(λk) for each 1 ≤k≤m. Define L∈ L(Fn)
byL(x) =Axin the standard basis E, which is to say [ L]E=A. It is immediate that λ1, . . . , λ m
are the distinct eigenvalues of L, and since PL=PAby Definition 6.16, it follows that PLsplits
overF. By Definition 6.30 αL(λk) =αA(λk) for each k, and since EL(λk) =EA(λk),
γL(λk) = dim( EL(λk)) = dim( EA(λk)) =γA(λk).
Hence αL(λk) =γL(λk) for all 1 ≤k≤m, and so Lis diagonalizable by Theorem 6.44(1). Since
eachBkthat is a basis for EA(λk) is also a basis for EL(λk), by Theorem 6.44(2) the set
B=m[
k=1Bk
is a spectral basis for L. We order the elements of B, where |B|=nsinceBis a basis for Fn,
so that B= (v1, . . . ,vn) is an ordered basis for Fn. Then D= [L]Bis a diagonal matrix by
Corollary 6.41, and by Corollary 4.33
IEBAI−1
EB=IEB[L]EI−1
EB= [L]B=D
as desired.
To obtain (6.25) , observe that if µkis the eigenvalue corresponding to eigenvector vkfor
each 1 ≤k≤n, then
D= [L]B= diag
µ1, . . . , µ n
by Corollary 6.41, and so from D=IEBAI−1
EBwe having, recalling Proposition 4.31 and Theorem
4.27,
A=I−1
EBDIEB=IBEdiag
µ1, . . . , µ n
I−1
BE
=h
[v1]E···[vn]Ei
diag
µ1, . . . , µ nh
[v1]E···[vn]Ei−1
=v1···vn
diag
µ1, . . . , µ nv1···vn−1,227
where the last equality is due to the simple fact that each symbol vkalready represents the
E-coordinates of a vector in Fn, so that [ vk]E=vk. ■
One particularly appealing feature of diagonal matrices is that, for any n∈N, their nth
powers are found simply by taking the nth powers of their entries.
Proposition 6.46. IfD= [dij]nis a diagonal matrix, then Dk= [dk
ij]nfor all k∈N.
Proof. The statement of the proposition is certainly true in the case when k= 1. Suppose
it is true for some arbitrary k∈N, so that Dk= [dk
ij]n. Since Dis diagonal we have dij= 0
whenever i̸=j. Fix 1 ≤i, j≤n. By Definition 2.4,

Dk+1
ij=
DkD
ij=nX
ℓ=1dk
iℓdℓj= 0 = dk+1
ij
ifi̸=j, and

Dk+1
ij=
DkD
jj=nX
ℓ=1dk
jℓdℓj=dk
jjdjj=dk+1
jj
ifi=j. In either case we see that the ij-entry of Dk+1isdk+1
ij, and so Dk+1= [dk+1
ij]n.
Therefore the statement of the proposition holds for all k∈Nby the Principle of Induction
and the proof is done. ■
Example 6.47. Determine whether
A=
−1 4−2
−3 4 0
−3 1 3

is diagonalizable in R. If it is, then find an invertible matrix Pand a diagonal matrix Dsuch
thatA=PDP−1.
Solution. In Example 6.20 we found that the characteristic polynomial PAsplits over Rby
direct factorization:
PA(t) =−t3+ 6t2−11t+ 6 = −(t−1)(t−2)(t−3).
In this way we determined that the eigenvalues of Aare 1, 2, and 3, and by inspection we see
that
αA(1) = αA(2) = αA(3) = 1 .
We also determined a basis for the eigenspace corresponding to each eigenvalue: for eigenspaces
EA(1),EA(2), and EA(3) we found bases
B1=


1
1
1


,B2=


2
3
3


,and B3=


1
3
4


,
respectively. Since |B1|=|B2|=|B3|= 1, we see that
γA(1) = γA(2) = γA(3) = 1 .228
Hence αA(λ) =γA(λ) for every eigenvalue λofA. Therefore Ais diagonalizable in Rby
Theorem 6.45.
Letting
v1=
1
1
1
,v2=
2
3
3
,and v3=
1
3
4
,
thenB= (v1,v2,v3) is an ordered set formed from the elements of B1∪ B 2∪ B 3which Theorem
6.45 implies is an ordered basis for R3. Now, since eigenvalues 1, 2, and 3 correspond to
eigenvectors v1,v2, and v3, respectively, by (6.25) we easily find that
A=v1v2v3
diag
1,2,3v1v2v3−1.
Thus if we let
P=v1v2v3
=
1 2 1
1 3 3
1 3 4
and D= diag
1,2,3
=
1 0 0
0 2 0
0 0 3
,
then we have A=PDP−1as desired. ■
In Example 6.47 there are other possible solutions. If we had chosen the ordered basis
(v3,v2,v1) instead of ( v1,v2,v3), then we would have
A=v3v2v1
diag
3,2,1v3v2v1−1,
which is to say A=PDP−1for
P=
1 2 1
3 3 1
4 3 1
and D=
3 0 0
0 2 0
0 0 1
.
One great use for diagonalization is that it makes it possible to calculate high powers of
square matrices with relative ease, as illustrated in the following example.
Example 6.48. Given
A=
−1 4−2
−3 4 0
−3 1 3
,
Find a formula for An, and use it to calculate A10.
Solution. From Example 6.47 we have A=PDP−1, with
P=
1 2 1
1 3 3
1 3 4
,D=
1 0 0
0 2 0
0 0 3
,P−1=
3−5 3
−1 3 −2
0−1 1
,
and so by Propositions 6.38 and 6.46, respectively,
An= (PDP−1)n=PDnP−1=
1 2 1
1 3 3
1 3 4

1 0 0
0 2n0
0 0 3n

3−5 3
−1 3 −2
0−1 1
229
=
3−2n+1−5 + 3·2n+1−3n3−2n+2+ 3n
3−3·2n−5 + 9·2n−3n+13−3·2n+1+ 3n+1
3−3·2n−5 + 9·2n−4·3n3−3·2n+1+ 4·3n
.
Therefore
A10=
3−211−5 + 3·211−3103−212+ 310
3−3·210−5 + 9·210−3113−3·211+ 311
3−3·210−5 + 9·210−4·3103−3·211+ 4·310

=
−2045 −52,910 54 ,956
−3069 −167,936 171 ,006
−3069 −226,985 230 ,055
,
a result far more easily obtained than calculating A10directly! ■
Example 6.49. Determine whether the linear operator L∈ L(R2×2) given by L(A) =A⊤is
diagonalizable. If it is, then find a spectral basis for L, and find the matrix corresponding to L
with respect to the spectral basis.
Solution. In Example 4.23 we found that the matrix corresponding to Lwith respect to the
standard basis E= (E11,E12,E21,E22) is
[L]E=
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
.
The characteristic polynomial of Lis thus
PL(t) =P[L]E(t) = det 4 
[L]E−tI4
=1−t0 0 0
0−t1 0
0 1 −t0
0 0 0 1 −t= (t−1)3(t+ 1),
which makes clear that PLsplits over R, and the eigenvalues of Lare±1 with αL(−1) = 1 and
αL(1) = 3.
Next we find bases for the eigenspaces of L. For the eigenvalue 1 we have
EL(1) =
X∈R2×2:L(X) =X	
,
where
X=L(X)⇔X=X⊤⇔
x y
z w
=
x z
y w
forx, y, z, w ∈R, implying that y=zand thus
EL(1) =
x y
z w
∈R2×2:y=z
=
x y
y w
:x, y, w ∈R
.
Letting x=s1,y=s2, and w=s3, we finally obtain
EL(1) =
1 0
0 0
s1+
0 1
1 0
s2+
0 0
0 1
s3:s1, s2, s3∈R
,230
which shows that EL(1) has basis
B1=
1 0
0 0
,
0 1
1 0
,
0 0
0 1
={E11,E12+E21,E22}
and therefore γL(1) = 3.
For the eigenvalue −1,
EL(−1) =
X∈R2×2:L(X) =−X	
,
where
−X=L(X)⇔ − X=X⊤⇔
−x−y
−z−w
=
x z
y w
forx, y, z, w ∈R, implying that x=−x,z=−y,−z=y, and w=−w. Thus x=w= 0, and
z=−y, so that
EL(−1) =
0y
−y0
:y∈R
.
Letting y=s, we obtain
EL(−1) =
0 1
−1 0
s:s∈R
,
which shows that EL(−1) has basis
B2=
0 1
−1 0
={E12−E21},
and therefore γL(−1) = 1.
By Theorem 6.44(1), since PLsplits over R,αL(1) = γL(1), and αL(−1) = γL(−1), the
operator Lis diagonalizable. By Theorem 6.44(2) the ordered set
B=B1∪ B 2= (v1,v2,v3,v4) = (E11,E12+E21,E22,E12−E21)
is a spectral basis for L. By Corollary 6.41 the B-matrix of Lis a diagonal matrix with kk-entry
the eigenvalue corresponding to the kth vector vkinB. Since v1=E11,v2=E12+E21, and
v3=E22are eigenvectors corresponding to 1, and v4=E12−E21is an eigenvector corresponding
to−1, we have
[L]B=
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 −1
.
It is in this sense that Lis “diagonalized” by finding a spectral basis. ■231
Problems
1. The matrix
A=
3 2
2 3
is diagonalizable.
(a) Find the characteristic polynomial of A, and use it to find the eigenvalues of A.
(b) For each eigenvalue of Afind the basis for the corresponding eigenspace.
(c) Find an invertible matrix Pand a diagonal matrix Dsuch that A=PDP−1.
(d) Find A50andA1/2.
2. Determine whether the matrix
A=
7−15
2−4
is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDP−1.
3. Determine whether the matrix
A=
4 0 4
0 4 4
4 4 8

is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDP−1.
4. Determine whether the matrix
A=
2 0−2
0 3 0
0 0 3

is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDP−1.
6.7 – Matrix Limits and Markov Chains232
6.8 – The Cayley-Hamilton Theorem
Proposition 6.50. LetVbe a finite-dimensional vector space over Fwith subspace W. IfWis
invariant under L∈ L(V), then the characteristic polynomial of L|Wdivides the characteristic
polynomial of L.
Proof. LetC={v1, . . . ,vm}be a basis for W. By Theorem 3.55 we can extend Cto a basis
B={v1, . . . ,vm,vm+1, . . . ,vn}
forV. Since WisL-invariant, for each vjwith 1 ≤j≤mwe have L(vj)∈W, and so there
exist a1j, . . . , a mj∈Fsuch that
L(vj) =mX
i=1aijvi.
Form+ 1≤j≤nwe have
L(vj) =nX
i=1aijvi.
Defining
A=
a11··· a1m.........
am1···amm
,B=
a1(m+1)··· a1n.........
am(m+1)···amn
,C=
a(m+1)(m+1)···a(m+1)n.........
an(m+1) ··· ann
,
by Corollary 4.21 the B-matrix for Lis
[L]B=h
L(v1)
B···
L(vn)
Bi
=A B
O C
and the C-matrix for L|Wis
[L|W]C=h
L|W(v1)
C···
L|W(vm)
Ci
=h
L(v1)
C···
L(vm)
Ci
=A.
Now by Example 5.21,
PL(t) = det n 
[L]B−tIn
= det n
A B
O C
−
tImO
OtIn−m
= det n
A−tIm B
O C −tIn−m
= det m(A−tIm) det n−m(C−tIn−m)
= det m 
[L|W]C−tIm
detn−m(C−tIn−m) =PL|W(t) det n−m(C−tIn−m),
and since det n−m(C−tIn−m) is a polynomial we conclude that PL|W(t) divides PL(t). ■
Definition 6.51. Suppose Vis a vector space, and v∈Vsuch that v̸=0. The L-cyclic
subspace of Vgenerated by Lis the subspace
Span{Lk(v) :k≥0}.233
As usual we take it as understood that L0=IV, the identity operator on V, so that
L0(v) =IV(v) =v.
Proposition 6.52. Suppose Vis a finite-dimensional vector space, v∈Vis a nonzero vector,
L∈ L(V), and Wis the L-cyclic subspace of Vgenerated by v. Ifdim(W) =m, then the
following hold.
1.The set {v, L(v), . . . , Lm−1(v)}is a basis for W.
2.Ifa0, . . . , a m−1∈Fare such that
m−1X
k=0akLk(v) +Lm(v) =0,
then
PL|W(t) = (−1)m m−1X
k=0aktk+tm!
.
Proof.
Proof of Part (1). Since v̸=0the set S0={v}is linearly independent. For each k≥0 let
Sk={v, L(v), . . . , Lk(v)},
and define
n= max {k:Skis a linearly independent set }.
Then Snis a linearly independent set and Sn+1=Sn∪ {Ln+1(v)}is linearly dependent, and by
Proposition 3.39 it follows that Ln+1(v)∈Span( Sn).
Fixj≥1 and suppose Ln+j(v)∈Span( Sn), so that there exist a0, . . . , a n∈Fsuch that
Ln+j(v) =nX
k=0akLk(v).
Now,
Ln+j+1(v) =L(Ln+j(v)) =L nX
k=0akLk(v)!
=nX
k=0akLk+1(v)
=a0L(v) +a1L2(v) +···+an−1Ln(v) +anLn+1(v),
and since ak−1Lk(v)∈Span (Sn) for all 1 ≤k≤n+ 1, we conclude that Ln+j+1(v)∈Span (S)
as well. Therefore Lk(v)∈Span( Sn) for all k≥0 by the principle of induction.
It is clear that
Span( Sn)⊆W= Span {Lk(v) :k≥0}.
Fixw∈W. Then there exist
a0, . . . , a r∈Fand 0 ≤k0< k 1<···< k r
such that
w=rX
j=0ajLkj(v)234
for some r∈W, and since ajLkj(v)∈Span (Sn) for each j, we have w∈Span (Sn) also, and
thus W⊆Span (Sn). It is now established that Span (Sn) =W, and since Snis a linearly
independent set, it follows that Snis a basis for Wand hence |Sn|= dim( W) =m. Therefore
Sn={v, L(v), . . . , Ln(v)}={v, L(v), . . . , Lm−1(v)},
as was to be shown.
Proof of Part (2) Suppose a0, . . . , a m−1∈Fare such that
Lm(v) =−a0v−a1L(v)− ··· − am−1Lm−1(v).
By Part (1) the ordered set C={v, L(v), . . . , Lm−1(v)}is a basis for W, and so
[L|W]C=h
L|W(v)
C
L|W(L(v))
C···
L|W(Lm−1(v))
Ci
=h
L(v)
C
L2(v)
C···
Lm−1(v)
C
Lm(v)
Ci
=
0 0 0 ··· 0−a0
1 0 0 ··· 0−a1
0 1 0 ··· 0−a2..................
0 0 0 ··· 0−am−2
0 0 0 ··· 1−am−1
.
It follows by Example 6.22 that the characteristic polynomial of [ L|W]Cis
(−1)m(a0+a1t+···+am−1tm−1+tm),
and therefore
PL|W(t) = (−1)m m−1X
k=0aktk+tm!
by Definition 6.16. ■
Definition 6.53. Letf∈ P n(F)be a polynomial function F→Fgiven by
f(t) =nX
k=0aktk.
IfVis a vector space over F,L∈ L(V), and A∈Fn×n, we define the mapping f(L)and matrix
f(A)by
f(L) =nX
k=0akLkand f(A) =nX
k=0akAk.
Some needed basic properties of mappings of the form f(L) and matrices of the form f(A)
which are routine to verify are the following.
Proposition 6.54. Suppose that Vis a vector space over F,L∈ L(V),A∈Fn×n, and
f, g, h ∈ P(F). Then235
1.f(L)∈ L(V)andf(A)∈Fn×n.
2.f(L)◦g(L) =g(L)◦f(L)andf(A)g(A) =g(A)f(A).
3.Ifh(t) =f(t)g(t), then h(L) =f(L)◦g(L)andh(A) =f(A)g(A).
Theorem 6.55 (Cayley-Hamilton Theorem ).LetVbe a finite-dimensional vector space. If
L∈ L(V), then PL(L) =OV.
Proof. Suppose L∈ L(V), and fix v∈Vsuch that v̸=0. Let Wbe the L-cyclic subspace of
Vgenerated by v, with m= dim( W). By Proposition 6.52(1) the set
B={v, L(v), . . . , Lm−1(v)}
is a basis for W, so that Lm(v)∈Span(B) and there exist scalars a0, . . . , a m−1∈Fsuch that
Lm(v) =−a0v−a1L(v)− ··· − am−1Lm−1(v).
By Proposition 6.52(2) it follows that
PL|W(t) = (−1)m(a0IV+a1t+a2t2+···+am−1tm−1+tm).
Now, by Proposition 6.50, the polynomial PL|Wdivides PL, which is to say there exists some
f∈ P(F) such that
PL(t) =f(t)PL|W(t),
and hence by Proposition 6.54(3)
PL(L) =f(L)◦PL|W(L).
However,
PL|W(L)(v) = 
(−1)m(a0IV+a1L+···+am−1Lm−1+Lm)
(v)
= (−1)m(a0v+a1L(v) +···+am−1Lm−1(v) +Lm(v))
= (−1)m(−Lm(v) +Lm(v)) = (−1)m0=0,
and so
PL(L)(v) = 
f(L)◦PL|W(L)
(v) =f(L) 
PL|W(L)(v)
=f(L)(0) =0,
where the last equality follows from the observation that f(L) is a linear operator by Proposition
6.54(1). Therefore PL(L)(v) =0for all nonzero v∈V, and since PL(L)(0) =0also, we
conclude that PL(L) =OV. ■
Corollary 6.56. IfA∈Fn×n, then PA(A) =On.
Proof. Suppose A∈Fn×n. Let L∈ L(Fn) be such that [ L]E=A, soL(x) =Axfor all x∈Fn.
By Definition 6.16 we have PL=PA, where deg( PA) =nby Proposition 6.27 and so
PA(t) =PL(t) =a0+a1t+···+antn
for some a0, . . . , a n∈F. By the Cayley-Hamilton Theorem PL(L) =O, the zero operator on Fn,
which is to say
PL(L)(x) = (a0I+a1L+···+anLn)(x) =O(x) =0236
for all x∈Fn, where Iis the identity operator on Fn. Now, PA(A)∈Fn×nis given by
PA(A) =a0In+a1A+···+anAn,
so that
PA(A)(x) = (a0In+a1A+···+anAn)(x) =a0x+a1Ax+···+anAnx
=a0I(x) +a1L(x) +···+anLn(x) = (a0I+a1L+···+anLn)(x)
=PL(L)(x) =O(x) =0
for all x∈Fn, and therefore PA(A) =Onby Proposition 2.12(2). ■237
7
Inner Product Spaces
7.1 – Inner Products
Recall that if zis a complex number, then ¯zdenotes the conjugate of z,Re(z) denotes the
real part of z, and Im( z) denotes the imaginary part of z. By definition,
a+bi=a−bi, Re(a+bi) =a,Im(a+bi) =b
for any a, b∈R. Throughout this chapter we take Fto represent any field that is a subfield of
the complex numbers C, which is to say Fis a field consisting of objects on which the operation
of conjugation may be done. This of course includes Citself, as well as the field of real numbers
R, rational numbers Q, and others.
Definition 7.1. Aninner product on a vector space VoverFis a function ⟨ ⟩:V×V→F
that associates each pair of vectors (u,v)∈V×Vwith a scalar ⟨u,v⟩ ∈Fin accordance with
the following axioms:
IP1.⟨u,v⟩=⟨v,u⟩for all u,v∈V
IP2.⟨u+v,w⟩=⟨u,w⟩+⟨v,w⟩for all u,v,w∈V
IP3.⟨au,v⟩=a⟨u,v⟩for all u,v∈Vanda∈F.
IP4.⟨u,u⟩>0for all u̸=0.
A vector space Vtogether with an associated inner product ⟨ ⟩is called an inner product
space and denoted by (V,⟨ ⟩).
Remark. Care must be taken to not confuse the symbol for the inner product of two vectors
⟨u,v⟩with, say, the symbol for a Euclidean vector ⟨x, y⟩ ∈R2that is used in some textbooks
(particularly calculus books). One features a pair of vectors between angle brackets, while the
other features a pair of scalars .
An inner product ⟨ ⟩associated with a vector space VoverCis generally complex-valued
and called a hermitian inner product or simply a hermitian product , in which case the
pair ( V,⟨ ⟩) is called a hermitian inner product space .238
Axiom IP1 is the conjugate symmetry property. If Vis a vector space over R(or some
subfield of R), then this axiom becomes
⟨u,v⟩=⟨v,u⟩for all u,v∈V
and is called the symmetry property.
Axioms IP2 and IP3 taken together are the linearity properties, and using them we easily
obtain
⟨u−v,w⟩=⟨u+ (−v),w⟩=⟨u,w⟩+⟨−v,w⟩=⟨u,w⟩ − ⟨v,w⟩.
Axiom IP4 is the positive-definiteness property. Products which satisfy all axioms save
IP4 (or which satisfy a modified version of IP4) are also of theoretical interest, but will not be
entertained in this chapter.
Theorem 7.2. Let(V,⟨ ⟩)be an inner product space over F. For u,v,w∈Vanda∈F, the
following properties hold:
1.⟨0,u⟩=⟨u,0⟩= 0.
2.⟨u,v+w⟩=⟨u,v⟩+⟨u,w⟩.
3.⟨u, av⟩= ¯a⟨u,v⟩.
4.⟨u,u⟩= 0if and only if u=0.
5.If⟨u,v⟩=⟨u,w⟩for all u∈V, then v=w.
Proof.
Proof of Part (1): Letu∈V. By Axiom IP2 we have
⟨0,u⟩=⟨0+0,u⟩=⟨0,u⟩+⟨0,u⟩.
Subtracting ⟨0,u⟩from the leftmost and rightmost expressions yields ⟨0,u⟩= 0 as desired.
Then
⟨u,0⟩=⟨0,u⟩=¯0 = 0
completes the proof.
Proof of Part (2): For any u,v,w∈Vwe have
⟨u,v+w⟩=⟨v+w,u⟩ Axiom IP1
=⟨v,u⟩+⟨w,u⟩ Axiom IP2
=⟨v,u⟩+⟨w,u⟩ Property of complex conjugates
=⟨u,v⟩+⟨u,w⟩ Axiom IP1
Proof of Part (3): For any u,v∈Vanda∈Fwe have
⟨u, av⟩=⟨av,u⟩ Axiom IP1
=a⟨v,u⟩ Axiom IP3
= ¯a⟨v,u⟩ Property of complex conjugates239
= ¯a⟨u,v⟩ Axiom IP1
Proof of Part (4): The contrapositive of Axiom IP4 states that if ⟨u,u⟩ ≤0, then u=0. Thus,
in particular, ⟨u,u⟩= 0 implies that u=0.
For the converse, suppose that u=0. Then, applying Axiom IP2,
⟨u,u⟩=⟨0,0⟩=⟨0+0,0⟩=⟨0,0⟩+⟨0,0⟩;
that is,
⟨0,0⟩+⟨0,0⟩=⟨0,0⟩,
from which we obtain ⟨0,0⟩= 0. We conclude that u=0implies that ⟨u,u⟩= 0.
Proof of Part (5): Suppose that ⟨u,v⟩=⟨u,w⟩for all u∈V. Then
⟨u,v−w⟩=⟨u,v+ (−1)w⟩=⟨u,v⟩+⟨u,(−1)w⟩
=⟨u,v⟩+ (−1)⟨u,w⟩=⟨u,v⟩ − ⟨u,w⟩
=⟨u,v⟩ − ⟨u,v⟩= 0
for all u∈V, making use of Proposition 3.3, parts (2) and (3), and the property
x+ (−1)y=x−y
forx, y∈F. Letting u=v−wsubsequently yields
⟨v−w,v−w⟩= 0,
so that v−w=0by part (4), and therefore v=w. ■
One sure result that obtains from Axiom IP4 and Theorem 7.2(4) is that ⟨u,u⟩ ≥0 for all
u∈V. This will be important when the discussion turns to norms in the next section.
Recall the Euclidean dot product as defined for vectors
x=
x1...
xn
and y=
y1...
yn

inRn:
x·y=y⊤x=y1···yn
x1...
xn
=nX
k=1xkyk.
It is easily verified that the Euclidean dot product applied to Rnsatisfies the four axioms of an
inner product, and so ( Rn,·) is an inner product space.
It might be assumed that ( Cn,·) is also an inner product space (where as usual Cnis taken
to have underlying field C), but this is not the case. Consider for instance the vector z= [ 1 i]⊤
inC2. We have
z·z=z⊤z=1i
1
i
= 12+i2= 1 + ( −1) = 0;240
that is, z·z= 0 even though z̸=0, and so Axiom IP4 fails! Or consider z= [i0 0 ]⊤inC3,
for which we find that
z·z=z⊤z=i0 0
i
0
0
=i2=−1<0
and again Axiom IP4 fails. To remedy the situation only requires a modest modification of the
dot product definition. For the definition we need the conjugate transpose matrix operation:
IfA= [aij]∈Cm×n, then set
A∗=A⊤= [aij]⊤.
Thus, in particular, if
z=
z1...
zn
∈Cn,
then
z∗=z1···zn
.
Definition 7.3. Ifw,z∈Cn, then the hermitian dot product ofwandzis
w·z=z∗w=z1···zn
w1...
wn
=nX
k=1wkzk. (7.1)
The natural isomorphism [ a]1×17→ais an implicit part of the definition, so that the hermitian
dot product produces a scalar value as expected.
Letting ·denote the hermitian dot product, we return to the vector [ 1 i]⊤∈C2and find
that 
1
i
·
1
i
=
1i
1
i
=1−i
1
i
= 1·1 +i(−i) = 1−i2= 1−(−1) = 2 ,
which is an outcome that does not run afoul of Axiom IP4 and so corrects the problem [ 1 i]⊤
presented for the Euclidean dot product above.
The hermitian dot product becomes the Euclidean dot product when applied to vectors in
Rn: letting x,y∈Rnwe have
x·y=y∗x=y⊤x=y1···yn
x1...
xn
=y1···yn
x1...
xn
=y⊤x,
since yk∈Rimplies that yk=ykfor each 1 ≤k≤n. For this reason we will henceforth always
assume (unless stated otherwise) that ·denotes the hermitian dot product, and call it simply
thedot product .
Example 7.4. Leta, b∈Rsuch that a < b , and let Vbe the vector space over Rconsisting of
all continuous functions f: [a, b]→R. Given f, g∈V, define
⟨f, g⟩=Zb
afg. (7.2)241
We verify that ( V,⟨ ⟩) is an inner product space. Since ⟨f, g⟩is real-valued for any f, g∈V, we
have
⟨f, g⟩=Zb
afg=Zb
agf=⟨g, f⟩=⟨g, f⟩
and thus Axiom IP1 is confirmed.
Next, for any f, g, h ∈Vwe have
⟨f+g, h⟩=Zb
a(f+g)h=Zb
a(fh+fg) =Zb
afh+Zb
afg=⟨f, h⟩+⟨f, g⟩,
confirming Axiom IP2.
Axiom IP3 obtains readily:
⟨af, g⟩=Zb
a(af)g=Zb
aa(fg) =aZb
afg=a⟨f, g⟩.
Next, for any f∈Vwe have f2(x)≥0 for all x∈[a, b], and so
⟨f, f⟩=Zb
af2≥0
follows from an established property of the definite integral. Finally, if
Zb
af2= 0
it follows from another property of definite integrals that f(x) = 0 for all x∈[a, b], which is to
sayf= 0 and therefore Axiom IP4 holds. ■
Example 7.5. Recall the notion of the trace of a square matrix, which is a linear mapping
tr :Fn×n→Fgiven by
tr(A) =nX
i=1aii
for each A= [aij]∈Fn×n. Letting F=R, define ⟨ ⟩: Symn(R)×Symn(R)→Rby
⟨A,B⟩= tr(AB).
The claim is that ( Symn(R),⟨ ⟩) is an inner product space. To substantiate the claim we must
verify that the four axioms of an inner product are satisfied.
LetA= [aij] and B= [bij] be elements of Symn(R). The ii-entry of ABisPn
j=1aijbji, and
so
tr(AB) =nX
i=1nX
j=1aijbji. (7.3)
Theii-entry of BAisPn
j=1bijaji, from which we obtain
tr(BA) =nX
i=1nX
j=1bijaji
=nX
j=1nX
i=1bjiaij (Interchange iandj)242
=nX
i=1nX
j=1aijbji (Interchange summations)
= tr(AB). (Equation (7.3))
Hence
⟨A,B⟩= tr(AB) = tr( BA) =⟨B,A⟩
and Axiom IP1 is confirmed to hold.
In Chapter 4 it was found that the trace operation is a linear mapping, and so for any
A,B,C∈Symn(R) and x∈Rwe have
⟨A+B,C⟩= tr(( A+B)C) = tr( AC+BC) = tr( AC) + tr( BC) =⟨A,C⟩+⟨B,C⟩
and
⟨xA,B⟩= tr(( xA)B) = tr( x(AB)) =xtr(AB) =x⟨A,B⟩,
which confirms Axioms IP2 and IP3.
Next, observing that A= [aij]∈Symn(R) if and only if aij=ajifor all 1 ≤i, j≤n, we
have
⟨A,A⟩= tr(A2) =nX
i=1nX
j=1aijaji=nX
i=1nX
j=1aijaij=nX
i=1nX
j=1a2
ij≥0.
It is easy to see that if tr(A2) = 0, then we must have aij= 0 for all 1 ≤i, j≤n, and thus
A=On. Axiom IP4 is confirmed. ■243
7.2 – Norms
Given an inner product space ( V,⟨ ⟩) and a vector u∈V, we define the norm ofuto be
the scalar
∥u∥=p
⟨u,u⟩.
If∥u∥= 1 we say that uis aunit vector . Notice that, by Axiom IP4, ∥u∥is always a
nonnegative real number. The distance d(u,v) between two vectors u,v∈Vis given by
d(u,v) =∥u−v∥,
also always a nonnegative real number. If
⟨u,v⟩= 0
we say that uandvareorthogonal and write u⊥v.
Proposition 7.6. Let(V,⟨ ⟩)be an inner product space. If W⊆Vis a subspace of V, then
W⊥={v∈V:⟨v,w⟩= 0 for all w∈W} (7.4)
is also a subspace of V.
Proof. Suppose u,v∈W⊥. Then for any w∈Wwe have
⟨u+v,w⟩=⟨u,w⟩+⟨v,w⟩= 0 + 0 = 0 ,
which shows that u+v∈W⊥. Moreover, for any a∈Fwe have
⟨au,w⟩=a⟨u,w⟩=a(0) = 0
for any w∈W, which shows that au∈W⊥. Since W⊥⊆Vis closed under scalar multiplication
and vector addition, we conclude that it is a subspace of V. ■
The subspace W⊥defined by (7.4) is called the orthogonal complement ofW.6Ifv∈W⊥,
then we say visorthogonal toWand write v⊥W.
Proposition 7.7. Let(V,⟨ ⟩)be an inner product space. Let w1, . . . ,wm∈V, and define the
subspace
U={v∈V:v⊥wifor all 1≤i≤m}.
IfW= Span {w1, . . . ,wm}, then U=W⊥.
Proof. It is a routine matter to verify that Uis indeed a subspace of V. Let v∈U. For any
w∈Wwe have
w=c1w1+···+cmwm
for some c1, . . . , c m∈F, and then since v⊥wiimplies ⟨wi,v⟩= 0 we obtain
⟨w,v⟩=DXm
i=1ciwi,vE
=Xm
i=1ci⟨wi,v⟩=Xm
i=1ci(0) = 0
6The symbol W⊥is often read as “ Wperp.”244
by Axioms IP2 and IP3. Hence v⊥wfor all w∈W, so that v∈W⊥and therefore U⊆W⊥.
Next, let v∈W⊥. Then ⟨w,v⟩= 0 for all w∈W, or equivalently
DXm
i=1ciwi,vE
= 0 (7.5)
for any c1, . . . , c m∈F. If for any 1 ≤i≤mwe choose ci= 1 and cj= 0 for j̸=i, then (7.5)
gives⟨wi,v⟩= 0. Thus v⊥wifor all 1 ≤i≤m, implying that v∈Uand so W⊥⊆U.
Therefore U=W⊥. ■
Letv∈(V,⟨ ⟩) such that ∥v∥ ̸= 0. Given any u∈(V,⟨ ⟩) there can be found some c∈F
such that
⟨v,u−cv⟩= 0.
Indeed
⟨v,u−cv⟩= 0⇔ ⟨v,u⟩ − ⟨v, cv⟩= 0⇔ ⟨v,u⟩ −¯c⟨v,v⟩= 0
⇔¯c⟨v,v⟩=⟨v,u⟩ ⇔ ¯c⟨v,v⟩=⟨v,u⟩
⇔c⟨v,v⟩=⟨u,v⟩ ⇔ c=⟨u,v⟩
⟨v,v⟩, (7.6)
where ⟨v,v⟩ ̸= 0 since ∥v∥ ̸= 0.
Definition 7.8. Let∥v∥ ̸= 0. The orthogonal projection of uonto vis given by
projvu=⟨u,v⟩
⟨v,v⟩v.
Theorem 7.9. Letu,v∈(V,⟨ ⟩).
1.Pythagorean Theorem: If u⊥v, then
∥u+v∥2=∥u∥2+∥v∥2.
2.Parallelogram Law:
∥u+v∥2+∥u−v∥2= 2∥u∥2+ 2∥v∥2.
3.Schwarz Inequality:
|⟨u,v⟩| ≤ ∥ u∥∥v∥.
4.Triangle Inequality:
∥u+v∥ ≤ ∥u∥+∥v∥.
5.Cauchy Inequality:
∥u∥∥v∥ ≤1
2∥u∥2+1
2∥v∥2.
Proof.
Pythagorean Theorem: Suppose u⊥v, so that ⟨u,v⟩=⟨v,u⟩= 0. By direct calculation we
have
∥u+v∥2=⟨u+v,u+v⟩=⟨u,u+v⟩+⟨v,u+v⟩ Axiom IP2
=⟨u,u⟩+⟨u,v⟩+⟨v,u⟩+⟨v,v⟩ Theorem 7.2(2)245
=⟨u,u⟩+⟨v,v⟩=∥u∥2+∥v∥2
Parallelogram Law: We have
∥u+v∥2=⟨u,u⟩+⟨u,v⟩+⟨v,u⟩+⟨v,v⟩=∥u∥2+⟨u,v⟩+⟨v,u⟩+∥v∥2(7.7)
from the proof of the Pythagorean Theorem, and
∥u−v∥2=⟨u−v,u−v⟩=∥u∥2− ⟨u,v⟩ − ⟨v,u⟩+∥v∥2. (7.8)
Adding equations (7.7) and (7.8) completes the proof.
Schwarz Inequality: Ifu=0orv=0, then by Theorem 7.2(1) we obtain
∥⟨u,v⟩∥=|0|= 0 = ∥u∥∥v∥,
which affirms the theorem’s conclusion.
Suppose u,v̸=0, and let
c=⟨u,v⟩
⟨v,v⟩=⟨u,v⟩
∥v∥2.
Now, by (7.6),
⟨u−cv, cv⟩=c⟨u−cv,v⟩=c⟨v,u−cv⟩=c(¯0) = c(0) = 0 .
Thus u−cvandcvare orthogonal, and by the Pythagorean Theorem
∥u∥2=∥(u−cv) +cv∥2=∥u−cv∥2+∥cv∥2.
Hence ∥cv∥2≤ ∥u∥2since∥u−cv∥2≥0. However, recalling that z¯z=|z|2for any z∈F, we
obtain
∥cv∥2=⟨cv, cv⟩=c¯c⟨v,v⟩=|c|2∥v∥2=|⟨u,v⟩|2
∥v∥4∥v∥2=|⟨u,v⟩|2
∥v∥2,
and so ∥cv∥2≤ ∥u∥2implies that
|⟨u,v⟩|2
∥v∥2≤ ∥u∥2.
Therefore we have
|⟨u,v⟩|2≤ ∥u∥2∥v∥2,
and taking the square root of both sides completes the proof.
Triangle Inequality: For any u,v∈Vwe have ⟨u,v⟩=a+bifor some a, b∈R, so that the real
part of ⟨u,v⟩isRe(⟨u,v⟩) =a. (IfVis a vector field over Rthen b= 0, but this will not affect
our analysis.) By the Schwarz Inequality we have
√
a2+b2=|a+bi|=|⟨u,v⟩| ≤ ∥ u∥∥v∥,
and since
Re 
⟨u,v⟩
=a≤ |a|=√
a2≤√
a2+b2,
it follows that
Re 
⟨u,v⟩
≤ ∥u∥∥v∥. (7.9)246
Recalling the property of complex numbers z+ ¯z= 2 Re( z), we have
⟨u,v⟩+⟨v,u⟩=⟨u,v⟩+⟨u,v⟩= 2 Re 
⟨u,v⟩
. (7.10)
Now,
∥u+v∥2=∥u∥2+⟨u,v⟩+⟨v,u⟩+∥v∥2Equation (7.7)
=∥u∥2+ 2 Re 
⟨u,v⟩
+∥v∥2Equation (7.10)
≤ ∥u∥2+ 2∥u∥∥v∥+∥v∥2, Inequality (7.9)
and so
∥u+v∥2≤ 
∥u∥+∥v∥2.
Taking the square root of both sides completes the proof.
Cauchy Inequality: This inequality in fact holds for all real numbers: if a, b∈R, then
0≤(a−b)2=a2−2ab+b2⇒2ab≤a2+b2⇒ab≤1
2a2+1
2b2,
and we’re done. ■
Proposition 7.10. Let(V,⟨ ⟩)be an inner product space, and let v1, . . . ,vn∈Vbe such that
vi̸=0for each 1≤i≤nandvi⊥vjwhenever i̸=j. Ifv∈Vand
ci=⟨v,vi⟩
⟨vi,vi⟩
for each 1≤i≤n, then
v−nX
i=1civi
is orthogonal to v1, . . . ,vn.
Proof. Fix 1≤k≤n. Since vk̸= 0 we have ⟨vk,vk⟩ ̸= 0. Also ⟨vi,vj⟩= 0 whenever i̸=j.
Now,*
v−nX
i=1civi,vk+
=⟨v,vk⟩ −*nX
i=1civi,vk+
=⟨v,vk⟩ −nX
i=1ci⟨vi,vk⟩
=⟨v,vk⟩ −ck⟨vk,vk⟩=⟨v,vk⟩ −⟨v,vk⟩
⟨vk,vk⟩⟨vk,vk⟩
=⟨v,vk⟩ − ⟨v,vk⟩= 0,
and therefore v−Pn
k=1ckvk⊥vkfor any 1 ≤k≤n. ■
Proposition 7.11. Let(V,⟨ ⟩)be an inner product space, and let v1, . . . ,vn∈Vbe such that
vi̸=0for each 1≤i≤nandvi⊥vjwhenever i̸=j. Ifv∈Vandci=⟨v,vi⟩/⟨vi,vi⟩for
each 1≤i≤n, then v−Xn
i=1civi≤v−Xn
i=1aivi
for any a1, . . . , a n∈F.247
Proof. Fixv∈Vanda1, . . . , a n∈F, and let ci=⟨v,vi⟩/⟨vi,vi⟩for each 1 ≤i≤n. First we
observe that for any scalars x1, . . . , x nwe haveD
v−Xn
k=1ckvk,Xn
i=1xiviE
=Xn
i=1D
v−Xn
k=1ckvk, xiviE
Theorem 7.2(2)
=Xn
i=1¯xiD
v−Xn
k=1ckvk,viE
Theorem 7.2(3)
=Xn
i=1¯xi(0) = 0 , Proposition 7.10
which is to say that v−Pn
k=1ckvkis orthogonal to any linear combination of the vectors
v1, . . . ,vn. In particular
v−nX
i=1civi⊥nX
i=1(ci−ai)vi,
and so by the Pythagorean Theorem
v−Xn
i=1aivi2
=v−Xn
i=1civi+Xn
i=1(ci−ai)vi2
=v−Xn
i=1civi2
+Xn
i=1(ci−ai)vi2
≥v−Xn
i=1civi2
.
Taking square roots completes the proof. ■
Problems
1. Let ( V,⟨ ⟩) be an inner product space, and let S⊆Vwith S̸=∅. Show that
S⊥={v∈V:⟨v,s⟩= 0 for all s∈S}
is a subspace of Veven if Sis not a subspace.248
7.3 – Orthogonal Bases
IfB={v1, . . . ,vn}is a basis for a vector space Vand⟨·,·⟩:V×V→Fis an inner product,
then we refer to Bas a basis for the inner product space ( V,⟨ ⟩).
Definition 7.12. LetB={v1, . . . ,vn}be a basis for an inner product space (V,⟨ ⟩). Ifvi⊥vj
whenever i̸=j, then Bis anorthogonal basis . IfBis an orthogonal basis such that ∥vi∥= 1
for all i, then Bis called an orthonormal basis .
Lemma 7.13. Letv1, . . . ,vn∈(V,⟨ ⟩)be nonzero vectors. If vi⊥vjwhenever i̸=j, then
v1, . . . ,vnare linearly independent.
Proof. Suppose that vi⊥vjwhenever i̸=j. Let x1, . . . , x n∈Fand set
x1v1+···+xnvn=0. (7.11)
Now, for each 1 ≤i≤n,*nX
k=1xkvk,vi+
=⟨0,vi⟩= 0.
On the other hand,*nX
k=1xkvk,vi+
=nX
k=1xk⟨vk,vi⟩=xi⟨vi,vi⟩.
Hence
xi⟨vi,vi⟩= 0,
and since vi̸=0implies ⟨vi,vi⟩ ̸= 0, it follows that xi= 0. Therefore (7.11) leads to the
conclusion that x1=···=xn= 0, and so v1, . . . ,vnare linearly independent. ■
Theorem 7.14 (Gram-Schmidt Orthogonalization Process ).Letm∈N. For any n∈N,
if(V,⟨ ⟩)is an inner product space over Fwith dim(V) =m+n,Wis a subspace of Vwith
orthogonal basis (wi)m
i=1, and
(w1, . . . ,wm,um+1, . . . ,um+n) (7.12)
is a basis for V, then an orthogonal basis for Vis(wi)m+n
i=1, where
wi=ui−i−1X
k=1⟨ui,wk⟩
⟨wk,wk⟩wk (7.13)
for each m+ 1≤i≤m+n. Moreover,
Span( wi)m+k
i=1= Span( w1, . . . ,wm,um+1, . . . ,um+k) (7.14)
for all 1≤k≤n.
Note that the existence of vectors um+1, . . . ,um+n∈Vsuch that (7.12) is a basis for Vis
assured by Theorem 3.55. Also observe that, since m, n∈Nimplies m+n≥2, the theorem
does not address one-dimensional vector spaces. This is because one-dimensional vector spaces
are not of much interest: any nonzero vector serves as an orthogonal basis!249
Proof. We carry out an argument by induction on nby first considering the case when
n= 1. That is, we let m∈Nbe arbitrary, and suppose ( V,⟨ ⟩) is an inner product space with
dim(V) =m+1,Wis a subspace of Vwith orthogonal basis ( wi)m
i=1, andB= (w1, . . . ,wm,um+1)
is a basis for V. Let
wm+1=um+1−mX
k=1⟨um+1,wk⟩
⟨wk,wk⟩wk.
Ifwm+1=0, then
um+1=mX
k=1⟨um+1,wk⟩
⟨wk,wk⟩wk
obtains, so that um+1∈Span (wi)m
i=1and by Proposition 3.39 it follows that Bis a linearly
dependent set—a contradiction. Hence wm+1̸=0is assured. Moreover wm+1is orthogonal
tow1, . . . ,wmby Proposition 7.10, implying that wi⊥wjfor all 1 ≤i, j≤m+ 1 such that
i̸=j. Since {w1, . . . ,wm+1}is an orthogonal set of nonzero vectors, by Lemma 7.13 it is also a
linearly independent set. Therefore, by Theorem 3.54, ( wi)m+1
i=1is a basis for Vthat is also an
orthogonal basis. We have proven that the theorem is true in the base case when n= 1.
Next, suppose the theorem is true for some particular n∈N. Fix m∈N, suppose ( V,⟨ ⟩) is
an inner product space with dim(V) =m+n+ 1,Wis a subspace of Vwith orthogonal basis
(wi)m
i=1, and
B= (w1, . . . ,wm,um+1, . . . ,um+n+1)
is a basis for V. Let V′=Span (B \ {um+n+1}), which is to say ( V′,⟨ ⟩) is an inner product
space with basis
B′= (w1, . . . ,wm,um+1, . . . ,um+n),
andWis a subspace of V′. Since dim(V′) =m+n, by our inductive hypothesis we conclude
that ( wi)m+n
i=1, where
wi=ui−i−1X
k=1⟨ui,wk⟩
⟨wk,wk⟩wk
for each m+ 1≤i≤m+n, is an orthogonal basis for V′.
Now, V′is a subspace of Vwith orthogonal basis ( wi)m+n
i=1, and
C= (w1, . . . ,wm+n,um+n+1)
is a basis for V. (To substantiate the latter claim use Proposition 3.39 twice: first to find that
um+n+1/∈Span(B′) =V′= Span( wi)m+n
i=1,
and then to find that Cis a linearly independent set. Now invoke Theorem 3.54.) Applying
the base case proven above, only with mreplaced by m+n, we conclude that ( wi)m+n+1
i=1 is an
orthogonal basis for V, where
wm+n+1=um+n+1−m+nX
k=1⟨um+n+1,wk⟩
⟨wk,wk⟩wk.
We have now shown that if the theorem holds when m∈Nis arbitrary and dim(V) =m+n,
then it holds when m∈Nis arbitrary and dim(V) =m+n+ 1. All but the last statement of
the theorem is now proven by the Principle of Induction.250
Finally, to see that (7.14) holds for each 1 ≤k≤n, simply note from (7.13) that each vector
in (wi)m+k
i=1lies in
Span( w1, . . . ,wm,um+1, . . . ,um+k),
and also each vector in
(w1, . . . ,wm,um+1, . . . ,um+k)
lies in Span( wi)m+k
i=1. ■
Corollary 7.15. If(V,⟨ ⟩)is an inner product space over Fof dimension n∈N, then it has an
orthonormal basis.
Example 7.16. Give the vector space R3the customary dot product, thereby producing the
inner product space ( R3,·). Let
u1=
1
1
1
,u2=
−1
1
0
,u3=
1
2
1
.
ThenB={u1,u2,u3}is a basis for ( R3,·). Use the Gram-Schmidt Process to transform Binto
an orthogonal basis for ( R3,·), and then find an orthonormal basis for ( R3,·).
Solution. Letw1=u1. Then {w1}is an orthogonal basis for the subspace W=Span{w1}.
Certainly W̸=R3, and we already know that {w1,u2,u3}is a basis for R3. Hence we have the
essential ingredients to commence the Gram-Schmidt Process and find vectors w2andw3so
that{w1,w2,w3}constitutes an orthogonal basis for ( R3,·). The formula for finding wi(where
i= 2,3) is
wi=ui−i−1X
k=1ui·wk
wk·wkwk.
Hence
w2=u2−u2·w1
w1·w1w1=
−1
1
0
−[−1,1,0]⊤·[1,1,1]⊤
[1,1,1]⊤·[1,1,1]⊤
1
1
1
=
−1
1
0
,
and
w3=u3−2X
k=1u3·wk
wk·wkwk=u3−u3·w1
w1·w1w1−u3·w2
w2·w2w2
=
1
2
1
−4
3
1
1
1
−1
2
−1
1
0
=
1/6
1/6
−1/3
.
(Note: it should not be surprising that w2=u2since u2is in fact already orthogonal to w1.)
We have obtained
{w1,w2,w3}=


1
1
1
,
−1
1
0
,
1/6
1/6
−1/3



as an orthogonal basis for ( R3,·).251
To find an orthonormal basis all we need do is normalize the vectors w1,w2andw3. We
have
ˆw1=w1
∥w1∥=1√
3,1√
3,1√
3⊤
,ˆw2=w2
∥w2∥=
−1√
2,1√
2,0⊤
,
and
ˆw3=w3
∥w3∥=1√
6,1√
6,−2√
6⊤
.
The set {ˆw1,ˆw2,ˆw3}is an orthonormal basis for ( R3,·). ■
Example 7.17. Recall the vector space P2(R) of polynomial functions of degree at most 2 with
coefficients in R, which here we shall denote simply by P2. Define
⟨p, q⟩=Z1
−1pq
for all p, q∈ P 2. The verification that ( P2,⟨ ⟩) is an inner product space proceeds in much the
same way as Example 7.4. Apply the Gram-Schmidt Process to transform the standard basis
E={1, x, x2}into an orthonormal basis for ( P2,⟨ ⟩).
Solution. Letw1= 1, the polynomial function with constant value 1. If W=Span{w1}, then
Wis a subspace of P2such that W̸=P2, and{w1}is an orthogonal basis for W. Starting
withw1, we employ the Gram-Schmidt Process to obtain w2andw3fromu2=xandu3=x2,
respectively. We have
w2=u2−⟨u2,w1⟩
⟨w1,w1⟩w1=x−⟨x,1⟩
⟨1,1⟩=x−R1
−1x dx
R1
−11dx=x−0
2=x,
and
w3=u3−⟨u3,w1⟩
⟨w1,w1⟩w1−⟨u3,w2⟩
⟨w2,w2⟩w2=x2−⟨x2,1⟩
⟨1,1⟩−⟨x2, x⟩
⟨x, x⟩x
=x2−R1
−1x2dx
R1
−11dx−R1
−1x3dx
R1
−1x2dxx=x2−1
3,
and so
{w1,w2,w3}=
1, x, x2−1
3	
is an orthogonal basis for P2.
To find an orthonormal basis we need only normalize the vectors w1,w2andw3. From
∥w1∥=p
⟨w1,w1⟩=p
⟨1,1⟩=qR1
−11dx=√
2,
∥w2∥=p
⟨w2,w2⟩=p
⟨x, x⟩=qR1
−1x2dx=q
2
3,
and
∥w3∥=p
⟨w3,w3⟩=q
x2−1
3, x2−1
3
=qR1
−1 
x2−1
32dx=q
8
45,
we obtain
ˆw1=w1
∥w1∥=1√
2,ˆw2=w2
∥w2∥=√
6
2x, ˆw3=w3
∥w3∥=√
10
4(3x2−1).252
The set {ˆw1,ˆw2,ˆw3}, which consists of the first three of what are known as normalized Legendre
polynomials, is an orthonormal basis for ( P2,⟨ ⟩). ■
Proposition 7.18. Let(V,⟨ ⟩)be an inner product space over Fof dimension n∈N, let
B={w1, . . . ,wr,u1, . . . ,us}
be an orthogonal basis for V, and let
W= Span {w1, . . . ,wr}and U= Span {u1, . . . ,us}.
Then U=W⊥,W=U⊥, and
dim(W) + dim( W⊥) = dim( V).
Proof. Letu∈U. Then there exist scalars x1, . . . , x s∈Fsuch that
u=sX
i=1xiui.
Letw∈Wbe arbitrary, so that
w=rX
j=1yjwj
for scalars y1, . . . , y r∈F. Now,
⟨u,w⟩=DXs
i=1xiui,wE
=Xs
i=1xi⟨ui,w⟩ (Axiom IP2)
=Xs
i=1
xiD
ui,Xr
j=1yjwjE
=Xs
i=1
xiXr
j=1⟨ui, yjwj⟩
(Theorem 7.2(2))
=Xs
i=1
xiXr
j=1¯yj⟨ui,wj⟩
(Theorem 7.2(3))
SinceBis an orthogonal basis we have ⟨ui,wj⟩= 0 for all 1 ≤i≤sand 1 ≤j≤r, so that
⟨u,w⟩=sX
i=1rX
j=1xi¯yj⟨ui,wj⟩= 0
and therefore u⊥w. Since w∈Wis arbitrary, we conclude that u∈W⊥and hence U⊆W⊥.
Next, let v∈W⊥. Since Bis a basis for V, there exist scalars x1, . . . , x s, y1, . . . , y r∈Fsuch
that
v=sX
i=1xiui+rX
j=1yjwj.
Fix 1≤k≤r. Since ykwk∈Wwe have
⟨v, ykwk⟩= 0. (7.15)253
On the other hand, since ⟨ui,wk⟩= 0 for all 1 ≤i≤s, and⟨wj,wk⟩= 0 for all j̸=k, we have
⟨v, ykwk⟩=sX
i=1xi¯yk⟨ui,wk⟩+rX
j=1yj¯yk⟨wj,wk⟩=yk¯yk⟨wk,wk⟩=|yk|2⟨wk,wk⟩.(7.16)
Combining (7.15) and (7.16) yields
|yk|2⟨wk,wk⟩= 0,
and since wk̸=0implies that ⟨wk,wk⟩ ̸= 0 by Axiom IP4, it follows that yk= 0. We conclude,
then, that
v=sX
i=1xiui∈U,
and so W⊥⊆U. Therefore U=W⊥, and by symmetry W=U⊥.
Finally, since {u1, . . . ,us}is a basis for Uand{w1, . . . ,wr}is a basis for W, we obtain
dim(V) =n=r+s= dim( W) + dim( U) = dim( W) + dim( W⊥),
which completes the proof. ■
The conclusions of Proposition 7.18 in fact apply to any arbitrary subspace of an inner
product space, as the next theorem establishes.
Theorem 7.19. LetWbe a subspace of an inner product space (V,⟨ ⟩)overFwithdim(V)∈N.
Then
(W⊥)⊥=W
and
dim(W) + dim( W⊥) = dim( V).
Proof. The proof is trivial in the case when dim(V) = 0, since the only possible subspace is
then{0}. So suppose henceforth that n= dim( V)>0.
IfW={0}, then W⊥=V. Now,
(W⊥)⊥=V⊥={0}=W,
and since dim( {0}) = 0 we have
dim(V) = dim( {0}) + dim( V) = dim( W) + dim( W⊥)
IfW=V, then W⊥={0}and a symmetrical argument to the one above leads to the same
conclusions.
Setm=dim(W), and suppose W̸={0}andW̸=V. Then m≤nby Theorem 3.56(2),
andm̸=nby Theorem 3.56(3), so that 0 < m < n . Since Wis a nontrivial vector space
in its own right, by Corollary 7.15 it has an orthogonal basis {w1, . . . ,wm}. Since W̸=Vit
follows by Theorem 7.14 that there exist wm+1, . . . ,wn∈Vsuch that B={w1, . . . ,wn}is an
orthogonal basis for V. Observing that W= Span {w1, . . . ,wm}and defining
U= Span {wm+1, . . . ,wn},254
by Proposition 7.18 we have U=W⊥,W=U⊥, and
dim(W) + dim( W⊥) = dim( V).
Finally, observe that
(W⊥)⊥=U⊥=W,
which finishes the proof. ■
The dimension equation in Theorem 7.19 amounts to a generalization of Proposition 4.46
from the setting of real Euclidean vector spaces (equipped specifically with the Euclidean dot
product) to that of abstract inner product spaces over an arbitrary field F.
Example 7.20. As a compelling application of some of the developments thus far, we give a
proof that the row rank of a matrix equals its column rank that is quite different (and shorter)
than the proof given in §3.6. Let A= [aij]∈Rm×n.
Define the linear mapping L:Rn→RmbyL(x) =Ax, and let a1, . . . ,am∈Rnbe such
thata⊤
1, . . . ,a⊤
mare the row vectors of A. Then Nul(L) is a subspace of the inner product space
(Rn,·) by Proposition 4.14, and so too is Row( A) = Span {a1, . . . ,am}. Now,
x∈Nul(L)⇔Ax=0⇔
a⊤
1x
...
a⊤
mx
=
x·a1...
x·am
=
0
...
0
⇔x⊥a1, . . . ,x⊥am,
so that
Nul(L) ={x∈Rn:x⊥aifor all 1 ≤i≤m}
and by Proposition 7.7 we have Nul( L) = Row( A)⊥. By Theorem 7.19
dim(Row( A)) + dim(Row( A)⊥) = dim( Rn),
whence
row-rank( A) + dim(Nul( L)) =n
and finally
row-rank( A) =n−dim(Nul( L)).
Next, by Theorem 4.37,
dim(Nul( L)) + dim(Img( L)) = dim( Rn),
and since Img( L) = Col( A) by Proposition 4.35, it follows that
n= dim( Rn) = dim(Nul( L)) + dim(Col( A)) = dim(Nul( L)) + col-rank( A)
and finally
col-rank( A) =n−dim(Nul( L)).
Therefore
row-rank( A) = col-rank( A) =n−dim(Nul( L)),
and we’re done. ■
Proposition 7.21. IfWis a subspace of an inner product space (V,⟨ ⟩)overF, then
V=W⊕W⊥.255
Proof. The situation is trivial in the cases when W={0}orW=V, so suppose Wis a
subspace such that W̸={0}, V. Let dim(W) =manddim(V) =n, and note that 0 < m < n .
Since ( W,⟨ ⟩) is a nontrivial inner product space, by Corollary 7.15 is has an orthogonal basis
{w1, . . . ,wm}. By Theorem 7.14 there exist wm+1, . . . ,wn∈Vsuch that B={w1, . . . ,wn}is
an orthogonal basis for V, and W⊥= Span {wm+1, . . . ,wn}by Proposition 7.18.
Letv∈V. Since Span( B) =V, there exist scalars c1, . . . , c n∈Fsuch that
v=nX
k=1ckwk=mX
k=1ckwk+nX
k=m+1ckwk,
and so v∈W+W⊥. Hence V⊆W+W⊥, and since the reverse containment is obvious we
have V=W+W⊥.
Suppose that v∈W∩W⊥. From v∈W⊥we have v⊥wfor all w∈W, and since v∈W
it follows that v⊥v. Thus ⟨v,v⟩= 0, and so v=0by Theorem 7.2(4). Hence W∩W⊥⊆ {0},
and since the reverse containment is obvious we have W∩W⊥={0}.
Since V=W+W⊥andW∩W⊥={0}, we conclude that V=W⊕W⊥. ■
Corollary 7.22. IfWis a subspace of an inner product space (V,⟨ ⟩)overF, then
dim(W⊕W⊥) = dim( W) + dim( W⊥).
Proof. By Proposition 7.21 we have V=W⊕W⊥, and thus dim(V) =dim(W⊕W⊥). The
conclusion then follows from Theorem 7.19. ■
The corollary could also be proved quite easily by utilizing Proposition 4.36, which applies
to abstract vector spaces over F.
For the following theorem we take all vectors in Fnto be, as ever, n×1 column matrices
(i.e. column vectors).
Theorem 7.23. Let(V,⟨ ⟩)be a finite-dimensional inner product space over F. IfOis an
ordered orthonormal basis for V, then
⟨u,v⟩= [v]∗
O[u]O (7.17)
for all u,v∈V.
Proof. The statement of the theorem is clearly true if V={0}, so assume dim(V) =n∈N
and set O= (w1, . . . ,wn). Let u,v∈V, so there exist u1, . . . , u n, v1, . . . , v n∈Fsuch that
u=u1w1+···+unwnand v=v1w1+···+vnwn,
and hence
[u]O=
u1...
un
and [ v]O=
v1...
vn
.
Now, because Ois orthonormal, ⟨wi,wj⟩= 0 whenever i̸=j, and⟨wi,wi⟩=∥wi∥2= 1 for all
i= 1, . . . , n . By Definition 7.1 and Theorem 7.2 we obtain
⟨u,v⟩=*nX
i=1uiwi,nX
j=1vjwj+
=nX
i=1nX
j=1ui¯vj⟨wi,wj⟩256
=nX
i=1ui¯vi⟨wi,wi⟩=nX
i=1ui¯vi= [v]∗
O[u]O,
as desired. ■
In the case when F=Rwe find that [ v]∗
O= [v]⊤
O, since the components of [ v]Oare all real
numbers, and thus we readily obtain the following.
Corollary 7.24. If(V,⟨ ⟩)is an inner product space over R, and O= (w1, . . . ,wn)is an
ordered orthonormal basis for V, then
⟨u,v⟩= [v]⊤
O[u]O
for all u,v∈V.
In Theorem 7.23, let φO:V→Fndenote the O-coordinate map, so that
φO(v) = [v]O
for all v∈V, and then (7.17) may be written as
⟨u,v⟩=φO(u)·φO(v),
recalling Definition 7.3. Now, if ∥ · ∥ Vdenotes the norm in Vand∥ · ∥Fnthe norm in Fn, then
∥v∥V=p
⟨v,v⟩=p
φO(v)·φO(v) =∥φO(v)∥Fn (7.18)
for all v∈V. In fact, if dVanddFnare the distance functions on VandFn, respectively, so
that for any u,v∈Vandx,y∈Fnwe have
dV(u,v) =∥u−v∥Vand dFn(x,y) =∥x−y∥Fn,
then it follows from (7.18) that
dV(u,v) =∥u−v∥V=∥φO(u−v)∥Fn=∥φO(u)−φO(v)∥Fn=dFn(φO(u), φO(v)),(7.19)
recalling that φOis an isomorphism.
Equation (7.18) exhibits a property of the mapping φOthat is called norm-preserving ,
and equation (7.19) exhibits the distance-preserving property of φO.
Definition 7.25. Let(U,⟨ ⟩U)and(V,⟨ ⟩V)be inner product spaces, and let ∥ · ∥ Uand∥ · ∥ V
denote the norms on UandVinduced by the inner products ⟨ ⟩Uand⟨ ⟩V, respectively. A linear
mapping L:U→Vis an isometry if it is norm-preserving; that is,
∥u∥U=∥L(u)∥V
for all u∈U. IfLis also an isomorphism, then (U,⟨ ⟩U)and(V,⟨ ⟩V)are said to be isomet-
rically isomorphic .
Thus we see that the mapping φOis an isometry as well as an isomorphism, where it must
not be forgotten that Orepresents an orthonormal basis for an inner product space ( V,⟨ ⟩) over
Fof dimension n≥1. By Corollary 7.15 every such inner product space admits an orthonormal
basis, and so must be isometrically isomorphic to ( Fn,·).257
Problems
1.LetR2have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2}into an orthonormal basis.
(a)u1= [1,−3]⊤,u2= [2,2]⊤.
(b)u1= [1,0]⊤,u2= [3,−5]⊤.
2.LetR3have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2,u3}into an orthonormal basis.
(a)u1= [1,1,1]⊤,u2= [−1,1,0]⊤,u3= [1,2,1]⊤.
(b)u1= [1,0,0]⊤,u2= [3,7,−2]⊤,u3= [0,4,1]⊤.
3.LetR4have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2,u3,u4}into an orthonormal basis:
u1=
0
2
1
0
,u2=
1
−1
0
0
,u3=
1
2
0
−1
,u4=
1
0
0
1
.
4. Let Wbe the subspace of R4spanned by the vectors
v1=
1
0
1
0
,v2=
3
0
2
0
,v3=
2
1
−1
3
.
(a)Beginning with the vector v1, use the Gram-Schmidt Orthogonalization Process to obtain
an orthogonal basis for W.
(b) Find an orthonormal basis for W.
5. Consider the matrix
A=
1 2 5
−1 1 −4
−1 4 −3
1−4 7
1 2 1
.
Letu1,u2,u3denote the column vectors of A.
(a) Show that {u1,u2,u3}is a basis for Col( A).
(b) Find an orthogonal basis for Col( A).
(c) Find an orthonormal basis for Col( A).258
(d)Letting r1, . . . ,r5∈R3denote the row vectors of A, find a basis for Row(A) of the form
R={r⊤
1,r⊤
i,r⊤
j}, where 1 < i < j ≤5 are such that iandjare as small as possible.7
(e) Use the basis Rfound in part (d) to obtain an orthogonal basis for Row( A).
(f) Find an orthonormal basis for Row( A).
7This ensures that there is only one possible answer.259
7.4 – Quadratic Forms
Recall from §7.1 that the vector space Cntogether with the operation given by
w·z=z∗w
forw,z∈Cnis an inner product space over C(and the product itself is called the hermitian
inner product). For the conjugate transpose operation z∗=z⊤we find that if zhas only
real-valued entries (so that z∈Rn) then z∗=z⊤. The norm of zis
∥z∥=√z·z=√
z∗z. (7.20)
For the statement and proof of the next theorem recall that the standard form for elements
ofCnisx+iy, where x,y∈Rn. In particular if z=x+iy∈Cforx, y∈R, then z=zimplies
zis real:
z=z⇒x−iy=x+iy⇒2iy= 0⇒y= 0⇒z=x.
Theorem 7.26. All eigenvalues of a real symmetric matrix Aare real, and if x+iy∈Cnis a
complex eigenvector corresponding to λ, then either xoryis a real eigenvector corresponding to
λ.
Proof. Suppose A∈Symn(R), soA=Asince Ais real and A⊤=Asince Ais symmetric,
and thus A∗=A. Let λbe an eigenvalue of Awith corresponding eigenvector z∈Cn, soz̸=0
is such that Az=λz. Now,
z∗Az=z∗λz=λ(z∗z) =λ∥z∥2,
and since ∥z∥>0 by Axiom IP4, we may write
λ=z∗Az
∥z∥2.
As a 1 ×1 matrix λis symmetric, so that
λ= (λ)⊤=λ∗=z∗Az
∥z∥2∗
=z∗A∗(z∗)∗
∥z∥2=z∗Az
∥z∥2=λ,
and hence λis real.
Next, z∈Cnimplies z=x+iyforx,y∈Rn, and then from Az=λzwe obtain
Ax+iAy=λx+iλy.
Since the entries of Aare real and λis real, it follows that
Ax=λxand Ay=λy.
Now, because z̸=0, either x̸=0ory̸=0. Therefore either xoryis a real eigenvector of A
corresponding to λ. ■260
Letaij∈Rfor all 1 ≤i, j≤n. A function f:Rn→Rgiven by
f(x) =nX
i=1nX
j=1aijxixj. (7.21)
for each x= [x1, . . . , x n]⊤is called a quadratic form onRn. An example of a quadratic form
onR2is
f(x, y) = 2 x2+ 10xy−2y2.
Letting
x=
x
y
and A=
2 5
5−2
,
it is easy to check that f(x) =x⊤Axif we identify the 1 ×1 matrix x⊤Axwith its scalar entry.
The fact that Ais a symmetric real matrix here is not an accident: any quadratic form on Rn
may be written in the form x⊤Axfor some A∈Symn(R).
Definition 7.27. IfA∈Symn(R), then the quadratic form associated with Ais the
function QA:Rn→Rgiven by
QA(x) =x⊤Ax
for all x∈Rn.
Again we note that, formally, x⊤Axis a 1×1 matrix, but the natural isomorphism [ c]7→c
is implicitly in play in Definition 7.27 so that QA(x) is a real number.
Example 7.28. Any quadratic form in R2may be written as
f(x, y) =ax2+ 2bxy+cy2
fora, b, c∈R. We wish to find a real symmetric 2 ×2 matrix Asuch that QA=fonR2. We
have
f(x, y) = (ax2+bxy) + (bxy+cy2) = (ax+by)x+ (bx+cy)y
=ax+by bx +cy
x
y
=x y
a b
b c
x
y
,
which shows that fis the quadratic form associated with
A=
a b
b c
.
■
Example 7.29. Let
A=
3−1 2
−1 1 4
2 4 −2
and x=
x
y
z
.
Then
QA(x) =x y z
3−1 2
−1 1 4
2 4 −2

x
y
z
=x y z
3x−y+ 2z
−x+y+ 4z
2x+ 4y−2z
261
=x(3x−y+ 2z) +y(−x+y+ 4z) +z(2x+ 4y−2z)
= 3x2−2xy+ 4xz+y2+ 8yz−2z2
is the quadratic form associated with A.
More generally, if
A=
a b c
b d e
c e f

then
QA(x) =ax2+ 2bxy+ 2cxz+dy2+ 2eyz+fz2(7.22)
is the associated quadratic form. ■
Forn∈Ndefine Snto be the set of all unit vectors in the vector space Rn+1with respect to
the Euclidean dot product:
Sn={x∈Rn+1:∥x∥= 1}=


x1...
xn+1
∈Rn+1:n+1X
k=1x2
k= 1

.
The set Snmay be referred to as the n-sphere or the ( n-dimensional) unit sphere .8Ifn= 1
we obtain a circle centered at ⟨0,0⟩,
S1=
x
y
∈R2:x2+y2= 1
,
and if n= 2 we obtain a sphere with center ⟨0,0,0⟩,
S2=


x
y
z
∈R2:x2+y2+z2= 1

.
The next proposition establishes an important property of the quadratic forms of symmetric
matrices that have, in particular, real-valued entries. It depends on a fact from analysis, not
proven here, that if f:S⊆Rn→Ris a continuous function and Sis a closed and bounded set,
then fattains a maximum value on S. That is, there exists some x0∈Ssuch that
f(x0) = max {f(x) :x∈S}.
Certainly Sn−1, as a subset of Rn, is closed and bounded with respect to the Euclidean dot
product. Also a cursory examination of (7.21) should make it clear that, for any A∈Rn×n,
the function QAis a polynomial function. Hence QAis continuous on Rnwith respect to the
Euclidean dot product, which easily implies that QAis continuous on Sn−1⊆Rn.
8It makes no difference whether we regard the elements of Snas vectors or points. For consistency’s sake we
keep on with the “vector interpretation” here, but later will make occasional use of the “point interpretation” to
aid intuitive understanding.262
Definition 7.30. LetU⊆Rbe an open set, and let f:U→Rnbe given by
f(t) =
f1(t)
...
fn(t)
,
where fk:U→Rfor each 1≤k≤n. If the derivatives f′
1(t0), . . . , f′
n(t0)are defined at t0∈U,
then the derivative of the vector-valued function fatt0is
f′(t0) =
f′
1(t0)
...
f′
n(t0)
.
Since all the eigenvalues of a symmetric real matrix Aare real by Theorem 7.26, it makes
sense to speak of the “smallest” and “largest” eigenvalue of A, as in the next theorem.
Theorem 7.31. Suppose A∈Symn(R), and let λminandλmaxbe the smallest and largest
eigenvalues of A, respectively.
1.If
QA(v1) = max {QA(x) :x∈Sn−1}and QA(v2) = min {QA(x) :x∈Sn−1},
thenv1andv2are eigenvectors of A.
2.For all x∈Sn−1,
λmin≤x⊤Ax≤λmax.
3.Forx∈Rnsuch that ∥x∥= 1,x⊤Ax=λmax(resp. λmin) iffxis an eigenvector of A
corresponding to λmax(resp. λmin).
Proof.
Proof of (1). Define U⊆Rnto be the set
U={u∈Rn:u·v1= 0}.
Since∥v1∥= 1 implies that v1̸=0, by Example 4.39 we find that Uis a subspace of Rnand
dim(U) =n−1. By Proposition 4.46
dim(U⊥) = dim( Rn)−dim(U) =n−(n−1) = 1 ,
and since clearly v1∈U⊥and{v1}is a linearly independent set, it follows by Theorem 3.54(1)
that{v1}is a basis for U⊥. Hence
U⊥= Span( v1) ={cv1:c∈R}.
Fixu∈Usuch that ∥u∥= 1, and define the vector-valued function f:R→Rnby
f(t) = sin( t)u+ cos( t)v1.
Since v1·v1=∥v1∥2= 1,u·u=∥u∥2= 1, and u·v1= 0, we find that
∥f(t)∥2=f(t)·f(t) = 
sin(t)u+ cos( t)v1
· 
sin(t)u+ cos( t)v1
= sin2(t)u·u+ 2 cos( t) sin(t)u·v1+ cos2(t)v1·v1263
= sin2(t) + cos2(t) = 1 ,
and so f(t)∈Sn−1for all t∈R. That is, the function fcan be regarded as defining a curve on
the unit sphere Sn−1, and f(0) = v1shows that the curve passes through the point v1. Letting
u=
u1...
un
and v1=
v1...
vn

we have
f(t) =
u1sin(t) +v1cos(t)
...
unsin(t) +vncos(t)
,
and so by definition
f′(t) =
u1cos(t)−v1sin(t)
...
uncos(t)−vnsin(t)
= cos( t)u−sin(t)v1.
Now, letting g=QA◦fand defining the function Afby (Af)(t) =Af(t) for t∈R, we have
g(t) =QA(f(t)) =f(t)⊤Af(t) =f(t)·Af(t) =f(t)·(Af)(t).
By the Product Rule of dot product differentiation,
g′(t) =f′(t)·(Af)(t) +f(t)·(Af)′(t) =f′(t)·Af(t) +f(t)·Af′(t)
=f′(t)⊤Af(t) +f(t)⊤Af′(t). (7.23)
Since f(t)⊤Af′(t) is a scalar it equals its own transpose, and so by Proposition 2.13 and the fact
thatA⊤=Awe obtain
f(t)⊤Af′(t) = 
f(t)⊤Af′(t)⊤=f′(t)⊤A⊤f(t) =f′(t)⊤Af(t).
Combining this result with (7.23) yields
g′(t) = 2f′(t)⊤Af(t). (7.24)
Because the function fmaps from RtoSn−1, the function QA:Sn−1→Rhas a maximum at
v1∈Sn−1, and
g(0) = QA(f(0)) = QA(v1),
it follows that the function g:R→Rhas a local maximum at t= 0. Thus, since g′(0) exists, it
further follows by Fermat’s Theorem in §4.1 of the Calculus Notes thatg′(0) = 0. From (7.24)
we have
u·Av 1=u⊤Av 1=f′(0)⊤Af(0) = 0 ,
and since u∈Uis arbitrary we conclude that Av 1⊥ufor all u∈U. Therefore
Av 1∈U⊥={x∈Rn:x⊥ufor all u∈U}= Span( v1),
and so there must exist some λ∈Rsuch that Av 1=λv1. Since v1∈Rnis nonzero, we
conclude that v1is an eigenvector of A. The proof that v2∈Sn−1is also an eigenvector of Ais264
much the same.
Proof of (2). By the previous result, letting λ1(resp. λ2) be the eigenvalue of Acorresponding
tov1(resp. v2), we have
x⊤Ax=QA(x)≤QA(v1) =v⊤
1Av 1=v⊤
1(λ1v1) =λ1(v⊤
1v1) =λ1≤λmax
and
x⊤Ax=QA(x)≥QA(v2) =v⊤
2Av 2=v⊤
2(λ2v2) =λ2(v⊤
2v2) =λ2≥λmin
for any x∈Sn−1.
Proof of (3). We provide only the proof of the statement concerning λmax, since the proof of the
other statement is similar. Let x∈Rnbe such that ∥x∥= 1.
Suppose x⊤Ax=λmax. Then
QA(x) = max {QA(u) :u∈Sn−1}
by part (2), and it follows by part (1) that xis an eigenvector of A. Let λbe the eigenvalue of
Acorresponding to x. We now have
λmax=x⊤Ax=x⊤(λx) =λx⊤x=λ,
and so xis an eigenvector of Acorresponding to λmax.
For the converse, suppose xis an eigenvector of Acorresponding to λmax. Then
x⊤Ax=x⊤(λmaxx) =λmaxx⊤x=λmax,
and the proof is done. ■
Example 7.32. Find the maximum and minimum value of the function φ:R3→Rgiven by
φ(x, y, z ) =x2−4xy+ 4y2−4yz+z2(7.25)
on the unit sphere S2.
Solution. Comparing (7.25) to equation (7.22) in Example 7.29, we see we have a= 1,b=−2,
c= 0,d= 4,e=−2, and f= 1. Thus the function φis the quadratic form associated with the
matrix
A=
a b c
b d e
c e f
=
1−2 0
−2 4 −2
0−2 1
.
The characteristic polynomial of Ais
PA(t) = det( A−tI3) =1−t−2 0
−2 4−t−2
0−2 1−t
= (−1)1+1(1−t)4−t−2
−2 1−t+ (−1)1+2(−2)−2−2
0 1−t
=−t3+ 6t2−t−4,265
and so
PA(t) = 0 ⇔t3−6t2+t+ 4 = 0 .
By the Rational Zeros Theorem of algebra, the only rational numbers that may be zeros of
PAare±1,±2, and ±4. It happens that 1 is in fact a zero, and so by the Factor Theorem of
algebra t−1 must be a factor of PA(t). Now,
t3−6t2+t+ 4
t−1=t2−5t−4,
whence we obtain
PA(t) = 0 ⇒(t−1)(t2−5t−4) = 0 ⇒t= 1 or t2−5t−4 = 0 ,
and so PA(t) = 0 implies that
t∈(
5 +√
41
2,5−√
41
2,1)
.
By Theorem 6.18 the eigenvalues of Aare
λ1=5 +√
41
2, λ 2=5−√
41
2, λ 3= 1,
so by Theorem 7.31 the maximum value of φonS2isλ1(approximately 5 .702) and the minimum
value is λ2(approximately −0.702). ■
Example 7.33. Find the maximum and minimum value of the function
f(x, y) =x2+xy+ 2y2
on the ellipse x2+ 3y2= 16.
Solution. We effect a change of variables so that, in terms of the new variables, the ellipse
becomes a unit circle. In particular we declare uandvto be such that 4 u=xand 4 v/√
3=y.266
8
Operator Theory
8.1 – The Adjoint of a Linear Operator
Many of the results developed in this chapter are of a technical nature which will be pressed
into service in due course to uncover some of the most wondrous and practical properties of
finite-dimensional vector spaces and the linear mappings between them.
Definition 8.1. Let(V,⟨ ⟩V)and(W,⟨ ⟩W)be inner product spaces over the field F, and let
L∈ L(V, W ). The adjoint ofLis the mapping L∗∈ L(W, V )satisfying
⟨L(v),w⟩W=⟨v, L∗(w)⟩V
for all v∈Vandw∈W.
Theorem 8.2. Let(V,⟨ ⟩V)and(W,⟨ ⟩W)be inner product spaces over F. For every L∈ L(V, W )
there exists a unique adjoint L∗∈ L(W, V ).
Given an inner product space ( V,⟨ ⟩) and an operator L∈ L(V), the adjoint of Lis the
unique operator L∗∈ L(V) satisfying
⟨L(u),v⟩=⟨u, L∗(v)⟩ (8.1)
for all u,v∈V.
Proposition 8.3. Let(V,⟨ ⟩)be an inner product space over F. IfL,ˆL∈ L(V)andc∈F,
then
1. (cL)∗= ¯cL∗
2. (L∗)∗=L
3. (L+ˆL)∗=L∗+ˆL∗
4. (L◦ˆL)∗=ˆL∗◦L∗
Proof.
Proof of Part (2). Letu,v∈Vbe arbitrary. By definition we have
⟨L(v),u⟩=⟨v, L∗(u)⟩,267
and thus
⟨L∗(u),v⟩=⟨u, L(v)⟩.
This shows that Lis the adjoint of L∗; that is, L= (L∗)∗.
Proof of Part (4). Letu,v∈V. By definition

(L◦ˆL)(u),v
=
u,(L◦ˆL)∗(v)
(8.2)
and
⟨ˆL(u),v⟩=⟨u,ˆL∗(v)⟩. (8.3)
Substituting ˆL(u) foruin (8.1), and L∗(v) forvin (8.3), we obtain

L(ˆL(u)),v
=⟨ˆL(u), L∗(v)⟩and ⟨ˆL(u), L∗(v)⟩=
u,ˆL∗(L∗(v))
,
and hence
(L◦ˆL)(u),v
=
u,(ˆL∗◦L∗)(v)
.
Comparing this equation with (8.2), and recalling that u,v∈Vare arbitrary, we see that both
(L◦ˆL)∗andˆL∗◦L∗are adjoints of L◦ˆL. Since the adjoint of a linear operator is unique, we
conclude that
(L◦ˆL)∗=ˆL∗◦L∗
as desired.
Proofs of the other parts of the proposition are left as exercises. ■
Definition 8.4. LetA∈Fm×n. The adjoint (orconjugate transpose ) ofAis the matrix
A∗∈Fn×mgiven by A∗= 
A⊤.
IfA= [aij]m×n, then the ij-entry of A∗is [A∗]ij=aji. It is an easy matter to verify that
 
A⊤=(A⊤)
(that is, the transpose of Ais the same as the conjugate of A⊤), so there would be no ambiguity
if we were to write simply AT. Hence,
A∗=A⊤=(A⊤) = 
A⊤. (8.4)
Also we define
A∗∗= (A∗)∗.
Proposition 8.5. IfA,B∈Fn×nandc∈F, then
1. (cA)∗= ¯cA∗
2.A∗∗=A
3. (A+B)∗=A∗+B∗
4. (AB)∗=B∗A∗268
Proof.
Proof of Part (2). Liberal use of equation (8.4) is prescribed here, as well as properties of the
transpose and conjugation operations established in chapters 2 and 6, respectively. We have
A∗=(A⊤), and so A∗=A⊤. Now,
A∗∗= (A∗)∗= 
A∗⊤= (A⊤)⊤=A,
as desired.
Proof of Part (4). For this we must recall Proposition 2.13 as well as equation (8.4):
(AB)∗= 
AB⊤=(AB)⊤=B⊤A⊤=B⊤A⊤= 
B⊤ 
A⊤=B∗A∗.
Proofs of the other parts of the proposition are left as exercises. ■
Theorem 8.6. Let(V,⟨ ⟩)be a finite-dimensional inner product space with ordered orthonormal
basisO, and let Λ, L∈ L(V). Then Λ =L∗if and only if [Λ]O= [L]∗
O.
Proof. Let [ ] represent [ ] Ofor simplicity. Suppose that Λ ∈ L(V) is such that [Λ] = [ L]∗,
where
[Λ] = [ L]∗⇔[Λ] = [L]⊤⇔[Λ] = [ L]⊤.
Now, for any u,v∈Vwe have, by Theorem 7.23,
⟨L(u),v⟩= [v]∗[L(u)] = [v]∗ 
[L][u]
= 
[v]∗[L]
[u]
= 
[v]∗[Λ]∗
[u] = 
[Λ][v]∗[u] = [Λ( v)]∗[u] =⟨u,Λ(v)⟩,
and therefore Λ = L∗. ■
With this theorem we have a way of finding the adjoint of a linear operator: given an
operator L∈ L(V), find an orthonormal basis OforV(perhaps using the Gram-Schmidt
Orthogonalization Process), then determine [ L]O(the matrix corresponding to Lwith respect
toO) using Corollary 4.21, and then obtain [ L]∗
Oby taking the conjugate of the transpose of
[L]O. The matrix [ L]∗
Odefines a new operator L∗∈ L(V) that will in fact be the adjunct of L.269
8.2 – Self-Adjoint and Unitary Operators
Definition 8.7. Let(V,⟨ ⟩)be an inner product space over F. A linear operator L∈ L(V)is
self-adjoint with respect to the inner product ⟨ ⟩ifL∗=L. A matrix A∈Fn×nisself-adjoint
ifA∗=A.
Observe that if A∈Rn×n, then Ais self-adjoint if and only if A=A⊤, since
A=A∗=A⊤=A⊤.
That is, “self-adjoint” and “symmetric” mean the same thing in the context of matrices with
real-valued entries. It is for this reason that a self-adjoint operator on an inner product space
over specifically the field Rmay also be called a symmetric operator. (Meanwhile, physicists
especially are fond of calling a self-adjoint operator on an inner product space over Cahermitian
operator.)
Theorem 8.8. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V). Then Lis
self-adjoint if and only if
⟨L(u),v⟩=⟨u, L(v)⟩
for all u,v∈V.
Proof. Suppose that Lis self-adjoint, so that L∗=L. From (8.1) we have
⟨L(u),v⟩=⟨u, L∗(v)⟩=⟨u, L(v)⟩
for all u,v∈V, as desired.
Now suppose that
⟨L(u),v⟩=⟨u, L(v)⟩ (8.5)
for all u,v∈V. By Theorem 8.2, L∗is the unique linear operator on Vfor which
⟨L(u),v⟩=⟨u, L∗(v)⟩ (8.6)
holds for all u,v∈V. Comparing (8.5) and(8.6), it is clear that L∗=L, and therefore Lis
self-adjoint. ■
The following proposition more firmly establishes the connection between the concepts of
self-adjoint operators and self-adjoint matrices.
Theorem 8.9. Let(V,⟨ ⟩)be a finite-dimensional inner product space over Fwith ordered
orthonormal basis O, and let L∈ L(V). Then Lis a self-adjoint operator if and only if
[L]∗
O= [L]O.
Proof. LetO= (w1, . . . ,wn), and let [ ] represent [ ] Ofor simplicity. Suppose that Lis
self-adjoint. By definition [ L]∈Fn×n, the matrix corresponding to Lwith respect to O, satisfies
[L][v] = [L(v)] (8.7)270
for all v∈V. By Corollary 4.21
[L] =h
L(w1)
···
L(wn)i
,
and so for any v∈V
[L]⊤[v] =
[L(w1)]⊤
...
[L(wn)]⊤
[v] =
[L(w1)]⊤[v]
...
[L(wn)]⊤[v]
=
⟨L(w1),v⟩
...
⟨L(wn),v⟩

=
⟨w1, L(v)⟩
...
⟨wn, L(v)⟩
=
[w1]⊤[L(v)]
...
[wn]⊤[L(v)]
=
[w1]⊤
...
[wn]⊤
[L(v)],
where the third and fifth equalities follow from Theorem 7.23, and the fourth equality is owing
toLbeing self-adjoint. But the n×nmatrix
[w1]⊤
...
[wn]⊤
∈Rn×n
is the identity matrix In, and so we obtain
[L]⊤[v] =[L(v)]. (8.8)
Taking the conjugate of both sides of (8.8) then yields the equation
[L]⊤[v] = [L(v)] (8.9)
for all v∈V. From (8.7) and(8.9) we conclude that [ L] and [L]⊤are matrices corresponding
toLwith respect to O. By Corollary 4.25 the matrix corresponding to Lwith respect to Ois
unique, and therefore it must be that
[L] =[L]⊤= [L]∗
as desired.
For the converse, suppose that [ L] = [L]∗. We have
[L] = [L]∗⇔[L] =[L]⊤⇔[L]⊤=[L],
and so by Theorem 7.23 we find that, for all u,v∈V,
⟨L(u),v⟩= [L(u)]⊤[v] = 
[L][u]⊤[v] = [u]⊤[L]⊤[v]
= [u]⊤[L][v] = [u]⊤[L(v)] =⟨u, L(v)⟩.
Therefore Lis a self-adjoint operator. ■
In Theorem 8.9, if we let F=Rin particular, then the entries of the matrix [ L]Oare real
valued, in which case
[L]⊤
O= [L]⊤
O271
and we obtain the following quite readily.
Corollary 8.10. Let(V,⟨ ⟩)be a finite-dimensional inner product space over Rwith orthonormal
basisO. Then L∈ L(V)is a self-adjoint operator if and only if [L]O= [L]⊤
O.
A self-adjoint operator Lon an inner product space over Ris called a symmetric operator
precisely because the matrix corresponding to Lwith respect to an orthonormal basis is a
symmetric matrix.
Corollary 8.11. Let(V,⟨ ⟩)be a finite-dimensional inner product space over Fwith orthonormal
basisO. IfL∈ L(V)is a self-adjoint operator, then [L∗]O= [L]∗
O.
Proof. Suppose that Lis self-adjoint. Then L=L∗and [ L]O= [L]∗
O, whereupon it follows
trivially that [ L∗]O= [L]∗
O. ■
Lemma 8.12 (Polarization Identity ).Let(V,⟨ ⟩)be an inner product space over F. If
L∈ L(V), then
⟨L(u+v),u+v⟩ − ⟨L(u−v),u−v⟩= 2
⟨L(u),v⟩+⟨L(v),u⟩
for all u,v∈V.
Proof. Suppose that L∈ L(V), and let u,v∈V. We have
⟨L(u+v),u+v⟩=⟨L(u),u⟩+⟨L(u),v⟩+⟨L(v),u⟩+⟨L(v),v⟩
and
⟨L(u−v),u−v⟩=⟨L(u),u⟩ − ⟨L(u),v⟩ − ⟨L(v),u⟩+⟨L(v),v⟩.
Subtraction then yields
⟨L(u+v),u+v⟩ − ⟨L(u−v),u−v⟩= 2⟨L(u),v⟩+ 2⟨L(v),u⟩,
the desired outcome. ■
Proposition 8.13. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V).
1.Suppose F=C. If⟨L(v),v⟩= 0for all v∈V, then L=OV.
2.Suppose F=C. Then Lis self-adjoint if and only if ⟨L(v),v⟩ ∈Rfor all v∈V.
3.IfLis self-adjoint and ⟨L(v),v⟩= 0for all v∈V, then L=OV.
Proof.
Proof of Part (1). Suppose that ⟨L(v),v⟩= 0 for all v∈V. From the polarization identity of
Lemma 8.12 we obtain
⟨L(u),v⟩+⟨L(v),u⟩= 0 (8.10)
for all u,v∈V. Thus we have, for all u,v∈V,
⟨L(u), iv⟩+⟨L(iv),u⟩=−i⟨L(u),v⟩+i⟨L(v),u⟩= 0,
whence
− ⟨L(u),v⟩+⟨L(v),u⟩= 0. (8.11)272
Adding equations (8.10) and (8.11) then gives
⟨L(v),u⟩= 0
for all u,v∈V. Letting vbe arbitrary and choosing u=L(v), we obtain
⟨L(v), L(v)⟩= 0,
and thus L(v) =0. Therefore L=OV.
Proof of Part (2). Suppose that Lis self-adjoint. Let v∈Vbe arbitrary. We have
⟨L(v),v⟩=⟨v, L(v)⟩=⟨L(v),v⟩,
which shows that ⟨L(v),v⟩ ∈R.
For the converse, suppose that ⟨L(v),v⟩ ∈Rfor all v∈V. Then
⟨L(v),v⟩=⟨v, L(v)⟩,
whence we obtain
⟨L(v),v⟩ − ⟨v, L(v)⟩=⟨v, L∗(v)⟩ − ⟨v, L(v)⟩=
v,(L∗−L)(v)
= 0.
That is,
(L∗−L)(v),v
= 0
for all v∈V, and so by Part (1) we conclude that L∗−L=OV. Therefore L∗=L.
Proof of Part (3). Suppose Lis self-adjoint and ⟨L(v),v⟩= 0 for all v∈V. The conclusion
follows by Part (1) if F=C, so we can assume that F=R. By the polarization identity we
obtain
⟨L(u),v⟩+⟨L(v),u⟩= 0,
whereupon commutativity gives
⟨L(u),v⟩+⟨u, L(v)⟩= 0,
and finally self-adjointness delivers
⟨L(u),v⟩+⟨L(u),v⟩= 0.
So⟨L(u),v⟩= 0 for all u,v∈V. Letting ube arbitrary and setting v=L(u), we find that
⟨L(u), L(u)⟩= 0, and thus L(u) =0. Therefore L=OV. ■
Definition 8.14. Let(V,⟨ ⟩)be an inner product space over F. An operator L∈ L(V)is
unitary with respect to the inner product ⟨ ⟩ifL∗=L−1. An invertible matrix A∈Fn×nis
unitary ifA∗=A−1.
It is common to call a unitary matrix Awith real-valued entries an orthogonal matrix,
and a unitary operator on an inner product space over Ranorthogonal operator. Note that a
unitary operator LonVis invertible: if v∈Vis such that L(v) =0, then
⟨v,v⟩=⟨L(v), L(v)⟩=⟨0,0⟩= 0273
implies v=0, so that Nul(L) ={0}and by the Invertible Operator Theorem we conclude that
Lis invertible.
Theorem 8.15. Let(V,⟨ ⟩)be a finite-dimensional inner product space over Fwith orthonormal
basisO, and let L∈ L(V). Then Lis a unitary operator if and only if [L]∗
O= [L]−1
O.
The proof of Theorem 8.15 is much the same as the proof of Theorem 8.9, and so it is left as
a problem.
Theorem 8.16. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V). The following
statements are equivalent:
1.Lis a unitary operator.
2.If∥v∥= 1, then ∥L(v)∥= 1.
3.∥L(v)∥=∥v∥for all v∈V.
4.⟨L(u), L(v)⟩=⟨u,v⟩for all u,v∈V.
Proof.
(1)→(2).Suppose that Lis a unitary operator. Fix v∈Vsuch that ∥v∥= 1. Then
∥L(v)∥2=⟨L(v), L(v)⟩=⟨v, L∗(L(v))⟩=⟨v, L−1(L(v))⟩=⟨v,v⟩=∥v∥2= 1,
and hence ∥L(v)∥= 1.
(2)→(3). Suppose that ∥L(v)∥= 1 for all v∈Vsuch that ∥v∥= 1. Fix v∈V. Ifv=0,
then∥L(0)∥=∥0∥obtains immediately, so suppose that v̸=0. Then ˆv=v/∥v∥is a vector in
Vsuch that ∥ˆv∥= 1, and so ∥L(ˆv)∥= 1 by hypothesis. Now,
∥L(v)∥=L 
∥v∥ˆv=∥v∥∥L(ˆv)∥=∥v∥.
Therefore ∥L(v)∥=∥v∥for all v∈V.
(3)→(4). Suppose that ∥L(v)∥=∥v∥, or equivalently ⟨L(v), L(v)⟩=⟨v,v⟩, for all v∈V.
Fixu,v∈V. By the Parallelogram Law given in Theorem 7.9,
∥L(u) +L(v)∥2+∥L(u)−L(v)∥2= 2∥L(u)∥2+ 2∥L(v)∥2= 2∥u∥2+ 2∥v∥2,
whence
∥L(u) +L(v)∥2+∥L(u−v)∥2=∥L(u) +L(v)∥2+∥u−v∥2= 2∥u∥2+ 2∥v∥2,
and then
⟨L(u) +L(v), L(u) +L(v)⟩+⟨u−v,u−v⟩= 2⟨u,u⟩+ 2⟨v,v⟩. (8.12)
Since
⟨L(u) +L(v), L(u) +L(v)⟩=⟨L(u), L(u)⟩+⟨L(u), L(v)⟩+⟨L(v), L(u)⟩+⟨L(v), L(v)⟩
=⟨u,u⟩+⟨L(u), L(v)⟩+⟨L(v), L(u)⟩+⟨v,v⟩
and
⟨u−v,u−v⟩=⟨u,u⟩ − ⟨u,v⟩ − ⟨v,u⟩+⟨v,v⟩,274
from (8.12) we obtain
⟨L(u), L(v)⟩+⟨L(v), L(u)⟩=⟨u,v⟩+⟨v,u⟩. (8.13)
IfF=R, then the inner product is commutative and (8.13) gives⟨L(u), L(v)⟩=⟨u,v⟩as
desired. If F=C, then substitute iuforuin (8.13) to obtain
⟨L(iu), L(v)⟩+⟨L(v), L(iu)⟩=⟨iu,v⟩+⟨v, iu⟩,
so by the linearity of L, Axiom IP2 in Definition 7.1, and Theorem 7.2(3),
i⟨L(u), L(v)⟩ −i⟨L(v), L(u)⟩=i⟨u,v⟩ −i⟨v,u⟩,
and thus
⟨L(u), L(v)⟩ − ⟨L(v), L(u)⟩=⟨u,v⟩ − ⟨v,u⟩. (8.14)
Finally, adding (8.13) and (8.14) gives ⟨L(u), L(v)⟩=⟨u,v⟩once again.
(4)→(1).Suppose that ⟨L(u), L(v)⟩=⟨u,v⟩for all u,v∈V. Thus, for any u,v∈V,
⟨L(u),v⟩=⟨L(u), L(L−1(v))⟩=⟨u, L−1(v)⟩,
which shows that L−1=L∗and therefore Lis a unitary operator. ■
Proposition 8.17. IfA,B∈Fn×nare unitary and c∈F, then A−1,cA,A+B, and ABare
unitary.
Proof. Suppose that A,B∈Fn×nare unitary and c∈F. By Proposition 8.5(2) we have
(A−1)∗= (A∗)∗=A∗∗=A= (A−1)−1;
that is, the adjoint of A−1equals the inverse of A−1, and therefore A−1is unitary. ■
Proposition 8.18. IfOandO′are two ordered orthonormal bases for an inner product space
(V,⟨ ⟩)overF, then the change of basis matrix IOO′is a unitary matrix.
Proof. Suppose that O= (w1, . . . ,wn) andO′= (w′
1, . . . ,w′
n) are each orthonormal bases for
an inner product space ( V,⟨ ⟩) over F. Then by Theorem 7.23
⟨u,v⟩= [u]⊤
O′[v]O′
for all u,v∈V, and also
⟨wi,wj⟩=δij=(
1,ifi=j
0,ifi̸=j
By Theorem 4.27
IOO′=h
[w1]O′···[wn]O′i
,
and so, letting I=IOO′for brevity,
I⊤I=
[w1]⊤
O′
...
[wn]⊤
O′
h
[w1]O′···[wn]O′i
.275
Thus the ij-entry of I⊤Iis

I⊤I
ij= [wi]⊤
O′[wj]O′=⟨wi,wj⟩=δij= [In]ij
for all 1 ≤i, j≤n, and therefore I⊤I=In. Now, since Iis invertible by Proposition 4.31, we
have
I⊤I=In⇔I⊤I=In⇔ 
I⊤I
I−1=InI−1⇔I⊤=I−1
and hence I∗=I−1. Therefore IOO′is a unitary matrix. ■276
8.3 – Normal Operators
Definition 8.19. Let(V,⟨ ⟩)be an inner product space over F. An operator L∈ L(V)is
normal if
L◦L∗=L∗◦L.
Proposition 8.20. All self-adjoint and unitary operators are normal operators
Proof. Suppose Lis a self-adjoint operator on ( V,⟨ ⟩). Then L∗=Lby definition, which
immediately implies that
L◦L∗=L∗◦L,
and hence Lis normal.
Now suppose that Lis a unitary operator on ( V,⟨ ⟩). Then L∗=L−1by definition, so that
L◦L∗=L◦L−1=IV=L−1◦L=L∗◦L
and hence Lis normal. ■
Proposition 8.21. Let(V,⟨ ⟩)be an inner product space over F. Then L∈ L(V)is a normal
operator if and only if ∥L(v)∥=∥L∗(v)∥for all v∈V.
Proof. Suppose that L∈ L(V) is a normal operator. Let v∈V. Then
∥L(v)∥2=
L(v), L(v)
=
v, L∗(L(v))
=
v,(L∗◦L)(v))
=
v,(L◦L∗)(v)
=
v, L(L∗(v))
=
L(L∗(v)),v
=
L∗(v), L∗(v)
=∥L∗(v)∥2,
and therefore ∥L(v)∥=∥L∗(v)∥.
Conversely, suppose that ∥L(v)∥=∥L∗(v)∥for all v∈V, or equivalently
⟨L(v), L(v)⟩=⟨L∗(v), L∗(v)⟩
for all v∈V. By Proposition 8.3,
(L◦L∗−L∗◦L)∗= (L◦L∗)∗−(L∗◦L)∗=L∗∗◦L∗−L∗◦L∗∗=L◦L∗−L∗◦L,
which shows that L◦L∗−L∗◦Lis self-adjoint. Now, for any v∈V,

v,(L∗◦L)(v)
=
v, L∗(L(v))
=
L(v), L(v)
=
L∗(v), L∗(v)
=
v, L(L∗(v))
=
v,(L◦L∗)(v)
,
and thus
(L◦L∗−L∗◦L)(v),v
=
(L◦L∗)(v),v
−
(L∗◦L)(v),v
= 0.
It follows by Proposition 8.13(3) that
L◦L∗−L∗◦L=OV,
and therefore L◦L∗=L∗◦L. ■277
Proposition 8.22. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V)be a normal
operator. If Uis a subspace of Vthat is invariant under L, then U⊥is also invariant under L∗.
Proof. Suppose that Uis a subspace of Vthat is invariant under L. Let q∈L∗(U⊥), so there
exists some p∈U⊥such that L(p) =q. Now, p∈U⊥implies that ⟨u,p⟩= 0 for all u∈U.
On the other hand L(u)∈Ufor all u∈U, and so
⟨L(u),p⟩= 0
for all u∈U. Now,
⟨L(u),p⟩= 0⇔ ⟨u, L∗(p)⟩= 0⇔ ⟨u,q⟩= 0,
which demonstrates that q⊥ufor all u∈U, and hence q∈U⊥. We conclude that
L∗(U⊥)⊆U⊥and therefore U⊥is invariant under L∗. ■278
8.4 – The Spectral Theorem
Recall from the previous chapter that if ( V,⟨ ⟩) is an inner product space over F, then a
linear operator LonVis called self-adjoint if
⟨L(u),v⟩=⟨u, L(v)⟩
for all u,v∈V. As the first part of the next theorem makes clear, any linear operator on
a nontrivial inner product space ( V,⟨ ⟩) over the field C, in particular, will always have an
eigenvector. If the underlying field of ( V,⟨ ⟩) isR, however, then something more is required for
the existence of an eigenvector to be assured: namely, the operator must be self-adjoint.
Theorem 8.23. Let(V,⟨ ⟩)be a vector space over Fof dimension n∈N, and let L∈ L(V).
1.IfF=C, then Lhas an eigenvector.
2.IfF=RandLis self-adjoint with respect to some inner product on V, then Lhas an
eigenvector.
Proof.
Proof of Part (1). Suppose F=C. LetBbe an ordered basis for V, and let [ L]Bbe the matrix
corresponding to Lwith respect to B. Then [ L]B∈Cn×nsince Vis a vector space over C, and
by Proposition 6.29(1) [ L]Bhas at least one eigenvalue λ∈C. Now Proposition 6.14 implies
that λis an eigenvalue of L, which is to say there exists some v∈Vsuch that v̸=0and
L(v) =λv. Therefore Lhas an eigenvector.
Proof of Part (2). Suppose F=RandLis self-adjoint with respect to some inner product on V.
By Corollary 7.15 there exists an ordered orthonormal basis OforV, and so [ L]O∈Symn(R)
by Corollary 8.10. It then follows by Theorem7.26 that [ L]Ohas an eigenvalue λ∈Rwith a
corresponding eigenvector in Rn, whereupon Proposition 6.14 implies that λis an eigenvalue of
L. Therefore Lhas an eigenvector. ■
Definition 8.24. LetVbe a vector space over F, letUbe a subspace, and let L∈ L(V)be a
linear operator. We say that Uisinvariant under L(orL-invariant ) ifL(U)⊆U.
Recall that L(U) =Img(L), and notice that a subspace Uof vector space Vis invariant
under L∈ L(V) if and only if L|U∈ L(U), where as usual L|Udenotes the restriction of the
function Lto the set U. Many times in proofs, however, we will continue to use the symbol L
to denote L|U, after writing either L∈ L(U) orL:U→U, say, to make clear that the domain
ofLis being restricted to U.
Proposition 8.25. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V)be a normal
operator.
1.Ifv∈Vis an eigenvector of Lwith corresponding eigenvalue λ, then vis an eigenvector of
L∗with eigenvalue λ.
2.Ifv1,v2∈Vare eigenvectors of Lwith corresponding eigenvalues λ1, λ2∈Fsuch that
λ1̸=λ2, then v1⊥v2.279
Proof.
Proof of Part (1). Suppose that v∈Vis an eigenvector of Lwith corresponding eigenvalue λ,
so that L(v) =λv. Define Λ = L−λIV, and note that Λ ∈ L(V). In fact Λ just so happens to
be a normal operator: recalling Proposition 8.3 and noting that I∗
V=IV, for any u∈Vwe have
(Λ◦Λ∗)(u) = Λ(( L∗−λIV)(u)) = Λ( L∗(u)−λu) =L(L∗(u)−λu)−λ(L∗(u)−λu)
= (L◦L∗)(u)−λL(u)−λL∗(u) +λλu
and
(Λ∗◦Λ)(u) = Λ∗((L−λIV)(u)) = Λ∗(L(u)−λu) =L∗(L(u)−λu)−λ(L(u)−λu)
= (L∗◦L)(u)−λL∗(u)−λL(u) +λλu.
Now, since L◦L∗=L∗◦L, we find that
Λ◦Λ∗=L◦L∗−λL−λL∗+λλIV= Λ∗◦Λ
and hence Λ is normal. Forging on, by Proposition 8.21 we obtain
∥(L∗−λIV)(v)∥=∥(L−λIV)∗(v)∥=∥(L−λIV)(v)∥=∥L(v)−λv∥=∥0∥= 0,
which implies that
(L∗−λIV)(v) =0.
That is, L∗(v) =λv.
Proof of Part (2). Suppose that v1,v2∈Vare eigenvectors of Lwith corresponding eigenvalues
λ1, λ2∈Fsuch that λ1̸=λ2. By Part (1), v1,v2∈Vare eigenvectors of L∗with corresponding
eigenvalues λ1andλ2, respectively. Now,
(λ1−λ2)⟨v1,v2⟩=λ1⟨v1,v2⟩ −λ2⟨v1,v2⟩=⟨λ1v1,v2⟩ − ⟨v1,λ2v2⟩
=⟨L(v1),v2⟩ − ⟨v1, L∗(v2)⟩= 0,
and since λ1−λ2̸= 0 we obtain ⟨v1,v2⟩= 0. Therefore v1⊥v2. ■
Whereas the proposition above establishes some eigen theory concerning normal operators,
the one below performs a similar favor for self-adjoint operators. The latter will be used to
prove the first part of the upcoming Spectral Theorem, the former the second part.
Proposition 8.26. Let(V,⟨ ⟩)be an inner product space over F, and let L∈ L(V)be a
self-adjoint operator.
1.All eigenvalues of Lare real.
2.Ifvis an eigenvector of Landu∈Vis such that u⊥v, then L(u)⊥valso.
Proof.
Proof of Part (1). Letλbe an eigenvalue of L. Then there exists some v∈Vsuch that v̸=0
andL(v) =λv. Because v̸=0we have ⟨v,v⟩>0, and because Lis self-adjoint we have
⟨L(v),v⟩=⟨v, L(v)⟩.280
Now,
⟨L(v),v⟩=⟨v, L(v)⟩ ⇔ ⟨ λv,v⟩=⟨v, λv⟩ ⇔ λ⟨v,v⟩=λ⟨v,v⟩ ⇔ λ=λ,
where the last equation obtains upon dividing by ⟨v,v⟩. Since only a real number can equal its
own conjugate, we conclude that λ∈R.
Proof of Part (2). Suppose that vis an eigenvector of Landu∈Vis such that u⊥v. Then
L(v) =λvfor some λ∈F, and⟨u,v⟩= 0. By Theorem 7.2(3),
0 =¯λ⟨u,v⟩=⟨u, λv⟩=⟨u, L(v)⟩=⟨L(u),v⟩,
which demonstrates that L(u)⊥v. ■
It is a trivial matter to verify that if Lis a normal (resp. self-adjoint) operator on V, and a
subspace UofVis invariant under L, then L|Uis a normal (resp. self-adjoint) operator on U.
This simple fact is assumed in the proof of the following momentous theorem.
Theorem 8.27 (Spectral Theorem ).Let(V,⟨ ⟩)be an inner product space over Fof dimension
n∈N.
1.LetF=R. Then L∈ L(V)is a self-adjoint operator if and only if Vhas an orthonormal
basis consisting of the eigenvectors of L.
2.LetF=C. Then L∈ L(V)is a normal operator if and only if Vhas an orthonormal basis
consisting of the eigenvectors of L.
Proof.
Proof of Part (1). We will first apply induction on dim(V) to prove that Vmust have an
orthogonal basis consisting of eigenvectors if L∈ L(V) is self-adjoint, whereupon it will be easy
to see that Vhas an orthonormal basis consisting of eigenvectors.
Letdim(V) = 1, and suppose Lis self-adjoint. Then Lhas an eigenvector wby Theorem
8.23(2), and thus B={w}is a basis for Vby Theorem 3.54(1) that is clearly orthogonal.
Suppose Part (1) of the statement of the theorem is true for some n∈N. Let ( V,⟨ ⟩) be an
inner product space over Rof dimension n+ 1, and let L:V→Vbe a self-adjoint operator.
Again, Lhas at least one eigenvector w0, so that L(w0) =λw0for some λ∈R. By the
Gram-Schmidt Process there exist vectors u1, . . . ,un∈Vsuch that B={w0,u1, . . . ,un}is an
orthogonal basis for V.
LetW=Span{w0}andU=Span{u1, . . . ,un}. For any v∈Wthere exists some c∈R
such that v=cw0, whereupon we obtain
L(v) =L(cw0) =cL(w0) =c(λw0) = (cλ)w0∈W (8.15)
and we see that Wis invariant under L. Since U=W⊥by Proposition 7.18, it follows by
Propositions 8.20 and 8.22 that Uis also invariant under L.
Now, dim(U) =nbecause {u1, . . . ,un}is a basis for U, and since ( U,⟨ ⟩) is an n-dimensional
inner product space over RandL:U→Uis a self-adjoint operator, it follows by the inductive
hypothesis that Uhas an orthogonal basis {w1, . . . ,wn}consisting of eigenvectors of L∈ L(U).281
Thus U=Span{w1, . . . ,wn}, where the vectors w1, . . . ,wnare mutually ortho-gonal. Moreover,
for each 1 ≤k≤n,
wk∈U⇒wk∈W⊥⇒ ⟨wk,w0⟩= 0,
and so w1, . . . ,wnare all orthogonal to w0. Hence O={w0, . . . ,wn}is a set of mutu-
ally orthogonal vectors, and by Lemma 7.13 we conclude that the vectors in Oare linearly
independent. Observing that |O|=n+ 1 = dim(V), Theorem 3.54(1) implies that Ois a basis
forV. That is, Ois an orthogonal basis for Vconsisting of eigenvectors of L∈ L(V).
So by induction we find that, for any n∈N, ifVis an inner product space over Rof
dimension nandLis a self-adjoint operator on V, then Vhas an orthogonal basis {w1, . . . ,wn}
consisting of eigenvectors of L. Defining
ˆwk=wk
∥wk∥
for 1≤k≤n, then {ˆw1, . . . , ˆwn}is an orthonormal basis consisting of eigenvectors of L.
For the converse, suppose that Vhas an orthonormal basis O={w1, . . . ,wn}consisting
of the eigenvectors of L∈ L(V). Since Vis a vector space over R, it follows that there exist
λk∈Rsuch that L(wk) =λkwkfor all 1 ≤k≤n. Let u,v∈V, so that
u=nX
k=1akwkand v=nX
k=1bkwk
for some ak, bk∈R, 1≤k≤n. Now, since λk=λk,
⟨L(u),v⟩=DX
kakλkwk,vE
=X
kλk⟨akwk,v⟩=X
kλkD
akwk,X
ℓbℓwℓE
=X
kX
ℓλk⟨akwk, bℓwℓ⟩=X
kλk⟨akwk, bkwk⟩=X
k⟨akwk, λkbkwk⟩
=X
kX
ℓ⟨aℓwℓ, λkbkwk⟩=DX
ℓaℓwℓ,X
kλkbkwkE
=⟨u, L(v)⟩
and therefore Lis self-adjoint.
Proof of Part (2). Letdim(V) = 1, and suppose Lis normal. Then Lhas an eigenvector wby
Theorem 8.23(1), and thus B={ˆw}is an orthonormal basis for Vby Theorem 3.54(1).
Suppose Part (2) of the statement of the theorem is true for some n∈N. Let ( V,⟨ ⟩) be
an inner product space over Cof dimension n+ 1, and let L∈ L(V) be normal. Again, Lhas
at least one eigenvector w0(which we can assume to be a unit vector), so that L(w0) =λw0
for some λ∈C. By the Gram-Schmidt Process there exist vectors u1, . . . ,un∈Vsuch that
B={w0,u1, . . . ,un}is an orthogonal basis for V.
LetW=Span{w0}andU=Span{u1, . . . ,un}. For any v∈Wthere exists some c∈C
such that v=cw0, whereupon (8.15) shows that Wis invariant under L. But Wis also
invariant under L∗, since by Proposition 8.25(1) we have
L∗(cw0) =cL∗(w0) =c(λw0) = (cλ)w0∈W.
Now, since L∗∈ L(V) is normal, by Proposition 8.22 we conclude that W⊥is invariant under
L∗∗=L, where W⊥=Uby Proposition 7.18.
Now, ( U,⟨ ⟩) is an n-dimensional inner product space over CandL∈ L(U) is a normal
operator, so by the inductive hypothesis Uhas an orthonormal basis {w1, . . . ,wn}consisting282
of eigenvectors of L. Thus U= Span {w1, . . . ,wn}, where the vectors w1, . . . ,wnare mutually
orthogonal, and as with the proof of Part (1) we find that w1, . . . ,wnare each orthogonal to
w0. Hence O={w0, . . . ,wn}is a set of mutually orthogonal vectors which, as before, we find
to be a basis for V. In particular, Ois an orthonormal basis for Vconsisting of eigenvectors of
L∈ L(V). By induction we conclude that Part (2) of the theorem is true for all n∈N.
Conversely, suppose that O={w1, . . . ,wn}is an orthonormal basis for Vconsisting of
eigenvectors of L. Thus there exist λk∈Csuch that L(wk) =λkwkfor all 1 ≤k≤n, and by
Proposition 8.25(1) we also have L∗(wk) =λkwkfor all 1 ≤k≤n. Let v∈V, so that
v=nX
k=1akwk
for some a1, . . . , a n∈C. Now,
⟨L(v), L(v)⟩=DX
kakλkwk,X
ℓaℓλℓwℓE
=X
kX
ℓ⟨akλkwk, aℓλℓwℓ⟩
=X
kX
ℓλkλℓ⟨akwk, aℓwℓ⟩=X
kX
ℓλkλℓ⟨aℓwℓ, akwk⟩
=X
kX
ℓ⟨aℓλℓwℓ, akλkwk⟩=X
kX
ℓ⟨aℓL∗(wℓ), akL∗(wk)⟩
=DX
ℓL∗(aℓwℓ),X
kL∗(akwk)E
=⟨L∗(v), L∗(v)⟩,
where the fourth equality is justified since ⟨akwk, aℓwℓ⟩is real-valued for all 1 ≤k, ℓ≤n:
⟨akwk, aℓwℓ⟩=(
0, ifk̸=ℓ
|ak|,ifk=ℓ
Hence we have
∥L(v)∥=p
⟨L(v), L(v)⟩=p
⟨L∗(v), L∗(v)⟩=∥L∗(v)∥,
and so by Proposition 8.21 we conclude that Lis a normal operator. ■
Corollary 8.28. Let(V,⟨ ⟩)be a nontrivial finite-dimensional inner product space over F, and
letλ1, . . . , λ mbe the distinct eigenvalues of L∈ L(V). IfLis self-adjoint, or if Lis normal
andF=C, then
V=EL(λ1)⊕ ··· ⊕ EL(λm). (8.16)
Moreover, EL(λi)⊥EL(λj)for all i̸=j.
Proof. Suppose that Lis self-adjoint, or Lis normal and F=C. By the Spectral Theorem
there exists an orthonormal basis O={w1, . . . ,wn}consisting of the eigenvectors of L, and
thus (8.16) follows by Theorem 6.40.
Next, let u∈EL(λi) and v∈EL(λj) for 1 ≤i < j ≤m. If either u=0orv=0, we
obtain u⊥v. Suppose that u,v̸=0. Then uis an eigenvector of Lwith corresponding
eigenvalue λi, and vis an eigenvector of Lwith corresponding eigenvalue λj. Since λi̸=λjand
Lis a normal operator, by Proposition 8.25(2) we conclude that u⊥vonce again. Therefore
EL(λi)⊥EL(λj). ■283
Example 8.29. Let (V,⟨ ⟩) be an n-dimensional inner product space over F, and suppose that
L∈ L(V) is a self-adjoint operator. By Proposition 8.20 Lis also a normal operator, and so by
the Spectral Theorem (regardless of whether FisRorC) there exist eigenvectors v1, . . . ,vn
such that B= (v1, . . . ,vn) is an ordered basis for V. By Proposition 8.26(1) the corresponding
eigenvalues λ1, . . . , λ nmust be real numbers, and so for each 1 ≤k≤nwe have λk∈Rsuch
thatL(vk) =λkvk. By Corollary 4.21 the B-matrix of Lis
[L]B=h
L(v1)
B···
L(vn)
Bi
=h
λ1v1
B···
λnvn
Bi
=h
λ1
v1
B···λn
vn
Bi
=
λ1
1
0
...
0
··· λn
0
0
...
1

=
λ10··· 0
0λ2··· 0
............
0 0 ··· λn
.
That is, [ L]Bis a diagonal matrix with real-valued entries, which makes it especially easy to
work with in applications.
We see, then, that the Spectral Theorem provides a means of diagonalizing self-adjoint
operators on nontrivial inner product spaces, and even normal operators if the underlying field
isC.
Proposition 8.30. IfA∈Fn×nis self-adjoint, then there exists a unitary matrix Usuch that
U−1AUis a diagonal matrix.
Proof. Suppose that A∈Fn×nis self-adjoint. Let Ebe the standard basis for Fn, and let
L∈ L(Fn) be the operator given by [ L(x)]E=A[x]E, so that the matrix corresponding to L
with respect to Eis [L]E=A. Since Eis an orthonormal basis and [ L]Eis self-adjoint, by
Theorem 8.9 the operator Lis self-adjoint, and therefore Lis normal by Proposition 8.20. By
the Spectral Theorem there exists an ordered orthonormal basis Oconsisting of the eigenvectors
ofL, and so [ L]Ois found to be a diagonal matrix by Corollary 4.21.
Consider IEO, the change of basis matrix from EtoO. Both bases are orthonormal, so IEO
is a unitary matrix by Proposition 8.18, and
[L]O=IEO[L]EI−1
EO (8.17)
by Corollary 4.33. Now, the inverse of a unitary matrix is also unitary by Proposition 8.17, so if
we let U=I−1
EO, then Uis unitary. Also we have U−1=IEOis unitary. From (8.17) comes
U−1AU= [L]O,
and the proof is done since [ L]Ois diagonal. ■284
9
Canonical Forms
9.1 – Generalized Eigenvectors
Recall that a vector v̸=0is an eigenvector of a linear operator L:V→VifL(v) =λvfor
some scalar λ, where
L(v) =λv⇔L(v)−λv=0⇔L(v)−λIV(v) =0⇔(L−λIV)(v) =0.(9.1)
We expand on this idea as follows.
Definition 9.1. LetVbe a vector space over F,L∈ L(V), and λ∈F. Ifv∈Vis a nonzero
vector such that (L−λIV)n(v) =0for some n∈N, then vis ageneralized eigenvector of
Lcorresponding to λ.
From (9.1) is it clear that the set of eigenvectors of Lis included in the set of generalized
eigenvectors of L, and any eigenvalue corresponding to an eigenvector necessarily also corresponds
to a generalized eigenvector. Suppose that v̸=0is a generalized eigenvector of Lcorresponding
toλ. Let
n= min {k∈N: (L−λIV)k(v) =0}.
Ifn≥2, then w= (L−λIV)n−1(v) is a nonzero vector in V, and
0= (L−λIV)n(v) = (L−λIV) 
(L−λIV)n−1(v)
= (L−λIV)(w) =L(w)−λw
implies that L(w) =λw. This result obtains immediately if n= 1, and so it follows that λis an
eigenvalue of Lwith ( L−λIV)n−1(v) as a corresponding eigenvector. We see that any eigenvalue
corresponding to a generalized eigenvector necessarily also corresponds to an eigenvector. It is
because a scalar λcorresponds to an eigenvector if and only if it corresponds to a generalized
eigenvector that we make no distinction between “eigenvalues” and “generalized eigenvalues.”
Definition 9.2. LetVbe a vector space over FandL∈ L(V). Suppose λ∈Fis an eigenvalue
ofL. The set
KL(λ) ={v∈V: (L−λIV)n(v) =0for some n∈N}
is the generalized eigenspace ofLcorresponding to λ.285
To prove the following proposition, note that if Wis an L-invariant subspace of a vector space
VoverF, then so too is Img(L), for the simple reason that L(W)⊆Wimplies L(L(W))⊆W,
and hence L(Img(L))⊆W. Also note that, for any f∈ P(F), the L-invariance of Wimplies
thef(L)-invariance of W.
Lemma 9.3. LetVbe a vector space over F,L∈ L(V), and λ∈F. For any n∈N,
(L−λIV)n◦L=L◦(L−λIV)n.
Proof. When n= 1 we have, by Theorem 4.50
L◦(L−λIV) =L◦L−λL◦IV=L◦L−λIV◦L= (L−λIV)◦L.
Suppose the conclusion of the lemma is true for some fixed n∈N. That is, if M=L−λIV,
then L◦Mn=Mn◦L. Now, making use of Theorem 4.49,
L◦Mn+1= (L◦Mn)◦M= (Mn◦L)◦M=Mn◦(L◦M)
=Mn◦(M◦L) = (Mn◦M)◦L=Mn+1◦L,
and therefore L◦Mn=Mn◦Lfor all n∈Nby induction. ■
Proposition 9.4. LetVbe a vector space over FandL∈ L(V). Suppose that λ∈Fis an
eigenvalue of L. Then
1.KL(λ)is an L-invariant subspace of Vsuch that EL(λ)⊆KL(λ).
2.For any µ∈Fsuch that µ̸=λ, the operator L−µIV:KL(λ)→Vis injective.
3.Ifµis an eigenvalue of Lsuch that µ̸=λ, then KL(µ)∩KL(λ) ={0}.
Proof.
Proof of Part (1). We have KL(λ)̸=∅since 0∈KL(λ). Let u,v∈KL(λ), so that
(L−λIV)m(u) =0and ( L−λIV)n(v) =0
for some m, n∈N. Then
(L−λIV)m+n(u+v) = (L−λIV)m+n(u) + (L−λIV)m+n(v)
= (L−λIV)n 
(L−λIV)m(u)
+ (L−λIV)m 
(L−λIV)n(v)
= (L−λIV)n(0) + (L−λIV)m(0) =0+0=0,
and we conclude that u+v∈KL(λ). Ifc∈F, then
(L−λIV)m(cu) =c(L−λIV)m(u) =c0=0
shows that cu∈KL(λ). Since KL(λ) is a nonempty subset of Vthat is closed under vector
addition and scalar multiplication, we conclude that it is a subspace of V. That EL(λ)⊆KL(λ)
is obvious.
Next, let v∈L(KL(λ)), so there is some u∈KL(λ) such that L(u) =v. There exists some
n∈Nsuch that ( L−λIV)n(u) =0, and hence by Lemma 9.3
(L−λIV)n(v) = (L−λIV)n(L(u)) =L 
(L−λIV)n(u)
=L(0) =0.286
Therefore v∈KL(λ), and we conclude that L(KL(λ))⊆KL(λ).
Proof of Part (2). Letv∈KL(λ) such that ( L−µIV)(v) =0. Let
n= min {k∈N: (L−λIV)k(v) =0}.
We have
(L−λIV) 
(L−λIV)n−1(v)
= (L−λIV)n(v) =0,
so that ( L−λIV)n−1(v)∈EL(λ). By Lemma 9.3,
(L−µIV) 
(L−λIV)n−1(v)
= (L−λIV)n−1 
(L−µIV)(v)
= (L−λIV)n−1(0) =0,
so that ( L−λIV)n−1(v)∈EL(µ). Since EL(λ)∩EL(µ) ={0}by Proposition 6.7(2), it follows
that
(L−λIV)n−1(v) =0.
Since nis the smallest positive integer for which ( L−λIV)n(v) =0holds, we must conclude
thatn−1 = 0, and so
0= (L−λIV)n−1(v) = (L−λIV)0(v) =v.
Therefore the null space of L−µIVrestricted to KL(λ) is{0}, and so L−µIV:KL(λ)→Vis
injective.
Proof of Part (3). Suppose that v∈KL(λ)∩KL(µ), so in particular ( L−µIV)n(v) =0for some
n∈N. Since KL(λ) isL-invariant by Part (1), it readily follows that KL(λ) is invariant under
L−µIV, and thus L−µIVis an injective operator on KL(λ) by Part (2). An easy induction
argument shows that ( L−µIV)nis likewise an injective operator on KL(λ), and since v∈KL(λ)
is such that ( L−µIV)n(v) =0, it follows that v=0and therefore KL(λ)∩KL(µ) =∅.■
Proposition 9.5. LetVbe a finite-dimensional vector space over FandL∈ L(V). Suppose
thatPLsplits over Fandλ∈σ(L)has algebraic multiplicity m. Then
1. dim( KL(λ))≤m.
2.KL(λ) = Nul(( L−λIV)m).
Proof.
Proof of Part (1). Letting dim( V) =n, and recalling Corollary 6.28 and Definition 6.30, there
exist a1, . . . , a n−m∈Fsuch that
PL(t) = (−1)n(t−λ)mn−mY
k=1(t−ak),
where ak̸=λfor all k. Let LK=L|KL(λ). By Proposition 9.4(1), LK∈ L(KL(λ)) and λis an
eigenvalue of LK. From the latter fact it follows by Theorem 6.18 that PLK(λ) = 0, so that
t−λis a factor of PLK(t).
Suppose that PLK(t) has a factor t−µfor some µ̸=λ, so that PLK(µ) = 0. Then µis
an eigenvalue of LKby Theorem 6.18 again, so there exists some v̸=0inKL(λ) such that
LK(v) =µv, whence ( LK−µI)(v) =0. But LK−µI:KL(λ)→KL(λ) is injective by287
Proposition 9.4(2), so that Nul(LK−µI) ={0}and hence v=0, which is a contradiction.
Thus there exists no µ̸=λsuch that t−µis a factor of PLK(t), and so
PLK(t) = (−1)r(t−λ)r(9.2)
for some 1 ≤r≤n. However, PLK(t) divides PL(t) by Proposition 4.55, and since PL(t) has
precisely mfactors of the form t−λ, we conclude that r≤m. Observing that deg(PLK) =r,
we finally obtain
dim(KL(λ)) = deg( PLK)≤m
by Corollary 6.28.
Proof of Part (2). Since KL(λ) is a finite-dimensional vector space and LK∈ L(KL(λ)), by the
Cayley-Hamilton Theorem and (9.2) we have
PLK(L) = (−1)r(LK−λIK)r=OK,
where IKandOKrepresent the identity and zero operators on KL(λ), and 1 ≤r≤m. Thus
(LK−λIK)r=OK, and so for any v∈KL(λ),
(LK−λIK)r(v) =0, (9.3)
and then
(LK−λIK)m(v) = (LK−λIK)m−r 
(LK−λIK)r(v)
= (LK−λIK)m−r(0) =0
shows that
v∈Nul(( LK−λIK)m)⊆Nul(( L−λIV)m).
(Observe that if r=m, then (9.3) delivers the desired outcome right away.) On the other hand
ifv∈Nul(( L−λIV)m), then it is immediate that v∈KL(λ).
Therefore KL(λ) = Nul(( L−λIV)m). ■
9.2 – Jordan Form288
10
The Geometry of Vector Spaces
10.1 – Convex Sets
Recall that, if Vis a vector space over Randu,v∈V, then the line segment joining uand
vis the set
Luv={(1−t)u+tv: 0≤t≤1}.
A set C⊆Vthat always contains the line segment joining two of its elements is of special
interest.
Definition 10.1. LetVbe a vector space over R. A set C⊆Visconvex ifLuv⊆Cfor every
u,v∈C.
Notice that any vector space Vis convex: if uandvare in V, then any linear combination
c1u+c2vis also in V, which certainly includes any linear combination of the form (1 −t)u+tv,
0≤t≤1, and therefore Luv⊆V.
Theorem 10.2. LetVbe a vector space over R. For any v1, . . . ,vn∈Vthe set
S=(nX
i=1tivit1, . . . , t n≥0andnX
i=1ti= 1)
(10.1)
is convex.
Proof. Letv1, . . . ,vn∈V. Fix u,w∈S, so that
u=u1v1+···+unvnand w=w1v1+···+wnvn
for some ui, wi∈Rsuch that ui, wi≥0 for all 1 ≤i≤n, and
nX
i=1ui=nX
i=1wi= 1.
Letx∈Luwbe arbitrary, so x= (1−s)u+swfor some s∈[0,1]. It must be shown that
x∈S. Now,
x= (1−s)(u1v1+···+unvn) +s(w1v1+···+wnvn)289
= [(1−s)u1+sw1]v1+···+ [(1−s)un+swn]vn,
where for each iwe clearly have (1 −s)ui+swi≥0, and
nX
i=1[(1−s)ui+swi] = (1 −s)nX
i=1ui+snX
i=1wi= (1−s)(1) + ( s)(1) = (1 −s) +s= 1.
Thus if we let xi= (1−s)ui+swifor each 1 ≤i≤n, then
x=x1v1+···+xnvn
with xi≥0 for all iandx1+···+xn= 1. Hence x∈S, and since x∈Luwis arbitrary it
follows that Luw⊆S. Since u,w∈Sare arbitrary we conclude that Luw⊆Sfor all u,w∈S,
and therefore Sis convex. ■
Proposition 10.3. LetVbe a vector space over RandC⊆Va convex set. If v1, . . . ,vn∈C
andt1, . . . , t n≥0withPn
i=1ti= 1, thenPn
i=1tivi∈C.
The proof of this proposition will be done by induction.
Proof. In the case when n= 1, the statement of the proposition reads as: “If v1∈Cand
t1≥0 with t1= 1, then t1v1∈C”. This is obviously true, and so the base case of the inductive
argument is established.
Now assume the statement of the proposition is true for some arbitrary integer n≥1.
Suppose that v1, . . . ,vn+1∈Candt1, . . . , t n+1≥0 with t1+···+tn+1= 1. It must be shown
thatt1v1+···+tn+1vn+1∈C.
Iftn+1= 1 then we must have ti= 0 for all 1 ≤i≤n, whence
n+1X
i=1tivi=vn+1∈C
and we’re done.
Assuming that tn+1̸= 1, observe that fromPn+1
i=1ti= 1 we have
nX
i=1ti= 1−tn+1,
and so
nX
i=1ti
1−tn+1= 1 (10.2)
obtains since 1 −tn+1̸= 0. Now,
n+1X
i=1tivi=nX
i=1tivi+tn+1vn+1= (1−tn+1)nX
i=1ti
1−tn+1vi+tn+1vn+1,
where by the inductive hypothesis
u=nX
i=1ti
1−tn+1vi290
Figure 11. The convex hull of some points in R2.
is an element of Cbecause of (10.2) and the observation that
ti
1−tn+1≥0
for all 1 ≤i≤n. Thus, since u,vn+1∈C,
n+1X
i=1tivi= (1−tn+1)u+tn+1vn+1
for some 0 ≤tn+1<1, and Cis convex, we conclude thatPn+1
i=1tivi∈C.
Therefore the statement of the proposition is true for n+ 1, and the proof is done. ■
We say that C′is the smallest convex set containing v1, . . . ,vnif, for any convex set C
such that v1, . . . ,vn∈C, we have C′⊆C.
Corollary 10.4. LetVbe a vector space and v1, . . . ,vn∈V. Then the set Sgiven by (10.1)
is the smallest convex set that contains v1, . . . ,vn.
Proof. LetCbe a convex set containing v1, . . . ,vn. For any x∈Swe have
x=nX
i=1tivi
Figure 12. Stereoscopic image of the convex hull of some points in R3.291
v1
v2
Figure 13. The parallelogram spanned by v1,v2.
for some t1, . . . , t n≥0 such thatPn
i=1ti= 1. But by Proposition 10.3 it then follows that
x∈C. Therefore S⊆C. ■
Theconvex hull of a set A, denoted here by Conv (A), is defined to be the smallest convex
set that contains A. It is easy to show that Conv (A) is equal to the intersection of all convex
setsCthat contain A:
Conv( A) =\
{C:A⊆CandCis convex }
Thus Corollary 10.3 states that
Conv({v1, . . . ,vn}) =(nX
i=1tivit1, . . . , t n≥0 andnX
i=1ti= 1)
.
See Figures 11 and 12.
Suppose that v1andv2are two linearly independent vectors in a vector space V. Then the
parallelogram spanned by v 1and v 2is the set of vectors (or points, if preferred)
{t1v1+t2v2: 0≤t1, t2≤1}.
Note that 0belongs to this set, as well as v1,v2,v1+v2. To see how the set forms a parallelogram
in the geometric sense, we return to the practice introduced in Chapter 1 of representing vectors
by arrows, which can still be done even if Vis not a Euclidean space (i.e. a vector space
consisting of Euclidean vectors). See Figure 13.
Definition 10.5. Let{v1, . . . ,vn}be a linearly independent set of vectors in V. The n-
dimensional box spanned by v1, . . . ,vnis the set
Bn=(nX
i=1tivi0≤t1, . . . , t n≤1)
.
v1v2
v3v1v2
v3
Figure 14. The parallelepiped spanned by v1,v2,v3.292
In particular B1is the line segment spanned by v1,B2theparallelogram spanned by v1
andv2andB3theparallelepiped spanned by v1,v2, and v3.
For a depiction of a parallelepiped (or box) spanned by v1,v2, and v3, see Figure 14. It can
be shown that the box Bnis a convex set for any n∈N.293
S
Symbol Glossary
A The matrix A.
A⊤The transpose of A= [aij]: the ij-entry of A⊤isaji.
A The conjugate of A= [aij]: the ij-entry of Aisaij.
A∗The adjoint of A:A∗= 
A⊤.
|A| The determinant of the matrix A.
Aij The submatrix of Aobtained by deleting the ith row and jth column of A.
Ai⋆ The submatrix of Aobtained by deleting the ith row of A.
A⋆j The submatrix of Aobtained by deleting the jth column of A.
(A)ij Same as Aij. Used for such expressions as ( A⊤)ijfor clarity.
[A]ij Theij-entry of matrix A.
[aij]m,n Anm×nmatrix with ij-entry aij.
[aij]n Ann×nsquare matrix with ij-entry aij.
[aij] A matrix with dimensions either unspecified or understood from context.
C The set of complex numbers.
δij The Kronecker delta: δij= 0 if i̸=j,δij= 1 if i=j.
ej Thejth standard basis element of RnorCn:ej= [δij]n×1.
EA(λ) The eigenspace corresponding to the eigenvalue λof matrix A.
EL(λ) The eigenspace corresponding to the eigenvalue λof operator L.294
F An unspecified field, the elements of which are called scalars.
FnF1×nin Chapter 1, otherwise Fn×1.
Fm×nSet of all m×nmatrices with entries in F.
φB The coordinate map, where φB(v) = [v]B.
γA(λ) The geometric multiplicity of eigenvalue λof matrix A.
γL(λ) The geometric multiplicity of eigenvalue λof operator L.
Img(L) Image of linear mapping L:V→W. Img( L) ={L(v) :v∈V}.
IBB′ The change of basis matrix from basis Bto basis B′.
In Then×nidentity matrix.
I An identity matrix, dimensions unspecified or understood from context.
IV The identity operator on vector space V:IV(v) =vfor all v∈V.
[L] Matrix corresponding to linear mapping Lwith respect to any basis.
[L]B Matrix corresponding to linear operator Lwith respect to the basis B.
[L]BC Matrix corresponding to linear mapping Lwith respect to bases BandC.
L(V) Image of Vunder L:V→W.L(V) = Img( L).
L(V) Set of all linear operators L:V→Von some vector space V.
L(V, W ) Set of all linear mappings L:V→Won given vector spaces VandW.
µA(λ) The algebraic multiplicity of eigenvalue λof matrix A.
µL(λ) The algebraic multiplicity of eigenvalue λof operator L.
Nul(L) Null space of linear mapping L:V→W. Nul( L) ={v∈V:L(v) =0}.
N The set of natural numbers (i.e. positive integers): N={1,2,3, . . .}.
O The zero mapping in L(V, W ):O(v) =0for all v∈V.
OV The zero operator on vector space V:OV(v) =0for all v∈V.
0 The zero vector.
F(S,F) The set of all functions S→F.
0V The zero vector of vector space V.295
Om,n Them×nzero matrix (all entries are 0).
On Then×nzero matrix (all entries are 0).
O A zero matrix, dimensions unspecified or understood from context.
O An orthonormal basis for an inner product space.
Pn(F) The vector space of all polynomials of degree at most nwith coefficients in F.
Q The set of rational numbers.
R The set of real numbers.
|S| The number of elements in the set S(i.e. the cardinality of S)
Symn(F) Set of all n×nsymmetric matrices with entries in F.
Skw n(F) Set of all n×nskew-symmetric matrices with entries in F.
s∼ The similar matrix relation.
σ(A) Set of all eigenvalues in Fof a matrix A∈Fn×n.
σ(L) Set of all eigenvalues in Fof an operator L∈ L(V).
v The vector v.
∥v∥ The norm or magnitude of v; that is, ∥v∥=√v·vor∥v∥=p
⟨v,v⟩
ˆv The normalization of vector v; that is, ˆv=v/∥v∥
[v]B TheB-coordinates of vector v.
W The set of whole numbers: W={0,1,2,3, . . .}.
Z The set of integers: Z={1,−1,2,−2,3,−3, . . .}.
⟨ ⟩ The inner product function.
⟨ ⟩V The inner product function of vector space V.
⇒ Symbol for logical implication. Read as “implies” or “implies that.”
⇔ Symbol for logical equivalence. Read as “is equivalent to” or “if and only if.”
>>>>>>> tailwind
