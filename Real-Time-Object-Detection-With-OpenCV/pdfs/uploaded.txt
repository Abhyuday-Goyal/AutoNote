<<<<<<< HEAD
1 
 
 
 
 
 
 
 
 
Effects of Rising Temperatures and Drought on Rice Production  
In the United States of America  
By: Nishkal Hundia  
Dr. Lars Olson, AREC280  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 2 
 
Abstract  
This paper aims to investigate the impact of climate elements, notably escalating 
temperatures and drought occurrences, on rice farming in the United States. By analyzing 
extensive datasets covering several decades, the research aims to discern patterns in  
temperature changes during critical phases of rice cultivation and the prevalence of drought 
events. The findings reveal a trend of steadily rising average temperatures during rice -growing 
months over recent decades. While maximum temperatures remained re latively consistent, the 
consistent increase in minimum temperatures led to a narrower temperature range, potentially 
placing strain on rice crops known to thrive within specific temperature thresholds. 
Temperature variances and rice yield growth, while being statistically significant , did not impact 
the yield growth much  at either local or national scales. Conversely, the influence of drought 
emerges as a significant and distinct factor affecting rice yield. There is an evident negative 
relationship betwee n yield growth and drought severity. A notable reduction in yield growth is 
observed as drought severity intensifies, displaying statistical significance across both localized 
and national levels. These outcomes emphasize the susceptibility of rice farming  in the US to 
drought events, highlighting the substantial challenge faced in maintaining consistent 
productivity.  
Significance Statement  
 Rice is a vital food crop on a global scale and analyzing the impact of climate change and 
drought on yield growth ca n reveal invaluable insights that will help improve yield in the future. 
Examining drought's impact on US rice yield highlights a critical correlation with climate change. 
Understanding these dynamics is essential for safeguarding rice production against 
environmental challenges especially because rice is considered to be a highly drought -
susceptible plant.  3 
 
Introduction  
 Rice is among the worlds most important food crops. Research into rice is crucial for the 
development of technologies that will increase productivity for farmers who rely on rice for 
their livelihood (*Zeigler & Barclay, 2008). Rice is a major staple among two -thirds of the 
worlds population (*Sen et al., 2020).  By the worlds standards, per capita rice consumption in 
the United States is  not very large, but it has been consistently increasing over the last several 
decades, reaching a level of 21.0 lb per capita annually currently (*Batres -Marquez et al., 2009). 
United States exports about 40 -45 percent of its rice crop each year, mostly t o Mexico, Central 
America, South America, the Caribbean, etc., and accounts for around 5 percent of the annual 
volume of global rice trade ( USDA ERS - Trade , n.d.). Rice is also a source of nutrition and health 
benefits for the U.S. population, as it is lo w in fat and sodium, cholesterol -free, and rich in 
complex carbohydrates and essential vitamins and minerals ( USDA ERS - Rice Sector at a 
Glance , n.d.).  
 Drought is the leading threat to agricultural food production, especially in the cultivation 
of rice, a semi -aquatic plant (*Oladosu et al., 2019). Rice is considered one of the most drought -
susceptible plants due to its small root system, thin cuticular wax, and swift stomata closure  
(*Sahebi et al., 2018). Drought is a very devastating and costly natura l disaster. From 1980 to 
2022, droughts caused extensive losses in the United States ($344.9B CPI -adjusted economic 
losses) accounting for roughly 13% of total losses from weather and climate disasters 
(noaa.gov, 2023). Similarly, increasing temperatures c an also have a drastic impact on rice 
production. Most of the rice in the US is currently cultivated in regions where temperatures are 
above the optimal temperature for the growth of rice, which is 22 -28 degrees Celsius or 71 -87 
degrees Fahrenheit (Krishna n et al., 2011). Any further increase in mean temperature or 
episodes of high temperatures during sensitive stages may reduce rice yields drastically.  This 4 
 
study aims to explore the impacts of drought and changing temperatures on the Yield Growth 
of Rice in the United States of America over time.  
Results:  
 To understand the potential impacts of changing temperatures, we will first assess 
trends over time of various temperature variables. Figure 1 shows the variation of the mean 
temperature during the rice -growing and harvesting months in the US (April - August) from 
1960 to 2020. As can be seen, there has been a steady increase in overall mean temperatures 
over the last few decades. Figure 2 shows the changes in the mean of the maximum (T max) and 
minimum ( Tmin) temperatures from 1960 to 2020 during the rice -growing and harvesting 
months in the US. The graph on the left in Figure 2 shows T min increasing significantly over time 
whereas the graph on the right shows T max not varying as much. This explains the d ecrease in 
the difference between T max and T min from 1960 to 2020 as shown in Figure 3.  
Figure 4 shows a graph of growth in the Yield of rice at the county level vs the change in 
the difference between T max and T min of their respective years. The graph al so contains a linear 
regression line which is almost horizontal . Fitting a linear regression model on the county  level  
showed that, although statistically significant,  change in the difference between minimum and 
maximum temperatures did not cause much cha nge in  yield growth, where a 1-degree  
Fahrenheit increase in  the differences leads to a 0.4% reduction in yield, as can be seen in Table 
1. Not much changed when fitting to country -level data as can be seen in Table 2 , with a 1 -
degree increase in the difference leading to a 1.9% reduction in yield. The country level case is 
also represented by Figure 4.  Moreover, changes in the mean temperature over time also 
proved  to be statistically significant when trying to model yield growth, as shown in Tables 1  
and 2 , but yet again, its impact on the value of yield growth was not major.  5 
 
Drought is another factor that was tested for any correlation to the yield growth of rice 
over time. Figure 5 represents the variation of the mean Palmer Drought Severity Index d uring 
the rice sowing and growing months (March -July) by year. It shows consistent repetition of 
drought occurrences over time in the US and the almost horizontal linear regression line is a 
further indication of the same. Figure 6 shows the graph of yield  growth vs the Change in Mean 
Palmer drought severity index. The blue line represents a linear regression line which shows a 
clear fall in yield growth with increasing drought severity index. The change in the value of 
drought severity is revealed to be st atistically significant at both the county level and the 
country level as revealed in Tables 1 and 2 respectively , with a 1.3% decrease in yield growth 
with a 1 -point change in the drought severity index at county level and a 2.2% decrease at the 
country l evel. The country level condition is represented in Figure 6.  
Discussion:  
The results show the intricate relationship between climate variables like rising 
temperatures and drought, and their impact on rice production in the United States. The 
increase in mean temperatures during crucial rice -growing months over the past few decades, 
as seen in Figures 1 and 2, presents a concerning trend. While the maximum temperatures 
(Tmax) have not changed as much, the consistent rise in minimum temperatures (T min) lead s to a 
reduction in the gap between the T max and T min as seen in Figure 3. This narrowing temperature 
range can be indicative of potential stress on rice crops , as these crops can only thrive within a 
specific temperature band (Krishnan et al., 2011).  
The correlation between change in temperature difference and rice yield growth, 
although statistically significant, was not very evident . This does not directly agree with the 
literature reviewed, but the statistical significance of temperature variables  sugge sts a complex 6 
 
relationship where other factors might interplay or mitigate the impact of temperature 
variations on rice yields.  
The impact of drought on rice yield growth, however, appears more pronounced and  is 
also statistically significant. The Palmer D rought Severity Index, one of the most widely used 
drought indicators, was used to measure the impact of drought on rice yield (*Wang et al., 
2022). demonstrates a recurring pattern of drought occurrences during the crucial rice sowing 
and growing months a s seen in Figure 5. The analysis depicted in Figure 6 shows a clear 
negative correlation between yield growth and change in drought severity at the country level. 
As the severity of drought increases, the yield growth reduces considerably, revealing a stro ng 
correlation  at both state and country levels, as represented in Tables 1 and 2. These results 
confirm the fact that rice is highly susceptible to drought , as suggested by Sahebi et. al.,  and 
further research must be done to find viable solutions to prev ent these recurring droughts from 
affecting rice yield in the US in the future.  
These findings show the vulnerability of rice cultivation in the US to drought events. 
While temperature fluctuations show a less direct  impact on yield growth, the effects of 
drought are starkly evident. As rice cultivation heavily relies on water availability, the recurring 
and consistent occurrence of drought poses a major challenge to sustained productivity. The 
lack of a clear statistical relationship between temperature va riations and yield growth could 
suggest the existence of adaptive mechanisms within rice cultivation practices or the need for 
further analysis that takes into account additional variables. Furthermore, while the 
temperature variations might not exhibit a direct linear correlation with yield growth, their 
indirect impacts on rice growth might still be of interest and warrant further investigation.  
Materials/Methods  7 
 
 This paper relied on county -level month -wise rice yield data obtained from the United 
State s Department of Agriculture (USDA) along with temperature and precipitation data 
collected over the years from 1960 -2020 by counties in the US (National Centers for 
Environmental Information (NCEI), 2023) . It also uses Palmer Drought Severity Index data at  the 
state level collected from 1960 -2020 (National Centers for Environmental Information (NCEI), 
2023). The Palmer Drought Severity Index was used because it is one of the most widely used 
indicators for drought monitoring and research (*Wang et al., 2022 ). All datasets were joined 
together in R. To graph how temperature has changed over time during the rice growing and 
harvesting months, the mean of the temperature was taken for these months and then a line 
plot was created by setting Year to the x -axis and the mean temperature to the y -axis. 
Similarly, changes in mean minimum and maximum temperatures were also graphed with 
Year as the x -axis. Linear regression lines were made for each of these graphs to show the 
trend over time. Drought severity was a lso graphed in a similar way over time. Scatterplots and 
linear regression models were created to investigate the relationship between temperature 
changes and drought severity with changes in the yield growth of rice. Yield growth was 
obtained by the formu la log(Yield)  log(lag(Yield)). Linear regression models were created at 
both the county level and at the country level. Since drought data was only available at the 
state level, every county in a state was assigned the same value for the drought index. F or the 
country level evaluation, mean was taken for all three explanatory variables: Change in 
Temperature, Change in the Difference between Minimum and Maximum Temperatures, and 
the Palmer Drought Severity Index as well as Yield Growth to get values at th e country level. A 
linear model was then fit to these datasets and summaries were generated which constitute 
tables 1 and 2.  8 
 
Figure 1 : Graph showing the variation of mean temperatures during crucial rice -growing and 
harvesting months (April -August) from 19 60-2020. The blue line in the figure represents a linear 
line of best fit for temperature values based on time.  
 
 
 
 
 
 
 
Figure 2 : Graphs showing the variation of the mean of the minimum (left) and maximum (right) 
temperatures during rice-growing and harvesting months (April -August) from 1960 -2020. They 
also include a blue linear line of best fit for temperature values based on time.  
Figure 3 : Graph showing the variation in the difference between the mean maximum and 
minimum temperatures during the rice -growing and harvesting months (April -August) from 
1960 -2020. Also includes a linear line of best fit for the difference in temperatures based on 
time.  
 
 
9 
 
Figure 4 : Graph showing the variation of yield growth of rice in the US b ased on the difference 
in mean minimum and maximum temperatures during the rice -growing and harvesting months. 
Also includes a linear line of best fit for the yield growth based on the difference in 
temperature.  
Figure 5 : Graph showing the variation in the  mean Palmer Drought Severity Index from 1960 -
2020 during the rice growing and sowing months (March -July). Also includes a linear line of best 
for the drought severity based on time.  
Figure 6 : Graph showing the variation of yield growth of rice in the US based on change in the 
mean drought severity during the rice -growing and sowing months. Also includes a linear line of 
best fit for the yield growth based on the drought severity.
10 
 
Table 1 : Yield growth of rice in the United States based on the change in mean temperature, change in 
drought severity, and change in the difference between minimum and maximum temperatures. Yield growth 
is given by log(Yield)  log(lag(Yield)). Temperature variables are at the co unty level while drought variable is 
at the state level.
 
Table 2 : Yield growth of rice in the United States based on the change in mean temperature, change in 
drought severity, and change in the difference between minimum and maximum temperatures. Yield  
growth is given by log(Yield)  log(lag(Yield)). All variables are at the country level. Values for the 
variables are the mean taken by year.  
11 
 
References  
1. *Batres -Marquez, S. P., Jensen, H. H., & Upton, J. (2009). Rice Consumption in the 
United States: Recent Evidence from Food Consumption Surveys . Journal of the 
American Dietetic Association , 109(10), 1719 â€“1727. 
https://doi.org/10.1016/j.jada.2009.07.010  
2. Krishnan, P., Ramakrishnan, B., Reddy, K. R., & Reddy, V. R. (2011, January 1). Chapter 
three - High -Temperature Effects on Rice Growth, Yield, and Grain Quality  (D. L. Sparks, 
Ed.). ScienceDirect; Academic Press.  
https://www.sciencedirect.com/science/article/abs/pii/B9780123876898000047  
3. noaa.gov. (2023). Billion -Dollar Weather and Climate Disasters | National Centers for 
Environmental Information  (NCEI). Www.ncei.noaa.gov. 
https://www.ncei.noaa.gov/access/billions/state -summary/US  
4. National Centers for Environmental Information (NCEI). (2023, June 23). NOAA Monthly 
U.S. Climate Divisional Database (NCLIMDIV) . 
https://www.ncei.noaa.gov/acce ss/metadata/landing -
page/bin/iso?id=gov.noaa.ncdc:C00005  
5. *Oladosu, Y., Rafii, M. Y., Samuel, C., Fatai, A., Magaji, U., Kareem, I., Kamarudin, Z. S., 
Muhammad, I., & Kolapo, K. (2019). Drought Resistance in Rice from Conventional to 
Molecular Breeding: A R eview. International Journal of Molecular Sciences , 20(14), 3519. 
https://doi.org/10.3390/ijms20143519  
6. *Sahebi, M., Hanafi, M. M., Rafii, M. Y., Mahmud, T. M. M., Azizi, P., Osman, M., Abiri, 
R., Tahe ri, S., Kalhori, N., Shabanimofrad, M., Miah, G., & Atabaki, N. (2018). 
Improvement of Drought Tolerance in Rice (Oryza sativa L.): Genetics, Genomic Tools, 
and the WRKY Gene Family . BioMed Research International , 2018, 1 â€“20. 
https://doi.org/10.1155/2018/3158474  
7. *Sen, S., Chakraborty, R., & Kalita, P. (2020). Rice - not just a staple food: A 
comprehensive review on its phytochemicals and therapeutic potential. Trends in Food 
Science and Technology , 97,  265â€“285. https://doi.org/10.1016/j.tifs.2020.01.022  
8. USDA ERS - Trade. (n.d.). Www.ers.usda.gov . 
https://www.ers.usda.gov/topics/crops/rice/trad e/ 
9. USDA ERS - Rice Sector at a Glance. (n.d.). Www.ers.usda.gov. 
https://www.ers.usda.gov/topics/crops/rice/rice -sector -at-a-glance/  
10. *Wang, Z., Yang, Y., Zhang, C., Guo,  H., & Hou, Y. (2022). Historical and future Palmer 
Drought Severity Index with improved hydrological modeling. Journal of Hydrology , 610, 
127941. https://doi.org/10.1016/j.jhydrol.2022.127941  
11. *Zeigler, R. S., & Barclay, A. (2008). The Relevance of Rice. Rice, 1(1), 3 â€“10. 
https://doi.org/10.1007/s12284 -008-9001 -z 
=======
Linear Algebra
Joe EricksonTable of Contents
1.Euclidean Vectors
1.1 Groups, Rings, and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Real Euclidean Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Located Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 The Dot Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
1.5 The Norm of a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12
1.6 Lines and Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.Matrices and Systems
2.1 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.2 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3 Row and Column Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .34
2.4 The Inverse of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.5 Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.6 Homogeneous Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.Vector Spaces
3.1 The Vector Space Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.3 Subspace Sums and Direct Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.4 Linear Combinations and Spans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.5 Linear Independence and Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .81
3.6 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
3.7 Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.8 The Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .99
4.Linear Mappings
4.1 Linear Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
4.2 Images and Null Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
4.3 Matrix Representations of Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.4 Change of Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .128
4.5 The Rank-Nullity Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.6 Dimension and Rank Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.7 Compositions of Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.8 The Inverse of a Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.9 Properties of Invertible Operators and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1555.Determinants
5.1 Determinants of Low Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.2 Determinants of Arbitrary Order . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
5.3 Applications of Determinants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.4 Determinant Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.5 Permutations and the Symmetric Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.6 The Leibniz Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.Eigen Theory
6.1 Eigenvectors and Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
6.2 The Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.3 Applications of the Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.4 Similar Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
6.5 The Theory of Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
6.6 Diagonalization Methods and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
6.7 Matrix Limits and Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .231
6.8 The Cayley-Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232
7.Inner Product Spaces
7.1 Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .237
7.2 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
7.3 Orthogonal Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
7.4 Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .259
8.Operator Theory
8.1 The Adjoint of a Linear Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8.2 Self-Adjoint and Unitary Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
8.3 Normal Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .276
8.4 The Spectral Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
9.Canonical Forms
9.1 Generalized Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .284
9.2 Jordan Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
10. The Geometry of Vector Spaces
10.1 Convex Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
A.Appendix
Symbol Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2931
1
Euclidean Vectors
1.1 â€“ Groups, Rings and Fields
It is assumed that the reader is well familiar with sets and functions. Given a set S, a
binary operation from SÃ—StoSis a function âˆ—:SÃ—Sâ†’S, so that for each ( a, b)âˆˆSÃ—S
we have âˆ—(a, b)âˆˆS. As is customary we will usually write aâˆ—binstead of âˆ—(a, b). The operation
âˆ—iscommutative if, for every a, bâˆˆS,
âˆ—(a, b) =aâˆ—b=bâˆ—a=âˆ—(b, a),
andassociative if, for every a, b, câˆˆS,
âˆ—(a,âˆ—(b, c)) =aâˆ—(bâˆ—c) = (aâˆ—b)âˆ—c=âˆ—(âˆ—(a, b), c).
Common binary operations are addition of real numbers, + : RÃ—Râ†’R, and multiplication
of real numbers, Â·:RÃ—Râ†’R, both of which are commutative and associative. Recall that
subtraction and division of real numbers is neither commutative nor associative, and indeed
aÃ·bis not even defined in the case when b= 0!
Linear algebra is foremost the study of vector spaces, and the functions between vector
spaces called mappings. However, underlying every vector space is a structure known as a field,
and underlying every field there is what is known as a ring. Thus we begin with the definition
of a ring and proceed thence.
Definition 1.1. Aring is a triple (R,+,Â·)consisting of a set Rof objects, along with binary
operations addition + :RÃ—Râ†’Randmultiplication Â·:RÃ—Râ†’Rsubject to the following
axioms:
R1.a+b=b+afor any a, bâˆˆR.
R2.a+ (b+c) = (a+b) +cfor any a, b, câˆˆR.
R3.There exists some 0âˆˆRsuch that a+ 0 = afor any aâˆˆR.
R4.For each aâˆˆRthere exists some âˆ’aâˆˆRsuch that âˆ’a+a= 0.
R5.aÂ·(bÂ·c) = (aÂ·b)Â·cfor any a, b, câˆˆR.
R6.aÂ·(b+c) =aÂ·b+aÂ·cfor any a, b, câˆˆR.2
As in elementary algebra it is common practice to denote multiplication by omitting the
symbol Â·and employing juxtaposition:
ab=aÂ·b, a (bc) =aÂ·(bÂ·c), a(b+c) =aÂ·(b+c),
and so on.
We call the object âˆ’ain Axiom R4 the additive identity ofa. From Axioms R1 and R4
we see that
(âˆ’a) +a=a+ (âˆ’a) = 0 .
As a matter of convenience we define a subtraction operation as follows:
aâˆ’b=a+ (âˆ’b),
so that
aâˆ’a= 0
obtains just as in elementary algebra.
Definition 1.2. A ring (R,+,Â·)iscommutative if it satisfies the additional axiom
R7.aÂ·b=bÂ·afor all a, bâˆˆR.
Definition 1.3. A commutative ring (R,+,Â·)is aunitary commutative ring if it satisfies
the additional axiom
R8.There exists some 1âˆˆRsuch that aÂ·1 =afor any aâˆˆR.
A ring that satisfies Axiom R8 but not R7 is simply called a unitary ring , but we will have
no need for such an entity.
Definition 1.4. Let(R,+,Â·)be a unitary ring. The multiplicative inverse of an object
aâˆˆRis an object aâˆ’1âˆˆRfor which
aÂ·aâˆ’1=aâˆ’1Â·a= 1.
We now have all the necessary pieces in place in order to give the following simple definition
of a field.
Definition 1.5. Afield is a unitary commutative ring (R,+,Â·)for which 1Ì¸= 0, and every
aâˆˆRsuch that aÌ¸= 0has a multiplicative inverse.
To summarize, a field is a set of objects F, together with binary operations +andÂ·onF,
that are subject to the following field axioms :
F1.a+b=b+afor any a, bâˆˆF.
F2.a+ (b+c) = (a+b) +cfor any a, b, câˆˆF.
F3.There exists some 0âˆˆFsuch that a+ 0 = afor any aâˆˆF.
F4.For each aâˆˆFthere exists some âˆ’aâˆˆFsuch that âˆ’a+a= 0.
F5.aÂ·(bÂ·c) = (aÂ·b)Â·cfor any a, b, câˆˆF.
F6.aÂ·(b+c) =aÂ·b+aÂ·cfor any a, b, câˆˆF.
F7.aÂ·b=bÂ·afor all a, bâˆˆF.
F8.There exists some 0Ì¸= 1âˆˆFsuch that aÂ·1 =afor any aâˆˆF.3
F9.For each 0Ì¸=aâˆˆFthere exists some aâˆ’1âˆˆFsuch that aaâˆ’1= 1.
Commonly encountered fields are the set of real numbers Runder the usual operations of
addition and multiplication, and also the set of complex numbers C. Many results in linear
algebra (but not all) are applicable to both the fields RandC, in which case we will employ
the symbol Fto denote either. That is, anywhere Fappears one can safely substitute either R
orCas desired. Throughout these notes a scalar will be taken to be an object belonging to a
field. Throughout the remainder of this chapter all scalars will be real numbers.
Example 1.6. The set of integers Zunder the usual operations of addition and multiplication
satisfies all the field axioms save for one: F9, the axiom that requires every nonzero element in
a set of objects to have a multiplicative inverse that also is an element of the set of objects. The
multiplicative inverse for 2 âˆˆZis 2âˆ’1, and of course 2âˆ’1= 1/2 does not belong to Z. Therefore
Zis not a field under the usual operations of addition and multiplication.
In contrast, the set of rational numbers
Q=p
q:p, qâˆˆZandqÌ¸= 0
is a field under the usual operations of addition and multiplication, since the reciprocal of any
nonzero rational number is also a rational number. â– 
Example 1.7. Afinite field is a field that contains a finite number of elements. One example
is the set Z2={0,1}, with a binary operation + defined by
0 + 0 = 0 ,0 + 1 = 1 ,1 + 0 = 1 ,1 + 1 = 0 ,
and a binary operation Â·defined by
0Â·0 = 0 ,0Â·1 = 0 ,1Â·0 = 0 ,1Â·1 = 1 .
Note that the only departure from â€œusualâ€ addition and multiplication in evidence is 1 + 1 = 0.
It is straightforward, albeit tedious, to directly verify that each of the nine field axioms are
satisfied. â– 4
1.2 â€“ Real Euclidean Space
LetRdenote the set of real numbers. Given a positive integer n, we define real Euclidean
n-space , or simply n-space , to be the set
Rn={(x1, x2, . . . , x n) :xiâˆˆRfor 1â‰¤iâ‰¤n}. (1.1)
Any ordered list of nobjects is called an n-tuple , and the n-tuple ( x1, x2, . . . , x n) of real
numbers, when regarded as an element of Rn, is called a point inn-space. Each value xiin
(x1, x2, . . . , x n) is called a coordinate of the point, with x1being the â€œfirstâ€ coordinate, x2the
â€œsecondâ€ coordinate, and so on. If xis a point in Rn, we write xâˆˆRnand take this as meaning
x= (x1, x2, . . . , x n)
for some real numbers x1, x2, . . . , x n. Ifxi= 0 for all 1 â‰¤iâ‰¤n, then we obtain the point
(0,0, . . . , 0) called the origin .
Euclidean 2-space is more commonly known as the plane , which is the set
R2={(x1, x2) :x1, x2âˆˆR},
with each point ( x1, x2) in the plane (or â€œon the planeâ€) being a 2-tuple usually called an
ordered pair . Euclidean 3-space is customarily called simply space , which is the set
R3={(x1, x2, x3) :x1, x2, x3âˆˆR},
with each point ( x1, x2, x3) in space being a 3-tuple usually called an ordered triple .
It is natural to assign a geometrical interpretation to the notion of a point on a plane or
in space. Specifically, in the case of a point p= (p1, p2)âˆˆR2(i.e. a point pon a plane), it is
convenient to think of pas being â€œlocatedâ€ somewhere on the plane relative to the origin (0 ,0).
Exactly how the coordinates p1andp2of the point pare used to determine a location for pon
the plane depends on the coordinate system being used. In R2the rectangular and polar
coordinate systems are most commonly employed. In R3there are the rectangular, cylindrical,
and spherical coordinate systems, among others. Unless otherwise specified, we will always use
the rectangular coordinate system! For those who may not have encountered the rectangular
coordinate system in R3, Figure 2 should suffice to make its workings known. In the figure the
x1x2
p
p1p2
p1
x1x2
p
p2
Figure 1. At left: p= (p1, p2) in the rectangular coordinate system. At right:
p= (p1, p2) in the polar coordinate system.5
p1p2p3
x1x2x3
p= (p1, p2, p3)
p1p2p3
x1x2x3
p= (p1, p2, p3)
Figure 2. Stereoscopic image of R3with p= (p1, p2, p3) in the rectangular
coordinate system.
positive xi-axis is labeled for i= 1,2,3, and so the point p= (p1, p2, p3) shown has coordinates
pi>0 for each i.
It will be convenient to designate operations that allow for â€œaddingâ€ points, as well as
â€œmultiplyingâ€ them by real numbers and â€œsubtractingâ€ them. The definitions for these operations
make use of the operations of addition and multiplication of real numbers which are taken to be
understood.
Definition 1.8. Letp= (p1, p2, . . . , p n)andq= (q1, q2, . . . , q n)be points in Rn, and câˆˆR.
Then we define the sum p+qofpandqto be the point
p+q= (p1+q1, p2+q2, . . . , p n+qn),
and the scalar multiple cpofpbycto be the point
cp= (cp1, cp2, . . . , cp n).
Defining âˆ’p= (âˆ’p1,âˆ’p2, . . . ,âˆ’pn), thedifference pâˆ’qofpandqis given to be
pâˆ’q=p+ (âˆ’q).6
1.3 â€“ Located Vectors
Alocated vector inn-space is an ordered pair of points p, qâˆˆRn. We denote such an
ordered pair by#â€pqrather than ( p, q), both to help distinguish it from a point in R2(which is an
ordered pair of numbers ), and also to reinforce the natural geometric interpretation of a located
vector as an â€œarrowâ€ in n-space that starts at pand ends at q. We call ptheinitial point of#â€pq, and qtheterminal point , and say that#â€pqis â€œlocated at p.â€ If the initial point pis at the
origin (0 ,0, . . . , 0), then the located vector#â€pqis called a position vector (a vector located at
the origin).
The situation in R2will be illustrative. In Figure 3 it can be seen that, if p= (p1, p2) and
q= (q1, q2), then#â€pqmay be characterized as an arrow with initial point pthat decomposes into
a horizontal translation of q1âˆ’p1and a vertical translation of q2âˆ’p2.
Two located vectors#â€pqand# â€uvareequivalent , written#â€pqâˆ¼# â€uv, ifqâˆ’p=vâˆ’u. Again
considering the situation in R2, ifp= (p1, p2),q= (q1, q2),u= (u1, u2), and v= (v1, v2), then
#â€pqâˆ¼# â€uvâ‡”qâˆ’p=vâˆ’u
â‡”(q1, q2)âˆ’(p1, p2) = (v1, v2)âˆ’(u1, u2)
â‡”(q1âˆ’p1, q2âˆ’p2) = (v1âˆ’u1, v2âˆ’u2)
â‡”q1âˆ’p1=v1âˆ’u1and q2âˆ’p2=v2âˆ’u2.
Thus#â€pqâˆ¼# â€uvinR2if and only if the arrows corresponding to the two located vectors decompose
into the same horizontal and vertical translations.
Ifo= (0, . . . , 0) is the origin in Rn,p= (p1, . . . , p n), and q= (q1, . . . , q n), then
#â€pqâˆ¼#               â€o(qâˆ’p).
This is verified by direct calculation:
qâˆ’p= (q1âˆ’p1, q2âˆ’p2) = (q1âˆ’p1, q2âˆ’p2)âˆ’(0,0) = ( qâˆ’p)âˆ’o.
Thus, any arbitrary location vector#â€pqis equivalent to some position vector, and in the exercises
it will be established that the position vector equivalent to#â€pqmust be unique.
xy
#â€pq
pq
p1 q1p2q2
q1âˆ’p1q2âˆ’p2
Figure 3. A vector in the plane R2located at p.7
xy
#â€ovv
o
Figure 4. Equivalent located vectors, all belonging to v.
Definition 1.9. Let#â€ovbe a position vector in Rn. The equivalence class of#â€ov, denoted by
v, is the set of all located vectors that are equivalent to#â€ov. That is,
v={#â€pq:#â€ovâˆ¼#â€pq}.
The equivalence class vof a located vector#â€ovis also called the vector v.
The symbol vis usually handwritten as# â€v. Ifv= (v1, . . . , v n), then it is common to denote
vby either
[v1, . . . , v n] orï£®
ï£°v1...
vnï£¹
ï£».
The row format exhibited in the first symbol will be used throughout this chapter, but later
on the column format of the second symbol will be favored. Thus v= [v1, . . . , v n] is the set of
located vectors that are equivalent to the position vector having v= (v1, . . . , v n) as its terminal
point. A vector of the form [ v1, . . . , v n], where the ithcoordinate viis a real number for
each 1 â‰¤iâ‰¤n, is called a Euclidean vector (orcoordinate vector ) to distinguish it from
the more abstract notion of vector that will be introduced in Chapter 3. Put another way, a
Euclidean vector is an equivalence class of located vectors in a Euclidean space Rn, and it is
fully determined by nreal-valued coordinates v1, . . . , v n.
The Euclidean zero vector is the vector 0whose coordinates are all equal to 0; thus if
0âˆˆRn, then
0= [ 0,0, . . . , 0|{z}
nzeros].
A useful way to think of a vector vÌ¸=0in Euclidean n-space is as an arrow with a fixed
length and direction, but varying location. For instance we can take the located vector#â€ov,
naturally depicted as an arrow with initial point at the origin oand terminal point at the point
v, and move the arrow around in a way that preserves its length and direction. See Figure 4.
Remark. If a located vector#â€pqis equivalent to#â€ov, then strictly speaking we say that#â€pqbelongs
to the equivalence class of located vectors known as vector v. However, sometimes the symbol#â€pqitself may be used to represent the vector v, which is in keeping with the common practice8
xy
uv
u+v
uu+v
o xy
uv
vu
uu+v
v
o
Figure 5. The geometry of vector addition.
in mathematics of letting any member of an equivalence class be a representative of that class.
Other times we may be given a located vector#â€pqin a situation when location is irrelevant, and
so refer to#â€pqas simply a vector.
Example 1.10. InR3letp= (2,âˆ’3,4) and q= (âˆ’5,âˆ’2,8). Find v= (v1, v2, v3) so that#â€pqâˆ¼#â€ov, where o= (0,0,0).
Solution. By definition#â€pqâˆ¼#â€ovmeans qâˆ’p=vâˆ’o, or
(âˆ’5,âˆ’2,8)âˆ’(2,âˆ’3,4) = ( v1, v2, v3)âˆ’(0,0,0) = ( v1, v2, v3).
Thus we have
v= (v1, v2, v3) = (âˆ’5âˆ’2,âˆ’2âˆ’(âˆ’3),8âˆ’4) = (âˆ’7,1,4).
It follows from this calculation that the located vector#â€pqbelongs to the equivalence class
of located vectors known as the vector v= [âˆ’7,1,4]. The symbol#â€pqitself could be used to
represent the vector [ âˆ’7,1,4], and we may even say that#â€pqand [âˆ’7,1,4] are the â€œsame vectorâ€
if location in R3is unimportant. â– 
As with points we define operations that allow for adding and subtracting Euclidean vectors,
and also multiplying them by real numbers.
Definition 1.11. Letu= [u1, . . . , u n]andv= [v1, . . . , v n]be Euclidean vectors in Rn, and
câˆˆR. Then we define the sum u+vofuandvto be the vector
u+v= [u1+v1, . . . , u n+vn],
and the scalar multiple cvofvbycto be the vector
cv= [cv1, . . . , cv n].
Defining âˆ’v= (âˆ’1)v, thedifference uâˆ’vofuandvis given to be
uâˆ’v=u+ (âˆ’v).9
There is some geometrical significance to the sum of two vectors, and it suffices to consider
the situation in R2to appreciate it. Define vectors u= [u1, u2] and v= [v1, v2] in the plane.
One representative of uis the located vector# â€ou. As for v, from
(u+v)âˆ’u=u+vâˆ’u=v=vâˆ’o
we have#                â€u(u+v)âˆ¼#â€ov,
and so#                â€u(u+v)is a located vectorâ€”in fact the only located vectorâ€”having initial point uthat
can represent v. Finally, a representative for u+vis the located vector
#                â€o(u+v).
Now, if the located vectors# â€ou,#                â€u(u+v), and#                â€o(u+v)are all drawn as arrows in R2, they will
be seen to form a triangle such as the one at left in Figure 5. Indeed if#â€ov, also representing v,
and#                â€v(u+v)
â€”easily seen to be another representative of uâ€”are also drawn as arrows, then a parallelogram
such as the one at right in Figure 5 results. In the figure, it should be pointed out, the various
located vectors are labeled only by the vector ( uorv) that they represent.
After this section we will refer to located vectors only infrequently, and instead focus mostly
on vectors. Until Chapter 3 the vectors will be strictly of the Euclidean variety, viewed naturally
as arrows in Rnwhich have length and direction but no particular location. Also we will often
use the symbol Rnto denote the set of all Euclidean vectors of the form [ x1, . . . , x n], rather
than the set of all points ( x1, . . . , x n). That is,
Rn={[x1, . . . , x n] :xiâˆˆRfor 1â‰¤iâ‰¤n}.
There is no substantive difference between this definition for Rnand the one given by equation
(1.1); there is only a difference in interpretation.
Definition 1.12. Two vectors u,vareparallel if there exists some scalar cÌ¸= 0such that
u=cv.10
1.4 â€“ The Dot Product
We have established operations that add and subtract vectors, and also multiply them by
real numbers. Now we define a way of â€œmultiplyingâ€ vectors that is known as the dot product.1
Definition 1.13. Letu= [u1, . . . , u n]andv= [v1, . . . , v n]be two vectors in Rn. Then the dot
product ofuandvis the real number
uÂ·v=u1v1+u2v2+Â·Â·Â·+unvn=nX
i=1uivi.
Thus, if uandvare vectors in R2, then
uÂ·v= [u1, u2]Â·[v1, v2] =u1v1+u2v2.
Some properties of the dot product now follow.
Theorem 1.14. For any vectors u,v,wâˆˆRnand scalar c,
1.uÂ·v=vÂ·u
2.uÂ·(v+w) =uÂ·v+uÂ·w
3. (cu)Â·v=c(uÂ·v) =uÂ·(cv)
4.uÂ·u>0ifuÌ¸=0
Proof. Proof of (2):
uÂ·(v+w) = [u1, . . . , u n]Â· 
[v1, . . . , v n] + [w1, . . . , w n]
= [u1, . . . , u n]Â·[v1+w1, . . . , v n+wn]
=nX
i=1ui(vi+wi) =nX
i=1(uivi+uiwi)
=nX
i=1uivi+nX
i=1uiwi=uÂ·v+uÂ·w,
using the established summation propertyP(ai+bi) =Pai+Pbi.
Proofs for the other dot product properties are left to the exercises. â– 
Definition 1.15. Two vectors u,vareorthogonal , written uâŠ¥v, ifuÂ·v= 0.
Orthogonal vectors are also said to be perpendicular , and in the next section we shall see
that this means precisely what we expect: the vectors form a right angle.
Example 1.16. Find two mutually perpendicular vectors in R3that are each perpendicular to
v= [2,âˆ’1,3]
1The dot product is also called the â€œscalar productâ€ in some books.11
Solution. We need to find vectors u= [u1, u2, u3] and w= [w1, w2, w3] such that
uÂ·w=uÂ·v=wÂ·v= 0.
From this we obtain a system of equations:
ï£±
ï£²
ï£³2u1âˆ’ u2+ 3 u3= 0
2w1âˆ’w2+ 3w3= 0
u1w1+u2w2+u3w3= 0
There are six variables but only three equations, and so we can expect that there are an infinite
number of solutions. To satisfy the third equation we may choose, quite arbitrarily, to let
u1w1= 1,u2w2=âˆ’2, and u3w3= 1, so that
w1=1
u1, w 2=âˆ’2
u2,and w3=1
u3. (1.2)
Substituting these into the systemâ€™s second equation yields
2
u1+2
u2+3
u3= 0. (1.3)
Now, from the systemâ€™s first equation we have u2= 2u1+ 3u3, which we substitute into (1.3) to
obtain2
u1+2
2u1+ 3u3+3
u3= 0.
From this, with a little algebra, we obtain a quadratic equation:
2u2
3+ 5u1u3+ 2u2
1= 0.
We employ the quadratic formula to solve this equation for u3:
u3=âˆ’5u1Â±p
25u2
1âˆ’4(2)(2 u2
1)
2(2)=âˆ’5u1Â±3|u1|
4.
If we set u1= 1 (again an arbitrary choice weâ€™re free to make), then we find that
u3=âˆ’5Â±3
4=âˆ’2,âˆ’1
2.
If we choose u3=âˆ’2, then we have
u2= 2u1+ 3u3= 2(1) + 3( âˆ’2) =âˆ’4,
and so u= [1,âˆ’4,âˆ’2]. Also from (1.2) we have
w=1
u1,âˆ’2
u2,1
u3
=
1,1
2,âˆ’1
2
.
Therefore
[1,âˆ’4,âˆ’2] and
1,1
2,âˆ’1
2
are two mutually perpendicular vectors that are each perpendicular to [2 ,âˆ’1,3]. There are
infinitely many other possibilities. â– 12
1.5 â€“ The Norm of a Vector
Definition 1.17. Thenorm of a vector vâˆˆRnisâˆ¥vâˆ¥=âˆšvÂ·v.
Ifv= [v1, . . . , v n], then
âˆ¥vâˆ¥=p
[v1, . . . , v n]Â·[v1, . . . , v n] =rXn
i=1v2
i (1.4)
The norm of a vector is also known as the vectorâ€™s magnitude orlength . Consider a located
vector#â€ovin the plane, which is a convenient representative of the vector v= [v1, v2]. In Â§1.2 we
saw that#â€ovmay be depicted as an arrow that starts at the origin o= (0,0) and ends at the
point v= (v1, v2). How long is the arrow? The answer is given by the conventional (Euclidean)
distance d(o, v) between oandvthat is derived from the familiar Pythagorean Theorem:
d(o, v) =p
(v1âˆ’0)2+ (v2âˆ’0)2=p
v2
1+v2
2.
On the other hand from (1.4) we have
âˆ¥vâˆ¥=p
v2
1+v2
2,
and so âˆ¥vâˆ¥=d(o, v), the length of the arrow#â€ovrepresenting v. Note that if#â€pqâˆ¼#â€ov, where
p= (p1, p2) and q= (q1, q2), then
d(p, q) =p
(q1âˆ’p1)2+ (q2âˆ’p2)2=p
v2
1+v2
2=d(o, v) =âˆ¥vâˆ¥
since q1âˆ’p1=v1andq2âˆ’p2=v2, and so it does not matter which located vector we choose
to represent v: the length of the arrow will be the same! These truths stay true in R3using
the usual Euclidean conception of distance in three-dimensional space. In fact, in light of the
following definition they remain true in Rnfor all n.
Definition 1.18. Letx,yâˆˆRn. The distance d(x,y), between xandyis given by
d(x,y) =âˆ¥xâˆ’yâˆ¥.
Thus if x= [x1, . . . , x n] and y= [y1, . . . , y n], then
d(x,y) =rXn
i=1(xiâˆ’yi)2,
which reduces to the usual formula for the distance between points xandywhen nequals 2 or
3. That is, d(x,y) =d(x, y) inR2orR3.
Remark. From now on we will frequently use the bold-faced symbol xfor the vector [x1, . . . , x n]
to represent the point x= (x1, . . . , x n). The logic of doing this is thus: a point xis naturally
identified with its corresponding position vector# â€ox, and# â€oxis naturally identified with x. Such
â€œvectorizationâ€ of points allows for a uniform notation in the statement of momentous results
in vector calculus and the sciences. Moreover it places everything under consideration in the
setting of a â€œvector space,â€ which is the main object of study in linear algebra. So it must be13
vu
ou
vcx
ou
vc v
Figure 6.
remembered: depending on context, x= [x1, . . . , x n] may be interpreted as a vector, a located
vector, or a point!2
We are now in a position to justify Definition 1.15, by which we mean ground the definition
in more familiar geometric soil. Suppose u,vâˆˆRnare orthogonal vectors, which is to say
uÂ·v= 0 and (since the dot product is commutative) vÂ·u= 0. Recall that located vectors
representing u,vandu+vmay be chosen so that their corresponding arrows form a triangle,
as at left in Figure 5. A triangle is a planar figure so it does not matter if the located vectors
are in an n-space for some n >2: we can always orient the situation so that it lies on a plane.
Now,âˆ¥u+vâˆ¥is the length of the longest side of the triangle, and âˆ¥uâˆ¥andâˆ¥vâˆ¥are the lengths
of the shorter sides. From the calculation
âˆ¥u+vâˆ¥2=p
(u+v)Â·(u+v)2
= (u+v)Â·(u+v)
= (u+v)Â·u+ (u+v)Â·v=uÂ·u+vÂ·u+uÂ·v+vÂ·v
=âˆ¥uâˆ¥2+âˆ¥vâˆ¥2,
it can be seen that the lengths of the triangleâ€™s sides obey the Pythagorean Theorem, and so it
must be that the triangle is a right triangle. That is, the sides formed by the located vectors
representing uandvmust meet at a right angle and therefore be perpendicular! It is in this
sense that orthogonal vectors are also said to be â€œperpendicular.â€
Proposition 1.19. Ifu,vâˆˆRnare orthogonal vectors, then âˆ¥u+vâˆ¥2=âˆ¥uâˆ¥2+âˆ¥vâˆ¥2.
The proof has already been furnished above.
Definition 1.20. LetvÌ¸=0. The orthogonal projection of uonto v,projvu, is given by
projvu=uÂ·v
vÂ·v
v.
Once again it should help to ground the definition in geometry, because ultimately it is
geometry that motivates the definition. Let u,vâˆˆRnwithvÌ¸=0. We represent these vectors
by located vectors with common initial point oas at left in Figure 6. For any câˆˆRletvc=cv.
We wish to find the value for cso that the vector xrepresented by located vector#   â€vcuat right in
Figure 6 is orthogonal to v. This means cmust be such that xÂ·v= 0, and since vc+x=uwe
obtain
(uâˆ’vc)Â·v= 0
2It was Henri PoincarÂ´ e who said â€œMathematics is the art of giving the same name to different things.â€14
and thus
uÂ·vâˆ’vcÂ·v=uÂ·vâˆ’(cv)Â·v= 0.
Since ( cv)Â·v=c(vÂ·v) we finally arrive at
c=uÂ·v
vÂ·v. (1.5)
Now, consider the right side of Figure 6 again. It can be seen that the vector vc, as pictured,
would be the shadow that uwould cast upon vwere a light to be directed upon the scene
from directly overhead. It is in this sense that vcis a projection of uontovâ€”in particular
theorthogonal projection, since the â€œlight raysâ€ casting the â€œshadowâ€ are perpendicular to v.
Multiplying both sides of equation (1.5) by vgives
vc=uÂ·v
vÂ·v
v,
which is projvuas given in Definition 1.20.
Lemma 1.21. Ifu,vâˆˆRn,vÌ¸=0, and cis as in (1.5), then uâˆ’cvis orthogonal to v.
Proof. Taking the dot product,
(uâˆ’cv)Â·v=uÂ·vâˆ’c(vÂ·v) =uÂ·vâˆ’uÂ·v
vÂ·v
(vÂ·v) =uÂ·vâˆ’uÂ·v= 0,
we immediately conclude that uâˆ’cvâŠ¥v. â– 
Itâ€™s a worthwhile exercise to verify that if uâŠ¥v, then uâŠ¥avfor any scalar a. The lemma
will be used to prove the following.
Theorem 1.22 (Schwarz Inequality ).Ifu,vâˆˆRn, then |uÂ·v| â‰¤ âˆ¥uâˆ¥âˆ¥vâˆ¥.
Proof. Suppose u,vâˆˆRn. Ifu=0orv=0, then
|uÂ·v|=|0|= 0 = âˆ¥uâˆ¥âˆ¥vâˆ¥,
which affirms the theoremâ€™s conclusion. So, suppose u,vÌ¸=0, and let câˆˆRbe given by (1.5).
Now,
(uâˆ’cv)Â·(cv) =c[(uâˆ’cv)Â·v] =c(0) = 0 ,
where ( uâˆ’cv)Â·v= 0 by Lemma 1.21. Thus uâˆ’cvandcvare orthogonal, and by Proposition
1.19
âˆ¥uâˆ¥2=âˆ¥(uâˆ’cv) +cvâˆ¥2=âˆ¥uâˆ’cvâˆ¥2+âˆ¥cvâˆ¥2.
Sinceâˆ¥uâˆ’cvâˆ¥2â‰¥0, this implies that âˆ¥cvâˆ¥2â‰¤ âˆ¥uâˆ¥2. However,
âˆ¥cvâˆ¥2=c2âˆ¥vâˆ¥2=uÂ·v
vÂ·v2
(vÂ·v) =(uÂ·v)2
vÂ·v=(uÂ·v)2
âˆ¥vâˆ¥2,
and so from âˆ¥cvâˆ¥2â‰¤ âˆ¥uâˆ¥2we obtain
(uÂ·v)2
âˆ¥vâˆ¥2â‰¤ âˆ¥uâˆ¥2,
whence comes ( uÂ·v)2â‰¤ âˆ¥uâˆ¥2âˆ¥vâˆ¥2. Taking the square root of both sides completes the proof. â– 15
From the Schwarz inequality we have
âˆ’ âˆ¥uâˆ¥âˆ¥vâˆ¥ â‰¤uÂ·vâ‰¤ âˆ¥uâˆ¥âˆ¥vâˆ¥,
and thus
âˆ’1â‰¤uÂ·v
âˆ¥uâˆ¥âˆ¥vâˆ¥â‰¤1
for any u,vÌ¸= 0. This observation justifies the following definition.
Definition 1.23. Letu,vâˆˆRnbe nonzero vectors. The angle between uandvis the number
Î¸âˆˆ[0, Ï€]for which
cosÎ¸=uÂ·v
âˆ¥uâˆ¥âˆ¥vâˆ¥. (1.6)
Since the function cos: [0, Ï€]â†’[âˆ’1,1] is one-to-one and onto, and the fraction in (1.6) only
takes values in [ âˆ’1,1], there will always exist a unique value Î¸âˆˆ[0, Ï€] that satisfies (1.6). From
Definition 1.23 we have a new formula for the dot product:
uÂ·v=âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸. (1.7)
Some textbooks give this formula as the definition of the dot product, but it is less desirable
since the idea of a dot product is then founded on a geometric notion of angle that becomes
problematic to visualize in Rnforn >3. However it is worthwhile verifying that the definition
of angle between vectors, as given here, agrees with our geometric intuition. For the sake of
simplicity we can assume that uandvare nonzero vectors in R2, though the situation does not
alter in Rnforn >2 since two vectors can always be represented by coplanar located vectors.3
The approach will be to let Î¸be the geometric angle between uandv, and then show that (1.7)
must necessarily follow.
Let 0 < Î¸ < Ï€ . The vectors u,v, and uâˆ’vmay be represented by located vectors that form
the triangle in Figure 7 (for convenience we depict Î¸as an acute angle).
By the Law of Cosines we obtain
âˆ¥uâˆ’vâˆ¥2=âˆ¥uâˆ¥2+âˆ¥vâˆ¥2âˆ’2âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸,
and since weâ€™re assuming that u= [u1, u2] and v= [v1, v2], we obtain uâˆ’v= [u1âˆ’v1, u2âˆ’v2]
so that
(u1âˆ’v1)2+ (u2âˆ’v2)2= (u2
1+u2
2) + (v2
1+v2
2)âˆ’2âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸,
3This is because two located vectors can be defined by three points p,q, and r, such as#â€pqand#â€pr, and three
points define a plane.
uuâˆ’v
v Î¸
Figure 7.16
and hence
âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸=u1v1+u2v2=uÂ·v.
In the cases when Î¸= 0 or Î¸=Ï€we find that v=ku= [ku1, ku 2] for some nonzero scalar
k; that is, uandvare parallel vectors, and we have
âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸=âˆ¥uâˆ¥âˆ¥kuâˆ¥cosÎ¸=|k|(u2
1+u2
2) cosÎ¸. (1.8)
IfÎ¸= 0, then k >0 so that |k|=kandcosÎ¸= 1; and if Î¸=Ï€, then k <0 so that |k|=âˆ’k
and cos Î¸=âˆ’1. In either case, from (1.8) we obtain
âˆ¥uâˆ¥âˆ¥vâˆ¥cosÎ¸=k(u2
1+u2
2) = [u1, u2]Â·[ku1, ku 2] =uÂ·v
as desired.
Example 1.24. Letu= [2,âˆ’1,5] and v= [âˆ’1,1,1].
(a) Find âˆ¥uâˆ¥andâˆ¥vâˆ¥.
(b) Find projvu, the orthogonal projection of uontov.
(c) Find projuv, the orthogonal projection of vontou.
(d) Find the angle between uandvto the nearest tenth of a degree.
Solution.
(a) We have
âˆ¥uâˆ¥=p
22+ (âˆ’1)2+ 52=âˆš
30 and âˆ¥vâˆ¥=p
(âˆ’1)2+ 12+ 12=âˆš
3.
(b) Since
uÂ·v= (2)( âˆ’1) + (âˆ’1)(1) + (5)(1) = 2 and vÂ·v=âˆ¥vâˆ¥2= (âˆš
3)2= 3,
we have
projvu=uÂ·v
vÂ·v
v=2
3[âˆ’1,1,1] =
âˆ’2
3,2
3,2
3
.
(c) Since
vÂ·u=uÂ·v= 2 and uÂ·u=âˆ¥uâˆ¥2= (âˆš
30)2= 30,
we have
projuv=vÂ·u
uÂ·u
u=2
30[2,âˆ’1,5] =2
15,âˆ’1
15,1
3
.
(d) By definition,
cosÎ¸=uÂ·v
âˆ¥uâˆ¥âˆ¥vâˆ¥=2âˆš
30âˆš
3=2
3âˆš
10,
and thus
Î¸= cosâˆ’12
3âˆš
10
â‰ˆ77.8â—¦.
â– 
Example 1.25. Find the measure of the angle Î¸between the diagonal of a cube and the
diagonal of one of its faces, as shown in Figure 8.17
Î¸ Î¸
Figure 8.
Solution. It will be convenient to regard the cube as existing in R3, with edges of length 1,
and the vertex where the two diagonals meet situated at the origin (0 ,0,0). We can then set
up coordinate axes such that the cube diagonal has endpoints (0 ,0,0) and (1 ,1,1), and the
face diagonal has endpoints (0 ,0,0) and (0 ,1,1). Thus the diagonals can be characterized as
positions vectors u= [1,1,1] and v= [0,1,1]. Now,
cosÎ¸=uÂ·v
âˆ¥uâˆ¥âˆ¥vâˆ¥=[1,1,1]Â·[0,1,1]âˆš
12+ 12+ 12âˆš
02+ 12+ 12=2âˆš
6,
and so
Î¸= cosâˆ’12âˆš
6
â‰ˆ35.264â—¦
is the angleâ€™s measure. â– 
Problems
1.Find the measure of the angle Î¸between the diagonal of a cube and one of its edges, as
shown below.
Î¸18
1.6 â€“ Lines and Planes
InR2a line Lis typically defined to be the solution set to an equation of the form ax+by=c
for constants a, b, câˆˆR, where aandbare not both zero. That is, Lis the set of points
{(x, y) :ax+by=C},
andax+by=Cis called the Cartesian equation (oralgebraic equation ) for L. InRnfor
n >2 we can still speak geometrically of lines, of course, but it becomes impossible to define the
line using a single Cartesian equation. The most convenient remedy for this is to use vectors,
thereby motivating the following definition.
Definition 1.26. Letp,vâˆˆRnwithvÌ¸=0. The line through pand parallel to vâˆˆRnis the
set of vectors of the form
{p+tv:tâˆˆR}.
Aparametric equation (orparametrization ) of a line L={p+tv:tâˆˆR} âŠ†Rnis any
vector-valued function x:Râ†’Rngiven by
x(t) =Ëœp+tËœv
for some ËœpâˆˆLand vector Ëœvparallel to v. (Here tis called a parameter .) Thus we find that
x(t) =p+tv
is one parametrization for L, but there are infinitely many others in existence.
Given a parametrization x(t) =p+tvfor some line in Rn, the vector p= [p1, . . . , p n] may
more naturally be thought of as the position vector#â€opof the point p= (p1, . . . , p n), and so in
everyday speech pmay be referred to as a point even though mathematically it is handled as a
vector. The same applies to the vector
x(t) = [x1(t), . . . , x n(t)]
for each tâˆˆR: we may regard it, if desired, as the position vector of the point
x(t) = (x1(t), . . . , x n(t)),
and so refer to it as a point. In contrast, for each tâˆˆRthe vector tvmay be thought of as a
localized vector (i.e. an arrow) with initial point at pand terminal point located at another
point on the line.
Definition 1.27. Theline segment inRnwith endpoints p,qâˆˆRnis the set of vectors of
the form
{p+t(qâˆ’p) :tâˆˆ[0,1]}.
A natural parametrization for a line segment with endpoints pandqis the vector-valued
function x: [0,1]â†’Rngiven by
x(t) =p+t(qâˆ’p), (1.9)19
though it is frequently the case in applications that other parametrizations may be considered.
In(1.9) we have x(0) = pandx(1) = q, and so as tincreases from 0 to 1 we see that we â€œtravelâ€
along the line segment from ptoq. However, the alternative parametrization
x(t) =q+t(pâˆ’q)
reverses the direction of travel.
Example 1.28. Find a parametrization x(t) of the line containing the points p= (2,âˆ’6,9)
andq= (0,8,1), such that x(1) = pandx(âˆ’2) =q.
Solution. We must have x(t) =p+f(t)(qâˆ’p) for some function fsuch that f(1) = 0 and
f(âˆ’2) = 1. The simplest such function is a linear one, which is to say f(t) =mt+bfor constants
mandb. With the condition f(1) = 0 we obtain b=âˆ’m, so that f(t) =m(tâˆ’1). With the
condition f(âˆ’2) = 1 we obtain 1 = m(âˆ’2âˆ’1), or m=âˆ’1/3, and hence b= 1/3. Now we have
x(t) =p+ 
âˆ’1
3t+1
3
(qâˆ’p)
forp= [2,âˆ’6,9] and q= [0,8,1], giving
x(t) =4
3,âˆ’4
3,19
3
+t2
3,âˆ’14
3,8
3
.
Other answers are possible if we choose fto be a nonlinear function. â– 
InR3a line Pis sometimes defined to be the solution set to an equation of the form
ax+by+cz=dfor constants a, b, c, d âˆˆR, where a,b,care not all zero. That is, Pis the set
of points
{(x, y, z ) :ax+by+cz=d},
where ax+by+cz=dis the Cartesian equation forP. InRnforn >3 we may still wish
to conceive of planes, but it is no longer possible to define the plane using a single Cartesian
equation. The following definition uses vectors to define the notion of a plane for all Rnwith
nâ‰¥3.
Definition 1.29. Letu,vâˆˆRnbe nonzero, nonparallel vectors. The plane through point
pâˆˆRnand parallel to u,vis the set of vectors of the form
{p+su+tv:s, tâˆˆR}.
Aparametric equation (orparametrization ) of a plane P={p+su+tv:tâˆˆR} âŠ†Rn
is any vector-valued function x:R2â†’Rngiven by
x(s, t) =Ëœp+sËœu+tËœv
for some ËœpâˆˆPand vectors ËœuandËœvparallel to uandv, respectively. (Here and sandtare
called the parameters .) Thus
x(s, t) =p+su+tv (1.10)
is one parametrization for Pamong infinitely many.
Anormal vector for a plane Phaving parametrization (1.10) is a nonzero vector nsuch
thatnÂ·u= 0 and nÂ·v= 0. A line Lis said to be orthogonal toPifLis parallel to n. IfL20
is orthogonal to PandpâˆˆLâˆ©P(i.e.pis the point of intersection between LandP), then the
distance between any point qâˆˆLandPis the length of the line segment pq.
Example 1.30. Find both a parametric and Cartesian equation for the plane Pcontaining the
point (0 ,0,0) that is orthogonal to the line Lhaving parametric equation
x(t) = [3 ,âˆ’2,1] +t[2,1,âˆ’3].
Solution. By definition any normal vector nforPmust be parallel to L, which in turn means
thatnmust be parallel to a direction vector of L. Since [2 ,1,âˆ’3] is an obvious direction vector
ofL, we may let n= [2,1,âˆ’3]. Geometrically speaking, since Pcontains the point o= (0,0,0),
Pwill consist precisely of those points ( x, y, z ) for which the vector [ x, y, z ]âˆ’[0,0,0] = [ x, y, z ]
is orthogonal to n. Since
nÂ·[x, y, z ] = 0 â‡”[2,1,âˆ’3]Â·[x, y, z ] = 0 â‡”2x+yâˆ’3z= 0,
we conclude that 2 x+yâˆ’3z= 0 is a Cartesian equation for P.
To find a parametric equation, we use the Cartesian equation to find two other points on P
besides (0 ,0,0), such as p= (1,âˆ’2,0) and q= (0,3,1). Now let
u=pâˆ’0= [1,âˆ’2,0] and v=qâˆ’0= [0,3,1].
A parametric equation for Pisx(s, t) =0+su+tv, or
x(s, t) =s[1,âˆ’2,0] +t[0,3,1]
fors, tâˆˆR. â– 
Example 1.31. Find a normal vector for the plane 3 x+ 2yâˆ’2z= 3.
Solution. We first find three points on the plane that are not collinear. This can be done by
substituting values for xandyin the equation, say, and then solving for z. In this way we find
points (0 ,0,1/7), (1 ,1,1), and (1 ,2,2).
Example 1.32. Find the distance between the point q= (1,âˆ’2,4) and the plane 3 x+2yâˆ’2z= 3.
Solution. Letting x=y= 0 in the planeâ€™s equation gives z= 1/7, so p= (0,0,1/7) is a point
on the plane. Let
v=#â€pq=qâˆ’p=
5,2,âˆ’22
7
.
A normal vector for the plane is n= [4,âˆ’4,7]. We project vonton:
projn(v) =vÂ·n
nÂ·n
n=âˆ’10
81[4,âˆ’4,7].
The magnitude of this vector,
D=âˆ¥projn(v)âˆ¥=10
9,
is the sought-after distance. â– 21
Problems
1.LetL1be the line given by x(t) = [1 ,1,1] +t[2,1,âˆ’1], and let L2be the line with Cartesian
equations
x= 5, yâˆ’4 =zâˆ’1
2.
(a) Show that the lines L1andL2intersect, and find the point of intersection.
(b) Find a Cartesian equation of the plane containing L1andL2.
2.LetPbe the plane in R3which has normal vector n= [1,âˆ’4,2] and contains the point
a= (5,1,3).
(a) Find a Cartesian equation for P.
(b) Find a parametric equation for P.22
2
Matrices and Systems
2.1 â€“ Matrices
Letm, nâˆˆN, and let Fbe a field. An mÃ—nmatrix over Fis a rectangular array of
elements of Farranged in mrows and ncolumns:
ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1n
a21a22Â·Â·Â· a2n............
am1am2Â·Â·Â· amnï£¹
ï£ºï£ºï£». (2.1)
The values mandnare called the dimensions of the matrix. The scalar (i.e. element of
F) in the ith row and jth column of the matrix, aij, is known as the ij-entry . To be clear,
throughout these notes the entries aijof a matrix are always taken to be elements of some field
F, which could be the real number system R, the complex number system C, or some other field.
A 1Ã—1 matrix [ a] is usually identified with the scalar aâˆˆFthat constitutes its sole entry.
Fornâ‰¥2, both nÃ—1 and 1 Ã—nmatrices are called vector matrices (or simply vectors ). In
particular an nÃ—1 matrixï£®
ï£¯ï£¯ï£°x1
x2...
xnï£¹
ï£ºï£ºï£»(2.2)
is acolumn vector (orcolumn matrix ), and a 1 Ã—nmatrix
x1x2Â·Â·Â·xn
is arow vector (orrow matrix ). Henceforth the Euclidean vector [ x1, . . . , x n] introduced in
Chapter 1 will most of the time be represented by its corresponding column vector (2.2) so as
to take advantage of the convenient properties of matrix arithmetic.
The matrix (2.1) we typically denote more compactly by the symbol
[aij]m,n,23
which indicates that the ij-entry is the scalar aij, where iâˆˆ {1, . . . , m }is the row number and
jâˆˆ {1, . . . , n }is the column number. We call the sets {1, . . . , m }and{1, . . . , n }therange of
the indexes iandj, respectively. If m=nthen a square matrix results, and we define
[aij]n= [aij]n,n.
(Care should be taken with this notation: [ aij]m,ndenotes an mÃ—nmatrix, while [ aij]mndenotes
anmnÃ—mnsquare matrix!) If the range of the indexes iandjare known or irrelevant, we will
write (2.1) as simply [ aij]. Another word about square matrices: The diagonal entries of a
square matrix [ aij]nare the entries with matching row and column number: a11, . . . , a nn.
Very often we have no need to make any reference to the entries of a matrix, in which case
we will usually designate the matrix by a bold-faced upper-case letter such as A,B,C, and so
on. The exception is vector matrices, which are normally labeled with bold-faced lower-case
letters such as a,b,x,yand so on. If we need to make reference to the ij-entry of a matrix A,
then the symbol [ A]ijstands ready to denote it. Thus if A= [aij]m,n, then
[A]ij=aij.
The set of all mÃ—nmatrices with entries in the field Fwill be denoted by FmÃ—n. That is,
FmÃ—n=
[aij]m,n:aijâˆˆFfor all 1 â‰¤iâ‰¤m, 1â‰¤jâ‰¤n	
.
From this point onward we also define
Fn=FnÃ—1
in these notes; that is, Fnis the set of matrices consisting of nentries from Farranged in a
single column. The exception has already been encountered: throughout the first chapter (and
only the first chapter) we always took Rnto signify R1Ã—n. In the wider world of mathematics
beyond these notes the symbol Fndenotes either row vectors (elements of F1Ã—n) or column
vectors (elements of FnÃ—1), depending on an authorâ€™s whim.
Ifaij= 0 for all 1 â‰¤iâ‰¤mand 1 â‰¤jâ‰¤n, then we obtain the mÃ—nzero matrix
Om,n= [0] m,n=ï£®
ï£¯ï£¯ï£°0 0Â·Â·Â· 0
0 0Â·Â·Â· 0
............
0 0Â·Â·Â· 0ï£¹
ï£ºï£ºï£»
having mrows and ncolumns of zeros. In particular we define
On=On,n.
In any case the symbol Owill always denote a zero matrix of some kind, whereas 0will continue
to denote more specifically a zero vector (i.e. a row or column matrix consisting of zeros).
Definition 2.1. IfA,BâˆˆFmÃ—nandcâˆˆF, then we define sum A+Bandscalar multiple
cAto be the matrices in FmÃ—nwithij-entry
[A+B]ij= [A]ij+ [B]ijand [cA]ij=c[A]ij
for all 1â‰¤iâ‰¤mand1â‰¤jâ‰¤n.24
Put another way, letting A= [aij] and B= [bij], we have
A+B= [aij+bij] =ï£®
ï£¯ï£¯ï£°a11+b11a12+b12Â·Â·Â· a1n+b1n
a21+b21a22+b22Â·Â·Â· a2n+b2n............
am1+bm1am2+bm2Â·Â·Â· amn+bmnï£¹
ï£ºï£ºï£»
and
cA= [caij] =ï£®
ï£¯ï£¯ï£°ca11ca12Â·Â·Â· ca1n
ca21ca22Â·Â·Â· ca2n............
cam1cam2Â·Â·Â· camnï£¹
ï£ºï£ºï£».
Thus matrix addition and matrix scalar multiplications is analogous to the addition and scalar
multiplication of Euclidean vectors. Clearly matrix addition is commutative, which is to say
A+B=B+A
for any A,BâˆˆFmÃ—n. We define the additive inverse ofAto be the matrix âˆ’Agiven by
âˆ’A= (âˆ’1)A= [âˆ’aij].
That
A+ (âˆ’A) =âˆ’A+A=O
is straightforward to check.
Definition 2.2. LetAâˆˆFmÃ—n. The transpose ofAis the matrix AâŠ¤âˆˆFnÃ—msuch that
[AâŠ¤]ij= [A]ji
for all 1â‰¤iâ‰¤nand1â‰¤jâ‰¤m.
Put another way, if A= [aij]m,n, then the transpose of Ais the matrix AâŠ¤= [Î±ji]n,mwith
Î±ji=aijfor each 1 â‰¤jâ‰¤n, 1â‰¤iâ‰¤m. Thus the number aijin the ith row and jth column of
Ais in the jth row and ith column of AâŠ¤, so that
AâŠ¤=ï£®
ï£¯ï£¯ï£°a11a21Â·Â·Â· am1
a12a22Â·Â·Â· am2............
a1na2nÂ·Â·Â· amnï£¹
ï£ºï£ºï£». (2.3)
Comparing (2.3) with (2.1), it can be seen that the rows of Asimply become the columns of
AâŠ¤. For example if
A=
âˆ’3 7 4
6âˆ’5 10
,
then
AâŠ¤=ï£®
ï£°âˆ’3 6
7âˆ’5
4 10ï£¹
ï£».25
It is easy to see that ( AâŠ¤)âŠ¤=A. We say Aissymmetric ifAâŠ¤=A, andskew-symmetric
ifAâŠ¤=âˆ’A. The set of all symmetric nÃ—nmatrices with entries in the field Fwill be denoted
by Symn(F); that is,
Symn(F) =
AâˆˆFnÃ—n:AâŠ¤=A	
.
The symbol Skw n(F) will denote the set of all skew-symmetric nÃ—nmatrices with entries in F:
Skw n(F) =
AâˆˆFnÃ—n:AâŠ¤=âˆ’A	
.
A standard approach to proving that two matrices AandBare equal is to first confirm
that they have the same dimensions, and then show that the ij-entry of the matrices are equal
for any iandj. Thus we verify that AandBaremÃ—nmatrices (a step that may be omitted
if it is clear), then verify that [ A]ij= [B]ijfor arbitrary 1 â‰¤iâ‰¤mand 1â‰¤jâ‰¤n. The proof of
the following proposition illustrates the method.
Proposition 2.3. LetA,BâˆˆFmÃ—n, and let câˆˆF. Then
1. (cA)âŠ¤=cAâŠ¤
2. (A+B)âŠ¤=AâŠ¤+BâŠ¤
3. (AâŠ¤)âŠ¤=A.
Proof.
Proof of Part (1). Fix 1â‰¤iâ‰¤mand 1 â‰¤jâ‰¤n. Then, applying Definitions 2.1 and 2.2,
[(cA)âŠ¤]ij= [cA]ji=c[A]ji=c[AâŠ¤]ij= [cAâŠ¤]ij.
So we see that the ij-entry of ( cA)âŠ¤equals the ij-entry of cAâŠ¤, and since iandjwere arbitrary,
it follows that ( cA)âŠ¤=cAâŠ¤.
Proof of Part (2). We have
[(A+B)âŠ¤]ij= [A+B]ji= [A]ji+ [B]ji= [AâŠ¤]ij+ [BâŠ¤]ij= [AâŠ¤+BâŠ¤]ij,
so the ij-entries of ( A+B)âŠ¤andAâŠ¤+BâŠ¤are equal. â– 
The proof of part (3) of Proposition 2.3, which can be done using the same â€œentrywiseâ€
technique, is left as a problem.
Thetrace of a square matrix A= [aij]n,n, written tr(A), is the sum of the diagonal entries
ofA:
tr(A) =nX
i=1aii. (2.4)
Since AâŠ¤= [Î±ij]n,nsuch that Î±ij=aji, we readily obtain
tr(AâŠ¤) =nX
i=1Î±ii=nX
i=1aii= tr(A).
Other properties of the trace and transpose operations will be established in future sections.
Ablock matrix is a matrix whose entries are themselves matrices. The matrices that
constitute a block matrix are called submatrices . In practice a block matrix is typically26
constructed from an ordinary matrix AâˆˆFmÃ—nby partitioning the entries into two or more
smaller arrays with the placement of vertical or horizontal rules, such as
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a11Â·Â·Â· a1s a1,s+1Â·Â·Â· a1n..................
ar1Â·Â·Â· ars ar,s+1Â·Â·Â· arn
ar+1,1Â·Â·Â· ar+1,sar+1,s+1Â·Â·Â· ar+1,n..................
am1Â·Â·Â· ams am,s+1Â·Â·Â· amnï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£», (2.5)
which partitions the matrix A= [aij]m,ninto four submatrices
ï£®
ï£°a11Â·Â·Â· a1s.........
ar1Â·Â·Â· arsï£¹
ï£»,ï£®
ï£°a1,s+1Â·Â·Â· a1n.........
ar,s+1Â·Â·Â· arnï£¹
ï£»,ï£®
ï£°ar+1,1Â·Â·Â· ar+1,s.........
am1Â·Â·Â· amsï£¹
ï£»,ï£®
ï£°ar+1,s+1Â·Â·Â· ar+1,n.........
am,s+1Â·Â·Â· amnï£¹
ï£»,
where of course 1 â‰¤r < m and 1 â‰¤s < n . If we designate the above submatrices as A1,A2,
A3, and A4, respectively, then we may write (2.5) as the block matrix
A1A2
A3A4
or
A1A2
A3A4
,
with the latter representation being preferred in these notes except in certain situations. A
block matrix is also known as a partitioned matrix .
Problems
1. Prove that ( AâŠ¤)âŠ¤=Afor any AâˆˆFmÃ—n.
2. Prove that ( A+B+C)âŠ¤=AâŠ¤+BâŠ¤+CâŠ¤for any A,B,CâˆˆFmÃ—n.
3. Prove that ( aA+bB)âŠ¤=aAâŠ¤+bBâŠ¤for any A,BâˆˆFmÃ—nanda, bâˆˆF.27
2.2 â€“ Matrix Multiplication
The definition of the product of two matrices is relatively more involved than that for
addition or scalar multiplication.
Definition 2.4. LetAâˆˆFmÃ—nandBâˆˆFnÃ—p. Then the product ofAandBis the matrix
ABâˆˆFmÃ—pwithij-entry given by
[AB]ij=nX
k=1[A]ik[B]kj
for1â‰¤iâ‰¤mand1â‰¤jâ‰¤p.
Letting A= [aij]m,nandB= [bij]n,p, it is immediate that AB= [cij]m,pwith ij-entry
cij=nX
k=1aikbkj.
That is,
AB= [aij]m,n[bij]n,p=hXn
k=1aikbkji
m,p, (2.6)
where itâ€™s understood that 1 â‰¤iâ‰¤mis the row number and 1 â‰¤jâ‰¤pis the column number
of the entryPn
k=1aikbkj.
Example 2.5. If
A=
âˆ’3 0 6
2 11 âˆ’5
and B=ï£®
ï£°4 9 âˆ’6
0âˆ’1 2
âˆ’4 0 âˆ’3ï£¹
ï£»,
so that Ais a 2Ã—3 matrix and Bis a 3Ã—3 matrix, then ABis a 2Ã—3 matrix given by
AB=
âˆ’3 0 6
2 11 âˆ’5ï£®
ï£°4 9 âˆ’6
0âˆ’1 2
âˆ’4 0 âˆ’3ï£¹
ï£»
=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’3 0 6ï£®
ï£°4
0
âˆ’4ï£¹
ï£»âˆ’3 0 6ï£®
ï£°9
âˆ’1
0ï£¹
ï£»âˆ’3 0 6ï£®
ï£°âˆ’6
2
âˆ’3ï£¹
ï£»
2 11 âˆ’5ï£®
ï£°4
0
âˆ’4ï£¹
ï£»2 11 âˆ’5ï£®
ï£°9
âˆ’1
0ï£¹
ï£»2 11 âˆ’5ï£®
ï£°âˆ’6
2
âˆ’3ï£¹
ï£»ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=
âˆ’36âˆ’27 0
28 7 25
.
The product BAis undefined. â– 28
Vectors may be used to better see how the product ABis formed. Let
ai=ai1Â·Â·Â·ain
denote the row vectors ofAfor 1â‰¤iâ‰¤m,
ï£®
ï£¯ï£¯ï£°ï£¹
ï£ºï£ºï£»a1â†’a11a12Â·Â·Â· a1n
a2â†’a21a22Â·Â·Â· a2n...............
amâ†’am1am2Â·Â·Â·amn=A(2.7)
and let
bj=ï£®
ï£°b1j...
bnjï£¹
ï£»
denote the column vectors ofBfor 1â‰¤jâ‰¤p,
B=b1b2Â·Â·Â·bp
â†“ â†“ â†“ï£®
ï£¯ï£¯ï£°ï£¹
ï£ºï£ºï£»b11b12Â·Â·Â·b1p
b21b22Â·Â·Â·b2p............
bn1bn2Â·Â·Â·bnp. (2.8)
Then by definition
AB= [aibj]m,p=ï£®
ï£¯ï£¯ï£°a1b1a1b2Â·Â·Â·a1bp
a2b1a2b2Â·Â·Â·a2bp............
amb1amb2Â·Â·Â·ambpï£¹
ï£ºï£ºï£»,
which makes clear that the ij-entry is
[AB]ij=aibj=
ai1Â·Â·Â·ainï£®
ï£°b1j...
bnjï£¹
ï£»=ai1b1j+ai2b2j+Â·Â·Â·+ainbnj=nX
k=1aikbkj,
in agreement with Definition 2.4. Note that ABis not defined if the number of columns in Ais
not equal to the number of rows in B!
It is commonâ€”and convenientâ€”to denote matrices (2.7) and (2.8) by the symbols
ï£®
ï£¯ï£¯ï£°a1
a2...
amï£¹
ï£ºï£ºï£»andb1b2Â·Â·Â·bp
,
respectively, and so we have
AB=ï£®
ï£¯ï£¯ï£°a1
a2...
amï£¹
ï£ºï£ºï£»b1b2Â·Â·Â·bp
=ï£®
ï£¯ï£¯ï£°a1b1a1b2Â·Â·Â·a1bp
a2b1a2b2Â·Â·Â·a2bp............
amb1amb2Â·Â·Â·ambpï£¹
ï£ºï£ºï£». (2.9)29
For any j= 1, . . . , p we have bjâˆˆFn, which is to say bjhasnrows and so Abjcan be computed
following the pattern of (2.9):
Abj=ï£®
ï£¯ï£¯ï£°a1
a2...
amï£¹
ï£ºï£ºï£»bj=ï£®
ï£¯ï£¯ï£°a1bj
a2bj...
ambjï£¹
ï£ºï£ºï£».
(This can be verified easily by working directly with Definition 2.4.) Comparing this result with
the right-hand side of (2.9), we see that Abjis the jth column vector of AB; that is, we have
the following.
Proposition 2.6. IfAâˆˆFmÃ—nandB= [b1Â·Â·Â·bp]âˆˆFnÃ—p, then
AB=A
b1Â·Â·Â·bp
=
Ab 1Â·Â·Â·Abp
.
We see how a judicious use of notation can reap significant labor-saving rewards, leading
from the unfamiliar characterization of ABgiven in Definition 2.4 to the perfectly natural
formula in Proposition 2.6.
Theorem 2.7. LetAâˆˆFmÃ—n,B,CâˆˆFnÃ—p,DâˆˆFpÃ—q, and câˆˆF. Then
1.A(cB) =c(AB).
2.A(B+C) =AB+AC(thedistributive property ).
3. (AB)D=A(BD)(theassociative property ).
Proof.
Proof of Part (1). Clearly A(cB) and c(AB) are both mÃ—pmatrices. Now, for any 1 â‰¤iâ‰¤m
and 1 â‰¤jâ‰¤p,
[A(cB)]ij=nX
k=1[A]ik[cB]kj Definition 2.4
=nX
k=1[A]ik 
c[B]kj
Definition 2.1
=cnX
k=1[A]ik[B]kj Definition 1.5(F5,6,7)
=c[AB]ij Definition 2.4 ,
= [c(AB)]ij Definition 2.1 ,
and so we see the ij-entries of A(cB) and c(AB) are equal.
Proof of Part (2). Clearly A(B+C) and AB+ACare both mÃ—pmatrices. For 1 â‰¤iâ‰¤m
and 1 â‰¤jâ‰¤p,
[A(B+C)]ij=nX
k=1[A]ik[B+C]kj Definition 2.430
=nX
k=1[A]ik 
[B]kj+ [C]kj
Definition 2.1
=nX
k=1[A]ik[B]kj+nX
k=1[A]ik[C]kj Definition 1.5(F6)
=AB+AC. Definition 2.4 ,
which shows equality of the ijentries.
Proof of Part (3). Both matrices will be mÃ—q. Using basic summation properties and Definition
2.4,
[(AB)D]ij=pX
k=1[AB]ik[D]kj=pX
k=1" nX
â„“=1[A]iâ„“[B]â„“k!
[D]kj#
=nX
â„“=1pX
k=1[A]iâ„“[B]â„“k[D]kj
=nX
â„“=1 
[A]iâ„“pX
k=1[B]â„“k[D]kj!
=nX
â„“=1[A]iâ„“[BD]â„“j= [A(BD)]ij,
and the proof is done. â– 
In light of the associative property of matrix multiplication it is not considered ambiguous
to write ABD , since whether we interpret it as meaning ( AB)DorA(BD) makes no difference.
The order of operations conventions dictate that ABD be computed in the order indicated by
(AB)D, however.
Proposition 2.8. IfAâˆˆFmÃ—n,BâˆˆFnÃ—p,CâˆˆFpÃ—q, and DâˆˆFqÃ—r, then
(AB)(CD) =A(BC)D.
Proof. LetAB=P. We have
(AB)(CD) =P(CD) = (PC)D= [(AB)C]D= [A(BC)]D, (2.10)
where the second and fourth equalities follow from Theorem 2.7(3). Next we obtain
[A(BC)]D=A(BC)D, (2.11)
since the order of operations in evaluating either expression is precisely the same: (1) execute B
times Cto obtain BC; (2) execute Atimes BCto obtain A(BC); (3) execute A(BC) times D
to obtain A(BC)D.
Combining (2.10) and (2.11) yields ( AB)(CD) =A(BC)D. â– 
There is no useful way to divide matrices, but we can easily define what it means to
exponentiate a matrix by a positive integer.31
Definition 2.9. IfAâˆˆFnÃ—nandmâˆˆN, then
Am=AAÂ·Â·Â·A|{z}
mfactors=mY
k=1A.
In particular A1=A.
The definition makes use of so-called product notation,
mY
k=1xk=x1x2x3Â·Â·Â·xm,
which does for products what summation notation does for sums.
TheKronecker delta is a function Î´ij:ZÃ—Zâ†’ {0,1}defined as follows for integers iand
j:
Î´ij=(
1,ifi=j
0,ifiÌ¸=j
We use the Kronecker delta to define the nÃ—nidentity matrix ,
In= [Î´ij]n=ï£®
ï£¯ï£¯ï£°1 0Â·Â·Â· 0
0 1Â·Â·Â· 0
............
0 0Â·Â·Â· 1ï£¹
ï£ºï£ºï£»,
thenÃ—nmatrix with diagonal entries 1 and all other entries 0. In particular we have
I2=
1 0
0 1
and I3=ï£®
ï£°1 0 0
0 1 0
0 0 1ï£¹
ï£»
Definition 2.10. For any AâˆˆFnÃ—nwe define A0=In.
If the dimensions of an identity matrix are known or irrelevant, then the abbreviated symbol
Imay be used. The reason Inis called the identity matrix is because, for any nÃ—nmatrix A,
it happens that
InA=AIn=A.
Thus Inacts as an identity with respect to matrix multiplication, just as 1 is the identity with
respect to multiplication of real numbers. In fact it can be shown that Inistheidentity for
matrix multiplication, as there can be no others.
Example 2.11. Show that I2is the only matrix for which I2A=AI2=Aholds for all 2 Ã—2
matrices A.
Solution. Given any 2 Ã—2 matrix
A=
a11a12
a21a22
,32
we have
AI2=
a11a12
a21a22
1 0
0 1
=
a11(1) + a12(0)a11(0) + a12(1)
a21(1) + a22(0)a21(0) + a22(1)
=
a11a12
a21a22
=A
and
I2A=
1 0
0 1
a11a12
a21a22
=
(1)a11+ (0) a12(0)a11+ (1) a12
(1)a21+ (0) a22(0)a21+ (1) a22
=
a11a12
a21a22
=A,
so certainly I2A=AI2=Aholds for all A.
Now, let Bbe a 2 Ã—2 matrix such that
BA=AB=A (2.12)
for all 2 Ã—2 matrices A. If we set A=I2in(2.12) we obtain BI2=I2in particular, whence
B=I2. Therefore I2is the only matrix for which I2A=AI2=Aholds for all A. â– 
To show more generally that Inis the only matrix for which
InA=AIn=A
for all AâˆˆFnÃ—ninvolves a nearly identical argument.
Proposition 2.12. LetAâˆˆFnÃ—n.
1.IfAx=xfor every nÃ—1column vector x, then A=In.
2.IfAx=0for every nÃ—1column vector x, then A=On.
Proof.
Proof of Part (1). Suppose that Ax=xfor all nÃ—1 column vectors x. For each 1 â‰¤jâ‰¤nlet
ej= [Î´ij]n,1=ï£®
ï£°Î´1j...
Î´njï£¹
ï£»,
where once again we make use of the Kronecker delta. Thus ejis the nÃ—1 column vector with
1 in the jth row and 0 in all other rows.
Now, for each 1 â‰¤jâ‰¤n,Aejis an nÃ—1 column vector with i1-entry equalling
nX
k=1aikÎ´kj=aijÎ´jj=aij.
for each 1 â‰¤iâ‰¤n. On the other hand Aej=ejby hypothesis, and so
aij=Î´ij=(
0,ifiÌ¸=j
1,ifi=j
for all 1 â‰¤i, jâ‰¤n. But this is precisely the definition for In, and therefore A=In. â– 33
The proof of part (2) of the proposition is similar and left as a problem. Observe that, in
the notation established in the proof of part (1), we have
In=h
[Î´i1]n,1Â·Â·Â·[Î´in]n,1i
=
e1Â·Â·Â·en
. (2.13)
Proposition 2.13. LetAâˆˆFmÃ—nandBâˆˆFnÃ—p. Then
(AB)âŠ¤=BâŠ¤AâŠ¤.
Proof. Note that BâŠ¤ispÃ—nandAâŠ¤isnÃ—m, so the product BâŠ¤AâŠ¤is defined as a pÃ—m
matrix. Fix 1 â‰¤iâ‰¤pand 1 â‰¤jâ‰¤m. We have, using Definition 2.4 and Definition 2.2 twice
each,
[BâŠ¤AâŠ¤]ij=nX
k=1[BâŠ¤]ik[AâŠ¤]kj=nX
k=1[B]ki[A]jk=nX
k=1[A]jk[B]ki= [AB]ji=
(AB)âŠ¤
ij.
Thus the ij-entry of BâŠ¤AâŠ¤is equal to the ij-entry of ( AB)âŠ¤, soBâŠ¤AâŠ¤= (AB)âŠ¤as was to be
shown. â– 
Problems
1. Given that
x=ï£®
ï£°3
âˆ’1
2ï£¹
ï£»,A=ï£®
ï£°1 2âˆ’3
3 0âˆ’1
âˆ’2 1 4ï£¹
ï£»,C=ï£®
ï£°âˆ’4 2
1âˆ’1
0 3ï£¹
ï£»
compute the following.
(a)xâŠ¤x
(b)xxâŠ¤
(c)AC34
2.3 â€“ Row and Column Operations
We start by establishing some necessary notation. The symbol En,lmwill denote the nÃ—n
matrix with lm-entry 1 and all other entries 0; that is,
En,lm= [Î´ilÎ´mj]n
for any fixed 1 â‰¤l, mâ‰¤n, making use of the Kronecker delta introduced in the last section.
Put yet another way, En,lmis the nÃ—nmatrix with ij-entry Î´ilÎ´mj:
[En,lm]ij=Î´ilÎ´mj. (2.14)
Usually the nin the symbol En,lmmay be suppressed without leading to ambiguity, so that
the more compact symbol Elmmay be used. This will usually be done except in the statement
of theorems.
Proposition 2.14. LetnâˆˆNand1â‰¤l, m, p, q â‰¤n.
1.En,lmEn,mp=En,lp.
2.IfmÌ¸=p, then En,lmEn,pq=On.
Proof.
Proof of Part (1). Using Definition 2.4 and equation (2.14), the ij-entry of ElmEmpis
[ElmEmp]ij=nX
k=1[Elm]ik[Emp]kj=nX
k=1(Î´ilÎ´mk)(Î´kmÎ´pj) = (Î´ilÎ´mm)(Î´mmÎ´pj) =Î´ilÎ´pj,
where the third equality is justified since Î´mk= 0 for all kÌ¸=m, and then we need only recall
thatÎ´mm= 1. So ElmEmpis the nÃ—nmatrix with ij-entry Î´ilÎ´pj, and therefore ElmEmp=Elp.
Proof of Part (2). Suppose mÌ¸=p. Again using Definition 2.4 and equation (2.14) , the ij-entry
ofElmEmpis
[ElmEpq]ij=nX
k=1[Elm]ik[Epq]kj=nX
k=1(Î´ilÎ´mk)(Î´kpÎ´qj) = 0 ,
where the third equality is justified since, for any 1 â‰¤kâ‰¤n, either kÌ¸=morkÌ¸=p, and so
either Î´mk= 0 or Î´kp= 0. Therefore ElmEpq=On. â– 
LetnâˆˆN. For any scalar cÌ¸= 0 define
Mi(c) =In+ (câˆ’1)Eii,
which is the nÃ—nmatrix obtained by multiplying the ith row of Inbyc. Also define
Mi,j=Inâˆ’Eiiâˆ’Ejj+Eij+Eji
fori, jâˆˆ {1, . . . , n }with iÌ¸=j, which is the matrix obtained by interchanging the ith and jth
rows of In(notice that Mi,j=Mj,i). Finally, for i, jâˆˆ {1, . . . , n }with iÌ¸=j, and scalar cÌ¸= 0,
define
Mi,j(c) =In+cEji,35
which is the matrix obtained by adding ctimes the ith row of Into the jth row of In. Any
matrix of the form Mi,j(c),Mi,j, orMi(c) is called an elementary matrix .
Definition 2.15. Given AâˆˆFmÃ—n, anelementary row operation onAis any one of the
multiplications
Mi,j(c)A,Mi,jA,Mi(c)A.
More specifically we call left-multiplication by Mi,j(c)anR1operation, left-multiplication by
Mi,janR2operation, and left-multiplication by Mi(c)AanR3operation. A matrix Aâ€²is called
row-equivalent toAif there exist elementary matrices M1, . . . ,Mksuch that
Aâ€²=MkÂ·Â·Â·M1A.
Anelementary column operation onAis any one of the multiplications
AMâŠ¤
i,j(c),AMâŠ¤
i,j,orAMâŠ¤
i(c).
More specifically we call right-multiplication by MâŠ¤
i,j(c)aC1operation, right-multiplication by
MâŠ¤
i,jaC2operation, and right-multiplication by MâŠ¤
i(c)aC3operation. A matrix Aâ€²is called
column-equivalent toAif there exist elementary matrices M1, . . . ,Mksuch that
Aâ€²=AMâŠ¤
1Â·Â·Â·MâŠ¤
k.
Itâ€™s understood that the elementary matrices in the first part of Definition 2.15 must all be
mÃ—mmatrices, and the elementary matrices in the second part must be nÃ—n. Also, to be
clear, we define MâŠ¤
i,j(c) = [Mi,j(c)]âŠ¤andMâŠ¤
i(c) = [Mi(c)]âŠ¤. Finally, we define any matrix Ato
be both row-equivalent and column-equivalent to itself.
When we need to denote a collection of, say, pelementary matrices in a general way, we will
usually use symbols M1, . . . ,Mp. So for each k= 1, . . . , p the symbol Mkcould represent any
one of the three basic types of elementary matrix given in Definition 2.15.
Proposition 2.16. Suppose AâˆˆFmÃ—nhas row vectors a1, . . . ,amâˆˆFn. Let cÌ¸= 0, and let
1â‰¤p, qâ‰¤mwithpÌ¸=q.
1.Mp,q(c)Ais the matrix obtained from Aby replacing the row vector aqbyaq+cap:
Mp,q(c)ï£®
ï£¯ï£°...
aq...ï£¹
ï£ºï£»=ï£®
ï£¯ï£°...
aq+cap...ï£¹
ï£ºï£».
2.Mp,qAis the matrix obtained from Aby interchanging apandaq:
Mp,qï£®
ï£¯ï£¯ï£¯ï£¯ï£°...amin{p,q}...amax{p,q}...ï£¹
ï£ºï£ºï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°...amax{p,q}...amin{p,q}...ï£¹
ï£ºï£ºï£ºï£ºï£».36
3.Mp(c)Ais the matrix obtained from Aby replacing apbycap:
Mp(c)ï£®
ï£¯ï£°...
ap...ï£¹
ï£ºï£»=ï£®
ï£¯ï£°...
cap...ï£¹
ï£ºï£».
Proof.
Proof of Part (1). Fix 1 â‰¤iâ‰¤mand 1 â‰¤jâ‰¤n. Here Mp,q(c) must be mÃ—m, so that
Mp,q(c) =Im+cEm,qp, since AismÃ—n. Then

Mp,q(c)A
ij=mX
k=1
Mp,q(c)
ik[A]kj=mX
k=1[Im+cEqp]ik[A]kj
=mX
k=1 
[Im]ik+c[Eqp]ik
[A]kj=mX
k=1[Im]ik[A]kj+cmX
k=1[Eqp]ik[A]kj
= [ImA]ij+cmX
k=1Î´iqÎ´pk[A]kj= [A]ij+cÎ´iq[A]pj,
where the last equality holds since Î´pk= 0 for all kÌ¸=p.
Now, if iÌ¸=q, then Î´iq= 0 and we obtain

Mp,q(c)A
ij= [A]ij
for all 1 â‰¤jâ‰¤n, which shows that the ith row vector of Mp,q(c)Aequals the ith row vector ai
ofAwhenever iÌ¸=q. On the other hand if i=q, then Î´iq=Î´qq= 1 and we obtain

Mp,q(c)A
qj= [A]qj+c[A]pj
for all 1 â‰¤jâ‰¤n, which shows that the qth row vector of Mp,q(c)Aequals the qth row vector of
Aplusctimes the pth row vector: aq+cap.
Proof of Part (2). For 1â‰¤iâ‰¤mand 1 â‰¤jâ‰¤n,
[Mp,qA]ij=mX
k=1[Mp,q]ik[A]kj=mX
k=1[Imâˆ’Eppâˆ’Eqq+Epq+Eqp]ik[A]kj
=mX
k=1 
[Im]ikâˆ’[Epp]ikâˆ’[Eqq]ik+ [Epq]ik+ [Eqp]ik
[A]kj
=mX
k=1[Im]ik[A]kjâˆ’mX
k=1[Epp]ik[A]kjâˆ’mX
k=1[Eqq]ik[A]kj+mX
k=1[Epq]ik[A]kj
+mX
k=1[Eqp]ik[A]kj
= [ImA]ijâˆ’mX
k=1Î´ipÎ´pk[A]kjâˆ’mX
k=1Î´iqÎ´qk[A]kj+mX
k=1Î´ipÎ´qk[A]kj37
+mX
k=1Î´iqÎ´pk[A]kj
= [A]ijâˆ’Î´ip[A]pjâˆ’Î´iq[A]qj+Î´ip[A]qj+Î´iq[A]pj. (2.15)
Now, if iÌ¸=p, q, then Î´ip=Î´iq= 0, and so for any 1 â‰¤jâ‰¤nwe find from (2.15) that
[Mp,qA]ij= [A]ij, which shows the ith row vector of Mp,qAequals the ith row vector of A.
Ifi=p, then from (2.15) we obtain
[Mp,qA]pj= [A]pjâˆ’Î´pp[A]pjâˆ’Î´pq[A]qj+Î´pp[A]qj+Î´pq[A]pj= [A]qj
for all 1 â‰¤jâ‰¤n, so that
[Mp,qA]p1Â·Â·Â·[Mp,qA]pn
=[A]q1Â·Â·Â·[A]qn
=aq,
and itâ€™s seen that the pth row vector of Mp,qAis the qth row vector of A.
Finally, if i=q, then from (2.15) we obtain
[Mp,qA]qj= [A]qjâˆ’Î´qp[A]pjâˆ’Î´qq[A]qj+Î´qp[A]qj+Î´qq[A]pj= [A]pj
for all 1 â‰¤jâ‰¤n, so that
[Mp,qA]q1Â·Â·Â·[Mp,qA]qn
=[A]p1Â·Â·Â·[A]pn
=ap,
and itâ€™s seen that the qth row vector of Mp,qAis the pth row vector of A.
We now see that Mp,qAis identical to Asave for a swap of the pth and qth row vectors, as
was to be shown. â– 
Proposition 2.17. Suppose AâˆˆFmÃ—nhas column vectors a1, . . . ,anâˆˆFm. Let cÌ¸= 0, and let
1â‰¤p, qâ‰¤nwithpÌ¸=q.
1.AMâŠ¤
p,q(c)is the matrix obtained from Aby replacing the column vector aqbyaq+cap:

Â·Â·Â·aqÂ·Â·Â·
MâŠ¤
p,q(c) =
Â·Â·Â·aq+capÂ·Â·Â·
.
2.AMâŠ¤
p,qis the matrix obtained from Aby interchanging apandaq:

Â·Â·Â·amin{p,q}Â·Â·Â·amax{p,q}Â·Â·Â·
MâŠ¤
p,q=
Â·Â·Â·amax{p,q}Â·Â·Â·amin{p,q}Â·Â·Â·
.
3.AMâŠ¤
p(c)is the matrix obtained from Aby replacing apbycap:

Â·Â·Â·apÂ·Â·Â·
MâŠ¤
p(c) =
Â·Â·Â·capÂ·Â·Â·
.
Proof.
Proof of Part (1). Observing that the row vectors of AâŠ¤âˆˆFnÃ—mareaâŠ¤
1, . . . ,aâŠ¤
n, by Proposition
2.16(1) we have,
Mp,q(c)AâŠ¤=Mp,q(c)ï£®
ï£¯ï£°...
aâŠ¤
q...ï£¹
ï£ºï£»=ï£®
ï£¯ï£°...
aâŠ¤
q+caâŠ¤
p...ï£¹
ï£ºï£»,38
and so by Proposition 2.13,
AMâŠ¤
p,q(c) = 
Mp,q(c)AâŠ¤âŠ¤=ï£®
ï£¯ï£°...
aâŠ¤
q+caâŠ¤
p...ï£¹
ï£ºï£»âŠ¤
=
Â·Â·Â·aq+capÂ·Â·Â·
.
Proof of Part (2). By Propositions 2.13 and 2.16(2),
AMâŠ¤
p,q= 
Mp,qAâŠ¤âŠ¤=ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­Mp,qï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°...
aâŠ¤
min{p,q}...
aâŠ¤
max{p,q}...ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸âŠ¤
=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°...
aâŠ¤
max{p,q}...
aâŠ¤
min{p,q}...ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»âŠ¤
=
Â·Â·Â·amax{p,q}Â·Â·Â·amin{p,q}Â·Â·Â·
,
and weâ€™re done. â– 
The proof of part (3) of Proposition 2.17 is left as a problem.
Definition 2.18. LetA= [aij]m,n. The ith pivot ofA,pi, is the first nonzero entry (from the
left) in the ith row of A:
pi=airi,where ri= min {j:aijÌ¸= 0}
Azero row of a matrix A, which is a row with all entries equal to 0, is said to have no
pivot.
Definition 2.19. A matrix is a row-echelon matrix (or has row-echlon form ) if the
following conditions are satisfied:
1.No zero row lies above a nonzero row.
2.Given two pivots pi1=ai1j1andpi2=ai2j2,j2> j 1whenever i2> i1.
In a row-echelon matrix, a pivot column is a column that has a pivot. An upper-
triangular matrix is a square matrix having row-echelon form. A lower-triangular matrix
is a square matrix Afor which AâŠ¤has row-echelon form.
The first condition requires that all zero rows be at the bottom of a matrix in row-echelon
form. The second condition requires that if the first kentries of the row iare zeros, then at
least the first k+ 1 entries of row i+ 1 must be zeros. Thus, all entries that lie below a pivot in
a given column must be zero. Examples of matrices in reduced-echelon form are the following,
with pientries indicating pivots (i.e. nonzero entries) and asterisks indicating entries whose39
values may be zero or nonzero:
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0p1âˆ— âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
0 0 p2âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
0 0 0 0 p3âˆ— âˆ— âˆ— âˆ—
0 0 0 0 0 0 0 p4âˆ—
0 0 0 0 0 0 0 0 p5
0 0 0 0 0 0 0 0 0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£»,ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°p1âˆ— âˆ— âˆ— âˆ—
0p2âˆ— âˆ— âˆ—
0 0 p3âˆ— âˆ—
0 0 0 p4âˆ—
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»,ï£®
ï£¯ï£¯ï£¯ï£¯ï£°p1âˆ— âˆ— âˆ— âˆ—
0p2âˆ— âˆ— âˆ—
0 0 p3âˆ— âˆ—
0 0 0 p4âˆ—
0 0 0 0 p5ï£¹
ï£ºï£ºï£ºï£ºï£».
The rightmost matrix is a square matrix and therefore happens to be in upper-triangular form.
Its transpose,ï£®
ï£¯ï£¯ï£¯ï£¯ï£°p10 0 0 0
âˆ—p20 0 0
âˆ— âˆ— p30 0
âˆ— âˆ— âˆ— p40
âˆ— âˆ— âˆ— âˆ— p5ï£¹
ï£ºï£ºï£ºï£ºï£»,
is an example of a matrix in lower-triangular form. The diagonal entries of a square matrix need
not be nonzero in order to have upper-triangular or lower-triangular form, however, so even
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°âˆ— âˆ— âˆ— âˆ— âˆ—
0âˆ— âˆ— âˆ— âˆ—
0 0 âˆ— âˆ— âˆ—
0 0 0 âˆ— âˆ—
0 0 0 0 âˆ—ï£¹
ï£ºï£ºï£ºï£ºï£»andï£®
ï£¯ï£¯ï£¯ï£¯ï£°âˆ—0 0 0 0
âˆ— âˆ— 0 0 0
âˆ— âˆ— âˆ— 0 0
âˆ— âˆ— âˆ— âˆ— 0
âˆ— âˆ— âˆ— âˆ— âˆ—ï£¹
ï£ºï£ºï£ºï£ºï£»
represent 5 Ã—5 triangular matrices regardless of what values we substitute for the asterisks.
Another way to define an upper-triangular matrix is to say it is a square matrix with all
entries below the diagonal equal to 0. Similarly, a lower-triangular matrix is a square matrix
with all entries above the diagonal equal to 0. A diagonal matrix is a square matrix [ aij]n
that is both upper-triangular and lower-triangular, so that aij= 0 whenever iÌ¸=j. Any identity
matrix Inor square zero matrix Onis a diagonal matrix, and (trivially) so too is any 1 Ã—1
matrix [ a].
Proposition 2.20. Every matrix is row-equivalent to a matrix in row-echelon form. Thus if A
is a square matrix, then it is row-equivalent to an upper-triangular matrix.
Proof. We start by observing that any 1 Ã—nmatrix is trivially in row-echelon form for any n.
LetmâˆˆNbe arbitrary, and suppose that an mÃ—nmatrix is row-equivalent to a matrix in
row-echelon form for any n. It remains to show that any ( m+ 1)Ã—nmatrix is row-equivalent
to a matrix in row-echelon form for any n, whereupon the proof will be finished by the Principle
of Induction.
Letnbe arbitrary. Fix A= [aij]m+1,n. We may express Aas a partitioned matrix,
Ba
bbn
,40
where B= [aij]m,nâˆ’1. Observing that [ B|a] is an mÃ—nmatrix, by our inductive hypothesis it
is row-equivalent to a matrix in row-echelon form [ R|c], and thus
Ba
bbn
âˆ¼Rr
bbn
(2.16)
Now, if
b|bn
=
b1Â·Â·Â·bn
consists of all zeros, or the pivot has column number greater than the pivot in the mth row,
then the matrix at right in (2.16) is in row-echelon form and we are done. Supposing neither is
the case, let bkbe the pivot for [ b|bn], and let row â„“be the lowest row in [ R|r] that does not
have a pivot which lies to the right of column k. (If k= 1 then set â„“= 0.) We now effect a
succession of R2 row operations,
Aâ€²=Mâ„“+2,â„“+1Â·Â·Â·Mm,mâˆ’1Mm+1,mRr
bbn
,
which have the effect of moving [ b|bn] to just below row â„“without altering the order of the
other rows. (If [ b|bn] has pivot in the first column it will become the top row since â„“= 0.) We
now have a matrix that either is in row-echelon form, or else rows â„“andâ„“+ 1 have pivots in the
same column.
Suppose the latter is the case. If â„“= 0, then the first entries of the first and second rows
are nonzero scalars x1andx2, respectively, and performing the R1 operation M1,2(âˆ’x2/x1) of
adding âˆ’x2/x1times the first row to the second row will put a 0 at the beginning of the second
row. If â„“ >0 we need do nothing, and proceed to partition Aâ€²as follows:
c1c
0C
Now, [ 0|C] is an mÃ—nmatrix, so by our inductive hypothesis it is row-equivalent to a matrix
[0|Râ€²] in row-echelon form. The resultant ( m+ 1)Ã—nmatrix,
c1c
0Râ€²
,
is in row-echelon form, and since
A=Ba
bbn
âˆ¼Rr
bbn
âˆ¼c1c
0C
âˆ¼c1c
0Râ€²
we conclude that Ais row-equivalent to a matrix in row-echelon form. â– 
In the example to follow, and frequently throughout the remainder of the text, we will
indicate the R1 elementary row operation of left-multiplying a matrix by Mi,j(c) by writing
cri+rjâ†’rj,
which may be read as â€œ ctimes row iis added to row jto yield a new row jâ€ (see Prop-osition
2.16(1)). Similarly an R2 operation, which occurs when left-multiplying by Mi,j, will be indicated
by
riâ†”rj,41
which may be read as â€œinterchange rows iandjâ€ (see Proposition 2.16(2)). Finally an R3
operation, which is the operation of left-multiplying by Mi(c), will be indicated by
criâ†’ri,
which may be read as â€œ ctimes row ito yield a new row iâ€ (see Proposition 2.16(3)).
Example 2.21. Using elementary row operations, find a row-equivalent matrix forï£®
ï£°0 1 3 âˆ’2
2 1âˆ’4 3
2 3 2 âˆ’1ï£¹
ï£»
that is in row-echelon form.
Solution. Call the matrix A. Then,
Ar1â†”r2âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°2 1âˆ’4 3
0 1 3 âˆ’2
2 3 2 âˆ’1ï£¹
ï£»âˆ’r1+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°2 1âˆ’4 3
0 1 3 âˆ’2
0 2 6 âˆ’4ï£¹
ï£»âˆ’2r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°2 1âˆ’4 3
0 1 3 âˆ’2
0 0 0 0ï£¹
ï£».
In terms of elementary matrices we computed
M2,3(âˆ’2)M1,3(âˆ’1)M1,2A,
multiplying from right to left. â– 
Example 2.22. Apermutation matrix is a square matrix Pwith exactly one entry equal to
1 in each row and in each column, and all other entries equal to 0. Any such matrix may be
obtained by rearranging (i.e. permuting) the rows of the identity matrix. Of course, Initself is
a permutation matrix for any nâˆˆN, as is the nÃ—nelementary matrix Mi,jthat results from
interchanging the ith and jth rows of In.
The matrix
P=ï£®
ï£°0 1 0
0 0 1
1 0 0ï£¹
ï£»
is a 3Ã—3 permutation matrix that is obtained from I3by performing the R2 operation r1â†”r2
followed by r2â†”r3. By Proposition 2.16(2), P=M2,3M1,2I3, or simply P=M2,3M1,2. Thus
for any 3 Ã—nmatrix Awe have
PA= (M2,3M1,2)A=M2,3(M1,2A),
which shows that left-multiplication of AbyPis equivalent to performing the following
operations: first, the top and middle rows of Awill be swapped to give a new matrix Aâ€²; and
second, the middle and bottom rows of Aâ€²will be swapped to give the final product. If a1,a2,
anda3are the row vectors of A, then left-multiplication of AbyPmay be characterized as the
action of assigning new positions to the row vectors of A. Namely, PAsends a1to row 3, a2to
row 1, and a3to row 2. Note how these three placement operations correspond to the placement
of the three entries equaling 1 in P: column 1, row 3; column 2, row 1; and column 3, row 2. â– 42
Problems
1. Show that, for any 1 â‰¤iâ‰¤n, the matrix En,iiis symmetric: EâŠ¤
n,ii=En,ii.
2.What matrix results from right-multiplication BPof an mÃ—3 matrix Bby the 3 Ã—3 matrix
Pin Example 2.22? What permutation matrix Qshould be used so that BQpermutes the
columns of Bthe same way that PApermutes the rows of a 3 Ã—nmatrix A?
3. Prove part (3) of Proposition 2.16.
4. Prove part (3) of Proposition 2.17.43
2.4 â€“ The Inverse of a Matrix
Definition 2.23. AnnÃ—nmatrix Aisinvertible if there exists a matrix Bsuch that
AB=BA=In,
in which case we call Btheinverse ofAand denote it by the symbol Aâˆ’1. A matrix that is
not invertible is said to be noninvertible ornonsingular .
From the definition we see that
AAâˆ’1=Aâˆ’1A=In,
provided that Aâˆ’1exists. Observe that Ondoes not have an inverse since AO n=Onfor any
nÃ—nmatrix A. Also observe that, of necessity, if Ais an nÃ—nmatrix, then Aâˆ’1must also be
nÃ—n.
Proposition 2.24. The inverse of a matrix Ais unique.
Proof. LetAbe an invertible nÃ—nmatrix and suppose that BandCare such that
AB=BA=Inand AC=CA=In.
From BA=Inwe obtain
(BA)C=InC=C,
and since matrix multiplication is associative by Theorem 2.7,
C= (BA)C=B(AC) =BIn=B.
That is, B=C, and so Acan have only one inverse. â– 
Proposition 2.25. IfAhas0as a row or column vector, then Ais not invertible.
Proof. LetAbe an nÃ—nmatrix with row vectors a1, . . . ,an. Suppose ai=0for some
1â‰¤iâ‰¤n. Let
B=b1Â·Â·Â·bn
be any nÃ—nmatrix. Since the ii-entry of ABis
aiÂ·bi=0Â·bi= 0,
it is seen that ABÌ¸=In. Since Bis arbitrary, we conclude that Ahas no inverse. That is, Ais
not invertible.
The proof that Ais not invertible if it has 0among its column vectors is similar. â– 
Theorem 2.26. LetkâˆˆN. IfA1, . . . ,AkâˆˆFnÃ—nare invertible, then A1Â·Â·Â·Akis invertible
and
(A1Â·Â·Â·Ak)âˆ’1=Aâˆ’1
kÂ·Â·Â·Aâˆ’1
1.44
Proof. An inductive argument is suitable. The case when k= 1 is trivially true. Let kâˆˆNbe
arbitrary, and suppose that the invertibility of kmatrices A1, . . . ,AkâˆˆFnÃ—nimplies A1Â·Â·Â·Ak
is invertible and ( A1Â·Â·Â·Ak)âˆ’1=Aâˆ’1
kÂ·Â·Â·Aâˆ’1
1.
Suppose that A1, . . . ,Ak+1are invertible nÃ—nmatrices. Let
B=A2Â·Â·Â·Ak+1and C=Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
2.
By the inductive hypothesis Bis invertible, with
Bâˆ’1= (A2Â·Â·Â·Ak+1)âˆ’1=Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
2=C,
and so by Proposition 2.8
(A1Â·Â·Â·Ak+1)(Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
1) = (A1B)(Bâˆ’1Aâˆ’1
1) =A1(BBâˆ’1)Aâˆ’1
1
=A1InAâˆ’1
1=A1Aâˆ’1
1=In. (2.17)
(The associativity of matrix multiplication is implicitly used to justify the penultimate equality.)
Next, let
P=A1Â·Â·Â·Akand Q=Aâˆ’1
kÂ·Â·Â·Aâˆ’1
1.
By the inductive hypothesis Qis invertible, with
Pâˆ’1= (A1Â·Â·Â·Ak)âˆ’1=Aâˆ’1
kÂ·Â·Â·Aâˆ’1
1=Q,
and so by Proposition 2.8
(Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
1)(A1Â·Â·Â·Ak+1) = (Aâˆ’1
k+1Q)(Qâˆ’1Ak+1) =Aâˆ’1
k+1(QQâˆ’1)Ak+1
=Aâˆ’1
k+1InAk+1=Aâˆ’1
k+1Ak+1=In. (2.18)
From (2.17) and(2.18) we conclude that Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
1is the inverse for A1Â·Â·Â·Ak+1. That is,
A1Â·Â·Â·Ak+1is invertible and
(A1Â·Â·Â·Ak+1)âˆ’1=Aâˆ’1
k+1Â·Â·Â·Aâˆ’1
1.
Therefore the statement of the theorem holds for all kâˆˆNby the Principle of Induction. â– 
We now proceed to establish some results that will help us determine whether a matrix has
an inverse, and then develop an algorithm for computing the inverse of any invertible matrix.
We start by examining elementary matrices, since the calculations involved are much simpler.
Proposition 2.27. An elementary matrix is invertible, with
Mâˆ’1
i,j(c) =Mi,j(âˆ’c),Mâˆ’1
i,j=Mi,j,Mâˆ’1
i(c) =Mi(câˆ’1).
Proof. LetnâˆˆNbe arbitrary, let cÌ¸= 0, and let i, jâˆˆ {1, . . . , n }with iÌ¸=j. Using the fact
thatE2
ji=Onby Proposition 2.14(2), we have
Mi,j(âˆ’c)Mi,j(c) = (Inâˆ’cEji)(In+cEji) =I2
n+cInEjiâˆ’cEjiInâˆ’c2E2
ji
=In+cEjiâˆ’cEjiâˆ’c2On=In,45
and
Mi,j(c)Mi,j(âˆ’c) = (In+cEji)(Inâˆ’cEji) =I2
nâˆ’cInEji+cEjiInâˆ’c2E2
ji
=Inâˆ’cEji+cEjiâˆ’c2On=In,
and therefore Mâˆ’1
i,j(âˆ’c) is the inverse for Mi,j(c).
Next we have
M2
i,j= (Inâˆ’Eiiâˆ’Ejj+Eij+Eji)2
=Inâˆ’Eiiâˆ’Ejj+Eij+Ejiâˆ’Eii+EiiEii+EiiEjjâˆ’EiiEijâˆ’EiiEji
âˆ’Ejj+EjjEii+EjjEjjâˆ’EjjEijâˆ’EjjEji+Eijâˆ’EijEiiâˆ’EijEjj
+EijEij+EijEji+Ejiâˆ’EjiEiiâˆ’EjiEjj+EjiEij+EjiEji
=Inâˆ’Eiiâˆ’Ejj+Eij+Ejiâˆ’Eii+Eiiâˆ’Eijâˆ’Ejj+Ejjâˆ’Eji+Eij
âˆ’Eij+Eii+Ejiâˆ’Eji+Ejj=In,
where the third equality owes itself to Proposition 2.14 and the understanding that iÌ¸=j, and
so for instance EiiEjj=On,EijEij=On,EiiEii=Eii,EijEji=Eii, and so on. Therefore Mi,j
is its own inverse.
Finally we show that the inverse for Mi(c) isMi(câˆ’1) for any fixed 1 â‰¤iâ‰¤nandcÌ¸= 0.
Since E2
ii=Eiiby Proposition 2.14(1), we have
Mi(c)Mi(câˆ’1) = 
In+ (câˆ’1)Eii 
In+ (câˆ’1âˆ’1)Eii
=In+ (câˆ’1âˆ’1)Eii+ (câˆ’1)Eii+ (câˆ’1)(câˆ’1âˆ’1)E2
ii
=In+ (câˆ’1âˆ’1)Eii+ (câˆ’1)Eiiâˆ’(câˆ’1)Eiiâˆ’(câˆ’1âˆ’1)Eii=In,
and similarly Mi(câˆ’1)Mi(c) =In. â– 
Proposition 2.28. Suppose Ais row-equivalent to B. Then Ais invertible if and only if Bis
invertible.
Proof. Since Ais row-equivalent to B, there exist elementary matrices M1, . . . ,Mksuch that
MkÂ·Â·Â·M1A=B. (2.19)
Now, suppose Ainvertible. The matrices M1, . . . ,Mkare invertible by Proposition 2.27, and
since Ais invertible by hypothesis, by Theorem 2.26 we conclude that Bis invertible.
Next, suppose Bis invertible. From (2.19) we have
A= (MkÂ·Â·Â·M1)âˆ’1B=Mâˆ’1
1Â·Â·Â·Mâˆ’1
kB,
where Mâˆ’1
1, . . . ,Mâˆ’1
kare all elementary matrices by Proposition 2.27. Thus Bis row-equivalent
toA, and since Bis invertible, the conclusion that Ais invertible follows from the first part of
the proof. â– 46
Proposition 2.29. IfAis invertible, then Ais row-equivalent to an upper-triangular matrix
with nonzero diagonal elements.
Proof. LetAbe an nÃ—nmatrix. Then Ais row-equivalent to an upper-triangular matrix
U= [uij]nby Proposition 2.20. Suppose that uii= 0 for some 1 â‰¤iâ‰¤n. Then ukk= 0 for
alliâ‰¤kâ‰¤n, and in particular unn= 0 so that the nth row vector of Uis0. Hence Uis not
invertible by Proposition 2.25, and since Aâˆ¼Uit follows that Ais not invertible by Proposition
2.28. We have now proven that if Ais row-equivalent to an upper-triangular matrix with a
diagonal element equalling 0, then Ais not invertible. This is equivalent to the statement of
the proposition. â– 
Theorem 2.30. AnnÃ—nmatrix Ais invertible if and only if Ais row-equivalent to In.
Proof. Suppose AâˆˆFnÃ—nis invertible. By Proposition 2.29 Ais row-equivalent to an upper
triangular matrix U= [uij]nwith nonzero diagonal elements. We multiply each row iofUby
uâˆ’1
ii(which of course is defined since uiiÌ¸= 0) to obtain a row-equivalent upper-triangular matrix
U1with diagonal entries all equal to 1:
M1(uâˆ’1
11)Â·Â·Â·Mn(uâˆ’1
nn)U=U1. (2.20)
In particular the first column of Uâ€²ise1as desired, recalling that In= [e1Â·Â·Â·en]. If we add
âˆ’u12times the second row of U1to the first row to obtain a row-equivalent matrix U2,
M2,1(âˆ’u12)U1=U2,
we find in particular that U2is upper-triangular with first column e1and second column e2.
Proceeding in this fashion to the jth column, we have an upper-triangular matrix
Ujâˆ’1=
e1Â·Â·Â·ejâˆ’1ujÂ·Â·Â·un
on which we perform a sequence of R1 row operations to obtain a row-equivalent matrix Uj:
 jâˆ’1Y
i=1Mj,i(âˆ’uij)!
Ujâˆ’1=Mj,1(âˆ’u1j)Â·Â·Â·Mj,jâˆ’1(âˆ’ujâˆ’1,j)Ujâˆ’1=Uj, (2.21)
where
Uj=
e1Â·Â·Â·ejuj+1Â·Â·Â·un
Equation (2.21) holds for j= 2, . . . , n , and gives Unas
Un= nâˆ’1Y
i=1Mn,i(âˆ’uin)! nâˆ’2Y
i=1Mnâˆ’1,i(âˆ’ui,nâˆ’1)!
Â·Â·Â· 1Y
i=1M2,i(âˆ’ui2)!
U1.
Observing that Un=In, and recalling (2.20), we finally obtain
In= nâˆ’1Y
i=1Mn,i(âˆ’uin)! nâˆ’2Y
i=1Mnâˆ’1,i(âˆ’ui,nâˆ’1)!
Â·Â·Â· 1Y
i=1M2,i(âˆ’ui2)! nY
i=1Mi(uâˆ’1
ii)!
U,
which demonstrates in explicit terms that Uis row-equivalent to In. Now, Aâˆ¼UandUâˆ¼In
imply that Aâˆ¼Inand the first part of the proof is finished.47
The converse is much easier to prove. Suppose that Ais row-equivalent to In. Since Inis
invertible, by Proposition 2.28 we conclude that Ais invertible. â– 
This theorem gives rise to a sure method for finding the inverse of any invertible matrix
A. IfAâˆˆFnÃ—nis invertible, then Aâˆ¼In, which is to say there exist elementary matrices
M1, . . . ,Mksuch that MkÂ·Â·Â·M1A=In. Now,
MkÂ·Â·Â·M1A=Inâ‡”(MkÂ·Â·Â·M1A)Aâˆ’1=InAâˆ’1
â‡”MkÂ·Â·Â·M1(AAâˆ’1) =Aâˆ’1
â‡”Aâˆ’1=MkÂ·Â·Â·M1In,
which demonstrates that the selfsame elementary row operations M1, . . . ,Mkthat transform
AintoInwill transform InintoAâˆ’1. In practice we set up a partitioned matrix [ A|In], and
apply identical sequences of elementary row operations to each submatrix until the submatrix
that started as Ahas become In. At that point the submatrix that started as Inwill be Aâˆ’1:
[A|In]âˆ¼[M1A|M1In]âˆ¼ Â·Â·Â· âˆ¼ [MkÂ·Â·Â·M1A|MkÂ·Â·Â·M1In] = [In|Aâˆ’1].
The next example illustrates the procedure.
Example 2.31. Find the inverse of the matrixï£®
ï£°2 4 3
âˆ’1 3 0
0 2 1ï£¹
ï£»
Solution. We employ the same sequence of elementary row operations on both AandI3, as
follows.ï£®
ï£°2 4 3 1 0 0
âˆ’1 3 0 0 1 0
0 2 1 0 0 1ï£¹
ï£»r2â†”r1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’r1â†’r1ï£®
ï£°1âˆ’3 0 0âˆ’1 0
2 4 3 1 0 0
0 2 1 0 0 1ï£¹
ï£»âˆ’2r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1âˆ’3 0 0âˆ’1 0
0 10 3 1 2 0
0 2 1 0 0 1ï£¹
ï£»r2â†”r3âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1âˆ’3 0 0âˆ’1 0
0 2 1 0 0 1
0 10 3 1 2 0ï£¹
ï£»âˆ’5r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1âˆ’3 0 0âˆ’1 0
0 2 1 0 0 1
0 0 âˆ’21 2 âˆ’5ï£¹
ï£»1
2r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1âˆ’3 0 0âˆ’1 0
0 1 1 /20 0 1 /2
0 0 âˆ’21 2 âˆ’5ï£¹
ï£»3r2+r1â†’r1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1 0 3 /20âˆ’1 3/2
0 1 1 /20 0 1 /2
0 0 âˆ’21 2 âˆ’5ï£¹
ï£»1
4r3+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
3
4r3+r1â†’r1ï£®
ï£°1 0 0 3/4 1/2âˆ’9/4
0 1 0 1/4 1/2âˆ’3/4
0 0âˆ’2 1 2 âˆ’5ï£¹
ï£»âˆ’1
2r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1 0 0 3/4 1/2âˆ’9/4
0 1 0 1/4 1/2âˆ’3/4
0 0 1 âˆ’1/2âˆ’1 5 /2ï£¹
ï£».48
Therefore
Aâˆ’1=ï£®
ï£¯ï£¯ï£°3
41
2âˆ’9
4
1
41
2âˆ’3
4
âˆ’1
2âˆ’15
2ï£¹
ï£ºï£ºï£»
is the inverse of A. â– 
Proposition 2.32. IfAâˆˆFnÃ—nis invertible, then AâŠ¤is invertible and
(AâŠ¤)âˆ’1= (Aâˆ’1)âŠ¤.
Proof. Suppose that AâˆˆFnÃ—nis invertible, so that Aâˆ’1exists. Now, by Proposition 2.13,
AAâˆ’1=Inâ‡’(AAâˆ’1)âŠ¤=IâŠ¤
nâ‡’(Aâˆ’1)âŠ¤AâŠ¤=In
and
Aâˆ’1A=Inâ‡’(Aâˆ’1A)âŠ¤=IâŠ¤
nâ‡’AâŠ¤(Aâˆ’1)âŠ¤=In.
Now,
(Aâˆ’1)âŠ¤AâŠ¤=AâŠ¤(Aâˆ’1)âŠ¤=In
shows that ( Aâˆ’1)âŠ¤is the inverse of AâŠ¤. Therefore AâŠ¤is invertible, and moreover ( AâŠ¤)âˆ’1=
(Aâˆ’1)âŠ¤. â– 
Example 2.33. IfPâˆˆFnÃ—nis a permutation matrix (see Example 2.22), then Pâˆ’1=PâŠ¤.
To see this, first observe that Pmay be obtained by permuting the rows of In, and since
any permutation of nobjects may be accomplished by performing at most ntranspositions
(i.e. the operation of swapping two objects), we may write P=M1M2Â·Â·Â·Mm, where mâ‰¤n,
and for each 1 â‰¤kâ‰¤mthe matrix Mkis an elementary matrix of the form Mi,jfor some
i, jâˆˆ {1, . . . , n }with iÌ¸=j.
Next, we claim that any elementary matrix Mi,jis symmetric: MâŠ¤
i,j=Mi,j. To show this,
since
Mi,j=Inâˆ’Eiiâˆ’Ejj+Eij+Eji,
we need to show that, generally, EâŠ¤
pp=Epp, and EâŠ¤
pq=Eqp. The former is a problem in Â§2.3, so
weâ€™ll show the latter. Let i, jâˆˆ {1, . . . , n }with iÌ¸=j. By Definition 2.2 and equation (2.14),
[EâŠ¤
pq]ij= [Epq]ji=Î´jpÎ´qi,
whereas by (2.14),
[Eqp]ij=Î´iqÎ´pj=Î´qiÎ´jp=Î´jpÎ´qi.
Hence [ EâŠ¤
pq]ij= [Eqp]ij, and therefore EâŠ¤
pq=Eqp. It is clear that Inis symmetric, so that IâŠ¤
n=In.
Now, by Proposition 2.3(2) and the foregoing findings,
MâŠ¤
i,j= (Inâˆ’Eiiâˆ’Ejj+Eij+Eji)âŠ¤=IâŠ¤
nâˆ’EâŠ¤
iiâˆ’EâŠ¤
jj+EâŠ¤
ij+EâŠ¤
ji
=Inâˆ’Eiiâˆ’Ejj+Eji+Eij=Mi,j.
Finally, we have
PâŠ¤= (M1M2Â·Â·Â·Mm)âŠ¤=MâŠ¤
mÂ·Â·Â·MâŠ¤
2MâŠ¤
1 (Proposition 2.13)49
=MmÂ·Â·Â·M2M1=Mâˆ’1
mÂ·Â·Â·Mâˆ’1
2Mâˆ’1
1 (Proposition 2.27)
= (M1M2Â·Â·Â·Mm)âˆ’1=Pâˆ’1, (Theorem 2.26)
as was to be shown. â– 50
2.5 â€“ Systems of Linear Equations
As usual let Fdenote a field. A system overFofmlinear equations in nunknowns
x1, . . . , x nis a set of equations of the form
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³a11x1+a12x2+Â·Â·Â·+a1nxn=b1
a21x1+a22x2+Â·Â·Â·+a2nxn=b2............
am1x1+am2x2+Â·Â·Â·+amnxn=bm(2.22)
for which aijâˆˆFandbiâˆˆFfor all integers 1 â‰¤iâ‰¤mand 1 â‰¤jâ‰¤n. The scalars aijare the
coefficients of the system, and b1, . . . , b mare the constant terms . IfSiis the solution set of
theith equation, which is to say
Si=ï£±
ï£²
ï£³ï£®
ï£°x1...
xnï£¹
ï£»âˆˆFn:ai1x1+ai2x2+Â·Â·Â·+ainxn=biï£¼
ï£½
ï£¾,
then the solution set of the system (2.22) is
S=S1âˆ© Â·Â·Â· âˆ© Sm=m\
i=1Si,
or equivalently
S=ï£±
ï£²
ï£³ï£®
ï£°x1...
xnï£¹
ï£»âˆˆFn:ï£®
ï£°x1...
xnï£¹
ï£»âˆˆSifor all 1 â‰¤iâ‰¤mï£¼
ï£½
ï£¾.
A system is consistent if its solution set Sis nonempty (i.e. the system has at least one
solution), and inconsistent ifS=âˆ…(i.e. the system has no solution). A consistent system is
dependent ifShas an infinite number of elements, and independent ifShas precisely one
element. As we will see later, a system of linear equations has either no solution, precisely one
solution, or an infinite number of solutions. There are no other possibilities.
If we define
A=ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1n
a21a22Â·Â·Â· a2n............
am1am2Â·Â·Â· amnï£¹
ï£ºï£ºï£»,x=ï£®
ï£¯ï£¯ï£°x1
x2...
xnï£¹
ï£ºï£ºï£»,and b=ï£®
ï£¯ï£¯ï£°b1
b2...
bmï£¹
ï£ºï£ºï£», (2.23)
then the system (2.22) may be written as the matrix equation Ax=b,
ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1n
a21a22Â·Â·Â· a2n............
am1am2Â·Â·Â· amnï£¹
ï£ºï£ºï£»ï£®
ï£¯ï£¯ï£°x1
x2...
xnï£¹
ï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£°b1
b2...
bmï£¹
ï£ºï£ºï£». (2.24)51
In this representation of the system, all solutions xare expressed as column vectors
x=ï£®
ï£°x1...
xnï£¹
ï£», (2.25)
so that the solution set Sis given as
S=ï£±
ï£²
ï£³ï£®
ï£°x1...
xnï£¹
ï£»âˆˆFnï£®
ï£°x1...
xnï£¹
ï£»âˆˆSifor all 1 â‰¤iâ‰¤mï£¼
ï£½
ï£¾.
As a further notational convenience we may express the matrix equation (2.24) as an
augmented matrix featuring only the coefficients and constant terms of the system,
ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1nb1
a21a22Â·Â·Â· a2nb2...............
am1am2Â·Â·Â· amnbmï£¹
ï£ºï£ºï£». (2.26)
We see the augmented matrix is just the partitioned matrix [ A|b]. The fact that there are n
columns of coefficients (understood to be the columns to the left of the vertical line) informs us
that there are nvariables, and since an n-variable system of equations is fully determined by its
coefficients and constant terms, no information is lost in doing this.
We now consider how the system (2.22) is affected if we left-multiply the corresponding
augmented matrix [ A|b] by any one of the three elementary matrices Mi,j(c),Mi,j, orMi(c).
By Proposition 2.16 we know that
Mi,j(c)[A|b]
will effect an R1 operation, specifically adding cÌ¸= 0 times the ith row of [ A|b] to the jth row.
What results is a new augmented matrix [ Aâ€²|bâ€²] corresponding to a new system of equations
in which ctimes the ith equation has been added to the jth equation. But is the solution set Sâ€²
of the new system [ Aâ€²|bâ€²] any different from the solution set Sof the original system [ A|b]?
In the system [ A|b] the ith and jth equations are
nX
k=1aikxk=biandnX
k=1ajkxk=bj, (2.27)
which have solution sets SiandSj, respectively; and in the system [ Aâ€²|bâ€²] the ith and jth
equations are
nX
k=1aikxk=biandnX
k=1(caik+ajk)xk=cbi+bj, (2.28)
which have solution sets Sâ€²
iandSâ€²
j, respectively. We will show that Siâˆ©Sj=Sâ€²
iâˆ©Sâ€²
j. To start,
we first observe that the ith equation of [ Aâ€²|bâ€²] is the same as the ith equation of [ A|b], so
Sâ€²
i=Siand our task becomes that of showing Siâˆ©Sj=Siâˆ©Sâ€²
j.52
LetxâˆˆSiâˆ©Sjbe given by (2.25) . Thus the scalars x1, . . . , x nare such that the equations
in (2.27) are satisfied. We have, using (2.27),
nX
k=1(caik+ajk)xk=cnX
k=1aik+nX
k=1ajkxk=cbi+bj,
which shows that x1, . . . , x nsatisfy the second equation in (2.28) and so xâˆˆSâ€²
j. From xâˆˆSi
andxâˆˆSâ€²
jwe have xâˆˆSiâˆ©Sâ€²
j, and therefore Siâˆ©SjâŠ†Siâˆ©Sâ€²
j.
Now suppose xâˆˆSiâˆ©Sâ€²
j, so that the scalars x1, . . . , x nare assumed to satisfy the equations
in (2.28). Multiplying the first equation by cyields
nX
k=1caikxk=cbi,
so thatnX
k=1(caik+ajk)xkâˆ’nX
k=1caikxk= (cbi+bj)âˆ’cbi
obtains from the second equation in (2.28), which in turn implies that
nX
k=1ajkxk=bj
and so xâˆˆSj. Since xâˆˆSialso, we conclude that xâˆˆSiâˆ©Sjand therefore Siâˆ©Sâ€²
jâŠ†Siâˆ©Sj.
We have now shown that Siâˆ©Sâ€²
j=Siâˆ©Sj, so that
Sâ€²= (Siâˆ©Sâ€²
j)âˆ© \
kÌ¸=i,jSk!
= (Siâˆ©Sj)âˆ© \
kÌ¸=i,jSk!
=m\
k=1Sk=S.
Thus, performing an R1 operation
Mi,j(c)[A|b] = [Aâ€²|bâ€²]
on the augmented matrix [ A|b] corresponding to a system of equations results in a new
augmented matrix [ Aâ€²|bâ€²] that corresponds to a new system of equations that has the same
solution set as the original system. This is clearly also the case whenever performing an R2
operation Mi,j[A|b], since the outcome yields an augmented matrix corresponding to a system
of equations that is identical to the original system except that the ith and jth equations have
traded places. (Again, a system of equations is a set of equations, and sets are blind to order.)
Finally, an R3 operation Mi(c)[A|b] results in an augmented matrix corresponding to a system
of equations that is identical to the original system except that the ith equation has been
multiplied by a nonzero scalar c, which does not alter the solution set of the ith equation and
therefore does not alter the solution set of the system as a whole. We have proven the following.
Proposition 2.34. Any elementary row operation performed on the augmented matrix [A|b]
of a system of linear equations results in an augmented matrix [Aâ€²|bâ€²]whose corresponding
system has the same solution set.
Definition 2.35. Two systems of linear equations are equivalent if their corresponding
augmented matrices are row-equivalent.53
In light of Proposition 2.34 it is immediate that equivalent systems of linear equations have
the same solution set. Thus, to solve a system of linear equations such as (2.22) , one fairly
efficient approach is to perform elementary row operations on its corresponding augmented
matrix until it is in row-echelon form, at which point it is easy to determine the systemâ€™s solution
set. The process is known as Gaussian elimination.
Example 2.36. Apply Gaussian elimination to determine the solution set of the system
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³3x+y+ 4z+w= 6
2x + 3z+ 4w= 13
yâˆ’2zâˆ’w= 0
xâˆ’y+z+w= 3
Solution. The corresponding augmented matrix for the system is
ï£®
ï£¯ï£¯ï£°3 1 4 1 6
2 0 3 4 13
0 1 âˆ’2âˆ’1 0
1âˆ’1 1 1 3ï£¹
ï£ºï£ºï£».
Weâ€™ll start by interchanging the 1st and 4th rows, since it will be convenient having a 1 at the
top of the 1st column. Also weâ€™ll interchange the 2nd and 3rd rows so as to move the 0 in the
2nd column down to a position where row-echelon form requires a 0 entry.
ï£®
ï£¯ï£¯ï£°3 1 4 1 6
2 0 3 4 13
0 1 âˆ’2âˆ’1 0
1âˆ’1 1 1 3ï£¹
ï£ºï£ºï£»r1â†”r4âˆ’ âˆ’ âˆ’ âˆ’ â†’
r2â†”r3ï£®
ï£¯ï£¯ï£°1âˆ’1 1 1 3
0 1 âˆ’2âˆ’1 0
2 0 3 4 13
3 1 4 1 6ï£¹
ï£ºï£ºï£»âˆ’2r1+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’3r1+r4â†’r4
ï£®
ï£¯ï£¯ï£°1âˆ’1 1 1 3
0 1 âˆ’2âˆ’1 0
0 2 1 2 7
0 4 1 âˆ’2âˆ’3ï£¹
ï£ºï£ºï£»âˆ’2r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’4r2+r4â†’r4ï£®
ï£¯ï£¯ï£°1âˆ’1 1 1 3
0 1 âˆ’2âˆ’1 0
0 0 5 4 7
0 0 9 2 âˆ’3ï£¹
ï£ºï£ºï£»âˆ’9
5r3+r4â†’r4âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£¯ï£¯ï£°1âˆ’1 1 1 3
0 1 âˆ’2âˆ’1 0
0 0 5 4 7
0 0 0 âˆ’26
5âˆ’78
5ï£¹
ï£ºï£ºï£»âˆ’5
26r4â†’r4âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£¯ï£¯ï£°1âˆ’1 1 1 3
0 1 âˆ’2âˆ’10
0 0 5 4 7
0 0 0 1 3ï£¹
ï£ºï£ºï£»
The fifth matrix above is in row-echelon form, so technically the last row operation is not
required. On the other hand it certainly is desirable to eliminate any fractions if thereâ€™s an easy
way to do it. We have obtained the following equivalent system of equations:
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³xâˆ’y+z+w= 3
yâˆ’2zâˆ’w= 0
5z+ 4w= 7
w= 354
We may now determine the solution to the system by employing so-called â€œbackward substitution.â€
Taking w= 3 from the 4th equation and substituting into the 3rd equation yields
5z+ 4(3) = 7 â‡’5z=âˆ’5â‡’z=âˆ’1.
Taking w= 3 and z=âˆ’1 and substituting into the 2nd equation yields
yâˆ’2(âˆ’1)âˆ’3 = 0 â‡’y= 1.
Finally, substituting w= 3,z=âˆ’1, and y= 1 into the 1st equation yields
xâˆ’1 + (âˆ’1) + 3 = 3 â‡’x= 2.
Therefore the only solution to the system is ( x, y, z, w ) = (2 ,1,âˆ’1,3), which is to say the
solution set is {(2,1,âˆ’1,3)}. â– 
Example 2.37. Apply Gaussian elimination to determine the solution set of the system
ï£±
ï£²
ï£³âˆ’3xâˆ’5y+ 36z= 10
âˆ’x + 7z= 5
x+yâˆ’10z=âˆ’4(2.29)
Write the solution set in terms of column vectors.
Solution. The corresponding augmented matrix for the system is
ï£®
ï£°âˆ’3âˆ’5 36 10
âˆ’1 0 7 5
1 1 âˆ’10âˆ’4ï£¹
ï£».
We transform this matrix into row-echelon form:ï£®
ï£°âˆ’3âˆ’5 36 10
âˆ’1 0 7 5
1 1 âˆ’10âˆ’4ï£¹
ï£»r1â†”r3âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 1 âˆ’10âˆ’4
âˆ’1 0 7 5
âˆ’3âˆ’5 36 10ï£¹
ï£»r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
3r1+r3â†’r3ï£®
ï£°1 1 âˆ’10âˆ’4
0 1 âˆ’3 1
0âˆ’2 6 âˆ’2ï£¹
ï£»
2r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 1âˆ’10âˆ’4
0 1 âˆ’3 1
0 0 0 0ï£¹
ï£».
We have obtained the equivalent system of equations
x+yâˆ’10z=âˆ’4
yâˆ’3z= 1
From the second equation we have
y= 3z+ 1,
which, when substituted into the first equation, yields
x= 10zâˆ’yâˆ’4 = 10 zâˆ’(3z+ 1)âˆ’4 = 7 zâˆ’5.
That is, we have x= 7zâˆ’5 and y= 3z+ 1, and zis free to assume any scalar value whatsoever.55
Any ordered triple [ x, y, z ]âŠ¤that satisfies (2.29) must be of the form
[7zâˆ’5,3z+ 1, z]âŠ¤
for some zâˆˆF, and therefore the solution set is
S=
[7zâˆ’5,3z+ 1, z]âŠ¤:zâˆˆF	
.
Since ï£®
ï£°7zâˆ’5
3z+ 1
zï£¹
ï£»=ï£®
ï£°âˆ’5
1
0ï£¹
ï£»+ï£®
ï£°7z
3z
zï£¹
ï£»=ï£®
ï£°âˆ’5
1
0ï£¹
ï£»+zï£®
ï£°7
3
1ï£¹
ï£»,
we may write
S=ï£±
ï£²
ï£³ï£®
ï£°âˆ’5
1
0ï£¹
ï£»+tï£®
ï£°7
3
1ï£¹
ï£»:tâˆˆFï£¼
ï£½
ï£¾.
â– 
The solution set Sin Example 2.37 is called a one-parameter solution set, meaning all
elements of Smay be specified by designating a value in the field Ffor a single parameter
(namely z). The solution set of the system in Example 2.36 is a zero-parameter solution set.
In general an n-parameter set is a set Swhose elements are determined by the values of n
independent variables x1, . . . , x ncalled parameters . If the values of x1, . . . , x nderive from a
setI(sometimes called the index set ), then Shas the form
S={f(x1, . . . , x n) :xiâˆˆIfor each 1 â‰¤iâ‰¤n}.
Here fis a function that pairs each n-tuple ( x1, . . . , x n) with a single element of S.56
Problems
In Exercises 1â€“4 use Gaussian elimination to determine the solution set of the system of linear
equations. Write all solution sets in terms of column vectors.
1.ï£±
ï£²
ï£³x+ 2yâˆ’z= 9
2x âˆ’z=âˆ’2
3x+ 5y+ 2z= 22
2.ï£±
ï£²
ï£³x âˆ’z= 1
âˆ’2x+ 3yâˆ’z= 0
âˆ’6x+ 6y =âˆ’2
3.ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³x +z+w= 4
yâˆ’z =âˆ’4
xâˆ’2y+ 3z+w= 12
2x âˆ’2z+ 5w=âˆ’1
4.ï£±
ï£²
ï£³3xâˆ’6yâˆ’z+w= 7
âˆ’x+ 2y+ 2z+ 3w= 1
4xâˆ’8yâˆ’3zâˆ’2w= 6
5. Consider the system of equationsï£±
ï£²
ï£³2x+y+z= 3
xâˆ’y+ 2z= 3
xâˆ’2y+Î»z= 4
Determine for which values of Î», if any, the system has:
(a) No solution.
(b) A unique solution, in which case give the solution.
(c) Infinitely many solutions, in which case give the solution.
6.Find conditions on the general vector bthat would make the equation Ax=bconsistent,
where
A=ï£®
ï£¯ï£¯ï£°1 0 âˆ’1
âˆ’2 3 âˆ’1
3âˆ’3 0
2 0 âˆ’2ï£¹
ï£ºï£ºï£».57
2.6 â€“ Homogeneous Systems
A system of linear equations in which all constant terms are equal to 0 is said to be
homogeneous :ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³a11x1+a12x2+Â·Â·Â·+a1nxn= 0
a21x1+a22x2+Â·Â·Â·+a2nxn= 0
............
am1x1+am2x2+Â·Â·Â·+amnxn= 0(2.30)
If we define Aandxas in (2.23) , which is to say A= [aij]m,nandx= [xi]n,1, then we may
write (2.30) as the matrix equation Ax=0. At a glance it is clear that setting
x1=Â·Â·Â·=xn= 0
will satisfy the system. This is called the trivial solution , and it may be represented as an
n-tuple (0 , . . . , 0), an nÃ—1 zero vector 0, or some analogous construct.
Theorem 2.38. LetA= [aij]m,nandx= [xi]n,1. Ifn > m , then the homogeneous system
Ax=0has a nontrivial solution.
Proof. The theorem states that, for each mâˆˆN, the system (2.30) has a nontrivial solution
whenever n > m . The proof will be accomplished using induction.
We consider the base case, when m= 1. For any n >1 the â€œsystemâ€ consists of a single
equation
a11x1+Â·Â·Â·+a1nxn= 0. (2.31)
Now, if a1j= 0 for all 1 â‰¤jâ‰¤n, then anychoice of scalars for x1, . . . , x nwill satisfy this
equation, and so in particular there exists a nontrivial solution. On the other hand if a1kÌ¸= 0 for
some 1 â‰¤kâ‰¤n, then we may choose any scalar values for x1, . . . , x kâˆ’1, xk+1, . . . , x n, and set
xk=âˆ’1
a1kX
jÌ¸=kaijxj
so as to satisfy (2.31) . Since there again exists a nontrivial solution, we see that the theorem is
true in the case when m= 1.
LetmâˆˆNbe arbitrary and suppose that the theorem is true for this mvalue. Consider the
systemï£±
ï£²
ï£³a11x1+a12x2+Â·Â·Â·+a1nxn= 0
............
am+1,1x1+am+1,2x2+Â·Â·Â·+am+1,nxn= 0(2.32)
where n > m + 1. Assume that a11Ì¸= 0. If [ A|0] is the corresponding augmented matrix, then
the sequence of elementary row operations
M1,m+1(âˆ’am+1,1/a11)Â·Â·Â·M1,2(âˆ’a21/a11)[A|0]58
will yield a new augmented matrix [ Aâ€²|0] that has zero entries under a11in the first column,
which is to say we have attained an equivalent system of the form
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³a11x1+a12x2+Â·Â·Â·+a1nxn= 0
aâ€²
22x2+Â·Â·Â·+aâ€²
2nxn= 0
.........
aâ€²
m+1,2x2+Â·Â·Â·+aâ€²
m+1,nxn= 0(2.33)
Contained within this system is the system
ï£±
ï£²
ï£³aâ€²
22x2+Â·Â·Â·+aâ€²
2nxn= 0
.........
aâ€²
m+1,2x2+Â·Â·Â·+aâ€²
m+1,nxn= 0
which has mequations and nvariables, where n > m . By our inductive hypothesis this smaller
system has a nontrivial solution ( Ë†x2, . . . , Ë†xn), so that there is some 2 â‰¤kâ‰¤nfor which Ë†xkÌ¸= 0.
Now, if we let
Ë†x1=âˆ’1
a11nX
j=2a1jË†xj,
then ( Ë†x1, . . . , Ë†xn) will satisfy all the equations in the system (2.33) . Since (2.32) is equivalent to
(2.33) it follows that ( Ë†x1, . . . , Ë†xn) is a solution to (2.32) , and moreover it is a nontrivial solution
since Ë† xkÌ¸= 0. We conclude that the theorem is true for m+ 1 at least when a11Ì¸= 0.
Ifa11= 0 but there exists some 2 â‰¤kâ‰¤nfor which a1kÌ¸= 0, we relabel our variables thus:
y1=xk,yk=x1, and yj=xjforjÌ¸= 1, k. We thereby obtain a system of the form
ï£±
ï£²
ï£³a1ky1+a12y2+Â·Â·Â·+a11ykÂ·Â·Â·+a1nyn= 0
...............
am+1,ky1+am+1,2y2+Â·Â·Â·+am+1,1ykÂ·Â·Â·+am+1,nyn= 0(2.34)
From this, much like before, we obtain an equivalent system in which the variable y1has been
eliminated from all equations save the first one. By our inductive hypothesis there exists a
nontrivial solution ( Ë†y2, . . . , Ë†yn) to the system consisting of the 2nd through ( m+ 1)st equations
of the equivalent system, whereupon setting
Ë†y1=âˆ’1
a1k(a12Ë†y2+Â·Â·Â·+a11Ë†yk+Â·Â·Â·+a1nË†yn)
gives an n-tuple ( Ë†y1, . . . , Ë†yn) that is nontrivial and satisfies (2.34) . It is then a routine matter to
verify that ( Ë†x1, . . . , Ë†xn) with Ë†x1=Ë†yk,Ë†xk=Ë†y1, and Ë†xj=Ë†yjforjÌ¸= 1, kis a nontrivial solution
to (2.32).
Ifa1j= 0 for all 1 â‰¤jâ‰¤n, then by our inductive hypothesis we may find a nontrivial
solution to the other mequations of (2.32) , and this solution must necessarily satisfy the first
equation.
We have now verified that the theorem is true for m+1 in all possible cases. By the Principle
of Induction, therefore, the theorem is proven. â– 
A system of equations Ax=bfor which bÌ¸=0isnonhomogeneous . The next example
shows the first of many intimate connections between a nonhomogeneous system Ax=band59
the corresponding homogeneous system Ax=0(i.e. the homogeneous system having the same
coefficient matrix A).
Example 2.39. LetAx=bbe a nonhomogeneous system of equations, and let xâ€²be a solution.
Show that if x0is a solution to the corresponding homogeneous system Ax=0, then xâ€²+x0is
another solution to Ax=b.
Solution. We have Axâ€²=bandAx0=0. Let y=xâ€²+x0. We must show that Ay=b. But
this is immediate:
Ay=A(xâ€²+x0) =Axâ€²+Ax 0=b+0=b,
using the Distributive Law of matrix multiplication established in Â§2.2. â– 
Given any nonempty SâŠ†Fnand nonzero xâˆˆFn, we define a new set
x+S={x+y:yâˆˆS}
called a coset ofS. We now improve on Example 2.39 with the following more general result.
Theorem 2.40. LetAx=bbe a nonhomogeneous system of linear equations with solution set
S, and let Shbe the solution set of the corresponding homogeneous system Ax=0. Ifxpis any
particular solution to Ax=b, then S=xp+Sh.
Proof. Suppose that xpis a particular solution to Ax=b. Let xâ€²âˆˆSbe arbitrary. Then
A(xâ€²âˆ’xp) =Axâ€²âˆ’Axp=bâˆ’b=0
shows that xâ€²âˆ’xpis a solution to Ax=0and hence xâ€²âˆ’xPâˆˆSh. Since
xâ€²=xp+ (xâ€²âˆ’xp)âˆˆ {xp+xh:xhâˆˆSh}=xp+Sh,
we conclude that SâŠ†xp+Sh.
Next, suppose that xâ€²âˆˆxp+Sh, soxâ€²=xp+xhfor some xhâˆˆSh. Since
Axâ€²=A(xp+xh) =Axp+Axh=b+0=b,
we conclude that xâ€²âˆˆSand hence xp+ShâŠ†S.
Therefore S=xp+Sh. â– 
To fully determine the solution set of any nonhomogeneous system Ax=b, according to
Theorem 2.40 it suffices to find just one solution to Ax=b(called a particular solution )
along with the complete solution set of Ax=0.
We close this chapter with a final result that will later become bound up in the Inverse
Matrix Theorem, which is a theorem that will bring together over a dozen seemingly disparate
statements that all turn out to be equivalent.
Proposition 2.41. IfAâˆˆFnÃ—nis invertible, then the homogeneous system Ax=0has only
the trivial solution.60
Proof. Suppose AâˆˆFnÃ—nis invertible. Clearly 0is a solution to Ax=0, and it only remains
to show it is a unique solution. Suppose that x0is a solution to the system, so that Ax 0=0.
Since Aâˆ’1exists, we have
Ax 0=0â‡’Aâˆ’1(Ax 0) =Aâˆ’10â‡’(Aâˆ’1A)x0=0â‡’Inx0=0â‡’x0=0.
Thus any x0given to be a solution to the system must in fact be 0, proving uniqueness. â– 61
3
Vector Spaces
3.1 â€“ The Vector Space Axioms
LetFbe a field. In practice Fis usually either the real number system Ror the complex
number system C, but in any case it is a set of objects that obey the field axioms given in Â§1.1.
Definition 3.1. Avector space overFis a set Vof objects, along with operations vector
addition VÃ—Vâ†’V(denoted by +) and scalar multiplication FÃ—Vâ†’V(denoted by Â·
or juxtaposition) subject to the following axioms:
VS1. u+v=v+ufor any u,vâˆˆV
VS2. u+ (v+w) = (u+v) +wfor any u,v,wâˆˆV
VS3. There exists some 0âˆˆVsuch that u+0=ufor any uâˆˆV
VS4. For each uâˆˆVthere exists some âˆ’uâˆˆVsuch that u+ (âˆ’u) =0
VS5. For any aâˆˆFandu,vâˆˆV,a(u+v) =au+av
VS6. For any a, bâˆˆFanduâˆˆV,(a+b)u=au+bu
VS7. For any a, bâˆˆFanduâˆˆV,a(bu) = (ab)u
VS8. For all uâˆˆV,1u=u
The elements of Vare called vectors and the elements of the underlying field Fare called
scalars .
Areal vector space is a vector space over R, and a complex vector space is a vector
space over C. AEuclidean n-space over Fis specifically a vector space consisting of n-tuples
[x1, . . . , x n], where xkâˆˆFfor all 1 â‰¤kâ‰¤n. In general a Euclidean space is any Euclidean
n-space over Ffor some unspecified nâˆˆNand field F. IfF=R, we obtain a real Euclidean
space ; and if F=C, we obtain a complex Euclidean space .
The object 0mentioned in Axiom VS3 is called the zero vector , and the vector âˆ’u
mentioned in Axiom VS4 is the additive inverse ofu.
We have in the statement of the definition that + : VÃ—Vâ†’V. That is, the vector addition
operation + takes any ordered pair ( u,v)âˆˆVÃ—Vand returns an object u+vâˆˆV. Thus
u+vmust be an object that belong to the set V! Similarly the scalar multiplication operation
is given to be a map Â·:FÃ—Vâ†’V, which means scalar multiplication takes any ordered pair
(a,u)âˆˆFÃ—Vand returns an object aÂ·u=auâˆˆV. Thus aumust also belong to V! Some62
books state these facets of the definition of a vector space as two additional axioms:
u+vâˆˆVfor any u,vâˆˆV (3.1)
and
auâˆˆVfor any aâˆˆFanduâˆˆV. (3.2)
We call (3.1) theclosure property of scalar multiplication , and (3.2) theclosure property
of addition . When property (3.1) holds for a set V, we say that Visclosed under addition ;
and when property (3.2) holds we say Visclosed under scalar multiplication .
Remark. A set Vtogether with given operations + and Â·defined on VÃ—VandFÃ—V,
respectively, is a vector space if and only if the eight axioms VS1â€“VS8 and the two closure
properties (3.1) and (3.2) are all satisfied!
Some seemingly â€œobviousâ€ results actually require careful reasoning to prove their validity in
the context of vector spaces, as the next two propositions show.
Proposition 3.2. LetVbe a vector space, uâˆˆV, and aâˆˆF. Then the following properties
hold.
1. 0u=0
2.a0=0
3.Ifau=0, then a= 0oru=0
Proof.
Proof of Part (1). Since uâˆˆVand 0 âˆˆF, we have 0 uâˆˆVby the closure property (3.2). Now,
0u= 0u+0 Axiom VS3
= 0u+ [u+ (âˆ’u)] Axiom VS4
= (0u+u) + (âˆ’u) Axiom VS2
= (0u+ 1u) + (âˆ’u) Axiom VS8
= (0 + 1) u+ (âˆ’u) Axiom VS6
= 1u+ (âˆ’u) Axiom F3
=u+ (âˆ’u) Axiom VS8
=0. Axiom VS4
The proofs of parts (2) and (3) are left to the exercises. â– 
Proposition 3.3. IfVis a vector space and uâˆˆV, then (âˆ’1)u=âˆ’u.
Proof. Suppose that Vis a vector space and uâˆˆV. Then ( âˆ’1)uâˆˆV, and
(âˆ’1)u= (âˆ’1)u+0 Axiom VS3
= (âˆ’1)u+ [u+ (âˆ’u)] Axiom VS4
= [(âˆ’1)u+u] + (âˆ’u) Axiom VS2
= [(âˆ’1)u+ 1u] + (âˆ’u) Axiom VS863
= (âˆ’1 + 1) u+ (âˆ’u) Axiom VS6
= 0u+ (âˆ’u) Axiom F4
=0+ (âˆ’u), Proposition 3.2(1)
=âˆ’u. Axiom VS3
â– 
As with Euclidean vectors we define vector subtraction by
uâˆ’v=u+ (âˆ’v)
for any u,vâˆˆV.
The objects belonging to a vector space are invariably called vectors, but they could be any
kind of mathematical entity either concrete or abstract. They often are the Euclidean vectors
encountered in Chapter 1, but they could also be matrices, polynomials, functions, or other
objects. This is part of the power of linear algebra.
Example 3.4. The set of coordinate vectors
Rn=ï£±
ï£²
ï£³ï£®
ï£°x1...
xnï£¹
ï£»x1, . . . , x nâˆˆRï£¼
ï£½
ï£¾,
together with the definitions of vector addition and real scalar multiplication as given in Â§1.2, is
easily verified to be a vector space over R. Similarly, the set of coordinate vectors
Cn=ï£±
ï£²
ï£³ï£®
ï£°z1...
znï£¹
ï£»z1, . . . , z nâˆˆCï£¼
ï£½
ï£¾,
with vector addition and complex scalar multiplication defined in analogous fashion to Rn, is a
vector space over C. Important: the underlying fields of RnandCnare always taken to be R
andC, respectively, unless otherwise specified! â– 
Example 3.5. The set FmÃ—nof all mÃ—nmatrices with entries in Fis a vector space under the
standard operations of matrix addition and scalar multiplication given by Definition 2.1. In
particular the set RmÃ—nofmÃ—nmatrices with real-valued entries is a real vector space, and
the set CmÃ—nofmÃ—nmatrices with complex-valued entries is a complex vector space. â– 
Example 3.6. Given an integer nâ‰¥0, let Pn(F) be the set of all polynomials of a single
variable xwith coefficients in Fand degree at most n; that is,
Pn(F) ={a0+a1x+Â·Â·Â·+anâˆ’1xnâˆ’1+anxn:aiâˆˆFfor 0â‰¤iâ‰¤n}.
By definition the polynomial 0 has degree âˆ’âˆ, and so 0 âˆˆ P n(F) in particular. We have
P0(R) ={a:aâˆˆR}=R,
P1(R) ={a+bx:a, bâˆˆR},
P2(R) ={a+bx+cx2:a, b, câˆˆR}.64
If we define polynomial addition and scalar multiplication in the customary fashion by
(a0+a1x+Â·Â·Â·+anxn) + (b0+b1x+Â·Â·Â·+bnxn)
= (a0+b0) + (a1+b1)x+Â·Â·Â·+ (an+bn)xn,
and
c(a0+a1x+Â·Â·Â·+anxn) =ca0+ca1x+Â·Â·Â·+canxn,
then it is straightforward to verify that Pn(F) is a vector space. â– 
Example 3.7. LetSâŠ†F, where as usual Fis some field. Let F(S,F) denote the set of all
functions Sâ†’F. Given fâˆˆ F(S,F) and câˆˆF, we define scalar multiplication of cwith fas
yielding a new function cfâˆˆ F(S,F) given by
(cf)(x) =cf(x)
for all xâˆˆS. Iff, gâˆˆ F(S,F), we define addition of fwith gas yielding a new function
f+gâˆˆ F(S,F) given by
(f+g)(x) =f(x) +g(x)
for all xâˆˆS. These operations are consonant with conventions established in elementary
algebra, and it is straightforward to verify that F(S,F) is in fact a vector space. The zero vector
is the function 0 given by 0( x) = 0 for all xâˆˆS. The additive inverse of any fâˆˆ F(S,F) is the
function âˆ’fgiven by ( âˆ’f)(x) =âˆ’f(x), since
(f+ (âˆ’f))(x) =f(x) + (âˆ’f)(x) =f(x) + (âˆ’f(x)) = 0 = 0( x)
for all xâˆˆS, and hence f+ (âˆ’f) = 0.
If a set Sis not specified at the outset of an analysis involving functions f1, f2, . . . , f n, then
we take
S=n\
i=1Dom( fi) = Dom( f1)âˆ©Dom( f2)âˆ© Â·Â·Â· âˆ© Dom( fn)
and carry out the analysis in the vector space F(S,F) provided that SÌ¸=âˆ…. â– 
Example 3.8. Show that the collection of functions4
C={f:Râ†’R|f(2) = 0 }
is a vector space over Runder the usual operations of function addition and scalar multiplication
(see Example 3.7).
Solution. First, itâ€™s worth noting that since F(R,R) is the set of all real-valued functions with
domain R, we have C âŠ† F (R,R).
Letf, g, h âˆˆ Canda, bâˆˆR. Let xâˆˆRbe arbitrary. We have f+gâˆˆ Candafâˆˆ Csince
(f+g)(2) = f(2) + g(2) = 0 + 0 = 0 and ( af)(2) = af(2) = a(0) = 0 ,
4Sets of functions (as well as sets of sets) are often referred to as â€œcollectionsâ€ or â€œfamiliesâ€ in the mathematical
literature.65
and so there is closure under addition and scalar multiplication. In what follows we make
frequent use of the field axioms of the real number system (see Â§1.1).
By the Commutative Property of Addition we have
(f+g)(x) =f(x) +g(x) =g(x) +f(x) = (g+f)(x),
so that f+g=g+f. Axiom VS1 holds.
We have
f(x) + [g(x) +h(x)] = [ f(x) +g(x)] +h(x)
by the Associative Property of Addition, and thus f+ (g+h) = (f+g) +h. Axiom VS2 holds.
Letobe the zero function. That is, o(x) = 0 for all xâˆˆR. Since o(2) = 0 we see that oâˆˆ C.
Now,
(o+f)(x) =o(x) +f(x) = 0 + f(x) =f(x)
and
(f+o)(x) =f(x) +o(x) =f(x) + 0 = f(x),
and so o+f=f+o=f. Axiom VS3 holds.
As usual âˆ’fis the function given by ( âˆ’f)(x) =âˆ’f(x), so in particular ( âˆ’f)(2) = âˆ’f(2) = 0
implies that âˆ’fâˆˆ C. Now,
(âˆ’f+f)(x) = (âˆ’f)(x) +f(x) =âˆ’f(x) +f(x) = 0 = o(x)
shows that âˆ’f+f=o. Similarly f+ (âˆ’f) =o. Axiom VS4 holds.
By the Distributive Property,
 
a(f+g)
(x) =a(f+g)(x) =a[f(x) +g(x)] =af(x) +ag(x)
= (af)(x) + (ag)(x) = (af+ag)(x),
which shows that a(f+g) =af+ag. Axiom VS5 holds.
Again by the Distributive Property,
 
(a+b)f
(x) = (a+b)f(x) =af(x) +bf(x) = (af)(x) + (bf)(x) = (af+bf)(x),
so (a+b)f=af+bf. Axiom VS6 holds.
By the Associative Property of Multiplication,
 
a(bf)
(x) =a(bf)(x) =a(bf(x)) = ( ab)f(x) = 
(ab)f
(x),
soa(bf) = (ab)f. Axiom VS7 holds.
Finally, since 1 âˆˆRis the multiplicative identity, we have (1 f)(x) = 1 f(x) =f(x). This
shows that 1 f=f, and Axiom VS8 holds. â– 66
Problems
In Exercises 1â€“4 a set of objects Vis given, along with definitions for operations of vector
addition and scalar multiplication. Determine whether or not Vis a vector space under the
given operations. If it is not, indicate which axioms and closure properties fail to hold.
1.V=R2, with vector addition and scalar multiplication defined by
u1
u2
+
v1
v2
=
u1+v1
u2+v2
and c
u1
u2
=
9cu1
9cu2
.
2.V=R2, with vector addition and scalar multiplication defined by
u1
u2
+
v1
v2
=
u1+v1+ 3
u2+v2+ 3
and c
u1
u2
=
cu1
cu2
.
3.Vis the set of 2 Ã—2 matrices of the form
a0
1b
with the standard operations of matrix addition and scalar multiplication.
4.Vis the set of real-valued one-to-one functions with domain ( âˆ’âˆ,âˆ), together with the
zero function x7â†’0. For any f, gâˆˆVandcâˆˆR, the sum f+gand scalar product cfare
defined in the standard way.
5. Prove part (2) of Proposition 3.2.
6. Prove part (3) of Proposition 3.2.67
3.2 â€“ Subspaces
Definition 3.9. LetVbe a vector space. If WâŠ†Vis a vector space under the vector addition
and scalar multiplication operations defined on VÃ—VandFÃ—V, respectively, then Wis a
subspace ofV.
In order for WâŠ†Vto be a vector space it must satisfy the statement of Definition 3.1
to the letter , except that the symbol Wis substituted for V. Straightaway this means we
must have WÌ¸=âˆ…since Axiom VS3 requires that 0âˆˆW. Moreover, vector addition must
map WÃ—Wâ†’Wand scalar multiplication must map FÃ—Wâ†’W, which is to say for any
u,vâˆˆWandaâˆˆFwe must have u+vâˆˆWandauâˆˆW. These observations prove the
forward implication in the following theorem.
Theorem 3.10. LetVbe a vector space and âˆ…Ì¸=WâŠ†V. Then Wis a subspace of Vif and
only if auâˆˆWandu+vâˆˆWfor all aâˆˆFandu,vâˆˆW.
Proof. We need only prove the reverse implication. So, suppose that for any aâˆˆFand
u,vâˆˆW, it is true that auâˆˆWandu+vâˆˆW. Then vector addition maps WÃ—Wâ†’W
and scalar multiplication maps FÃ—Wâ†’W, and it remains to confirm that Wsatisfies the
eight axioms in Definition 3.1. But it is clear that Axioms VS1, VS2, VS5, VS6, VS7, and VS8
must hold. For instance if u,vâˆˆW, then u+v=v+usince u,vâˆˆVandVis given to be a
vector space, and so Axiom VS1 is confirmed.
LetuâˆˆW. Since auâˆˆWfor any aâˆˆF, it follows that ( âˆ’1)uâˆˆWin particular. Now,
(âˆ’1)u=âˆ’uby Proposition 3.3, and so âˆ’uâˆˆW. That is, for every uâˆˆWwe find that
âˆ’uâˆˆWas well, where u+ (âˆ’u) =âˆ’u+u=0. This shows that Axiom VS4 holds for W.
Finally, since auâˆˆWfor any aâˆˆF, it follows that 0 uâˆˆW. By Proposition 3.2 we have
0u=0, so0âˆˆWand Axiom VS3 holds for W.
We conclude that WâŠ†Vis a vector space under the vector addition and scalar multiplication
operations defined on VÃ—VandFÃ—V, respectively. Therefore Wis a subspace of Vby
Definition 3.9. â– 
The following result is immediate, and provides a checklist that commonly is employed to
quickly determine whether a subset of a vector space is a subspace.
Corollary 3.11. LetVbe a vector space, and let WâŠ†V. Then Wis a subspace of Vif the
following conditions hold:
1.0âˆˆW.
2.auâˆˆWfor all uâˆˆWandaâˆˆF.
3.u+vâˆˆWfor all u,vâˆˆW.
In practice, to determine whether any given subset of a vector space Vis a subspace the
first thing one usually checks is whether or not it contains the zero vector 0. IfWâŠ†Vdoes
not contain 0, then it is not a subspace.68
Example 3.12. Consider the set
U=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3:xyz= 0ï£¼
ï£½
ï£¾.
Certainly Uis a subset of R3, but is it a subspace of R3? Two vectors belonging to Uare
u1=ï£®
ï£°1
0
0ï£¹
ï£»and u2=ï£®
ï£°0
1
1ï£¹
ï£»,
since (1)(0)(0) = 0 and (0)(1)(1) = 0. However, the vector
u1+u2=ï£®
ï£°1
0
0ï£¹
ï£»+ï£®
ï£°0
1
1ï£¹
ï£»=ï£®
ï£°1
1
1ï£¹
ï£»
does not belong to Usince (1)(1)(1) Ì¸= 0. Since Uis not closed under vector addition, it is not
a subspace of R3. â– 
Example 3.13. Consider the set Skw n(R) ofnÃ—nskew-symmetric matrices with entries in R:
Skw n(R) ={AâˆˆRnÃ—n:AâŠ¤=âˆ’A}.
Clearly Skw n(R) is a subset of the vector space RnÃ—n, and since OâŠ¤
n=âˆ’Onwe see that Skw n(R)
contains the â€œzero vectorâ€ of RnÃ—n. Let A,BâˆˆSkw n(R) and câˆˆR. By Proposition 2.3,
(cA)âŠ¤=cAâŠ¤=c(âˆ’A) =âˆ’(cA)
and
(A+B)âŠ¤=AâŠ¤+BâŠ¤=âˆ’A+ (âˆ’B) =âˆ’(A+B),
which shows that cAâˆˆSkw n(R) and A+BâˆˆSkw n(R). Therefore Skw n(R) is a subspace by
Corollary 3.11. â– 
Example 3.14. As we saw in Â§2.5, a system of mlinear equations in nunknowns
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³a11x1+a12x2+Â·Â·Â·+a1nxn=b1
a21x1+a22x2+Â·Â·Â·+a2nxn=b2............
am1x1+am2x2+Â·Â·Â·+amnxn=bm(3.3)
may be written as a matrix equation Ax=b, where
A=ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1n
a21a22Â·Â·Â· a2n............
am1am2Â·Â·Â· amnï£¹
ï£ºï£ºï£»,x=ï£®
ï£¯ï£¯ï£°x1
x2...
xnï£¹
ï£ºï£ºï£»,b=ï£®
ï£¯ï£¯ï£°b1
b2...
bmï£¹
ï£ºï£ºï£». (3.4)69
Here each vector xinFnis represented by a column matrix as in (3.4), so that
Fn=ï£±
ï£²
ï£³ï£®
ï£°x1...
xnï£¹
ï£»x1, . . . , x nâˆˆFï£¼
ï£½
ï£¾.
As previously established, a vector
s=ï£®
ï£°s1...
snï£¹
ï£»
is asolution toAx=bif and only if substituting sforxinAx=bmakes the equation true,
and this will be the case if and only if the n-tuple ( s1, . . . , s n) is a solution to the system of
equations (3.3).
Now, if we set b=0, we obtain the matrix equation Ax=0representing the homogeneous
system in which the right-hand side of every equation in (3.3) is 0. The solution set for Ax=0
is the set
S={xâˆˆFn:Ax=0},
so clearly SâŠ†Fn. But is Sasubspace ofFn? Certainly A0=0is true, so 0âˆˆSandSÌ¸=âˆ….
To determine definitively whether Sis a subspace we use Corollary 3.11.
LetsâˆˆSandaâˆˆF. Since sis inSwe have As=0, and then
A(as) =a(As) =a0=0
shows that asâˆˆS. Next, if s,sâ€²âˆˆS, so that As=0andAsâ€²=0both hold, then
A(s+sâ€²) =As+Asâ€²=0+0=0
shows that s+sâ€²âˆˆSalso.
Therefore Sis a subspace of Fnby Corollary 3.11. We call Sthesolution space of the
system Ax=0. â– 
Definition 3.15. Thenull space ofAâˆˆFmÃ—nis the set
Nul(A) ={xâˆˆFn:Ax=0}.
Proposition 3.16. IfAâˆˆFmÃ—n, then Nul(A)is a subspace of Fn.
Proof. This follows easily from the proceedings of Example 3.14 since the null space of a matrix
Acorresponds to the solution space of the homogeneous system of linear equations Ax=0.â– 
Definition 3.17. LetVbe a subspace of Rn. The orthogonal complement ofVis the set
VâŠ¥={xâˆˆRn:xÂ·v= 0 for all vâˆˆV}.
Proposition 3.18. IfVis a subspace of Rn, then VâŠ¥is also a subspace of Rn.70
Proof. LetVbe a subspace of Rn. Suppose x,yâˆˆVâŠ¥. Then for any vâˆˆVwe have
(x+y)Â·v=xÂ·v+yÂ·v= 0 + 0 = 0 ,
which shows that x+yâˆˆVâŠ¥. Moreover, for any câˆˆRwe have
(cx)Â·v=c(xÂ·v) =c(0) = 0
for any vâˆˆV, which shows that cxâˆˆVâŠ¥. Since VâŠ¥âŠ†Rnis closed under scalar multiplication
and vector addition, we conclude that it is a subspace of Rn. â– 
Problems
1. Determine whether the set
W=
[x, y, z ]âŠ¤:y= 2xâˆ’z	
is a subspace of R3. If it is, prove it; otherwise show how it fails to be a subspace.
2.Prove or disprove that the set is a subspace of the vector space R2Ã—2of all 2 Ã—2 matrices
with real entries.
(a)
AâˆˆR2Ã—2:AâŠ¤=A	
(b)
a b
0c
:a, b, câˆˆR
(c)
a0
0a2
:aâˆˆR
(d)
a20
0b2
:a, bâˆˆR
3.Determine whether Symn(R), the set of nÃ—nsymmetric matrices with real entries, is a
subspace of RnÃ—n. If it is, prove it; otherwise show how it fails to be a subspace.
4.Prove or disprove that the set is a subspace of the vector space F(R,R) of all real-valued
functions fwith domain R.
(a){fâˆˆ F(R,R) :f(x)â‰¤0 for all xâˆˆR}
(b){fâˆˆ F(R,R) :f(0) = 0 }
(c){fâˆˆ F(R,R) :f(0) = 9 }
(d){fâˆˆ F(R,R) :fis a constant function }
(e){fâˆˆ F(R,R) :f(x) =acosx+bsinxfor some a, bâˆˆR}71
3.3 â€“ Subspace Sums and Direct Sums
Definition 3.19. LetUandWbe subspaces of a vector space V. The sum ofUandWis the
set of vectors
U+W={vâˆˆV:v=u+wfor some uâˆˆUandwâˆˆW}.
More generally, if U1, . . . , U nare subspaces of V, then the sum ofU1, . . . , U nis the set of
vectors
nX
k=1Uk=(
vâˆˆV:v=nX
k=1ukfor some ukâˆˆUk)
.
Equivalently we may write
U+W={u+w:uâˆˆUandwâˆˆW}
for subspaces UandWofV, and
nX
k=1Uk=(nX
k=1uk:ukâˆˆUk)
for subspaces U1, . . . , U kofV.
Proposition 3.20. IfU1, . . . , U nare subspaces of a vector space VoverF, then U1+Â·Â·Â·+Un
is also a subspace of V.
Proof. Suppose U1, . . . , U nare subspaces of a vector space V, and let U=U1+Â·Â·Â·+Un. Clearly
0âˆˆU, soUÌ¸=âˆ…. Let u,vâˆˆU, so that
u=nX
k=1ukand v=nX
k=1vk
for vectors uk,vkâˆˆUk, 1â‰¤kâ‰¤n. Now, uk+vkâˆˆUksince each Ukis closed under vector
addition, and hence
u+v=nX
k=1(uk+vk)âˆˆnX
k=1Uk=U
and we conclude that Uis closed under vector addition. Also, for any câˆˆFwe have cukâˆˆUk
since each Ukis closed under scalar multiplication, and hence
cu=nX
k=1cukâˆˆnX
k=1Uk=U
and we conclude that Uis closed under scalar multiplication. Therefore Uis a subspace of V
by Corollary 3.11. â– 72
Definition 3.21. LetUandWbe subspaces of a vector space V. We say Vis the direct sum
ofUandW, written V=UâŠ•W, ifV=U+WandUâˆ©W={0}.
More generally, let U1, . . . , U nbe subspaces of V. Then Vis the direct sum ofU1, . . . , U n,
written
V=nM
k=1Uk,
ifV=Pn
k=1Ukand
Uiâˆ©X
kÌ¸=iUk={0} (3.5)
for each i= 1, . . . , n .
In (3.5) itâ€™s understood that the sum is taken over all 1 â‰¤kâ‰¤nnot equal to i; that is,
X
kÌ¸=iUk=U1+Â·Â·Â·+Uiâˆ’1+Ui+1+Â·Â·Â·+Un.
Thus, in particular, if U1,U2, and U3are subspaces of V, then
V=3M
k=1Uk=U1âŠ•U2âŠ•U3
if and only if
V=U1+U2+U3
and
U1âˆ©(U2+U3) =U2âˆ©(U1+U3) =U3âˆ©(U1+U2) ={0}.
Proposition 3.22. LetUandWbe subspaces of V. Then V=UâŠ•Wif and only if for each
vâˆˆVthere exist unique vectors uâˆˆUandwâˆˆWsuch that v=u+w.
To say there exist unique vectors uâˆˆUandwâˆˆWsuch that v=u+wmeans, specifically,
that if u,uâ€²âˆˆUandw,wâ€²âˆˆWare such that v=u+wandv=uâ€²+wâ€², then we must have
u=uâ€²andw=wâ€². We now prove the proposition.
Proof. Suppose that V=UâŠ•W, and let vâˆˆV. Since V=U+Wthere exist some uâˆˆU
andwâˆˆWsuch that v=u+w. Suppose uâ€²âˆˆUandwâ€²âˆˆWare such that v=uâ€²+wâ€². Then
0=vâˆ’v= (u+w)âˆ’(uâ€²+wâ€²) = (uâˆ’uâ€²) + (wâˆ’wâ€²),
which implies that
uâˆ’uâ€²=wâ€²âˆ’w
and hence uâˆ’uâ€²,wâ€²âˆ’wâˆˆUâˆ©Wsinceuâˆ’uâ€²âˆˆUandwâ€²âˆ’wâˆˆW. However, from V=UâŠ•W
we have Uâˆ©W={0}, leading to
uâˆ’uâ€²=wâ€²âˆ’w=0
and therefore uâ€²=uandwâ€²=w.73
Conversely, suppose that for each vâˆˆVthere exists unique vectors uâˆˆUandwâˆˆWsuch
thatv=u+w. Then vâˆˆU+W, so clearly V=U+W. Suppose that vâˆˆUâˆ©W. Then we
may take uâˆˆUto be v, and wâˆˆWto be 0, so that
u+w=v+0=v;
on the other hand if we let uâ€²=0andwâ€²=v, then uâ€²âˆˆUandwâ€²âˆˆWare such that
v=uâ€²+wâ€². By our uniqueness hypothesis we must have u=uâ€²andw=wâ€². That is, u=0
andw=0, so that v=0and we obtain vâˆˆ {0}. From this we conclude that Uâˆ©WâŠ† {0},
and since the reverse containment is obvious, we find that both Uâˆ©W={0}andV=U+W
are true. Therefore V=UâŠ•W. â– 
Proposition 3.22 and its proof are presented largely for pedagogical reasons. The more
general result is given next, though it takes a bit more work to prove and will have limited
applicability in the next few chapters.
Theorem 3.23. LetU1, . . . , U nbe subspaces of V. Then V=U1âŠ• Â·Â·Â· âŠ• Unif and only if for
eachvâˆˆVthere exist unique vectors u1âˆˆU1, . . . ,unâˆˆUnsuch that v=u1+Â·Â·Â·+un.
Proof. Suppose that V=U1âŠ• Â·Â·Â· âŠ• Un. Let vâˆˆV, so for each 1 â‰¤kâ‰¤nthere exists some
ukâˆˆUksuch that v=Pn
k=1uk. Now, suppose that v=Pn
k=1uâ€²
k, where uâ€²
kâˆˆUkfor each k.
Fix 1â‰¤iâ‰¤n. We have uâ€²
iâˆ’uiâˆˆUi, and from
nX
k=1(ukâˆ’uâ€²
k) =nX
k=1ukâˆ’nX
k=1uâ€²
k=vâˆ’v=0
we obtain
uâ€²
iâˆ’ui=X
kÌ¸=i(ukâˆ’uâ€²
k)âˆˆX
kÌ¸=iUk.
That is,
uâ€²
iâˆ’uiâˆˆUiâˆ©X
kÌ¸=iUk={0},
so that uâ€²
iâˆ’ui=0and hence uâ€²
i=ui. Since 1 â‰¤iâ‰¤nis arbitrary we conclude that
uâ€²
1=u1, . . . ,uâ€²
n=un, and therefore the vectors u1âˆˆU1, . . . ,unâˆˆUnfor which v=Pn
k=1uk
are unique.
Next, suppose that for each vâˆˆVthere exist unique vectors u1âˆˆU1, . . . ,unâˆˆUnsuch that
v=Pn
k=1uk. Then it is clear that
V=nX
k=1Uk. (3.6)
Fix 1â‰¤iâ‰¤n, and suppose that
vâˆˆUiâˆ©X
kÌ¸=iUk.
Thus vâˆˆUiimplies we have u=Pn
k=1uk, where ukâˆˆUkis0forkÌ¸=i, and ui=vâˆˆUi. On
the other hand vâˆˆP
kÌ¸=iUkimplies that, for each kÌ¸=ithere exists some uâ€²
kâˆˆUksuch that74
v=P
kÌ¸=iuâ€²
k, and so if we let uâ€²
iâˆˆUibe0, we obtain v=Pn
k=1uâ€²
k. Now, by our uniqueness
hypothesis it must be that uk=uâ€²
kfor each 1 â‰¤kâ‰¤n. In particular,
v=ui=uâ€²
i=0,
and so vâˆˆ {0}. This shows that Uiâˆ©P
kÌ¸=iUkâŠ† {0}, and since the reverse containment is
obvious, we conclude that
Uiâˆ©X
kÌ¸=iUk={0}. (3.7)
Now, the equations (3.6) and (3.7) imply that V=U1âŠ• Â·Â·Â· âŠ• Un. â– 75
3.4 â€“ Linear Combinations and Spans
Definition 3.24. A vector vis called a linear combination of the vectors v1, . . . ,vnif there
exist scalars c1, . . . , c nsuch that
v=c1v1+Â·Â·Â·+cnvn=nX
i=1civi.
Example 3.25. Define u,v,wâˆˆR3by
u=ï£®
ï£°2
âˆ’3
5ï£¹
ï£»,v=ï£®
ï£°0
7
âˆ’1ï£¹
ï£»,and w=ï£®
ï£°4
1
9ï£¹
ï£».
Show that wis a linear combination of uandv.
Solution. We must find scalars aandbsuch that
w=au+bv=ï£®
ï£°2a
âˆ’3a
5aï£¹
ï£»+ï£®
ï£°0
7b
âˆ’bï£¹
ï£»=ï£®
ï£°2a
âˆ’3a+ 7b
5aâˆ’bï£¹
ï£».
That is, we need aandbto satisfy
ï£®
ï£°2a
âˆ’3a+ 7b
5aâˆ’bï£¹
ï£»=ï£®
ï£°4
1
9ï£¹
ï£»,
which is the system of equations
(2a = 4
âˆ’3a+ 7b= 1
5aâˆ’b= 9
From the first equation we have a= 2. Substituting this into the second equation yields
âˆ’6 + 7 b= 1, or b= 1. Now we must determine whether ( a, b) = (2 ,1) satisfies the third
equation, in which general is unlikely but in this case works:
5aâˆ’b= 9â‡’5(2)âˆ’1 = 9 â‡’9 = 9 .
Sow= 2u+v, and therefore wis a linear combination of uandv. â– 
Example 3.26. Define u,v,wâˆˆR3by
u=ï£®
ï£°2
âˆ’3
5ï£¹
ï£»,v=ï£®
ï£°0
7
âˆ’1ï£¹
ï£»,and w=ï£®
ï£°4
âˆ’13
9ï£¹
ï£».
Show that wis not a linear combination of uandv.76
Solution. We must show that there exist no scalars aandbsuch thatï£®
ï£°4
âˆ’13
9ï£¹
ï£»=w=au+bv=ï£®
ï£°2a
âˆ’3a+ 7b
5aâˆ’bï£¹
ï£»,
which sets up the system of equations
(2a = 4
âˆ’3a+ 7b=âˆ’13
5aâˆ’b= 9
The first equation gives a= 2. Substituting this into the second equation yields âˆ’6 + 7b=âˆ’13,
orb=âˆ’1. However, putting ( a, b) = (2 ,âˆ’1) into the third equation yields a contradiction:
5aâˆ’b= 9â‡’5(2)âˆ’(âˆ’1) = 9 â‡’11 = 9 .
Hence the system of equations has no solution, which is to say there are no scalars aandbfor
which w=au+bv. â– 
Definition 3.27. LetVbe a vector space and v1, . . . ,vnâˆˆV. We say vectors v1, . . . ,vnspan
V, orVisspanned by the set {v1, . . . ,vn}, if for every vâˆˆVthere exist scalars c1, . . . , c n
such that v=c1v1+Â·Â·Â·+cnvn.5
Thus vectors v1, . . . ,vnspan Vif and only if every vector in Vis expressible as a linear
combination of v1, . . . ,vn. Define the span ofv1, . . . ,vnto be the set
Span{v1, . . . ,vn}=(nX
i=1civi:c1, . . . , c nâˆˆF)
,
which is to say Span{v1, . . . ,vn}is the set of allpossible linear combinations of the vectors
v1, . . . ,vnâˆˆV. It is easy to see in light of the closure properties (3.1) and(3.2) that Vis
spanned by {v1, . . . ,vn}if and only if
V= Span {v1, . . . ,vn}.
IfSis an arbitrary subset of a vector space VoverF, then Span (S) is defined to be the set
of all linear combinations of finitely many vectors belonging to S. Precisely put,
Span( S) =(nX
k=1ckvk:nâˆˆN,v1, . . . ,vnâˆˆS, and c1, . . . , c nâˆˆF)
.
This definition allows us to speak meaningfully of the span of an infinite set, in particular.
Example 3.28. Determine whether the vectors
v1=ï£®
ï£°1
1
1ï£¹
ï£»,v2=ï£®
ï£°2
2
0ï£¹
ï£»,v3=ï£®
ï£°3
0
0ï£¹
ï£»
spanR3.
5Some books say v1, . . . , vngenerate V, orVisgenerated by the set {v1, . . . , vn}.77
Solution. Let
x=ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3.
We attempt to find scalars c1, c2, c3âˆˆRsuch that c1v1+c2v2+c3v3=x; that is,
c1ï£®
ï£°1
1
1ï£¹
ï£»+c2ï£®
ï£°2
2
0ï£¹
ï£»+c3ï£®
ï£°3
0
0ï£¹
ï£»=ï£®
ï£°x
y
zï£¹
ï£».
This yields the system(c1+ 2c2+ 3c3=x
c1+ 2c2 =y
c1 =z
which indeed has a solution:
(c1, c2, c3) =
z,yâˆ’z
2,xâˆ’y
3
.
Thus every vector in R3is expressible as a linear combination of v1,v2, and v3, which shows
that the set {v1,v2,v3}spans R3. â– 
Example 3.29. Determine whether the vectors
v1=ï£®
ï£°2
âˆ’1
3ï£¹
ï£»,v2=ï£®
ï£°4
1
2ï£¹
ï£»,v3=ï£®
ï£°8
âˆ’1
8ï£¹
ï£»
spanR3.
Solution. Let
x=ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3.
We attempt to find scalars c1, c2, c3âˆˆRsuch that c1v1+c2v2+c3v3=x. This yields the system
(2c1+ 4c2+ 8c3=x
âˆ’c1+c2âˆ’c3=y
3c1+ 2c2+ 8c3=z
This can be cast as an augmented matrix and manipulated using elementary row operations:
ï£®
ï£°2 4 8 x
âˆ’1 1âˆ’1y
3 2 8 zï£¹
ï£»âˆ¼ï£®
ï£°âˆ’1 1âˆ’1y
2 4 8 x
3 2 8 zï£¹
ï£»âˆ¼ï£®
ï£°âˆ’1 1âˆ’1 y
0 6 6 2y+x
0 5 5 3y+zï£¹
ï£»
âˆ¼ï£®
ï£¯ï£°1âˆ’1 1 âˆ’y
0 1 12y+x
6
0 5 5 3y+zï£¹
ï£ºï£»âˆ¼ï£®
ï£¯ï£°1âˆ’1 1 âˆ’y
0 1 12y+x
6
0 0 0 3y+zâˆ’5 2y+x
6ï£¹
ï£ºï£»78
We see that in order for xto be expressed as a linear combination of v1,v2, and v3, we need x,
y, and zsuch that
3y+zâˆ’52y+x
6
= 0,
or
5xâˆ’8yâˆ’6z= 0.
This leads to 1 = 0 if we choose x= 0, y= 0, and z= 1, for instance. That is, we cannot
expressï£®
ï£°0
0
1ï£¹
ï£»
as a linear combination of v1,v2, and v3. We conclude that {v1,v2,v3}does not span R3.â– 
Proposition 3.30. LetVbe a vector space. If v1, . . . ,vnâˆˆV, then W=Span{v1, . . . ,vn}is
a subspace of V.
Proof. Suppose v1, . . . ,vnâˆˆV. First we observe that
0= 0v1+Â·Â·Â·+ 0vnâˆˆW.
Now, let aâˆˆF, and let uâˆˆWso that there exist c1, . . . , c nâˆˆFsuch that
u=c1v1+Â·Â·Â·+cnvn.
Since
au=a(c1v1+Â·Â·Â·+cnvn) =ac1v1+Â·Â·Â·+acnvn
forac1, . . . , ac nâˆˆF, it follows that auâˆˆWalso.
Next, let u,vâˆˆW. Then there exist c1, . . . , c n, d1, . . . , d nâˆˆFsuch that
u=c1v1+Â·Â·Â·+cnvnand v=d1v1+Â·Â·Â·+dnvn,
and then
u+v= (c1+d1)v1+Â·Â·Â·+ (cn+dn)vn
forc1+d1, . . . , c n+dnâˆˆFshows that u+vâˆˆWalso.
Therefore Wis a subspace of Vby Corollary 3.11. â– 
Proposition 3.31. LetVbe a vector space, and let S={v1, . . . ,vn} âŠ†V. IfaâˆˆFis nonzero,
then
Span( S) = Span 
(S\ {vi})âˆª {vi+avj}
for any i, jâˆˆ {1, . . . , n }withiÌ¸=j.
Proof. Suppose aâˆˆF\{0}, and let i, jâˆˆ {1, . . . , n }with iÌ¸=j. Let T= (S\{vi})âˆª{vi+avj},
and note that Tis the set obtained from Sby replacing viwithvi+avj. Suppose vâˆˆSpan (S),
so that v=Pn
k=1ckvkfor some c1, . . . , c nâˆˆF. Now,
v=civi+cjvj+X
kÌ¸=i,jckvk=ci(vi+avj) + (cjâˆ’aci)vj+X
kÌ¸=i,jckvk,79
which shows that vis a linear combination of the elements of Tand hence vâˆˆSpan( T).
Next, suppose that vâˆˆSpan( T), so there exists c1, . . . , c nâˆˆFsuch that
v=ci(vi+avj) +X
kÌ¸=ickvk,
and hence
v=nX
k=1câ€²
kvk
with câ€²
k=ckforkÌ¸=jandcâ€²
j=aci+cj, which shows that vis a linear combination of elements
ofSand hence vâˆˆSpan( S).
Since Span (S)âŠ†Span (T) and Span (T)âŠ†Span (S), we conclude that Span (S) =Span (T)
as was to be shown. â– 80
Problems
1. Let
u1=
âˆ’1
3
and u2=
2
âˆ’6
.
Prove or disprove that Span {u1,u2}=R2.
2. Determine which of the following are linear combinations of vectors
u=ï£®
ï£°1
âˆ’1
3ï£¹
ï£»and v=ï£®
ï£°2
4
0ï£¹
ï£».
(a)w= [âˆ’1,âˆ’11,9]âŠ¤
(b)w= [3,7,âˆ’2]âŠ¤
3. Express each polynomial as linear combinations of
p1= 2 + x+ 4x2, p 2= 1âˆ’x+ 3x2,and p3= 3 + 2 x+ 5x2.
(a) 6
(b) 2 + 6 x2
(c) 5 + 9 x+ 5x2
4. Determine whether the given vectors span R3.
(a)v1= [3,3,3]âŠ¤,v2= [âˆ’2,âˆ’2,0]âŠ¤,v3= [1,0,0]âŠ¤
(b)v1= [1,âˆ’1,3]âŠ¤,v2= [4,0,2]âŠ¤,v3= [6,âˆ’1,6]âŠ¤
(c)v1= [3,1,4]âŠ¤,v2= [2,âˆ’3,5]âŠ¤,v3= [5,âˆ’2,9]âŠ¤,v4= [1,4,âˆ’1]âŠ¤
(d)v1= [1,3,3]âŠ¤,v2= [1,3,4]âŠ¤,v3= [1,4,3]âŠ¤,v4= [6,2,1]âŠ¤
5. Determine whether the polynomials
p1= 1 + 2 xâˆ’x2p2= 3 + x2
p3= 5 + 4 xâˆ’x2p4=âˆ’2 + 2 xâˆ’2x2
span the vector space P2(R) ={a+bx+cx2:a, b, câˆˆR}.81
3.5 â€“ Linear Independence and Bases
Definition 3.32. LetVbe a vector space and A={v1, . . . ,vn} âŠ†Vbe nonempty. If the
equation
c1v1+Â·Â·Â·+cnvn=0 (3.8)
admits only the trivial solution c1=Â·Â·Â·=cn= 0, then we call Aalinearly independent set
andv1, . . . ,vnarelinearly independent vectors . Otherwise we call Aalinearly dependent
setandv1, . . . ,vnarelinearly dependent vectors .
An arbitrary set SâŠ†Vislinearly independent if every finite subset of Sis linearly
independent. Otherwise Sislinearly dependent .
It is straightforward to show that the definition for linear independence of an arbitrary set S
is equivalent to the definition for linear independence of A={v1, . . . ,vn} Ì¸=âˆ…in the case when
Sis a nonempty finite set. Thus, the second paragraph of Definition 3.32 is the more general
definition of linear independence.
A careful reading of Definition 3.32 should make clear that vectors v1, . . . ,vnâˆˆVare
linearly dependent if and only if there exist scalars c1, . . . , c nnot all zero such that (3.8) is
satisfied. Also, an arbitrary set Sis linearly dependent if and only if there exists some finite set
{v1, . . . ,vn} âŠ†Sfor which (3.8) has a nontrivial solution.
Theorem 3.33. LetAbe a row-echelon matrix. Then the nonzero row vectors of Aare linearly
independent, and the column vectors of Athat contain a pivot are linearly independent.
Proof. We shall prove the second statement concerning the column vectors using induction,
and leave the proof of the first statement (which is quite similar) as a problem.
LetmâˆˆNbe arbitrary. It is clear that if AâˆˆFmis a row-echelon matrix with a pivot, then
its single column vector constitutes a linearly independent set. Let nâˆˆNbe arbitrary, and
suppose that the pivot columns of any row-echelon matrix AâˆˆFmÃ—nare linearly independent.
LetAâˆˆFmÃ—(n+1)be a row-echelon matrix. Then the matrix BâˆˆFmÃ—nthat results from
deleting column n+ 1 from Ais also a row-echelon matrix, and so its pivot columns p1, . . . ,pr
are linearly independent by inductive hypothesis. Now, if column n+ 1 of Ais not a pivot
column, then the pivot columns of Aare precisely p1, . . . ,pr, and we conclude that the pivot
columns of Aare linearly independent.
Suppose rather that column n+ 1 of Ais a pivot column. Then the pivot columns of A
are precisely p1, . . . ,prandq, where q= [q1Â·Â·Â·qm]âŠ¤denotes column n+ 1 of A. For each
1â‰¤jâ‰¤rlet
pj=ï£®
ï£°p1j...
pmjï£¹
ï£».
Since qis a pivot column, there exists some 1 â‰¤â„“â‰¤msuch that qâ„“is a pivot of A, and then
by the definition of a pivot we have pâ„“j= 0 for all 1 â‰¤jâ‰¤r. Suppose c1, . . . , c r, aâˆˆFare such
that
c1p1+Â·Â·Â·+crpr+aq=0. (3.9)82
This yields
c1pâ„“1+Â·Â·Â·+crpâ„“r+aqâ„“= 0,
which reduces to aqâ„“= 0, and since qâ„“Ì¸= 0 on account of being a pivot, we finally obtain a= 0.
Hence
c1p1+Â·Â·Â·+crpr=0,
and since p1, . . . ,prare linearly independent, it follows that cj= 0 for 1 â‰¤jâ‰¤r. This shows
that(3.9) only admits the trivial solution, and therefore {p1, . . . ,pr,q}is a linearly independent
set. That is, the pivot columns of AâˆˆFmÃ—(n+1)are linearly independent, and we conclude by
induction that the pivot columns of any row-echelon matrix are linearly independent. â– 
Recall the vector space F(S,F) of functions Sâ†’Fintroduced in Example 3.7. A linear
combination of f1, f2, . . . , f nâˆˆ F(S,F) is an expression of the form
c1f1+c2f2+Â·Â·Â·+cnfn
for some choice of constants c1, c2, . . . , c nâˆˆF, which of course is itself a function in F(S,F)
given by
(c1f1+c2f2+Â·Â·Â·+cnfn)(x) =c1f1(x) +c2f2(x) +Â·Â·Â·+cnfn(x)
for all xâˆˆS. To write
c1f1+c2f2+Â·Â·Â·+cnfn= 0 (3.10)
means
(c1f1+c2f2+Â·Â·Â·+cnfn)(x) = 0
for all xâˆˆS; that is, c1f1+c2f2+Â·Â·Â·+cnfnis the zero function 0 : Sâ†’ {0}.
We say f1, f2, . . . , f nâˆˆ F(S,F) arelinearly independent on Sif (3.10) implies that
c1=c2=Â·Â·Â·=cn= 0.
Functions that are not linearly independent on Sare said to be linearly dependent on S.
Thus, f1, f2, . . . , f nare linearly dependent on Sif there can be found constants c1, c2, . . . , c nâˆˆF,
not all zero , such that ( c1f1+c2f2+Â·Â·Â·+cnfn)(x) = 0 for all (and it must beall)xâˆˆS.
Example 3.34. Consider the functions f, g:Râ†’Rgiven by
f(t) =eatand g(t) =ebt
fora, bÌ¸= 0 such that aÌ¸=b. To show that fandg(as vectors in the vector space RR) are
linearly independent on R, we start by supposing that c1, c2âˆˆRare such that
c1f+c2g= 0.
That is, the constants c1andc2are such that
c1eat+c2ebt=c1f(t) +c2g(t) = (c1f+c2g)(t) = 0
for all tâˆˆR. Thus, by choosing t= 0 and t= 1, we have in particular
c1+c2= 0 and c1ea+c2eb= 0.83
From the first equation we have
c2=âˆ’c1, (3.11)
which, when put into the second equation, yields
c1eaâˆ’c1eb= 0,
and thus
c1(eaâˆ’eb) = 0 . (3.12)
From aÌ¸=bwe have
ea= exp( a)Ì¸= exp( b) =eb
since the exponential function is one-to-one as established in Â§7.2 of the Calculus Notes , so
eaâˆ’ebÌ¸= 0 and from equations (3.12) and(3.11) we conclude that c1=c2= 0. Therefore the
functions fandg, which is to say eatandebt, are linearly independent on Rfor any distinct
nonzero real numbers aandb. â– 
Example 3.35. Show that the functions 1, x, and x2are linearly independent on any open
interval IâŠ†(0,âˆ).
Solution. LetIbe an interval in (0 ,âˆ), so that I= (a, b) for some 0 < a < b â‰¤ âˆ. From
analysis we know there can be found some Ï >1 such that a < Ïa < 2Ïa < 3Ïa < b . To show
that the functions 1, x, and x2(as vectors in the space RI) are linearly independent on I, we
suppose that c1, c2, c3âˆˆRare such that
c1+c2x+c3x2= 0. (3.13)
for all xâˆˆI. Substituting Ïa, 2Ïa, and 3 Ïaforxin (3.13) yields the system
ï£±
ï£²
ï£³c1+ (Ïa)c2+ (Ïa)2c3= 0
c1+ (2Ïa)c2+ (2Ïa)2c3= 0
c1+ (3Ïa)c2+ (3Ïa)2c3= 0
We can employ Gaussian Elimination to help solve this system for c1,c2, and c3:ï£®
ï£°1Ïa (Ïa)20
1 2Ïa4(Ïa)20
1 3Ïa9(Ïa)20ï£¹
ï£»âˆ’r1+r2â†’r2
âˆ’r1+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1Ïa (Ïa)20
0Ïa3(Ïa)20
0 2Ïa8(Ïa)20ï£¹
ï£»âˆ’2r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1Ïa (Ïa)20
0Ïa3(Ïa)20
0 0 2( Ïa)20ï£¹
ï£»r2Ã·Ïaâ†’r2
r3Ã·2(Ïa)2â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1Ïa(Ïa)20
0 1 3 Ïa0
0 0 1 0ï£¹
ï£».
Thus we now have the system
ï£±
ï£²
ï£³c1+ (Ïa)c2+ (Ïa)2c3= 0
c2+ (3Ïa)c3= 0
c3= 0
from which it easily follows that c1=c2=c3= 0. This shows that the set {1, x, x2}is a linearly
independent set of functions in RIfor any open interval IâŠ†(0,âˆ). â– 84
Remark. The basic approach exhibited in Example 3.35 can, with minor modifications, be
used to show that
{1, x, x2, . . . , xn}
is linearly independent in RIforanyinterval IâŠ†Rand integer nâ‰¥0.
Example 3.36. Consider the functions
x7â†’cos 2x, x 7â†’cos2x, x 7â†’sin2x
with domain R. Suppose c1, c2, c3âˆˆRare such that
c1cos 2x+c2cos2x+c3sin2x= 0 (3.14)
for all xâˆˆR. The functions cos2x,cos2x, and sin2xare linearly independent on Rif and only
if the only way to satisfy (3.14) for all xâˆˆRis to have c1=c2=c3= 0. However, it is true
that
cos 2x= cos2xâˆ’sin2x
onR, and hence (3.14) is equivalent to the equation
c1(cos2xâˆ’sin2x) +c2cos2x+c3sin2x= 0.
Now notice that this equation, and subsequently (3.14) , is satisfied for all xâˆˆRif we let c1= 1,
c2=âˆ’1, and c3= 1. So (3.14) has a nontrivial solution on R, and therefore the functions
cos 2x, cos2x, and sin2xare linearly dependent on R. â– 
Proposition 3.37. LetVbe a vector space.
1.The set {0} âŠ†Vis linearly dependent.
2.The empty set âˆ…is linearly independent.
Proof.
Proof of Part (1). The equation c0=0is satisfied by letting c= 1. Since this is a nontrivial
solution, it follows that {0}is linearly dependent.
Proof of Part (2). From Definition 3.32 an arbitrary set Sis linearly independent if and only if
the following statement (P) is true: â€œIf v1, . . . ,vnâˆˆS, then v1, . . . ,vnare linearly independent.â€
However if S=âˆ…, then the statement â€œ v1, . . . ,vnâˆˆSâ€ is necessarily false, and therefore (P) is
vacuously true. We conclude that âˆ…is linearly independent. â– 
Proposition 3.38. LetVbe a vector space. If v1, . . . ,vnâˆˆVare linearly independent and
x1v1+Â·Â·Â·+xnvn=y1v1+Â·Â·Â·+ynvn
for scalars x1, . . . , x nandy1, . . . , y n, then xi=yifor all 1â‰¤iâ‰¤n.85
Proof. Suppose that v1, . . . ,vnâˆˆVare linearly independent and
nX
i=1xivi=nX
i=1yivi
for scalars xiandyi. Then
nX
i=1(xiâˆ’yi)vi=0,
and since the vectors viare linearly independent, it follows that xiâˆ’yi= 0 for 1 â‰¤iâ‰¤n. That
is,xi=yifor 1â‰¤iâ‰¤n. â– 
Proposition 3.39. Suppose Vis a vector space, and S={v1, . . . ,vn}is a linearly independent
set in V. Given wâˆˆV, the set Sâˆª {w}is linearly dependent if and only if wâˆˆSpan( S).
Proof. Suppose that Sâˆª {w}is linearly dependent. Then the equation
x1v1+Â·Â·Â·+xnvn+xn+1w= 0
has a nontrivial solution, which is to say at least one of the coefficients x1, . . . , x n+1is nonzero.
Ifxn+1= 0, then xkÌ¸= 0 for some 1 â‰¤kâ‰¤n, in which case
x1v1+Â·Â·Â·+xnvn= 0
has a nontrivial solution and we conclude that v1, . . . ,vnare linearly dependentâ€”a contradiction.
Hence xn+1Ì¸= 0, and we may write
w=nX
k=1âˆ’xk
xn+1vk,
which shows that wâˆˆSpan( S).
Conversely, suppose that wâˆˆSpan( S), so that
w=a1v1+Â·Â·Â·+anvn
for some a1, . . . , a nâˆˆF. If we choose xk=âˆ’akfor each 1 â‰¤kâ‰¤n, and let xn+1= 1, then
x1v1+Â·Â·Â·+xnvn+xn+1w=âˆ’a1v1âˆ’ Â·Â·Â· âˆ’ anvn+w
=âˆ’(a1v1+Â·Â·Â·+anvn) + (a1v1+Â·Â·Â·+anvn)
=0,
and hence
x1v1+Â·Â·Â·+xnvn+xn+1w=0.
has a nontrivial solution. Therefore Sâˆª {w}is a linearly dependent set. â– 
Definition 3.40. Abasis for a vector space Vis a linearly independent set B âŠ†Vsuch that
Span (B) =V. In the case of the trivial vector space {0}we take the basis to be âˆ…, the empty
set.86
A basis Bis frequently indexed ; that is, there exists an index set Iof positive integers
together with a function Iâ†’ B that pairs each element of Bwith a unique kâˆˆI. Typically I
is either {1, . . . , n }for some nâˆˆN, or else I=N. In this fashion the vectors in Bareordered
according to the integers to which they are paired, with a symbol such as vkbeing used to
denote the vector that is paired with the integer kâˆˆI. IfBis an indexed set containing n
vectors that we wish to list explicitly, then the list is most properly presented as an n-tuple,
B= (v1, . . . ,vn),
rather than as a set B={v1, . . . ,vn}. We will adhere to this practice in all situations in which
the order of the vectors in Bis important.
Theorem 3.41. If{v1, . . . ,vn}is a basis for V, then for any vâˆˆVthere exist unique scalars
x1, . . . , x nfor which v=x1v1+Â·Â·Â·+xnvn.
Proof. Suppose that {v1, . . . ,vn}is a basis for V, and let vâˆˆV. Since v1, . . . ,vnspan V,
there exist scalars x1, . . . , x nsuch that
v=x1v1+Â·Â·Â·+xnvn.
Now, suppose
v=y1v1+Â·Â·Â·+ynvn
for scalars y1, . . . , y n, so that
x1v1+Â·Â·Â·+xnvn=y1v1+Â·Â·Â·+ynvn.
Then since v1, . . . ,vnare linearly independent we must have yi=xifor all 1 â‰¤iâ‰¤n
by Proposition 3.38. Therefore the scalars x1, . . . , x nfor which v=x1v1+Â·Â·Â·+xnvnare
unique. â– 
The following proposition pertaining to R2will be verified using only the most basic algebra.
A more general result applying to Rnfor all nâ‰¥2 must wait until later, when more sophisticated
machinery will have been built to allow for a far more elegant proof.
Proposition 3.42. Let[a, b]âŠ¤,[c, d]âŠ¤âˆˆR2.
1. [a, b]âŠ¤and[c, d]âŠ¤are linearly dependent if and only if adâˆ’bc= 0.
2.If[a, b]âŠ¤and[c, d]âŠ¤are linearly independent, then they form a basis for R2.
Proof.
Proof of Part (1). Suppose that [ a, b]âŠ¤and [ c, d]âŠ¤are linearly dependent. Then there exist
scalars rands, not both zero, such that
r
a
b
+s
c
d
=
0
0
.
This vector equation gives rise to the system
ar+cs= 0,(Ïµ1)
br+ds= 0,(Ïµ2)87
IfrÌ¸= 0, then d(Ïµ1)âˆ’c(Ïµ2) (i.e. dtimes equation ( Ïµ1) minus ctimes equation ( Ïµ2)) yields
adrâˆ’bcr= 0, or ( adâˆ’bc)r= 0. Since rÌ¸= 0, we conclude that adâˆ’bc= 0.
IfsÌ¸= 0, then âˆ’b(Ïµ1) +a(Ïµ2) yields âˆ’bcs+ads= 0, or ( adâˆ’bc)s= 0. Since sÌ¸= 0, we
conclude that adâˆ’bc= 0 once more.
Now, we have that either rÌ¸= 0 or sÌ¸= 0, both of which lead to the conclusion that adâˆ’bc= 0
and so the forward implication of part (1) is proven.
Suppose next that adâˆ’bc= 0. We must find scalars xandy, not both 0, such that
x
a
b
+y
c
d
=
0
0
. (3.15)
This vector equation gives rise to the system

ax+cy= 0,(Ïµ3)
bx+dy= 0,(Ïµ4)
Assume first that aÌ¸= 0. Then from ( Ïµ3) we have x=âˆ’cy/a, and from âˆ’b(Ïµ3) +a(Ïµ4) we obtain
âˆ’bcy+ady= 0 and then ( adâˆ’bc)y= 0. Since adâˆ’bc= 0, we may satisfy ( adâˆ’bc)y= 0 by
letting y=a, and then x=âˆ’cy/a =âˆ’c. Itâ€™s easy to check that x=âˆ’candy=aÌ¸= 0 will
satisfy (3.15), and thus [ a, b]âŠ¤and [ c, d]âŠ¤are linearly dependent.
Now assume that a= 0. Then adâˆ’bc= 0 implies that bc= 0, and so either b= 0 or c= 0.
Butb= 0 leads us to [ a, b]âŠ¤= [0,0]âŠ¤, in which case [ a, b]âŠ¤and [ c, d]âŠ¤are linearly dependent.
Suppose that c= 0 and bÌ¸= 0. Then equation ( Ïµ3) in the system above vanishes, and only ( Ïµ4)
remains to give x=âˆ’dy/b. If we let y=b, then x=âˆ’dy/b =âˆ’d. Itâ€™s easy to check that
x=âˆ’dandy=bÌ¸= 0, together with our assumptions that a= 0 and c= 0, will satisfy (3.15) .
Since either a= 0 or aÌ¸= 0 must be the case, and both lead to the conclusion that xand
ymay be chosen such that both arenâ€™t zero and (3.15) is satisfied, it follows that [ a, b]âŠ¤and
[c, d]âŠ¤must be linearly dependent. The reverse implication of part (1) is proven.
Proof of Part (2). Suppose that [ a, b]âŠ¤and [ c, d]âŠ¤are linearly independent. To show that the
vectors form a basis for R2, we need only verify that
R2= Span
a
b
,
c
d
.
Let [x1, x2]âŠ¤âˆˆR2. Scalars s1ands2must be found so that

x1
x2
=s1
a
b
+s2
c
d
. (3.16)
This gives rise to the system
as1+cs2=x1,(Ïµ5)
bs1+ds2=x2,(Ïµ6)
in which s1ands2are the unknowns. From âˆ’b(Ïµ5) +a(Ïµ6) comes ( adâˆ’bc)s2=ax2âˆ’bx1, and
since by part (1) the linear independence of [ a, b] and [ c, d]âŠ¤implies that adâˆ’bcÌ¸= 0, we obtain
s2=ax2âˆ’bx1
adâˆ’bc88
Putting this into ( Ïµ5) and solving for s1yields
s1=1
a
x1âˆ’ax2âˆ’bx1
adâˆ’bcc
if we assume that aÌ¸= 0, which shows that there exist scalars s1ands2that satisfy (3.16).
Ifa= 0, then adâˆ’bcÌ¸= 0 becomes bcÌ¸= 0 and thus b, cÌ¸= 0. Since ( Ïµ5) is now just cs2=x1
andcÌ¸= 0, we obtain s2=x1/c. Putting this into ( Ïµ6) gives
bs1+dx1
c=x2â‡’s1=1
b
x2âˆ’dx1
c
,
since bÌ¸= 0. Once again there exist scalars satisfying (3.16).
Therefore [ a, b]âŠ¤and [ c, d]âŠ¤spanR2, and we conclude that the set {[a, b]âŠ¤,[c, d]âŠ¤}forms a
basis for R2. This proves part (2). â– 
The two parts of Proposition 3.42, when combined, provide an easy test to determine whether
two given vectors in R2are linearly independent.
Example 3.43. Show that [1 ,âˆ’3]âŠ¤and [5 ,6]âŠ¤form a basis for R2.
Solution. Here we have [ a, b]âŠ¤= [1,âˆ’3]âŠ¤and [ c, d]âŠ¤= [5,6]âŠ¤, and since
adâˆ’bc= (1)(6) âˆ’(âˆ’3)(5) = 21 Ì¸= 0
we conclude by part (1) of Proposition 3.42 that the vectors are linearly independent. Then, by
part (2), it follows that the vectors do indeed form a basis for R2. â– 
Problems
1. Let
u1=ï£®
ï£°2
0
âˆ’1ï£¹
ï£»,u2=ï£®
ï£°3
1
0ï£¹
ï£»,u3=ï£®
ï£°âˆ’2
3
2ï£¹
ï£».
(a) Show that {u1,u2,u3}is a linearly independent set.
(b) The ordered set B= (u1,u2,u3) is a basis for R3. Given
v=ï£®
ï£°âˆ’6
âˆ’10
âˆ’5ï£¹
ï£»,
find [v]B, the coordinates of vwith respect to the basis B.
2. Write down a basis for the yz-plane in R3.
3. The plane Pgiven by x+ 2yâˆ’3z= 0 is a subspace of R3. Find a basis for it.89
3.6 â€“ Dimension
The first proposition we consider is useful mainly for proving more momentous results in
this section.
Proposition 3.44. LetVbe a vector space such that V=Span{v1, . . . ,vm}. Ifu1, . . . ,unâˆˆV
for some n > m , then the vectors u1, . . . ,unare linearly dependent.
Proof. Letu1, . . . ,unâˆˆVfor some n > m . Since the vectors v1, . . . ,vmspan V, there exist
scalars aijsuch that
uj=mX
i=1aijvi=a1jv1+a2jv2+Â·Â·Â·+amjvm (3.17)
for each 1 â‰¤jâ‰¤n.
Now, by Theorem 2.38 the homogeneous system of equations
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³a11x1+a12x2+Â·Â·Â·+a1nxn= 0
a21x1+a22x2+Â·Â·Â·+a2nxn= 0
............
am1x1+am2x2+Â·Â·Â·+amnxn= 0
has a nontrivial solution since n(the number of variables) is greater than m(the number of
equations). That is, there exists a solution ( x1, . . . , x n) = ( c1, . . . , c n) such that not all the
scalars cjare equal to 0.
We now have
nX
j=1aijcj=ai1c1+Â·Â·Â·+aincn= 0
for each 1 â‰¤iâ‰¤m, which implies that
mX
i=1nX
j=1aijcjvi=nX
j=1a1jcjv1+Â·Â·Â·+nX
j=1amjcjvm= 0v1+Â·Â·Â·+ 0vm=0. (3.18)
But, recalling (3.17), we may also write
mX
i=1nX
j=1aijcjvi=nX
j=1mX
i=1aijcjvi=nX
j=1 
cjmX
i=1aijvi!
=nX
j=1cjuj. (3.19)
Combining (3.18) and (3.19), we find that
nX
j=1cjuj=c1u1+Â·Â·Â·+cnun=0
forc1, . . . , c nnot all equal to 0.
Therefore u1, . . . ,unare linearly dependent. â– 90
Theorem 3.57 at the end of this section states that every vector space Vhas a basis, but
it leaves open two mutually-exclusive possibilities: either Vhas a finite basis (i.e. a basis
containing a finite number of vectors), or it does not. If Vhas a finite basis, then it is called a
finite-dimensional vector space; otherwise it is an infinite-dimensional vector space. Note
that the trivial vector space {0}, which has basis âˆ…by definition, is finite-dimensional.
Remark. If a vector space Vis finite-dimensional, so that it has a finite basis B={v1, . . . ,vm},
then it is an immediate consequence of Proposition 3.44 and the fact that V=Span{v1, . . . ,vm}
thatVcannot possess any basis that is infinite. Indeed no set of more than mvectors can even
be linearly independent!
While it is usually the case that many different sets of vectors can serve as a basis for a
finite-dimensional vector space V(the trivial vector space being the sole exception), it turns
out that every basis for a finite-dimensional vector space must contain the same number of
vectors. In what follows we let |S|denote the number of elements of a set S, also known as the
cardinality ofS.
Theorem 3.45. IfB1andB2are two bases for a finite-dimensional vector space V, then
|B1|=|B2|.
Proof. The remark made above makes clear that B1andB2must both be finite sets, so
B1={v1, . . . ,vm}andB2={u1, . . . ,un}for integers mandn, and we have |B1|=mand
|B2|=n.
Since Span (B1) =V, ifn > m thenu1, . . . ,unmust be linearly dependent by Proposition
3.44, which contradicts the hypothesis that B2is a basis for V. Hence nâ‰¤m.
Since Span (B2) =V, ifn < m thenv1, . . . ,vmmust be linearly dependent by Proposition
3.44, which contradicts the hypothesis that B1is a basis for V. Hence nâ‰¥m.
Therefore m=n, which is to say |B1|=|B2|. â– 
Throughout these notes, if a vector space is not said to be finite-dimensional, then it can
be assumed to be either finite- or infinite-dimensional. It is the fact that the cardinality of all
the bases of a given finite-dimensional vector space is a constant that allows us to make the
following definition.
Definition 3.46. Thedimension of a finite-dimensional vector space V,dim(V), is the
number of elements in any basis for V. That is, if Bis a basis for V, then dim(V) =|B|.
Remark. Since the basis for the trivial vector space {0}isâˆ…by Definition 3.40, it follows
that the dimension of {0}is|âˆ…|= 0. If a vector space Vis infinite-dimensional then we might
be tempted to write dim(V) =âˆ, but there is little use in doing this since there are in fact
different â€œsizesâ€ of infinity. We will not make a study of such matters in these notes, for it is
more properly the domain of a book on the subject of functional analysis.
Example 3.47. A basis for the vector space R2isE2={e1,e2}, where
e1=
1
0
and e2=
0
1
.91
Since there are two elements in the set we conclude that dim( R2) = 2.
More generally, as we have seen, a basis for Rnis provided by the set
En={e1, . . . ,en}
of standard unit vectors. Since |En|=n, we see that dim( Rn) =n.
Example 3.48. The vector space RmÃ—nofmÃ—nmatrices with real-valued entries has as a
basis the set
Emn={Eij: 1â‰¤iâ‰¤m,1â‰¤jâ‰¤n},
where Eijis the mÃ—nmatrix with ij-entry 1 and all other entries 0. There are mnelements in
Emn, and thus dim( RmÃ—n) =mn.
Example 3.49. Example 3.13 showed that Skw n(R) is a subspace of RnÃ—n, and thus is a vector
space over Rin its own right. The goal now is to find the dimension of Skw n(R). The first thing
to notice is that the diagonal entries of any skew-symmetric matrix A= [aij] must all be zero:
AâŠ¤=âˆ’Aâ‡’aii=âˆ’aiiâ‡’aii= 0.
So, in the case when n= 2, we must have
A=
0a
âˆ’a0
for some aâˆˆR, which is to say
Skw 2(R) =
0a
âˆ’a0
:aâˆˆR
=
a
0 1
âˆ’1 0
:aâˆˆR
= Span
0 1
âˆ’1 0
.
Thus we see that the set
B2=
0 1
âˆ’1 0
={E2,12âˆ’E2,21}
spans Skw 2(R), where the definitions of the matrices E2,12andE2,21are given by Equation
(2.14) . Since B2is a linearly independent set it follows that B2is a basis for Skw 2(R), and
therefore dim(Skw 2(R)) =|B2|= 1.
When n= 3 we find that
Skw 3(R) =ï£±
ï£²
ï£³ï£®
ï£°0a b
âˆ’a0c
âˆ’bâˆ’c0ï£¹
ï£»:a, b, câˆˆRï£¼
ï£½
ï£¾
=ï£±
ï£²
ï£³aï£®
ï£°0 1 0
âˆ’1 0 0
0 0 0ï£¹
ï£»+bï£®
ï£°0 0 1
0 0 0
âˆ’1 0 0ï£¹
ï£»+cï£®
ï£°0 0 0
0 0 1
0âˆ’1 0ï£¹
ï£»:a, b, câˆˆRï£¼
ï£½
ï£¾
= Spanï£«
ï£­ï£®
ï£°0 1 0
âˆ’1 0 0
0 0 0ï£¹
ï£»,ï£®
ï£°0 0 1
0 0 0
âˆ’1 0 0ï£¹
ï£»,ï£®
ï£°0 0 0
0 0 1
0âˆ’1 0ï£¹
ï£»ï£¶
ï£¸
= Span 
{E3,12âˆ’E3,21,E3,13âˆ’E3,31,E3,23âˆ’E3,32}
.92
The set
B3={E3,12âˆ’E3,21,E3,13âˆ’E3,31,E3,23âˆ’E3,32}
is linearly independent, and so dim(Skw 3(R)) = 3.
More generally, for arbitrary nâˆˆN, we find A= [aij]nis such that aii= 0 for 1 â‰¤iâ‰¤n, and
aij=âˆ’ajiwhenever iÌ¸=j. Thus the entries of Aare fully determined by just the entries above
the main diagonal, since each entry below the diagonal must be the negative of the corresponding
entry above the diagonal. The entries above the diagonal are aijfor 1â‰¤i < jâ‰¤n, and it is
straightforward to check that
Bn={En,ijâˆ’En,ji: 1â‰¤i < jâ‰¤n}
is a linearly independent set such that
Skw n(R) = Span 
{En,ijâˆ’En,ji: 1â‰¤i < jâ‰¤n}
,
and so
dim(Skw n(R)) =|Bn|= (nâˆ’1) + ( nâˆ’2) +Â·Â·Â·+ 1 =nâˆ’1X
k=1k=n(nâˆ’1)
2.
This is just the number of entries in an nÃ—nmatrix that are above the main diagonal. â– 
Definition 3.50. LetVbe a vector space and AâŠ†Va nonempty set. We call BâŠ†Aa
maximal subset of linearly independent vectors if the following are true:
1.Bis a linearly independent set.
2.For all SâŠ†Awith|S|>|B|,Sis a linearly dependent set.
Thus if BâŠ†Ais a maximal subset of linearly independent vectors and |B|=r, then there
exist rlinearly independent vectors in A, but there cannot be found r+ 1 linearly independent
vectors in A. It may be that only one combination of rvectors in Acan be used to construct
the set B, or there may be many different possible combinations.
Theorem 3.51. LetVbe a vector space, and let A={v1, . . . ,vn} âŠ†Vbe such that V=
Span( A). Then
1.The dimension of Vis at most n:dim(V)â‰¤n.
2.IfBâŠ†Ais a maximal subset of linearly independent vectors, then Bis a basis for V.
Proof.
Proof of Part (1). By Proposition 3.44 any set containing more than nvectors in Vmust be
linearly dependent, so if Bis a basis for V, then we must have dim( V) =|B| â‰¤ n.
Proof of Part (2). Suppose that BâŠ†Ais a maximal subset of linearly independent vectors.
Reindexing the elements of Aif necessary, we may assume that B={v1, . . . ,vr}. Ifr=n,
then B=A, and so Bspans Vand we straightaway conclude that Bis a basis for Vand weâ€™re
done. Suppose, then, that 1 â‰¤r < n . For each 1 â‰¤iâ‰¤nâˆ’rlet
Bi=Bâˆª {vr+i}={v1, . . . ,vr,vr+i}.93
The set Biis linearly dependent since |Bi|>|B|, and so there exist scalars ai1, . . . , a ir, bi, not
all zero, such that
ai1v1+Â·Â·Â·+airvr+bivr+i=0. (3.20)
We must have biÌ¸= 0, since otherwise (3.20) becomes
ai1v1+Â·Â·Â·+airvr=0,
whereupon the linear independence of v1, . . . ,vrwould imply that ai1=Â·Â·Â·=air= 0 and
so contradict the established fact that not all the scalars ai1, . . . , a ir, biare zero! From the
knowledge that biÌ¸= 0 we may write (3.20) as
vr+i=âˆ’ai1
biv1âˆ’ Â·Â·Â· âˆ’air
bivr=rX
j=1aij
âˆ’bivj=rX
j=1dijvj, (3.21)
where we define dij=âˆ’aij/bifor each 1 â‰¤iâ‰¤nâˆ’rand 1 â‰¤jâ‰¤r. Hence the vectors
vr+1, . . . ,vnare each expressible as a linear combination of v1, . . . ,vr.
LetuâˆˆVbe arbitrary. Since v1, . . . ,vnspan Vthere exist scalars c1, . . . , c nsuch that
u=c1v1+Â·Â·Â·+cnvn,
and then from (3.21) we have
u=c1v1+Â·Â·Â·+crvr+nâˆ’rX
i=1cr+ivr+i=rX
j=1cjvj+nâˆ’rX
i=1
cr+irX
j=1dijvj
=rX
j=1cjvj+nâˆ’rX
i=1rX
j=1cr+idijvj=rX
j=1cjvj+rX
j=1nâˆ’rX
i=1cr+idijvj
=rX
j=1
cjvj+nâˆ’rX
i=1cr+idijvj
=rX
j=1
cj+nâˆ’rX
i=1cr+idij
vj.
Setting
Ë†cj=cj+nâˆ’rX
i=1cr+idij
for each 1 â‰¤jâ‰¤r, we finally obtain
u= Ë†c1v1+Â·Â·Â·+ Ë†crvr
and so conclude that uâˆˆSpan{v1, . . . ,vr}= Span( B).
Therefore V= Span( B), and so Bis a basis for V. â– 
Closely related to the concept of a maximal subset of linearly independent vectors is the
following.
Definition 3.52. LetVbe a vector space. A set BâŠ†Vis amaximal set of linearly
independent vectors inVif the following are true:
1.Bis a linearly independent set.
2.For all wâˆˆVsuch that w/âˆˆB, the set Bâˆª {w}is linearly dependent.94
Theorem 3.53. IfVis a vector space and Sa maximal set of linearly independent vectors in
V, then Sis a basis for V.
Proof. Suppose that Vis a vector space and S={v1, . . . ,vn}is a maximal set of linearly
independent vectors. Let uâˆˆV. Then the set {v1, . . . ,vn,u}is linearly dependent, and so
there exist scalars c0, . . . , c nnot all zero such that
c0u+c1v1+Â·Â·Â·+cnvn=0. (3.22)
Now, if c0were 0 we would obtain c1v1+Â·Â·Â·+cnvn=0, whereupon the linear independence of
Swould imply that c1=Â·Â·Â·=cn= 0 and so contradict the established fact that not all the
scalars c0, . . . , c nare zero. Hence we must have c0Ì¸= 0, and (3.22) gives
u=âˆ’c1
c0v1âˆ’ Â·Â·Â· âˆ’cn
c0vn.
That is, every vector in Vis expressible as a linear combination of vectors in S, so that
Span( S) =Vand we conclude that Sis a basis for V. â– 
Theorem 3.54. LetVbe a finite-dimensional vector space, and let SâŠ†Vwith|S|=dim(V).
1.IfSis a linearly independent set, then Sis a basis for V.
2.IfSpan( S) =V, then Sis a basis for V.
Proof.
Proof of Part (1). Setting n=dim(V), suppose S={v1, . . . ,vn} âŠ†Vis a linearly independent
set. Any basis for Vwill span Vand have nvectors, so by Proposition 3.44 the set Sâˆª {w}
must be linearly dependent for every wâˆˆVsuch that w/âˆˆS. Hence Sis a maximal set of
linearly independent vectors, and therefore Sis a basis for Vby Theorem 3.53.
Proof of Part (2). Again set n=dim(V), and suppose S={v1, . . . ,vn}is such that Span (S) =
V. Assume Sis not a basis for V. Then Smust not be a linearly independent set. Let BâŠ†S
be a maximal subset of linearly independent vectors. Then Bcannot contain all of the vectors
inS, so|B|<|S|=n. By Theorem 3.51(2) it follows that Bis a basis for V, and so
dim(V) =|B|< n.
Since this is a contradiction, we conclude that Smust be a linearly independent set and therefore
Sis a basis for V. â– 
Theorem 3.55. LetVbe a vector space with dim(V) =n >0. Ifv1, . . . ,vrâˆˆVare linearly
independent vectors for some r < n , then vectors vr+1, . . . ,vnâˆˆVmay be found such that
{v1, . . . ,vn}is a basis for V.
Proof. Suppose that v1, . . . ,vrâˆˆVare linearly independent vectors, where r < n . The set
Sr={v1, . . . ,vr}cannot be a basis for Vsince by Definition 3.46 any basis for Vmust contain
nvectors. Hence Srcannot be a maximal set of linearly independent vectors by Theorem 3.53,
and so there must exist some vector vr+1âˆˆVsuch that the set
Sr+1=Srâˆª {vr+1}={v1, . . . ,vr+1}95
is linearly independent. Now, if r+ 1 = n, then Theorem 3.54 implies that Sr+1is a basis for
Vand the proof is done. If r+ 1< n, then we repeat the arguments made above to obtain
successive sets of linearly independent vectors
Sr+i=Sr+iâˆ’1âˆª {vr+i}={v1, . . . ,vr+i}
until such time that r+i=n, at which point the linearly independent set
Sn=Snâˆ’1âˆª {vn}={v1, . . . ,vr,vr+1, . . . ,vn}
will be a basis for V. â– 
Theorem 3.56. LetVbe a finite-dimensional vector space, and let Wbe a subspace of V.
Then
1.Wis finite-dimensional.
2. dim( W)â‰¤dim(V).
3.Ifdim(W) = dim( V), then W=V.
Proof. IfW={0}, then all three conclusions of the theorem follow trivially. Thus, we will
henceforth assume WÌ¸={0}, so that dim( V) =nâ‰¥1.
Proof of Part (1). Suppose Wis infinite-dimensional. Let w1be a nonzero vector in W. The set
{w1}cannot be a maximal set of linearly independent vectors in Wsince otherwise Theorem
3.53 would imply that {w1}is a basis for Wand hence dim(W) = 1, a contradiction. Thus for
some kâ‰¥2 additional vectors w2, . . . ,wkâˆˆWmay be found such that Sk={w1, . . . ,wk}is a
linearly independent set of vectors in W. However, for no kâˆˆNcanSkbe a maximal set of
linearly independent vectors in W, since otherwise Theorem 3.53 would imply that dim(W) =k.
It follows that there exists, in particular, a linearly independent set
{w1, . . . ,wn+1} âŠ†WâŠ†V,
which is impossible since by Proposition 3.44 there can be no linearly independent set in V
containing more than nvectors. Therefore Wmust be finite-dimensional.
Proof of Part (2). By Part (1) it is known that Wis finite-dimensional, so there exists a basis
B={w1, . . . ,wm}forW, where mâˆˆN. Since Bis a linearly independent set in V, and by
Proposition 3.44 there can be no linearly independent set in Vcontaining more than dim(V) =n
vectors, it follows that dim( W) =mâ‰¤n= dim( V).
Proof of Part (3). Suppose that dim(W) =dim(V) =n, where nis some integer since Vis
given to be finite-dimensional. Let B={w1, . . . ,wn}be a basis for W, so that W=Span (B).
Since dim(V) =nandw1, . . . ,wnâˆˆVare linearly independent, Bis a basis for Vby Theorem
3.54. Thus V= Span( B), and we have V=W. â– 
Given a matrix AâˆˆFmÃ—n, recall from Â§3.1 that the set of all xâˆˆFnfor which Ax=0
is true is a subspace of Fncalled the null space ofA, denoted by Nul(A). Later on we will
frequently be concerned with determining the dimension of Nul(A), which we will often refer to
as the nullity ofA. That is,
nullity( A) = dim(Nul( A)).96
Theorem 3.57. Every vector space has a basis.
Proof. LetVbe a vector space over a field F. By definition âˆ…is the basis for {0}, so assume
thatVis nontrivial. Let Sbe the collection of all linearly independent subsets of V:
S={AâŠ†V:Ais a linearly independent set }.
(Note that Scontains at least one singleton {v}withvÌ¸=0since Vis nontrivial.) Then Sis
a nonempty partially ordered set under the inclusion relation âŠ†. LetC âŠ† S be a chain in S.
We have C={Ci:iâˆˆI}for some index set I, and for every A, Bâˆˆ Ceither AâŠ†BorBâŠ†A.
Claim:
U=[
iâˆˆICi
is an upper bound for the chain Csuch that Uâˆˆ S. It is clear that CiâŠ†Ufor all iâˆˆI. Suppose
that U /âˆˆ S, which is to say Uis not a linearly independent set in V. This implies that, for
some nâˆˆN, there exist u1, . . . ,unâˆˆUsuch that {u1, . . . ,un}is linearly dependent, which in
turn implies that for each 1 â‰¤kâ‰¤nthere is some ikâˆˆIwithukâˆˆCik. For convenience we
may assume the vectors u1, . . . ,unare indexed such that
Ci1âŠ†Ci2âŠ† Â·Â·Â· âŠ† Cin,
recalling that each Cikis an element of the totally ordered set C. Thus u1, . . . ,unâˆˆCin, which
shows that Cinis not a linearly independent set and hence Cin/âˆˆ Sâ€”a contradiction. We
conclude that Umust be a linearly independent set, and hence Uis an upper bound for Cwith
Uâˆˆ S. Since every chain in Shas an upper bound in S, Zornâ€™s Lemma implies that Shas a
maximal element M.
LetvâˆˆVbe arbitrary. Suppose, for all nâˆˆN(or 1 â‰¤nâ‰¤ |M|ifMis finite) and
v1, . . . ,vnâˆˆM, the only r1, . . . , r n, râˆˆFthat satisfy the equation
nX
k=1rkvk+rv=0 (3.23)
arer1=Â·Â·Â·=rn=r= 0. Then Mâˆª {v}is a linearly independent set, which implies
that Mâˆª {v} âˆˆ S . Since MâŠ†Mâˆª {v}andMis a maximal element of S, we must have
M=Mâˆª {v}and therefore vâˆˆM. In particular we see that vâˆˆSpan( M).
Suppose, in contrast, that for some nâˆˆNandv1, . . . ,vnâˆˆMthe equation (3.23) admits a
nontrivial solution. Since v1, . . . ,vnare linearly independent this means we must have rÌ¸= 0
(otherwise we are forced to embrace the trivial solution). Since Fis a field there exists some
râˆ’1âˆˆFsuch that râˆ’1r= 1. Hence
rv=âˆ’nX
k=1rkvkâ‡’v=râˆ’1nX
k=1rkvk=nX
k=1(râˆ’1rk)vk,
and we see that vâˆˆSpan (M) once more. Thus V=Span (M), and since Mis a linearly
independent set we conclude that Mis a basis for V. â– 97
Problems
1.Find the dimension of P3(R), the vector space over Rof polynomials in xof degree at most
3 with real coefficients.
2. Recall that Symn(R) denotes the vector space of nÃ—nsymmetric matrices over R.
(a) Find a basis for Sym2(R). What is the dimension of Sym2(R)?
(b) Find a basis for Sym3(R). What is the dimension of Sym3(R)?
(c) Find a basis for Sym4(R). What is the dimension of Sym4(R)?
(d) Find a basis for Symn(R). What is the dimension of Symn(R)?98
3.7 â€“ Product Spaces
Definition 3.58. LetUandVbe vector spaces over F. The product ofUandVis the set
UÃ—V={(u,v) :uâˆˆU,vâˆˆV}.
More generally, let V1, . . . , V nbe vector spaces over F. The product ofV1, . . . , V nis the set
nY
k=1Vk={(v1, . . . ,vn) :vkâˆˆVkfor each 1â‰¤kâ‰¤n}
We see that the product of two or more vector spaces amounts to nothing more than the
Cartesian product of the sets of objects contained within the vector spaces. Let
u,vâˆˆnY
k=1Vk
be the n-tuples
u= (u1, . . . ,un) and v= (v1, . . . ,vn),
and let câˆˆF. If we define the sum ofuandvby
u+v= (u1+v1, . . . ,un+vn),
and the scalar product ofcwith vby
cv= (cv1, . . . , c vn),
then it is a routine matter to verify thatQn
k=1Vkbecomes a vector space in its own right, called
theproduct space ofV1, . . . , V n.99
3.8 â€“ The Rank of a Matrix
LetAâˆˆFmÃ—n, so that
A=ï£®
ï£¯ï£¯ï£°a11a12Â·Â·Â· a1n
a21a22Â·Â·Â· a2n............
am1am2Â·Â·Â· amnï£¹
ï£ºï£ºï£». (3.24)
Denote the column vectors of Aby
cj=ï£®
ï£°a1j...
amjï£¹
ï£»
for 1â‰¤jâ‰¤n, and denote the row vectors of Aby
ri=ai1. . . a in
for 1â‰¤iâ‰¤m. The column space ofAis defined to be the set
Col(A) = Span {c1, . . . ,cn},
and the row space ofAis defined to be the set
Row(A) = Span {râŠ¤
1, . . . ,râŠ¤
m}.
Proposition 3.30 implies that Col(A) is a subspace of FmandRow(A) is a subspace of Fn. The
column rank ofAis the dimension of the column space of A:
col-rank( A) = dim[Col( A)].
Therow rank ofAis the dimension of the row space:
row-rank( A) = dim[Row( A)].
Proposition 3.59. LetAâˆˆFmÃ—n, with c1, . . . ,cnâˆˆFmthe column vectors of Aand
r1, . . . ,rmâˆˆFnthe row vectors of A.
1.IfSâŠ† {c1, . . . ,cn}is a maximal subset of linearly independent vectors, then
col-rank( A) =|S|.
2.IfSâŠ† {r1, . . . ,rm}is a maximal subset of linearly independent vectors, then
row-rank( A) =|S|.
Proof.
Proof of Part (1). Suppose SâŠ† {c1, . . . ,cn}is a maximal subset of linearly independent
vectors. Let col-rank (A) =k. Since Col(A) is a vector space, Col(A) =Span{c1, . . . ,cn}, and
SâŠ† {c1, . . . ,cn}is a maximal subset of linearly independent vectors, it follows by Theorem
3.51 that Sis a basis for Col(A). Now, because the dimension of Col(A) isk, we must have
|S|=k= col-rank( A) as was to be shown.
Proof of Part (2). Done similarly, and so left as a problem. â– 100
In the proof of Proposition 3.59(1), since |S|=col-rank (A) =k, we can conclude that S
consists of kelements of the set {c1, . . . ,cn}, and so we may write S={cn1, . . . ,cnk}for some
n1, . . . , n kâˆˆ {1, . . . , n }. That is, {cn1, . . . ,cnk}is a maximal subset of linearly independent
vectors, which is to say the maximum number of linearly independent column vectors of Ais
col-rank( A).
What we ultimately want to show is that the row and column ranks of a matrix are always
equal. It is not an obvious fact, and so a few more results will need to be developed before we
are in a position to prove it.
Lemma 3.60. LetVandWbe vector spaces, with
SV={v1, . . . ,vn} âŠ†Vand SW={w1, . . . ,wn} âŠ†W.
If
nX
k=1xkvk=0â‡”nX
k=1xkwk=0
for all x1, . . . , x nâˆˆF, then dim(Span SV) = dim(Span SW).
Proof. Suppose that, for all x1, . . . , x nâˆˆF,Pn
i=1xivi=0if and only ifPn
i=1xiwi=0. We
shall refer to this hypothesis as (H). Let
RV={vi1, . . . ,vir} âŠ†SV
be a maximal subset of linearly independent vectors for SV, which means any subset of SVwith
more than relements must be linearly dependent. By Theorem 3.51 RVis a basis for Span (SV),
and so dim(Span SV) =|RV|=r.
LetRW={wi1, . . . ,wir} âŠ†SW. Suppose that
xi1wi1+Â·Â·Â·+xirwir=0.
Then by (H) we have
xi1vi1+Â·Â·Â·+xirvir=0
as well, and since vi1, . . . ,virare linearly independent we conclude that xi1=Â·Â·Â·=xir= 0.
That is,Pr
k=1xikwik=0necessarily implies that xik= 0 for all 1 â‰¤kâ‰¤r, and so RWis itself
a linearly independent set of vectors.
Next, assume B={wj1, . . . ,wjt} âŠ†SWis such that |B|=t > r . Set
xj1vj1+Â·Â·Â·+xjtvjt=0. (3.25)
Since any subset of SVcontaining more than relements must be linearly dependent, it follows
thatvj1, . . . ,vjtmust be linearly dependent and there exist scalars xj1, . . . , x jt,not all equal to
zero, which satisfy (3.25). By (H) these same scalars must satisfy
xj1wj1+Â·Â·Â·+xjtwjt=0,
which shows that wj1, . . . ,wjtmust also be linearly dependent. Hence there does not exist any
linearly independent set BâŠ†SWfor which |B|> r.
We conclude that RWâŠ†SWis a maximal subset of linearly independent vectors. By
Theorem 3.51 RWis a basis for Span( SW), and so dim(Span SW) =|RW|=r.101
Therefore dim(Span SV) =r= dim(Span SW). â– 
Lemma 3.61. Suppose AâˆˆFmÃ—mis invertible, and let BâˆˆFmÃ—n. Then col-rank (AB) =
col-rank( B).
Proof. Letb1, . . . ,bnâˆˆFmbe the column vectors of B, so that
B=b1Â·Â·Â·bn
,
and thus by Proposition 2.6
AB=Ab 1Â·Â·Â·Abn
,
where Ab 1, . . . ,AbnâˆˆFm. Let x1, . . . , x nâˆˆF. IfPn
j=1xjbj=0, then by Theorem 2.7 we have
nX
j=1xj(Abj) =nX
j=1A(xjbj) =AnX
j=1xjbj=A0=0;
and ifPn
j=1xj(Abj) =0, then since Ais invertible we have
nX
j=1xjbj=nX
j=1xj(Aâˆ’1Abj) =Aâˆ’1nX
j=1xj(Abj) =Aâˆ’10=0.
Therefore
col-rank( AB) = dim 
Span{Ab 1, . . . ,Abn}
= dim 
Span{b1, . . . ,bn}
= col-rank( B)
by Lemma 3.60. â– 
Proposition 3.62. LetAâˆˆFmÃ—n.
1.IfAâ€²row-equivalent to A, then
Row(A) = Row( Aâ€²)and col-rank( A) = col-rank( Aâ€²).
2.IfAâ€²column-equivalent to A, then
Col(A) = Col( Aâ€²)and row-rank( A) = row-rank( Aâ€²)
Thus both col-rank (A)androw-rank (A)are invariant under arbitrary finite sequences of ele-
mentary row and column operations applied to A.
Proof.
Proof of Part (1). Suppose that Aâ€²is row-equivalent to A. This means there exists a finite
sequence of elementary matrices M1, . . . ,MkâˆˆFmÃ—msuch that
Aâ€²=MkÂ·Â·Â·M1A.
By Proposition 2.27 each matrix Mjis invertible, and hence M=MkÂ·Â·Â·M1is invertible by
Theorem 2.26. Therefore
col-rank( A) = col-rank( MA) = col-rank( Aâ€²)
by Lemma 3.61.
To show that Row(A) =Row(Aâ€²), it is sufficient to show that Row(A) is invariant under
each one of the three elementary row operations. By Proposition 2.16(1) an R1 operation102
Mi,j(c)Areplaces the row vector ajofAbyaj+cai, and thus the row space of the resultant
(row-equivalent) matrix is equal to Row(A) by Proposition 3.31. By Proposition 2.16(2) an R2
operation Mi,jAmerely interchanges two row vectors of A, which clearly does not alter the row
space. Finally by Proposition 2.16(3) an R3 operation Mi(c)Amultiplies the row vector aiofA
by the nonzero scalar c, and the straightforward formal verification that the row space of the
resultant matrix equals Row( A) is left as a problem.
Proof of Part (2). Suppose Aâ€²is column-equivalent to A, so there are elementary matrices
M1, . . . ,MkâˆˆFnÃ—nsuch that
Aâ€²=AMâŠ¤
1Â·Â·Â·MâŠ¤
k,
and hence (taking the transpose of both sides and applying Proposition 2.13) we have
(Aâ€²)âŠ¤=MkÂ·Â·Â·M1AâŠ¤.
Again M=MkÂ·Â·Â·M1is invertible, so Lemma 3.61 implies that
col-rank( AâŠ¤) = col-rank( MAâŠ¤) = col-rank(( Aâ€²)âŠ¤).
Since the column spaces of AâŠ¤and (Aâ€²)âŠ¤are the row spaces of AandAâ€², respectively, we
finally obtain row-rank( A) = row-rank( Aâ€²).
The proof that Col(A) =Col(Aâ€²) is nearly identical to the proof that Row(A) =Row(Aâ€²)
in part (1), only Proposition 2.17 is employed instead of Proposition 2.16. â– 
In brief, elementary row operations do not change the row space of a matrix, and elementary
column operations do not change the column space. On the other hand elementary row (resp.
column) operations may change the column (resp. row) space of a matrix, but the dimension of
the column (resp. row) space will remain the same. That is, any elementary row operation may
change the span of the column vectors, and any elementary column operation may change the
span of the row vectors.
Example 3.63. Find a basis for the column space of the matrix
A=ï£®
ï£¯ï£¯ï£°2 0 3 4 1
0 1 1 âˆ’1 3
3 1 0 2 âˆ’6
1 0âˆ’4 2 1ï£¹
ï£ºï£ºï£»
Solution. One way to proceed is to use elementary column operations to put the matrix into
row-echelon form.
ï£®
ï£¯ï£¯ï£°2 0 3 4 1
0 1 1 âˆ’1 3
3 1 0 2 âˆ’6
1 0âˆ’4 2 1ï£¹
ï£ºï£ºï£»1
2c4+c1â†’c1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
2c4+c3â†’c3ï£®
ï£¯ï£¯ï£°4 0 11 4 1
âˆ’1
21âˆ’1âˆ’1 3
2 1 4 2 âˆ’6
0 0 0 2 1ï£¹
ï£ºï£ºï£»âˆ’1
2c3+c1â†’c1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
c2â†”c3103
ï£®
ï£¯ï£¯ï£°âˆ’3
211 0 4 1
0âˆ’1 1âˆ’1 3
0 4 1 2 âˆ’6
0 0 0 2 1ï£¹
ï£ºï£ºï£»âˆ’4c3+c2â†’c2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’2c1ï£®
ï£¯ï£¯ï£°3 11 0 4 1
0âˆ’5 1âˆ’1 3
0 0 1 2 âˆ’6
0 0 0 2 1ï£¹
ï£ºï£ºï£»âˆ’2c5+c4â†’c4âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£¯ï£¯ï£°3 11 0 2 1
0âˆ’5 1âˆ’7 3
0 0 1 14 âˆ’6
0 0 0 0 1ï£¹
ï£ºï£ºï£»=Aâ€²
The first, second, third, and fifth column vectors of the row-echelon matrix Aâ€²,
c1=ï£®
ï£¯ï£¯ï£°3
0
0
0ï£¹
ï£ºï£ºï£»,c2=ï£®
ï£¯ï£¯ï£°11
âˆ’5
0
0ï£¹
ï£ºï£ºï£»,c3=ï£®
ï£¯ï£¯ï£°0
1
1
0ï£¹
ï£ºï£ºï£»,c5=ï£®
ï£¯ï£¯ï£°1
3
âˆ’6
1ï£¹
ï£ºï£ºï£»,
contain pivots, and so are linearly independent by Theorem 3.33. Since dim(R4) = 4 and
c1,c2,c3,c5âˆˆR4are linearly independent, by Theorem 3.54(1) the set
S={c1,c2,c3,c5}
is a basis for R4, and so Span (S) =R4. By Proposition 3.44 any subset of R4containing more
vectors than S(i.e. more than four vectors) must be linearly dependent, and therefore Smust
be a maximal set of linearly independent vectors in Col(Aâ€²) since any vector in Col(Aâ€²) is
necessarily a vector in R4. By Theorem 3.53 we conclude that Sis a basis for Col(Aâ€²). Now,
because Aâ€²is column-equivalent to Awe have Col(Aâ€²) =Col(A) by Proposition 3.62. Therefore
Sis a basis for Col( A) and we are done. â– 
The next theorem is momentous. It tells us that the column rank of a matrix Aalways equals
the row rank, so that we may simply refer to the rank ofA,rank(A), without discriminating
between the column and row spaces. That is,
rank(A) = dim(Col( A)) = dim(Row( A)).
Also the theorem provides a definitive strategy for determining rank( A).
Theorem 3.64. IfAâˆˆFmÃ—nis such that row-rank (A) =r, then Ais equivalent via elementary
row and column operations to the mÃ—nmatrixIrO
OO
. (3.26)
Hence col-rank( A) = row-rank( A).
Proof. Suppose AâˆˆFmÃ—nwith row-rank (A) =r. By Proposition 2.20, Ais row-equivalent to
a matrix Aâ€²in row-echelon form. Since the nonzero row vectors of Aâ€²are linearly independent
androw-rank (Aâ€²) =rby Proposition 3.62, it follows that the top rrows of Aâ€²must be nonzero
row vectors while the bottom mâˆ’rrows must consist solely of zero entries.
Now, the pivots p1, . . . , p rin the top rrows of Aâ€²are nonzero entries having only zero
entries to the left of them. Each nonzero entry xto the right of p1we may â€œeliminateâ€ by104
performing a C1 operation: namely, if p1is in column iandxis in column j > i , then add
âˆ’x/p 1times column ito column j. Since all entries below p1are zero, this affects no other
entries in the matrix beyond replacing the 1 j-entry xwith 0. In the end we obtain a matrix in
which p1is the only nonzero entry in its row and column, and we repeat the process for p2,p3,
and finally pr. The resultant matrix will have only p1, . . . , p ras nonzero entries, still in their
original row-echelon formation. Now we perform C2 operations to make pitheii-entry for each
1â‰¤iâ‰¤r. Finally we perform C3 operations: we multiply column iby 1/piso that the ii-entry
is 1 for each 1 â‰¤iâ‰¤r, thereby securing the desired matrix (3.26).
The matrix (3.26) clearly has row rank and column rank both equal to r, and since the matrix
was obtained from Aby applying a finite sequence of elementary row and column operations,
Proposition 3.62 implies that the row rank and column rank of Aare likewise both equal to r.
This finishes the proof. â– 
Example 3.65. Apply a sequence of elementary row and column operations to
A=ï£®
ï£°1 1 2 1
1 0 1 2
2 1 3 4ï£¹
ï£»
to obtain an equivalent matrix of the form (3.26) . Show that the row vectors of Aare linearly
independent, and that row-rank( A) = col-rank( A). State the rank of A.
Solution. First we will get a matrix in row-echelon form using strictly elementary row operations:ï£®
ï£°1 1 2 1
1 0 1 2
2 1 3 4ï£¹
ï£»âˆ’r1+r2â†’r2
âˆ’2r1+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 1 2 1
0âˆ’1âˆ’1 1
0âˆ’1âˆ’1 2ï£¹
ï£»âˆ’r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 1 2 1
0âˆ’1âˆ’1 1
0 0 0 1ï£¹
ï£»
Now elementary column operations will be used to first put zeros to the right of the ii-entries,
and then to obtain a diagonal of 1â€™s:ï£®
ï£°1 1 2 1
0âˆ’1âˆ’1 1
0 0 0 1ï£¹
ï£»âˆ’c1+c2â†’c2
âˆ’2c1+c3â†’c3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 0 0 1
0âˆ’1âˆ’1 1
0 0 0 1ï£¹
ï£»âˆ’c1+c4â†’c4âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 0 0 0
0âˆ’1âˆ’1 1
0 0 0 1ï£¹
ï£»c3â†”c4âˆ’ âˆ’ âˆ’ â†’
ï£®
ï£°1 0 0 0
0âˆ’1 1âˆ’1
0 0 1 0ï£¹
ï£»c2+c3â†’c3
âˆ’c2+c4â†’c4âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1 0 0 0
0âˆ’1 0 0
0 0 1 0ï£¹
ï£»âˆ’c2âˆ’ âˆ’ â†’ï£®
ï£°1 0 0 0
0 1 0 0
0 0 1 0ï£¹
ï£».
The row vectors of the final matrix are [1 ,0,0,0], [0,1,0,0], and [0 ,0,1,0], which are linearly
independent, and so the row rank is 3. By Proposition 3.62 it follows that row-rank (A) = 3
as well, and therefore the row vectors of Amust be linearly independent by Theorem 3.54(2).
The nonzero column vectors of the final matrix are [1 ,0,0]âŠ¤, [0,1,0]âŠ¤, and [0 ,0,1]âŠ¤, which are
linearly independent, and so col-rank( A) = 3 by Proposition 3.62. Thus we have
row-rank( A) = col-rank( A) = 3 ,
and therefore rank( A) = 3. â– 
In Example 3.65 it should be noted that rank(A) could have been determined rather easily
early on, right after performing the R1 row operation âˆ’r2+r3â†’r3. The row vectors at that105
stage were [1 ,1,2,1], [0,âˆ’1,âˆ’1,1], and [0 ,0,0,1], which can be seen to be linearly independent
on account of the placement of the zeros. Thus rank( A) = row-rank( A) = 3.
With our definition of rank in hand, the findings of Proposition 3.62 and Theorem 3.64
combine to yield the following result.
Theorem 3.66. IfAis row-equivalent or column-equivalent to Aâ€², then rank(A) = rank( Aâ€²).
The next example makes use of a variety of results developed throughout this chapter. What
once may have required much tedious calculation now can be accomplished quickly and elegantly.
Example 3.67. Let
v1=ï£®
ï£°âˆ’1
1
1ï£¹
ï£»and v2=ï£®
ï£°1
2
1ï£¹
ï£».
Show that B={v1,v2}is a basis for the vector space WâŠ†R3given by
W=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»xâˆ’2y+ 3z= 0ï£¼
ï£½
ï£¾.
Solution. Define the matrix
B=v1v2
=ï£®
ï£°âˆ’1 1
1 2
1 1ï£¹
ï£»,
and consider the first two row vectors [ a, b] = [âˆ’1,1] and [ c, d] = [1 ,2]. Since
adâˆ’bc= (âˆ’1)(2)âˆ’(1)(1) = âˆ’3Ì¸= 0,
these row vectors of Bare linearly independent by Proposition 3.42, and so row-rank (B)â‰¥2.
On the other hand Bhas only two columns, so col-rank( B)â‰¤2. Hence, by Theorem 3.64,
2â‰¤row-rank( B) = rank( B) = col-rank( B)â‰¤2,
which implies that rank(B) = 2. Since v1andv2are the column vectors of B, it follows that v1
andv2are linearly independent.
It is easily verified that v1,v2âˆˆW, so that S=Span (B) is a subspace of Wand thus
dim(S)â‰¤dim(W) by Theorem 3.56(2). Since Bis a basis for S, we have dim(S) = 2; and sinceï£®
ï£°1
0
0ï£¹
ï£»/âˆˆW,
so that Wis a subspace of R3that does not equal R3, it follows by Theorem 3.56 that
dim(W)<dim(R3) = 3 .
That is,
2 = dim( S)â‰¤dim(W)â‰¤2,
which shows that dim( W) = 2, and therefore Bis a basis for Wby Theorem 3.54(1). â– 106
Problems
1. For any matrix Ashow that Col( A) = Row( AâŠ¤) and Row( A) = Col( AâŠ¤).
2. Show that rank( A) = rank( AâŠ¤) for any matrix A.
3. Let AâˆˆFmÃ—nandBâˆˆFnÃ—p.
(a) Show that rank( AB)â‰¤rank(A).
(b) Show that rank( AB)â‰¤rank(B).107
4
Linear Mappings
4.1 â€“ Linear Mappings
Amapping (ortransformation ) is nothing more than a function, but usually a function
between sets that have some additional structure such as a vector space. We have encountered
mappings already in the definition of a vector space V: namely the scalar multiplication and
vector addition functions, whose ranges both consist of elements of V. As with functions in
general, to say a mapping Tmaps a set Xinto a set Y, written T:Xâ†’Y, means that T
maps each object xâˆˆXto a unique object yâˆˆY. We denote this by writing T(x) =y, or
sometimes Tx=y, and call Xthedomain ofTandYthecodomain . A little more formally
a mapping Tis a set of ordered pairs ( x, y)âˆˆXÃ—Ywith the property that
âˆ€xâˆˆX
âˆƒyâˆˆY 
((x, y)âˆˆT)âˆ§(Ë†yÌ¸=yâ†’(x,Ë†y)/âˆˆT)
.
We call T(x) the value of Tatx. Given any set AâŠ†X, we define the image of Aunder T
to be the set
T(A) ={T(x) :xâˆˆA} âŠ†Y,
with T(X) in particular being called the image ofT(also known as the range ofT) and
denoted by Img( T).
A common practice is to write x7â†’yto indicate a mapping. For instance x7â†’3âˆšxmay be
written to denote a mapping T:Râ†’Rfor which T(x) =3âˆšxfor all xâˆˆR. The symbol â†’is
placed between sets, while 7â†’is placed between elements of sets.
Definition 4.1. A mapping T:Xâ†’Yisinjective (orone-to-one ) if
T(x1) =T(x2)â‡’x1=x2.
for all x1, x2âˆˆX. Thus if x1Ì¸=x2, then T(x1)Ì¸=T(x2).
A mapping T:Xâ†’Yissurjective (oronto ) if for each yâˆˆYthere exists some xâˆˆX
such that T(x) =y. Thus we have T(X) =Y.
If a mapping is both injective and surjective, then it is called a bijection .
A large part of linear algebra is occupied with the study of a special kind of mapping known
as a linear mapping.108
Definition 4.2. LetVandWbe vector spaces over F. A mapping L:Vâ†’Wis called a
linear mapping if the following properties hold.
LT1. L(u+v) =L(u) +L(v)for all u,vâˆˆV
LT2. L(cu) =cL(u)for all câˆˆFanduâˆˆV.
Whenever L:Vâ†’Wis given to be a linear mapping, it is understood that VandWmust
be vector spaces. A linear operator is a linear mapping L:Vâ†’V, which may be more
specifically referred to as a linear operator on Vwhenever the occasion warrants.
Proposition 4.3. IfL:Vâ†’Wis a linear mapping, then
1.L(0) =0
2.L(âˆ’v) =âˆ’L(v)for any vâˆˆV.
3.For any c1, . . . , c nâˆˆF,v1, . . . ,vnâˆˆV,
L nX
k=1ckvk!
=nX
k=1ckL(vk).
Proof.
Proof of Part (1). Using the linearity property LT1, we have
L(0) =L(0+0) =L(0) +L(0).
Subtracting L(0) from the leftmost and rightmost sides then gives
L(0)âˆ’L(0) = [L(0) +L(0)]âˆ’L(0),
and thus 0=L(0).
Proof of Part (2). LetvâˆˆVbe arbitrary. Using property LT1 and part (1), we have
L(v) +L(âˆ’v) =L(v+ (âˆ’v)) =L(0) =0.
This shows that L(âˆ’v) is the additive inverse of L(v). That is, L(âˆ’v) =âˆ’L(v).
Proof of Part (3). We have L(c1v1) =c1L(v1) by property LT2. Let nâˆˆNand suppose that
L(c1v1+Â·Â·Â·+cnvn) =c1L(v1) +Â·Â·Â·+cnL(vn) (4.1)
for any c1, . . . , c nâˆˆF,v1, . . . ,vnâˆˆV. Let c1, . . . , c n+1âˆˆFandv1, . . . ,vn+1âˆˆVbe arbitrary.
Then
LXn+1
i=1civi
=L 
(c1v1+Â·Â·Â·+cnvn) +cn+1vn+1
=L(c1v1+Â·Â·Â·+cnvn) +L(cn+1vn+1) Property LT1
=c1L(v1) +Â·Â·Â·+cnL(vn) +L(cn+1vn+1) Hypothesis (4.1)
=c1L(v1) +Â·Â·Â·+cnL(vn) +cn+1L(vn+1) Property LT2
=Xn+1
i=1ciL(vi)
The proof is complete by the Principle of Induction. â– 109
In part (1) of Proposition 4.3 the vector 0on the left side of L(0) =0is the zero vector in
V, and the 0on the right side is the zero vector in W. Occasionally there may arise a need
to distinguish between these two zero vectors, in which case we will denote 0âˆˆVby0Vand
0âˆˆWby0W.
Example 4.4. LetVandWbe vectors spaces. The mapping Vâ†’Wgiven by v7â†’0Wfor all
vâˆˆVis called the zero mapping and denoted by O. Thus we may write O:Vâ†’Wsuch
thatO(v) =0for all vâˆˆV, where the symbol 0on the right side is understood to be the zero
vector in W. It is easy to verify that Ois a linear mapping. â– 
Example 4.5. Given a vector space V, the mapping IV:Vâ†’Vgiven by IV(v) =vfor all
vâˆˆVis called the identity mapping . It is a linear mapping as well, and may be denoted by
Iif the vector space it is acting on is not in question. â– 
Example 4.6. Given a vector space VandaâˆˆV, a mapping Ta:Vâ†’Vgiven by Ta(v) =v+a
for all vâˆˆVis atranslation by a . Note that this mapping is not linear unless a=0, in
which case it is simply an identity mapping. One geometric interpretation is to regard vas a
â€œpointâ€ in V, and v+ais a new â€œpointâ€ obtained by translating vbya.
For example, fixing a nonzero vector
a=
a
b
âˆˆR2,
we may define Ta:R2â†’R2by
Ta(x) =x+a=
x
y
+
a
b
=
x+a
y+b
(4.2)
for each x= [x, y]âŠ¤âˆˆR2.
Very often a mapping L:Rnâ†’Rnis taken to be a change in coordinates, for instance
in order to effect a change of variables in a double or triple integral in vector calculus. In
the case of Ta:R2â†’R2we may regard the mapping as taking the coordinates of a point
(x, y) inxy-coordinates and converting them to uv-coordinates ( u, v) by setting u=x+aand
v=y+b. Thus, if we let the symbol R2
xyrepresent R2inxy-coordinates, and let R2
uvrepresent
xy
ax0
Ta(x0)
x0y0
x0+ay0+b
Figure 9. Taas a translation by ainR2.110
xy
x0
x0y0Ta
uv
u0
x0+ax0+b
Figure 10. Taas a change in coordinates R2
xyâ†’R2
uv.
R2inuv-coordinates, then we may define the mapping Tadefined by (4.2) to be the mapping
Ta:R2
xyâ†’R2
uvgiven by
Ta: (x, y)7â†’(u, v) = (x+a, y+b).
In vector notation we may still write Ta:x7â†’x+a, since it makes no difference, mathematically,
whether we talk of points (x, y) and ( u, v), or vectors

x
y
and
u
v
.
Thus, translation by ainR2corresponds to a change in coordinates from the xy-system R2
xyto
theuv-system R2
uv. Figure 9 shows the translation by ainR2interpretation of Tain the case
when a >0 and b <0, letting
x0
y0
=x0;
and Figure 10 shows the change in coordinates interpretation of Taletting

u0
v0
=u0=x0+a.
â– 
Example 4.7. LetA= [aij] be an mÃ—nmatrix and define L:Rnâ†’RmbyL(x) =Ax; that
is,
L(x) =ï£®
ï£¯ï£¯ï£°a11Â·Â·Â· a1n
a21Â·Â·Â· a2n.........
am1Â·Â·Â· amnï£¹
ï£ºï£ºï£»ï£®
ï£¯ï£¯ï£°x1
x2...
xnï£¹
ï£ºï£ºï£»
for each xâˆˆRn. The mapping Lis easily shown to be linear using properties of matrix
arithmetic established in Chapter 2: for each câˆˆRandxâˆˆRnwe have
L(cx) =A(cx) =c(Ax) =cL(x),111
and for each x,yâˆˆRnwe have
L(x+y) =A(x+y) =Ax+Ay=L(x) +L(y).
This verifies properties LT1 and LT2. â– 
Definition 4.8. Given linear mappings L1, L2:Vâ†’W, we define the mapping L1+L2:Vâ†’
Wby
(L1+L2)(v) =L1(v) +L2(v)
for each vâˆˆV.
Given linear mapping L:Vâ†’WandcâˆˆF, we define cL:Vâ†’Wby
(cL)(v) =cL(v)
for each vâˆˆV. In particular we define âˆ’L= (âˆ’1)L.
Given vector spaces VandWoverF, the symbol L(V, W ) will be used to denote the set of
all linear mappings Vâ†’W; that is,
L(V, W ) ={L:Vâ†’W|Lis a linear mapping }.
As it turns out, L(V, W ) is a vector space in its own right.
Proposition 4.9. IfVandWare vector spaces over F, then L(V, W )is a vector space under
the operations of vector addition and scalar multiplication given in Definition 4.8.
Proof. LetL1, L2âˆˆ L(V, W ). For any u,vâˆˆV,
(L1+L2)(u+v) =L1(u+v) +L2(u+v) Definition 4.8
=L1(u) +L1(v) +L2(u) +L2(v) Property LT1
= [L1(u) +L2(u)] + [L1(v) +L2(v)] Axioms VS1 and VS2
= (L1+L2)(u) + (L1+L2)(v). Definition 4.8
For any câˆˆF,
(L1+L2)(cv) =L1(cv) +L2(cv) Definition 4.8
=cL1(v) +cL2(v) Property LT2
=c[L1(v) +L2(v)] Axioms VS5
=c(L1+L2)(v). Definition 4.8
Thus L1+L2:Vâ†’Wsatisfies properties LT1 and LT2, implying that L1+L2âˆˆ L(V, W ) and
therefore L(V, W ) is closed under vector addition. The proof that L(V, W ) is also closed under
scalar multiplication is left as a problem. It remains to verify the eight axioms VS1â€“VS8 given
in Definition 3.1.
For any vâˆˆVwe have
(L1+L2)(v) =L1(v) +L2(v) =L2(v) +L1(v) = (L2+L1)(v),112
where the middle equality follows from VS1 for W. Thus L1+L2=L2+L1, verifying VS1 for
L(V, W ).
LetL3âˆˆ L(V, W ). For any vâˆˆV,
(L1+ (L2+L3))(v) =L1(v) + (L2+L3)(v) =L1(v) + (L2(v) +L3(v))
= (L1(v) +L2(v)) +L3(v) = (L1+L2)(v) +L3(v)
= ((L1+L2) +L3)(v),
where the middle equality follows from VS2 for W. Thus
L1+ (L2+L3) = (L1+L2) +L3,
verifying VS2 for L(V, W ).
The zero mapping O:Vâ†’Wis a linear mapping, as mentioned in Example 4.4, and thus
Oâˆˆ L(V, W ). It is straightforward to verify that O+L=L+O=Lfor any Lâˆˆ L(V, W ),
and thus L(V, W ) satisfies VS3.
For any Lâˆˆ L(V, W ) we have âˆ’Lâˆˆ L(V, W ) also, since âˆ’L= (âˆ’1)Lby Definition 4.8, and
it has been already verified that L(V, W ) is closed under scalar multiplication. Now, for any
vâˆˆV,
(L+ (âˆ’L))(v) =L(v) + (âˆ’L)(v) =L(v) + ((âˆ’1)L)(v)
=L(v) + (âˆ’1)L(v) =L(v) + (âˆ’L(v)) =0,
where the first three equalities follow from Definition 4.8, the fourth equality from Proposition
3.3, and the fifth equality from VS4 for W. Thus L+ (âˆ’L) =O, verifying VS4 for L(V, W ).
LetaâˆˆF. For any vâˆˆV,
(a(L1+L2))(v) =a(L1+L2)(v) =a(L1(v) +L2(v))
=aL1(v) +aL2(v) = (aL1)(v) + (aL2)(v)
= (aL1+aL2)(v),
where the middle equality follows from VS5 for W. Thus a(L1+L2) =aL1+aL2, verifying
VS5 for L(V, W ).
The verification of Axiom VS6 is left as a problem, as is the verification of VS7.
Finally, for any Lâˆˆ L(V, W ) and vâˆˆVwe have
(1L)(v) = 1 L(v) =L(v),
by application of Definition 4.8 and VS8 for W. Thus 1 L=L, verifying VS8 for L(V, W ).â– 
Definition 4.10. A bijective linear mapping is called an isomorphism .
IfVandWare vector spaces and there exists a linear mapping L:Vâ†’Wthat is an
isomorphism, then VandWare said to be isomorphic and we write Vâˆ¼=W.
Isomorphic vector spaces are truly identical in all respects save for the symbols used to
represent their elements. In fact any vector space Vof dimension ncan be shown to be113
isomorphic to Rn. To see this, let B= (v1, . . . ,vn) be an ordered basis for Vand observe that
the operation of taking a vector
v=x1v1+Â·Â·Â·+xnvn
inVand giving its B-coordinates,
[v]B=ï£®
ï£°x1...
xnï£¹
ï£»,
is actually a mapping v7â†’[v]Bfrom VtoRncalled the B-coordinate map (or the coordinate
map determined by B) and is denoted by Ï†B. Thus, by definition,
Ï†B(v) = [v]B
for all vâˆˆV. The mapping Ï†Bis a well-defined function: given vâˆˆV, by Theorem 3.41 there
exist unique scalars x1, . . . , x nfor which v=x1v1+Â·Â·Â·+xnvn, and therefore
Ï†B(v) =ï£®
ï£°x1...
xnï£¹
ï£»
is the only possible definition for Ï†B. The mapping Ï†Bis, in fact, linear, injective, and surjective,
which is to say it is an isomorphism.
Theorem 4.11. LetB= (v1, . . . ,vn)be an ordered basis for a vector space VoverF. Then
the coordinate map Ï†B:Vâ†’Fnis an isomorphism.
Proof. Suppose u,vâˆˆVare such that
Ï†B(u) =ï£®
ï£°a1...
anï£¹
ï£»=ï£®
ï£°b1...
bnï£¹
ï£»=Ï†B(v).
Then u=Pn
i=1aiviandv=Pn
i=1bivisuch that ai=bifori= 1, . . . , n , whence
uâˆ’v=nX
i=1aiviâˆ’nX
i=1bivi=nX
i=1(aiâˆ’bi)vi=nX
i=10vi=0,
and so u=v. Thus Ï†Bis injective.
Next, letï£®
ï£°x1...
xnï£¹
ï£»âˆˆFn
be arbitrary. Defining vâˆˆVbyv=Pn
i=1xivi, we observe that
Ï†B(v) =ï£®
ï£°x1...
xnï£¹
ï£»,
and thus Ï†Bis surjective.114
Finally, for any u=Pn
i=1aiviandv=Pn
i=1biviinVwe have u+v=Pn
i=1(ai+bi)vi, so
Ï†B(u+v) =ï£®
ï£°a1+b1...
an+bnï£¹
ï£»=ï£®
ï£°a1...
anï£¹
ï£»+ï£®
ï£°b1...
bnï£¹
ï£»=Ï†B(u) +Ï†B(v)
by the definition of vector addition in Fn. Also for any câˆˆFwe have cv=Pn
i=1caivi, so
Ï†B(cu) =ï£®
ï£°ca1...
canï£¹
ï£»=cï£®
ï£°a1...
anï£¹
ï£»=cÏ†B(v)
by the definition of scalar multiplication in Fn. Hence Ï†Bis a linear mapping.
Therefore Ï†Bis an isomorphism. â– 
Example 4.12. Consider the vector space WâŠ†R3given by
W=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»xâˆ’2y+ 3z= 0ï£¼
ï£½
ï£¾.
Two ordered bases for Ware
B=ï£«
ï£­ï£®
ï£°âˆ’1
1
1ï£¹
ï£»,ï£®
ï£°1
2
1ï£¹
ï£»ï£¶
ï£¸ and C=ï£«
ï£­ï£®
ï£°2
1
0ï£¹
ï£»,ï£®
ï£°âˆ’3
0
1ï£¹
ï£»ï£¶
ï£¸.
Given
v=ï£®
ï£°5
7
3ï£¹
ï£»âˆˆW,
find [v]Band [v]C.
Solution. Since ( x, y, z ) = (5 ,7,3) is a solution to the equation xâˆ’2y+ 3z= 0, it is clear that
vâˆˆW. To find the B-coordinates of v, we find a, bâˆˆRsuch that
aï£®
ï£°âˆ’1
1
1ï£¹
ï£»+bï£®
ï£°1
2
1ï£¹
ï£»=ï£®
ï£°5
7
3ï£¹
ï£»,
which is to say we solve the systemï£±
ï£²
ï£³âˆ’a+b= 5
a+ 2b= 7
a+b= 3
The only solution is ( a, b) = (âˆ’1,4), and therefore
[v]B=
âˆ’1
4
.115
To find the C-coordinates of v, we find a, bâˆˆRsuch that
aï£®
ï£°2
1
0ï£¹
ï£»+bï£®
ï£°âˆ’3
0
1ï£¹
ï£»=ï£®
ï£°5
7
3ï£¹
ï£»,
giving the systemï£±
ï£²
ï£³2aâˆ’3b= 5
a+ 0b= 7
0a+b= 3
which immediately yields the unique solution ( a, b) = (7 ,3), and therefore
[v]C=
7
3
.
â– 116
4.2 â€“ Images and Null Spaces
The image (or range) of a mapping was already defined in Â§4.1, but for convenience we give
the definition again in a slightly different guise. We also narrow the focus to linear mappings in
particular.
Definition 4.13. LetL:Vâ†’Wbe a linear mapping. The image ofLis the set
Img(L) ={wâˆˆW:L(v) =wfor some vâˆˆV},
and the null space (orkernel ) ofLis the set
Nul(L) ={vâˆˆV:L(v) =0}.
Note that for L:Vâ†’Wwe have Img(L) =L(V). Another term for the null space of Lis
thekernel ofL, denoted by Ker( L) in many books.
Proposition 4.14. LetL:Vâ†’Wbe a linear mapping. Then the following hold.
1. Img( L)is a subspace of W.
2. Nul( L)is a subspace of V.
Proof.
Proof of Part (1). As we have shown in the previous section L(0) =0, and so 0âˆˆImg(L).
Suppose that w1,w2âˆˆImg(L). Then there exist vectors v1,v2âˆˆVsuch that L(v1) =w1
andL(v2) =w2. Now, since v1+v2âˆˆVand
L(v1+v2) =L(v1) +L(v2) =w1+w2,
we conclude that w1+w2âˆˆImg(L). Hence Img( L) is closed under vector addition.
Finally, let câˆˆRand suppose wâˆˆImg(L). Then there exists some vâˆˆVsuch that
L(v) =w, and since cvâˆˆVand
L(cv) =cL(v) =cw,
we conclude that cwâˆˆImg(L). Hence Img( L) is closed under scalar multiplication.
Therefore Img( L)âŠ†Wis a subspace.
Proof of Part (2). Since L(0) =0we immediately obtain 0âˆˆNul(L).
Suppose that v1,v2âˆˆNul(L). Then L(v1) =L(v2) =0, and since
L(v1+v2) =L(v1) +L(v2) =0+0=0,
it follows that v1+v2âˆˆNul(L) and so Nul( L) is closed under vector addition.
Finally, let câˆˆRand suppose vâˆˆNul(L). Then L(v) =0, and since
L(cv) =cL(v) =c0=0
we conclude that cvâˆˆNul(L) and so Nul( L) is closed under scalar multiplication.
Therefore Nul( L)âŠ†Vis a subspace. â– 117
Proposition 4.15. LetL:Vâ†’Wbe a linear mapping. Then Lis injective if and only if
Nul(L) ={0}.
Proof. Suppose that L:Vâ†’Wis injective. Let vâˆˆNul(L), so that L(v) =0. By Proposition
4.3 we have L(0) =0also, and since Lis injective it follows that v=0. Hence Nul(L)âŠ† {0},
andL(0) =0shows that {0} âŠ†Nul(L). Therefore Nul( L) ={0}.
Next, suppose that Nul(L) ={0}. Suppose that L(v1) =L(v2), soL(v1)âˆ’L(v2) =0. Then
L(v1âˆ’v2) =L(v1)âˆ’L(v2) =0
shows that v1âˆ’v2âˆˆNul(L) ={0}and thus v1âˆ’v2=0. Therefore v1=v2and we conclude
thatLis injective. â– 
Proposition 4.16. LetL:Vâ†’Wbe an injective linear mapping. If v1, . . . ,vnâˆˆVare
linearly independent, then L(v1), . . . , L (vn)âˆˆWare linearly independent.
Proof. Suppose v1, . . . ,vnare linearly independent vectors in V. Let a1, . . . , a nâˆˆFbe such
that
a1L(v1) +Â·Â·Â·+anL(vn) =0.
From this we obtain
L(a1v1+Â·Â·Â·+anvn) =0,
and since Nul( L) ={0}it follows that
a1v1+Â·Â·Â·+anvn=0.
Now, since v1, . . . ,vnare linearly independent, it follows that a1=Â·Â·Â·=an= 0.
Therefore the vectors L(v1), . . . , L (vn) inWare linearly independent. â– 
Example 4.17. Define the mapping T:FnÃ—nâ†’FnÃ—nby
T(A) =Aâˆ’AâŠ¤
2.
(a) Show that Tis a linear mapping.
(b) Find the null space of T, and give its dimension.
(c) Find the image of T, and give its dimension.
Solution.
(a) Let A,BâˆˆFnÃ—nandcâˆˆF. Recalling Proposition 2.3, we have
T(cA) =(cA)âˆ’(cA)âŠ¤
2=cAâˆ’cAâŠ¤
2=cAâˆ’AâŠ¤
2
=cT(A),
and
T(A+B) =(A+B)âˆ’(A+B)âŠ¤
2=(A+B)âˆ’(AâŠ¤+BâŠ¤)
2
=A+Bâˆ’AâŠ¤âˆ’BâŠ¤
2=Aâˆ’AâŠ¤
2+Bâˆ’BâŠ¤
2118
=T(A) +T(B),
and therefore Tis linear.
(b) By definition we have
Nul(T) ={AâˆˆFnÃ—n:T(A) =On},
where Onis the nÃ—nzero matrix. Now,
T(A) =Onâ‡”Aâˆ’AâŠ¤
2=Onâ‡”Aâˆ’AâŠ¤=Onâ‡”A=AâŠ¤,
and therefore
Nul(T) ={AâˆˆFnÃ—n:AâŠ¤=A}= Symn(F).
That is, the null space of Tconsists of the set of all nÃ—nsymmetric matrices. In a problem at
the end of Â§3.6 it is found that dim(Symn(F)) =n(n+ 1)/2, and therefore
dim(Nul( T)) =n(n+ 1)
2
as well.
(c) By definition we have
Img(T) ={T(A) :AâˆˆFnÃ—n}=Aâˆ’AâŠ¤
2:AâˆˆFnÃ—n
.
Now, appealing to Proposition 2.3 once more, we find that
Aâˆ’AâŠ¤
2âŠ¤
=1
2(Aâˆ’AâŠ¤)âŠ¤=1
2[AâŠ¤âˆ’(AâŠ¤)âŠ¤] =1
2(AâŠ¤âˆ’A) =âˆ’Aâˆ’AâŠ¤
2,
and so the elements
Aâˆ’AâŠ¤
2
in the image of Tare skew-symmetric. Let Skw n(F) denote the set of nÃ—nskew-matrices with
entries in F:
Skw n(F) ={AâˆˆFnÃ—n:AâŠ¤=âˆ’A}.
We have just shown that Img(T)âŠ†Skw n(F). Suppose BâˆˆSkw n(F), so that BâŠ¤=âˆ’B. Now,
it happens that
T(B) =Bâˆ’BâŠ¤
2=Bâˆ’(âˆ’B)
2=2B
2=B,
and since there exists some matrix Afor which T(A) =B(namely we can let AbeBitself),
it follows that BâˆˆImg(T) and hence Skw n(F)âŠ†Img(T). Therefore Img(T) =Skw n(F). In
Example 3.49 we found that dim(Skw n(F)) =n(nâˆ’1)/2, and therefore
dim(Img( T)) =n(nâˆ’1)
2
as well. â– 119
Example 4.18. LetVbe a vector space over Fwith dim(V) = nand basis B, and let
Ï†B:Vâ†’Fnbe the B-coordinate map. Now, suppose Wis a subspace of Vwith dim(W) =m,
and consider the restriction Ï†B|W:Wâ†’Fn. By Proposition 4.14, Img(Ï†B|W) is a subspace of
Fn. For brevity we define
[W]B= Img( Ï†B|W) =Ï†B(W).
What is the dimension of [ W]B? Let ( wi)m
i=1be any ordered basis for W. We wish to show
thatC= ([wi]B)m
i=1is a basis for [ W]B. Since Ï†Bis injective on Vby Theorem 4.11, it is also
injective on W, and thus Cis a linearly independent set by Proposition 4.16.
Ifxâˆˆ[W]B, then x= [w]Bfor some wâˆˆW, where w=c1w1+Â·Â·Â·+cmwmfor some
c1, . . . , c mâˆˆF. Now, using the linearity properties of Ï†B,
x= [c1w1+Â·Â·Â·+cmwm]B=c1[w1]B+Â·Â·Â·+cm[wm]BâˆˆSpan(C)
Conversely, if xâˆˆSpan(C), so that
x=mX
i=1ci[wi]B
for some choice of constants c1, . . . , c mâˆˆF, then the vector wâˆˆWgiven by
w=mX
i=1ciwi
is such that Ï†B(w) = [w]B=x, and thus xâˆˆÏ†B(W) = [W]B. Hence Span (C) = [W]B, and we
conclude that Cis a basis for [ W]B. It follows that dim([ W]C) =|C|=m, and therefore
dim([ W]C) = dim( W)
for any choice of basis CforW. â– 
Problems
1.LetAx=bbe a nonhomogeneous system of linear equations, where Ais an mÃ—nmatrix.
Define L:Rnâ†’RmbyL(x) =Ax. Without using Theorem 2.40, prove that if x0is a
solution to the system then the systemâ€™s solution set is
x0+ Nul( L) ={x0+y:yâˆˆNul(L)}.120
4.3 â€“ Matrix Representations of Mappings
We begin with the case of Euclidean vector spaces. Let L:Rnâ†’Rmbe an arbitrary linear
mapping. For each 1 â‰¤jâ‰¤nletejbe the jth standard unit vector of Rn, represented as an
nÃ—1 column vector. Thus, as ever, ej= [Î´ij]nÃ—1such that
Î´ij=(
0,ifiÌ¸=j
1,ifi=j
Also, for each 1 â‰¤iâ‰¤mletÏµibe the ith standard unit vector of Rm, represented as an mÃ—1
column vector. Choosing En={ej: 1â‰¤jâ‰¤n}andEm={Ïµi: 1â‰¤iâ‰¤m}to be the bases
forRnandRm, respectively, we view the elements of both Euclidean spaces as being column
vectors in what follows.
For each 1 â‰¤jâ‰¤nwe have L(ej)âˆˆRmso that
L(ej) =mX
i=1aijÏµi
for some scalars a1j, . . . , a mj, and so the Em-coordinates of L(ej) are
L(ej) =ï£®
ï£°a1j...
amjï£¹
ï£».
(We could write [ L(ej)]Em, but since both L(ej) and [ L(ej)]Emare elements of Rmis would be
overly fastidious.) Now,
L(ej) =a1jÏµ1+Â·Â·Â·+amjÏµm=a1jï£®
ï£¯ï£¯ï£°1
0
...
0ï£¹
ï£ºï£ºï£»+Â·Â·Â·+amjï£®
ï£¯ï£¯ï£°0
0
...
1ï£¹
ï£ºï£ºï£»=ï£®
ï£°a1j...
amjï£¹
ï£».
Now, for any xâˆˆRnthere exist scalars x1, . . . , x nsuch that
x=nX
j=1xjej,
and so the En-coordinates of xare
x=ï£®
ï£°x1...
xnï£¹
ï£».
By the linearity of Lwe have
L(x) =L nX
j=1xjej!
=nX
j=1xjL(ej) =nX
j=1xjï£®
ï£°a1j...
amjï£¹
ï£»,121
and hence, defining A= [aij]m,n,
L(x) =ï£®
ï£¯ï£°Pn
j=1xja1j
...Pn
j=1xjamjï£¹
ï£ºï£»=ï£®
ï£°a11Â·Â·Â· a1n.........
am1Â·Â·Â·amnï£¹
ï£»ï£®
ï£°x1...
xnï£¹
ï£»=Ax.
That is, the linear mapping Lhas a corresponding matrix A, called the matrix corresponding
toL:Rnâ†’Rmwith respect to EnandEm. Since Lis arbitrary we have shown that every
linear mapping between Euclidean spaces has a corresponding matrix, and moreover we have
devised a means for determining the entries of the matrix.
Example 4.19. LetL:R3â†’R2be given by
Lï£«
ï£­ï£®
ï£°x1
x2
x3ï£¹
ï£»ï£¶
ï£¸=
3x1+ 2x2âˆ’7x3
4x1âˆ’6x2+ 5x3
Find the matrix corresponding to Lwith respect to the standard bases for R2andR3.
Solution. We must find some matrix A= [aij]2Ã—3such that L(x) =Axfor all xâˆˆR3; that is,

a11a12a13
a21a22a23ï£®
ï£°x1
x2
x3ï£¹
ï£»=
3x1+ 2x2âˆ’7x3
4x1âˆ’6x2+ 5x3
.
This straightaway yields

a11x1+a12x2+a13x3
a21x1+a22x2+a23x3
=
3x1+ 2x2âˆ’7x3
4x1âˆ’6x2+ 5x3
,
from which we immediately obtain
A=
a11a12a13
a21a22a23
=
3 2 âˆ’7
4âˆ’6 5
and weâ€™re done. â– 
Now that we have examined the lay of the land in the case of real Euclidean vector spaces,
it is time to turn our attention to abstract vector spaces. Recall that once an ordered basis B
for any finite-dimensional vector space Vover a field Fhas been chosen, every vector vâˆˆV
can be represented by coordinates with respect to Busing the coordinate map Ï†B, where
Ï†B(v) = [v]B
as discussed in Â§4.1. Depending on whatever is most convenient in a given situation, we may
write [ v]Bas a column or row matrix,
ï£®
ï£°x1...
xnï£¹
ï£»,x1Â·Â·Â·xn
,
or more compactly as [ x1, . . . , x n].122
LetL:Vâ†’Wbe a linear mapping, and let B= (v1, . . . ,vn) be an ordered basis for Vand
C= (w1, . . . ,wm) an ordered basis for W. For each 1 â‰¤jâ‰¤nwe have L(vj)âˆˆW, and since
w1, . . . ,wmspan Wit follows that
L(vj) =a1jw1+Â·Â·Â·+amjwm (4.3)
for some scalars aijâˆˆF, 1â‰¤iâ‰¤m. In terms of coordinates with respect to the bases BandC
we may write (4.3) for each 1 â‰¤jâ‰¤nas

L(vj)
C=ï£®
ï£°a1j...
amjï£¹
ï£».
(Recall that [ vj]B, written as a column matrix, will have 1 in the jth row and 0 in all other
rows.) Now, given any vâˆˆV, there exist scalars x1, . . . , x nsuch that v=x1v1+Â·Â·Â·+xnvn,
and so
[v]B=ï£®
ï£°x1...
xnï£¹
ï£».
Now, by the linearity of LandÏ†C,

L(v)
C=
L(x1v1+Â·Â·Â·+xnvn)
C=x1
L(v1)
C+Â·Â·Â·+xn
L(vn)
C
=x1ï£®
ï£°a11...
am1ï£¹
ï£»+Â·Â·Â·+xnï£®
ï£°a1n...
amnï£¹
ï£»=ï£®
ï£¯ï£°Pn
j=1xja1j
...Pn
j=1xjamjï£¹
ï£ºï£». (4.4)
If we define the mÃ—nmatrix
[L]BC=h
L(v1)
CÂ·Â·Â·
L(vn)
Ci
=ï£®
ï£°a11Â·Â·Â· a1n.........
am1Â·Â·Â·amnï£¹
ï£»,
then we see from (4.4) that

L(v)
C=ï£®
ï£°a11Â·Â·Â· a1n.........
am1Â·Â·Â·amnï£¹
ï£»ï£®
ï£°x1...
xnï£¹
ï£»,
or equivalently

L(v)
C= [L]BC[v]B (4.5)
for all vâˆˆV. The matrix [ L]BCis the matrix corresponding to Lâˆˆ L(V, W )with respect
toBandC, also called the BC-matrix ofL. We may write (4.5) simply as L(v) = [L]BCvif it
is understood that BandCare the bases for VandW, respectively. In any case [ L]BC[v]Bis
seen to give the C-coordinates (as a column matrix) of the vector L(v)âˆˆW. We formalize the
foregoing findings as a theorem for later use.123
Theorem 4.20. LetL:Vâ†’Wbe a linear mapping, with B= (v1, . . . ,vn)an ordered basis
forVandC= (w1, . . . ,wm)an ordered basis for W. The BC-matrix of Lis
[L]BC=h
L(v1)
CÂ·Â·Â·
L(vn)
Ci
, (4.6)
and
[L(v)]C= [L]BC[v]B.
for all vâˆˆV.
The situation simplifies somewhat in the commonly encountered case when Lis a linear
operator on a vector space Vfor which we consider only a single ordered basis B= (v1, . . . ,vn).
To begin with, the BB-matrix of L, [L]BB, is denoted by the more compact symbol [ L]B, and
referred to as either the matrix corresponding to Lwith respect to Bor the B-matrix of
L. The following corollary is immediate.
Corollary 4.21. IfLâˆˆ L(V)andB= (v1, . . . ,vn)is an ordered basis for V, then the B-matrix
ofLis
[L]B=h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
, (4.7)
and
[L(v)]B= [L]B[v]B.
for all vâˆˆV.
Example 4.22. LetL:R2â†’R3be a linear mapping for which
L
1
1
=ï£®
ï£°1
1
1ï£¹
ï£»and L
1
âˆ’1
=ï£®
ï£°1
âˆ’1
âˆ’1ï£¹
ï£». (4.8)
Find the matrix corresponding to Lwith respect to the standard bases for R2andR3, and then
find an expression for L([x, y]âŠ¤).
Solution. The vectors [1 ,1]âŠ¤and [1 ,âˆ’1]âŠ¤are linearly independent and hence form a basis
forR2, so that (4.8) in fact uniquely determines L. Let [ L] denote the matrix corresponding
toLwith respect to the standard bases for R2andR3, which weâ€™ll denote by {e1,e2}andE
respectively. Theorem 4.20 informs us that
[L] =h
L(e1)
E
L(e2)
Ei
=L(e1)L(e2)
,
where the last equality is simply a recognition of the fact that, for any xâˆˆR2, the vector
L(x)âˆˆR3is already in E-coordinates. The problem is we donâ€™t know the values of L(e1) and
L(e2). These could be figured out with a little clever tinkering using the linearity properties of
L, but the tack weâ€™ll take is one which will work in general.
Setting
B=
1 1
1âˆ’1
,124
by Proposition 2.6 and the definition of [ L] we have
[L]B="
[L]
1
1
[L]
1
âˆ’1#
=ï£®
ï£°1 1
1âˆ’1
1âˆ’1ï£¹
ï£». (4.9)
By the methods of Â§2.4 we find that Bis invertible, with
Bâˆ’1="
1
21
2
1
2âˆ’1
2#
.
Right-multiplying through (4.9) by Bâˆ’1at once gives us [ L]:
[L] =ï£®
ï£°1 1
1âˆ’1
1âˆ’1ï£¹
ï£»Bâˆ’1=ï£®
ï£°1 1
1âˆ’1
1âˆ’1ï£¹
ï£»"
1
21
2
1
2âˆ’1
2#
=ï£®
ï£°1 0
0 1
0 1ï£¹
ï£».
Now for any [ x, y]âŠ¤âˆˆR2we have
L
x
y
= [L]
x
y
=ï£®
ï£°1 0
0 1
0 1ï£¹
ï£»
x
y
=ï£®
ï£°x
y
yï£¹
ï£».
The image of Lis easily verified to be Col([ L]), which is the plane y=zinR3. â– 
Example 4.23. LetLâˆˆ L(R2Ã—2) be given by L(A) =AâŠ¤, and let E=E22, the standard
ordered basis for R2Ã—2. Find [ L]E, theE-matrix of L.
Solution. We have E= (E11,E12,E21,E22), where
E11=
1 0
0 0
,E12=
0 1
0 0
,E21=
0 0
1 0
,E22=
0 0
0 1
.
Since
E11= 1E11+ 0E12+ 0E21+ 0E22,E12= 0E11+ 1E12+ 0E21+ 0E22,
and so on, the E-coordinates of the elements of Eare
[E11]E=ï£®
ï£¯ï£¯ï£°1
0
0
0ï£¹
ï£ºï£ºï£»,[E12]E=ï£®
ï£¯ï£¯ï£°0
1
0
0ï£¹
ï£ºï£ºï£»,[E21]E=ï£®
ï£¯ï£¯ï£°0
0
1
0ï£¹
ï£ºï£ºï£»,[E22]E=ï£®
ï£¯ï£¯ï£°0
0
0
1ï£¹
ï£ºï£ºï£».
Now, in general,
L
a b
c d
=
a c
b d
,
so that
L(E12) =L
0 1
0 0
=
0 0
1 0
=E21
and
L(E21) =L
0 0
1 0
=
0 1
0 0
=E12,125
while L(E11) =E11andL(E22) =E22. By Corollary 4.21,
[L]E=h
L(E11)
E
L(E12)
E
L(E21)
E
L(E22)
Ei
=
[E11]E[E21]E[E12]E[E22]E
,
and therefore
[L]E=ï£®
ï£¯ï£¯ï£°1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1ï£¹
ï£ºï£ºï£»
is the E-matrix of L. â– 
Theorem 4.24. IfVandWare vector spaces over Fwithdim(V) =nanddim(W) =m, then
L(V, W )âˆ¼=FmÃ—n.
Proof. LetB= (v1, . . . ,vn) andC= (w1, . . . ,wm) be ordered bases for VandW, respectively.
Proposition 4.9 established that L(V, W ) is a vector space, so define Î¦ : L(V, W )â†’FmÃ—nby
Î¦(L) = [L]BC. By Theorem 4.20,
Î¦(L) =h
Ï†C 
L(v1)
Â·Â·Â·Ï†C 
L(vn)i
,
which shows that Î¦ is a well-defined function since the C-coordinate map Ï†C:Wâ†’Fmis a
well-defined function by the discussion preceding Theorem 4.11. We will show that Î¦ is an
isomorphism.
LetL1, L2âˆˆ L(V, W ). Then by Theorem 4.20,
Î¦(L1+L2) = [L1+L2]BC=h
(L1+L2)(v1)
CÂ·Â·Â·
(L1+L2)(vn)
Ci
;
that is, Î¦( L1+L2) is the matrix with jth column vector [( L1+L2)(vn)]Cfor 1â‰¤jâ‰¤n. Now,
since Ï†C:Wâ†’Fmis linear by Theorem 4.11,

(L1+L2)(vj)
C=
L1(vj) +L2(vj)
C=Ï†C 
L1(vj) +L2(vj)
=Ï†C 
L1(vj)
+Ï†C 
L2(vj)
=
L1(vj)
C+
L2(vj)
C,
and so by the definition of matrix addition,
Î¦(L1+L2) =h
L1(v1)
C+
L2(v1)
CÂ·Â·Â·
L1(vn)
C+
L2(vn)
Ci
=h
L1(v1)
CÂ·Â·Â·
L1(vn)
Ci
+h
L2(v1)
CÂ·Â·Â·
L2(vn)
Ci
= Î¦(L1) + Î¦( L2).
Next, for any câˆˆFandLâˆˆ L(V, W ), again recalling that Ï†Cis linear,
Î¦(cL) =h
Â·Â·Â·
(cL)(vj)
CÂ·Â·Â·i
=h
Â·Â·Â·
cL(vj)
CÂ·Â·Â·i
=h
Â·Â·Â·c
L(vj)
CÂ·Â·Â·i
=ch
Â·Â·Â·
L(vj)
CÂ·Â·Â·i
=cÎ¦(L),
where the fourth equality follows from the definition of scalar multiplication of a matrix. We
see that Î¦ satisfies properties LT1 and LT2, and hence is a linear mapping.126
LetLâˆˆNul(Î¦), so that
Î¦(L) = [L]BC=h
L(v1)
CÂ·Â·Â·
L(vn)
Ci
=Om,n.
Thus, for each 1 â‰¤jâ‰¤n,
Ï†C(L(vj)) =
L(vj)
C= [0] m,1,
which shows that L(vj)âˆˆNul(Ï†C). However Ï†Cis injective, so Nul(Ï†C) ={0}by Proposition
4.15, and hence L(vj) =0. Now, for any vâˆˆVthere exist c1, . . . , c kâˆˆFsuch that
v=nX
j=1cjvj,
and then
L(v) =L nX
j=1cjvj!
=nX
j=1cjL(vj) =nX
j=1cj0=0.
Thus L(v) =0for all vâˆˆV, which is to say L=O, the zero mapping. It follows that
Nul(Î¦)âŠ† {O}, and since the reverse containment obtains from Proposition 4.3(1), we have
Nul(Î¦) = {O}. Hence Î¦ is injective by Proposition 4.15.
Next, let AâˆˆFmÃ—n, so
A=a1Â·Â·Â·an
with
aj=ï£®
ï£°a1j...
amjï£¹
ï£»âˆˆFm
for each 1 â‰¤jâ‰¤n. Let Lâˆˆ L(V, W ) be such that
L(vj) =a1jw1+Â·Â·Â·+amjwm
for each 1 â‰¤jâ‰¤n, so that
[L(vj)]C=aj.
Then
Î¦(L) =h
L(v1)
CÂ·Â·Â·
L(vn)
Ci
=a1Â·Â·Â·an
=A,
which shows that Î¦ is surjective.
Since Î¦ : L(V, W )â†’FmÃ—nis linear, injective, and surjective, we conclude that it is an
isomorphism. Therefore L(V, W )âˆ¼=FmÃ—n. â– 
Corollary 4.25. LetVandWbe finite-dimensional vector spaces over Fwith ordered bases B
andC, respectively. For every mapping Lâˆˆ L(V, W )there is a unique matrix AâˆˆFmÃ—nsuch
thatA= [L]BC. For every matrix AâˆˆFmÃ—nthere is a unique mapping Lâˆˆ L(V, W )such that
[L]BC=A.
Proof. In the proof of Theorem 4.24 it was found that Î¦ : L(V, W )â†’FmÃ—ngiven by Î¦( L) =
[L]BCis an isomorphism. The first statement of the corollary follows from the fact that Î¦ is
a well-defined function on L(V, W ), and the second statement follows from the fact that Î¦ is
surjective and injective. â– 127
Example 4.26. Another way to argue that, in particular, there is a unique matrix Acorre-
sponding to a linear mapping L:Vâ†’Wwith respect to bases BandCis as follows. Suppose
that [ L]BC,[L]â€²
BCâˆˆFmÃ—nare two matrices corresponding to Lwith respect to BandC. Then
[L(v)]C= [L]BC[v]Band [ L(v)]C= [L]â€²
BC[v]B,
and thus 
[L]BCâˆ’[L]â€²
BC
[v]B=0
for all vâˆˆV. Now, setting B= [L]BCâˆ’[L]â€²
BCand observing that [ vj]B= [Î´ij]nÃ—1, we have
B[vj]B=0for each 1 â‰¤jâ‰¤n, or
B[vj]B=ï£®
ï£°a11Â·Â·Â· a1n.........
am1Â·Â·Â·amnï£¹
ï£»ï£®
ï£°Î´1j...
Î´njï£¹
ï£»=ï£®
ï£°Pn
k=1b1kÎ´kj...Pn
k=1bmkÎ´kjï£¹
ï£»=ï£®
ï£°b1j...
bmjï£¹
ï£»=ï£®
ï£°0
...
0ï£¹
ï£».
Thus the jth column vector of Bis0, and since 1 â‰¤jâ‰¤nis arbitrary we conclude that all the
columns of Bconsist of zeros and so B=Om,n. Therefore [ L]BCâˆ’[L]â€²
BC=Om,n, and it follows
that [ L]BC= [L]â€²
BC. â– 
Though there cannot be two distinct matrices corresponding to the same linear mapping
L:Vâ†’Wwith respect to the same bases BandC, a different choice of basis for either Vor
Wwill result in a different corresponding matrix for L. This turns the discussion toward the
idea of changing from one basis Bof a vector space Vto another basis Bâ€², the subject of the
next section.
Problems
1. Suppose that L:R2â†’R3is the linear transformation given by
L
x1
x2
=ï£®
ï£°x2
âˆ’5x1+ 13x2
âˆ’7x1+ 16x2ï£¹
ï£».
Find [ L]BC, the matrix corresponding to Lwith respect to the ordered bases
B=
3
1
,
5
2
and C=ï£«
ï£­ï£®
ï£°1
0
âˆ’1ï£¹
ï£»,ï£®
ï£°âˆ’1
2
2ï£¹
ï£»,ï£®
ï£°0
1
2ï£¹
ï£»ï£¶
ï£¸.128
4.4 â€“ Change of Basis
LetVbe an n-dimensional vector space over F, where nâ‰¥1. Let
B={v1, . . . ,vn}and Bâ€²={vâ€²
1, . . . ,vâ€²
n}
be two distinct bases for V. We would like to devise a ready means of expressing any vector
vâˆˆVgiven in B-coordinates in terms of Bâ€²-coordinates instead. In other words we seek a
mapping Fnâ†’Fngiven by [ v]B7â†’[v]Bâ€²for all vâˆˆV. How to find the mapping? Consider
the identity mapping IV:Vâ†’V, which of course is linear. By Theorem 4.20 the matrix
corresponding to IVwith respect to BandBâ€²is
[IV]BBâ€²=h
IV(v1)
Bâ€²Â·Â·Â·
IV(vn)
Bâ€²i
=h
[v1]Bâ€²Â·Â·Â·[vn]Bâ€²i
, (4.10)
and for all vâˆˆV
[IV]BBâ€²[v]B= [IV(v)]Bâ€²= [v]Bâ€².
This is it! To convert any vâˆˆVfromB-coordinates to Bâ€²-coordinates we simply multiply the
column vector [ v]Bby the matrix [ IV]BBâ€², which happens to be the matrix corresponding to the
identity matrix IVwith respect to BandBâ€², but we will call it the change of basis matrix
from BtoBâ€²(or the coordinate transformation matrix from BtoBâ€²) and denote it by
IBBâ€². We have proven the following.
Theorem 4.27. LetB= (v1, . . . ,vn)andBâ€²= (vâ€²
1, . . . ,vâ€²
n)be two ordered bases for V, and
define IBBâ€²âˆˆFnÃ—nby
IBBâ€²=h
[v1]Bâ€²Â·Â·Â·[vn]Bâ€²i
.
Then
IBBâ€²[v]B= [v]Bâ€²
for all vâˆˆV.
Clearly to find IBBâ€²we must determine the Bâ€²-coordinates of the vectors in B. For each
1â‰¤jâ‰¤nthere exist scalars a1j, . . . , a njsuch that
vj=a1jvâ€²
1+Â·Â·Â·+anjvâ€²
n,
and so
[vj]Bâ€²=ï£®
ï£°a1j...
anjï£¹
ï£».
As the following example illustrates, the task of determining IBBâ€²in practice amounts to solving
a system of equations that has a unique solution.
Example 4.28. LetVbe a vector space with ordered basis B= (v1,v2).
(a)Show that Bâ€²= (vâ€²
1,vâ€²
2) with vâ€²
1=âˆ’v1+ 2v2andvâ€²
2= 3v1+v2is another ordered basis
for the vector space V.
(b) Determine the change of basis matrix from BtoBâ€²,IBBâ€².
(c) Determine the change of basis matrix from Bâ€²toB,IBâ€²B.129
Solution.
(a) We see that dim(V) =|B|= 2, and so by Theorem 3.54(1) to show that Bâ€²is a basis for V
is suffices to show that vâ€²
1andvâ€²
2are linearly independent. Suppose that c1, c2âˆˆFare such that
c1vâ€²
1+c2vâ€²
2=0. (4.11)
This implies that
c1(âˆ’v1+ 2v2) +c2(3v1+v2) =0,
or equivalently
(âˆ’c1+ 3c2)v1+ (2c1+c2)v2=0
Since v1andv2are linearly independent we must have
âˆ’c1+ 3c2= 0
2c1+c2= 0
This system readily informs us that c1=c2= 0, and so (4.11) admits only the trivial solution.
Therefore vâ€²
1andvâ€²
2must be linearly independent.
(b) By Theorem 4.27 we have
IBBâ€²=h
[v1]Bâ€²[v2]Bâ€²i
.
We set
[v1]Bâ€²=
x1
x2
and [ v2]Bâ€²=
y1
y2
,
which is to say
x1vâ€²
1+x2vâ€²
2=v1and y1vâ€²
1+y2vâ€²
2=v2
Using the fact that the coordinate map Ï†Bis a linear mapping, we obtain
1
0
= [v1]B=Ï†B(v1) =Ï†B(x1vâ€²
1+x2vâ€²
2) =x1Ï†B(vâ€²
1) +x2Ï†B(vâ€²
2)
=x1[vâ€²
1]B+x2[vâ€²
2]B=x1
âˆ’1
2
+x2
3
1
=
âˆ’1 3
2 1
x1
x2
, (4.12)
and similarly
0
1
= [v2]B=y1[vâ€²
1]B+y2[vâ€²
2]B=y1
âˆ’1
2
+y2
3
1
=
âˆ’1 3
2 1
y1
y2
. (4.13)
From (4.12) and (4.13) we obtain the systems
âˆ’x1+ 3x2= 1
2x1+x2= 0and
âˆ’y1+ 3y2= 0
2y1+y2= 1
Solving these systems yields x1=âˆ’1/7,x2= 2/7,y1= 3/7, and y2= 1/7. Therefore we have
IBBâ€²=h
[v1]Bâ€²[v2]Bâ€²i
=
x1y1
x2y2
=
âˆ’1/7 3/7
2/7 1/7
.
(c) As for the change of basis matrix from Bâ€²toB, thatâ€™s relatively straightforward since the
vectors in Bâ€²were given in terms of the vectors in B:
IBâ€²B=h
[vâ€²
1]B[vâ€²
2]Bi
=
âˆ’1 3
2 1130
Observe that
IBâ€²BIBBâ€²=IBBâ€²IBâ€²B=I2,
so that IBâ€²BandIBBâ€²are in fact inverses of one another. â– 
Example 4.29. Consider the vector space WâŠ†R3given by
W=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»xâˆ’2y+ 3z= 0ï£¼
ï£½
ï£¾.
Two ordered bases for Ware
B= (u1,u2) =ï£«
ï£­ï£®
ï£°âˆ’1
1
1ï£¹
ï£»,ï£®
ï£°1
2
1ï£¹
ï£»ï£¶
ï£¸ and C= (v1,v2) =ï£«
ï£­ï£®
ï£°2
1
0ï£¹
ï£»,ï£®
ï£°âˆ’3
0
1ï£¹
ï£»ï£¶
ï£¸.
Find the change of basis matrix from BtoC, and use it to find the C-coordinates of vâˆˆW
given that [ v]B= [âˆ’1 4]âŠ¤
Solution. By Theorem 4.27 we have
IBC=h
[u1]C[u2]Ci
,
and so we must find the C-coordinates of u1andu2. Starting with u1, we find a, bâˆˆRsuch
thatav1+bv2=u1; that is,
aï£®
ï£°2
1
0ï£¹
ï£»+bï£®
ï£°âˆ’3
0
1ï£¹
ï£»=ï£®
ï£°âˆ’1
1
1ï£¹
ï£»,
which has ( a, b) = (1 ,1) as the only solution, and hence
[u1]C=
1
1
.
Next, we find a, bâˆˆRsuch that av1+bv2=u2; that is,
aï£®
ï£°2
1
0ï£¹
ï£»+bï£®
ï£°âˆ’3
0
1ï£¹
ï£»=ï£®
ï£°1
2
1ï£¹
ï£»,
which has ( a, b) = (2 ,1) as the only solution, and hence
[u2]C=
2
1
.
Therefore
IBC=
1 2
1 1
is the change of basis matrix from BtoC. Now,
[v]C=IBC[v]B=
1 2
1 1
âˆ’1
4
=
7
3
,
which agrees with the results of Example 4.12. â– 131
Example 4.30. Two ordered bases for the vector space
P2(R) ={a0+a1x+a2x2:a0, a1, a2âˆˆR}
are
B= (1, x, x2) and D= (1,1 +x,1 +x+x2).
(a) Find the change of basis matrix from BtoD.
(b) Find the change of basis matrix from DtoB.
Solution.
(a) We have B= (v1,v2,v3) with v1= 1,v2=x, and v3=x2, and D= (vâ€²
1,vâ€²
2,vâ€²
3) with
vâ€²
1= 1,vâ€²
2= 1 + x, and vâ€²
3= 1 + x+x2. By Theorem 4.27,
IBD=h
[1]D[x]D[x2]Di
.
Setting
[1]D=ï£®
ï£°a1
a2
a3ï£¹
ï£»,[x]D=ï£®
ï£°b1
b2
b3ï£¹
ï£»,[x2]D=ï£®
ï£°c1
c2
c3ï£¹
ï£»,
three equations arise:
a1(1) + a2(1 +x) +a3(1 +x+x2) = 1 ,
b1(1) + b2(1 +x) +b3(1 +x+x2) =x,
c1(1) + c2(1 +x) +c3(1 +x+x2) =x2.
Rewriting these equations as
(a1+a2+a3) + (a2+a3)x+a3x2= 1,
(b1+b2+b3) + (b2+b3)x+b3x2=x,
(c1+c2+c3) + (c2+c3)x+c3x2=x2,
we obtain the systems
(a1+a2+a3= 1
a2+a3= 0
a3= 0(b1+b2+b3= 0
b2+b3= 1
b3= 0(c1+c2+c3= 0
c2+c3= 0
c3= 1
which have solutionsï£®
ï£°a1
a2
a3ï£¹
ï£»=ï£®
ï£°1
0
0ï£¹
ï£»,ï£®
ï£°b1
b2
b3ï£¹
ï£»=ï£®
ï£°âˆ’1
1
0ï£¹
ï£»,ï£®
ï£°c1
c2
c3ï£¹
ï£»=ï£®
ï£°0
âˆ’1
1ï£¹
ï£».
Therefore
IBD=ï£®
ï£°1âˆ’1 0
0 1 âˆ’1
0 0 1ï£¹
ï£».132
(b) By Theorem 4.27
IDB=h
[1]B[1 +x]B[1 +x+x2]Bi
.
Clearly
[1]B=ï£®
ï£°1
0
0ï£¹
ï£»,[1 +x]B=ï£®
ï£°1
1
0ï£¹
ï£»,[1 +x+x2]B=ï£®
ï£°1
1
1ï£¹
ï£»,
and therefore
IDB=ï£®
ï£°1 1 1
0 1 1
0 0 1ï£¹
ï£».
â– 
Proposition 4.31. LetBandBâ€²be ordered bases for a finite-dimensional vector space V. Then
the change of basis matrix IBBâ€²is invertible, with
Iâˆ’1
BBâ€²=IBâ€²B.
Proof. By Theorem 4.27
IBBâ€²[v]B= [v]Bâ€²and IBâ€²B[v]Bâ€²= [v]B,
for all vâˆˆV, and so
IBâ€²BIBBâ€²[v]B=IBâ€²B[v]Bâ€²= [v]B (4.14)
and
IBBâ€²IBâ€²B[v]Bâ€²=IBBâ€²[v]B= [v]Bâ€² (4.15)
for all vâˆˆV.
Letn=dim(V), and fix xâˆˆFn. By Theorem 4.11 the coordinate maps Ï†B, Ï†Bâ€²:Vâ†’Fn
are isomorphisms, and so there exist unique vectors u,uâ€²âˆˆVsuch that
Ï†B(u) = [u]B=xand Ï†Bâ€²(uâ€²) = [uâ€²]Bâ€²=x,
whereupon equations (4.14) and (4.15) give
IBâ€²BIBBâ€²x=xand IBBâ€²IBâ€²Bx=x,
respectively. Since xâˆˆFnis arbitrary, we conclude that
(IBâ€²BIBBâ€²)x=xand ( IBBâ€²IBâ€²B)x=x
for all xâˆˆFn. It follows by Proposition 2.12(1) that
IBâ€²BIBBâ€²=Inand IBBâ€²IBâ€²B=In,
and therefore IBBâ€²is invertible with Iâˆ’1
BBâ€²=IBâ€²B. â– 133
Now suppose that Lis a linear operator on a vector space V, which is to say Lis a linear
mapping Vâ†’V. Let B= (v1, . . . ,vn) be an ordered basis for V. For any vâˆˆVwe have
L(v)âˆˆVgiven by
[L(v)]B= [L]B[v]B,
where
[L]B=h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
by Corollary 4.21. If Bâ€²= (vâ€²
1, . . . ,vâ€²
n) is another ordered basis for V, then another corresponding
matrix [ L]Bâ€²is obtained for the operator L:
[L]Bâ€²=h
L(vâ€²
1)
Bâ€²Â·Â·Â·
L(vâ€²
n)
Bâ€²i
,
where for any vâˆˆVwe have L(v)âˆˆVgiven by
[L(v)]Bâ€²= [L]Bâ€²[v]Bâ€².
We would like to determine the relationship between [ L]Band [ L]Bâ€².
Recall that if ABis defined and B= [b1Â·Â·Â·bn], then by Proposition 2.6
AB=Ab1Â·Â·Â·bn
=Ab 1Â·Â·Â·Abn
.
We therefore have
IBBâ€²[L]B=IBBâ€²h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
=h
IBBâ€²
L(v1)
BÂ·Â·Â·IBBâ€²
L(vn)
Bi
=h
L(v1)
Bâ€²Â·Â·Â·
L(vn)
Bâ€²i
= [L]BBâ€², (4.16)
the last equality a direct consequence of Theorem 4.20. On the other hand,
[L]Bâ€²IBBâ€²= [L]Bâ€²h
[v1]Bâ€²Â·Â·Â·[vn]Bâ€²i
=h
[L]Bâ€²[v1]Bâ€²Â·Â·Â·[L]Bâ€²[vn]Bâ€²i
=h
L(v1)
Bâ€²Â·Â·Â·
L(vn)
Bâ€²i
= [L]BBâ€² (4.17)
Comparing (4.16) and (4.17), we have proven the following.
Proposition 4.32. Suppose Vis a finite-dimensional vector space and Lâˆˆ L(V). IfBandBâ€²
are ordered bases for V, then
IBBâ€²[L]B= [L]Bâ€²IBBâ€²= [L]BBâ€².
Note that [ L]BBâ€²is the matrix corresponding to the operator L:Vâ†’Vin the case when
each input vforLis given in B-coordinates but the output L(v) is given in Bâ€²-coordinates.
That is, the Vcomprising the domain of Lhas basis Band the Vcomprising the codomain of
Lhas basis Bâ€²!
Corollary 4.33. Suppose Vis a finite-dimensional vector space and Lâˆˆ L(V). IfBandBâ€²
are ordered bases for V, then
[L]Bâ€²=IBBâ€²[L]BIâˆ’1
BBâ€².134
Proof. Suppose that BandBâ€²are ordered bases for V. Then IBBâ€²[L]B= [L]Bâ€²IBBâ€²by Proposition
4.32, and thus
IBBâ€²[L]B= [L]Bâ€²IBBâ€²â‡’IBBâ€²[L]BIâˆ’1
BBâ€²= [L]Bâ€²IBBâ€²Iâˆ’1
BBâ€²â‡’[L]Bâ€²=IBBâ€²[L]BIâˆ’1
BBâ€²
since the matrix IBBâ€²is invertible by Proposition 4.31. â– 
Problems
1. The ordered sets
E=
1
0
,
0
1
and B=
1
2
,
âˆ’2
1
are bases for R2(the former being the standard basis).
(a) Find the change of basis matrix IEBfor changing from the basis Eto the basis B.
(b) Use IEBto find the B-coordinates of x= [2,âˆ’5]âŠ¤.
(c) Find IBEusing Proposition 4.31.
2. The ordered sets
B=
7
5
,
âˆ’3
âˆ’1
and C=
1
âˆ’5
,
âˆ’2
2
are bases for R2. Find the change of basis matrices IBCandICB.135
4.5 â€“ The Rank-Nullity Theorem
Given a matrix AâˆˆFnÃ—n, recall from Â§3.5 that the nullity of Ais defined to be nullity (A) =
dim(Nul(A)), and also recall from Â§3.6 that the rank of Amay be characterized as rank(A) =
dim(Col( A)). We now attribute similar terminology to linear mappings.
Definition 4.34. LetL:Vâ†’Wbe a linear mapping. The rank ofLis the dimension of the
image of L,
rank( L) = dim(Img( L)),
and the nullity ofLis the dimension of the null space of L,
nullity( L) = dim(Nul( L)).
From here onward we will use the new notation rank(L) and nullity (L) interchangeably with
the old notation dim(Img(L)) and dim(Nul(L)), since both are used extensively in the literature.
The motivation behind Definition 4.34 will become more apparent presently.
In the statement of the next proposition we take
[Img( L)]C=Ï†C 
Img(L)
={Ï†C(w) :wâˆˆImg(L)}={[w]C:wâˆˆImg(L)}.
Proposition 4.35. LetVandWbe finite-dimensional vector spaces with ordered bases Band
C, respectively. If L:Vâ†’Wis a linear mapping, then
[Img( L)]C= Col([ L]BC).
Proof. LetB= (b1, . . . ,bn), and suppose L:Vâ†’Wis a linear mapping. By Theorem 4.20
we have
[L]BC=h
L(b1)
CÂ·Â·Â·
L(bn)
Ci
.
Now fix yâˆˆ[Img(L)]C. Then y= [w]Cfor some wâˆˆImg(L), and so there exists some
vâˆˆV, where
[v]B=ï£®
ï£°v1...
vnï£¹
ï£»,
such that w=L(v). Since v=v1b1+Â·Â·Â·+vnbn, and both LandÏ†Care linear mappings, we
have
y= [w]C= [L(v1b1+Â·Â·Â·+vnbn)]C=v1[L(b1)]C+Â·Â·Â·+vn[L(bn)]CâˆˆCol([L]BC),
and therefore [Img( L)]CâŠ†Col([L]BC).
Conversely, yâˆˆCol([L]BC) implies that
y=x1[L(b1)]C+Â·Â·Â·+xn[L(bn)]C
for some x1, . . . , x nâˆˆF, and then
y= [L(x1b1+Â·Â·Â·+xnbn)]C
forL(x1b1+Â·Â·Â·+xnbn)âˆˆImg(L) shows yâˆˆ[Img( L)]C. Hence Col([ L]BC)âŠ†[Img( L)]C.â– 136
As a consequence of Proposition 4.35 we have
rank([ L]BC) = dim(Col([ L]BC)) = dim([Img( L)]C) = dim(Img( L)) = rank( L),
where the third equality follows from Example 4.18. Thus the rank of a linear mapping
L:Vâ†’Wequals the rank of its corresponding matrix with respect to any choice of ordered
bases for VandW, and so the thrust behind Definition 4.34 should now be clear. We have
proven the following.
Corollary 4.36. IfVandWare finite-dimensional and Lâˆˆ L(V, W ), then
rank( L) = rank([ L]),
where [L]is the matrix corresponding to Lwith respect to any choice of ordered bases for Vand
W.
Theorem 4.37 (Rank-Nullity Theorem for Mappings ).LetVbe a finite-dimensional
vector space. If L:Vâ†’Wis a linear mapping, then
rank( L) + nullity( L) = dim( V).
Proof. Letn=dim(V),p=nullity (L), and q=rank(L). We must demonstrate that n=p+q.
Ifnullity (L) =n, then Nul(L) =Vby Theorem 3.56(3); that is, L(v) =0for all vâˆˆV, so
Img(L) ={0}and therefore
nullity( L) + rank( L) = dim( V) + dim( {0}) =n+ 0 = n= dim( V)
as desired.
Ifnullity (L) = 0, then Nul(L) ={0}. Let {v1, . . . ,vn}be a basis for V. The set
{L(v1), . . . , L (vn)} âŠ†Img(L) is linearly independent by Proposition 4.16. Now,
S= Span {L(v1), . . . , L (vn)} âŠ†Img(L)
since Img(L) is a subspace of W. Let wâˆˆImg(L), so that w=L(v) for some vâˆˆV. There
exist scalars c1, . . . , c nsuch that v=c1v1+Â·Â·Â·+cnvn, and then
w=L(v) =L nX
i=1civi!
=nX
i=1ciL(vi)âˆˆSpan{L(v1), . . . , L (vn)}=S
shows that Img(L)âŠ†S. Hence S=Img(L) and weâ€™ve shown that Sis a basis for Img(L).
Therefore
nullity( L) + rank( L) = 0 + |S|= 0 + n= dim( V)
once again.
Finally, assume that 0 <nullity (L)< n, so that Nul(L) is neither {0}norV. Since
Nul(L)Ì¸=Vthere exists some vâˆˆVsuch that L(v)Ì¸=0, which implies that Img(L)Ì¸={0}
and hence rank(L) =qâ‰¥1. Also Nul(L)Ì¸={0}implies that nullity (L) =pâ‰¥1. Thus
Img(L) has some basis {w1, . . . ,wq} Ì¸=âˆ…, and Nul(L) has some basis {u1, . . . ,up} Ì¸=âˆ…. Since
{w1, . . . ,wq} âŠ†Img(L), for each 1 â‰¤iâ‰¤qthere exists some viâˆˆVsuch that L(vi) =wi. The
claim is that
B={u1, . . . ,up,v1, . . . ,vq} (4.18)137
is a basis for V.
LetvâˆˆV. Then L(v) =wfor some wâˆˆW, and since wâˆˆImg(L) there exist scalars
b1, . . . , b qsuch that
w=b1w1+Â·Â·Â·+bqwq.
Hence, by the linearity of L,
L(v) =w=b1L(v1) +Â·Â·Â·+bqL(vq) =L(b1v1+Â·Â·Â·+bqvq),
and so
L(vâˆ’(b1v1+Â·Â·Â·+bqvq)) =L(v)âˆ’L(b1v1+Â·Â·Â·+bqvq) =0.
So we have vâˆ’(b1v1+Â·Â·Â·+bqvq)âˆˆNul(L), and since {u1, . . . ,up}is a basis for Nul(L) there
exist scalars a1, . . . , a psuch that
vâˆ’(b1v1+Â·Â·Â·+bqvq) =a1u1+Â·Â·Â·+apup.
From this we obtain
v=a1u1+Â·Â·Â·+apup+b1v1+Â·Â·Â·+bqvqâˆˆSpan{u1, . . . ,up,v1, . . . ,vq},
and therefore
V= Span {u1, . . . ,up,v1, . . . ,vq}.
It remains to shows that u1, . . . ,up,v1, . . . ,vqare linearly independent. Suppose that
a1u1+Â·Â·Â·+apup+b1v1+Â·Â·Â·+bqvq=0. (4.19)
Then
0=L(a1u1+Â·Â·Â·+apup+b1v1+Â·Â·Â·+bqvq)
=L(a1u1+Â·Â·Â·+apup) +L(b1v1+Â·Â·Â·+bqvq)
=a1L(u1) +Â·Â·Â·+apL(up) +b1L(v1) +Â·Â·Â·+bqL(vq)
=a10+Â·Â·Â·+ap0+b1w1+Â·Â·Â·+bqwq
=b1w1+Â·Â·Â·+bqwq,
and since w1, . . . ,wqare linearly independent we obtain b1=Â·Â·Â·=bq= 0. Now (4.19) becomes
a1u1+Â·Â·Â·+apup= 0, but since u1, . . . ,upare linearly independent we obtain a1=Â·Â·Â·=ap= 0.
Hence all coefficients in (4.19) are zero and we conclude that the set Bin(4.18) is a linearly
independent set.
We have now shown that Bis a basis for V, from which is follows that
dim(V) =|B|=p+q= nullity( L) + rank( L)
and the proof is done. â– 
Notice that the rank-nullity theorem we have just proved holds even in the case when Wis
an infinite-dimensional vector space!138
Example 4.38. Recall the mapping T:FnÃ—nâ†’FnÃ—ngiven by
T(A) =Aâˆ’AâŠ¤
2.
in Example 4.17. We found that Nul(T) =Symn(F) in part (b) of the example, and so by
Theorem 4.37 we obtain
dim(Img( T)) = dim( FnÃ—n)âˆ’dim(Nul( T)) =n2âˆ’n(n+ 1)
2=n(nâˆ’1)
2,
recalling from Example 3.48 that dim(FnÃ—n) =n2. We see that, with Theorem 4.37 in hand,
the determination of dim(Img(T)) does not depend on knowing that Img(T) =Skw n(F). Once
the dimensions of a linear mappingâ€™s domain and null space are known, the dimension of the
image follows immediately. â– 
Example 4.39. Determine the dimension of the subspace UofRngiven by
U={xâˆˆRn:aÂ·x= 0},
where nâ‰¥1 and aÌ¸=0.
Solution. Define the mapping L:Rnâ†’Rby
L(x) =aÂ·x,
which is easily verified to be linear using properties of the Euclidean dot product established in
Â§1.4: for any x= [x1, . . . , x n] and y= [y1, . . . , y n] inRnandcâˆˆRwe have
L(x+y) =aÂ·(x+y) =aÂ·x+aÂ·y=L(x) +L(y)
and
L(cx) =aÂ·(cx) =c(aÂ·x) =cL(x).
Moreover,
Nul(L) ={xâˆˆRn:L(x) = 0}={xâˆˆRn:aÂ·x= 0}=U.
Now, Img(L) is a subspace of Rby Proposition 4.14. Since dim(R) = 1, by Theorem 3.56(2)
dim(Img(L)) is either 0 or 1. But dim(Img(L)) = 0 if and only if Img(L) ={0}, which cannot
be the case since aÌ¸=0implies that
L(a) =aÂ·a=âˆ¥aâˆ¥2Ì¸= 0,
and therefore dim(Img(L)) = 1. (By Theorem 3.56(3) it further follows that Img(L) =R
since dim(Img(L)) = dim(R), but we do not need this fact.) Recalling that dim(Rn) =nand
Nul(L) =U, by Theorem 4.37 we have
n= dim( Rn) = dim(Nul( L)) + dim(Img( L)) = dim( U) + 1,
and hence dim( U) =nâˆ’1. That is, Uis a hyperplane in Rn. â– 
Theorem 4.40 (Rank-Nullity Theorem for Matrices ).IfAâˆˆFmÃ—n, then
rank(A) + nullity( A) =n.139
Proof. Suppose that AâˆˆFmÃ—n. Let L:Fnâ†’Fmbe given by L(x) =Ax. Then Lis a linear
mapping such that
Nul(L) ={xâˆˆFn:L(x) =0}={xâˆˆFn:Ax=0}= Nul( A).
Also by Proposition 4.35 we have
Img(L) = Col( A).
with respect to the standard bases. Now by the Rank-Nullity Theorem for Mappings we have
n= dim( Fn) = rank( L) + nullity( L) = dim(Img( L)) + dim(Nul( L))
= dim(Col( A)) + dim(Nul( A)) = rank( A) + nullity( A).
That is, rank( A) + nullity( A) =n, as desired. â– 
Example 4.41. Find the dimension of the solution space Sfor the system of equations
4x1+ 7x2âˆ’Ï€x3= 0
2x1âˆ’x2+x3= 0
and also find a basis for S.
Solution. Letting
x=ï£®
ï£°x1
x2
x3ï£¹
ï£»and A=
4 7 âˆ’Ï€
2âˆ’1 1
,
we find that Sis the set of all xâˆˆR3that satisfy the matrix equation Ax=0, and so
S= Nul( A). By the Rank-Nullity Theorem for Matrices we have
dim(S) = nullity( A) = dim( R3)âˆ’rank(A) = 3âˆ’rank(A).
Since
A=
4 7 âˆ’Ï€
2âˆ’1 1
âˆ’2r2+r1â†’r1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
0 9 âˆ’2âˆ’Ï€
2âˆ’1 1
and the row rank of the matrix on the right is clearly 2, it follows that rank(A) = 2 and so
dim(S) = 3âˆ’2 = 1.
Next we set to the task of finding a basis for S. From the second equation in the system we
have
x2= 2x1+x3. (4.20)
Putting this into the first equation then yields
4x1+ 7(2 x1+x3)âˆ’Ï€x3= 0,
and thus
x1=Ï€âˆ’7
18x3. (4.21)
Substituting this into (4.20), we get
x2= 2Ï€âˆ’7
18x3
+x3=Ï€+ 2
9x3. (4.22)140
From (4.21) and (4.22), replacing x3with t, we find that
S=(
tÏ€âˆ’7
18,Ï€+ 2
9,1âŠ¤
:tâˆˆR)
, (4.23)
which shows that
B=(Ï€âˆ’7
18,Ï€+ 2
9,1âŠ¤)
would qualify as a basis for S. This is not the only possibility, however, since any nonzero element
ofSwill span S. For instance, if we set t= 18 we find from (4.23) that [ Ï€âˆ’7,2Ï€+ 4,18]âŠ¤is in
S, and so
B=
[Ï€âˆ’7,2Ï€+ 4,18]âŠ¤	
is a basis for S. â– 
Example 4.42. Find the dimension of the subspace of R7consisting of all vectors that are
orthogonal to the vectors
r1= [1,1,âˆ’2,3,4,5,6]âŠ¤and r2= [0,0,2,1,0,7,0]âŠ¤,
Solution. The subspace of R7in question consists of the set of vectors
S={xâˆˆR7:r1Â·x= 0 and r2Â·x= 0}.
Indeed, if we define AâˆˆFmÃ—nby
A=
1 1âˆ’2 3 4 5 6
0 0 2 1 0 7 0
then we find that
S={xâˆˆR7:Ax=0}= Nul( A).
By Theorem 4.40 we have
dim(S) = dim(Nul( A)) = 7 âˆ’rank(A).
Now, Ais already in row-echelon form, and so it should be clear that the row vectors of A,
which are râŠ¤
1andrâŠ¤
2, are linearly independent. Thus rank(A) =row-rank (A) = 2, and therefore
dim(S) = 7âˆ’2 = 5. â– 141
4.6 â€“ Dimension and Rank Formulas
Proposition 4.43. IfUandWare subspaces of a vector space V, then
dim(U+W) = dim( U) + dim( W)âˆ’dim(Uâˆ©W).
Proof. Suppose that UandWare subspaces of a vector space V. The product space UÃ—W
defined in section 3.1 is a vector space, and so we define a mapping L:UÃ—Wâ†’Vby
L(u, w) =uâˆ’w. For any ( u,w),(uâ€²,wâ€²)âˆˆUÃ—WandcâˆˆRwe have
L((u,w) + (uâ€²,wâ€²)) =L(u+uâ€²,w+wâ€²) = (u+uâ€²)âˆ’(w+wâ€²)
= (uâˆ’w) + (uâ€²âˆ’wâ€²) =L(u,w) +L(uâ€²,wâ€²)
and
L(c(u,w)) =L(cu, cw) =cuâˆ’cw=c(uâˆ’w) =cL(u,w),
soLis a linear mapping.
IfvâˆˆImg(L), then there exists some ( u,w)âˆˆUÃ—Wsuch that
L(u,w) =uâˆ’w=v,
sov=u+ (âˆ’w)âˆˆU+Wand we have Img(L)âŠ†U+W. IfvâˆˆU+W, then v=u+wfor
some uâˆˆUandwâˆˆW, and then
L(u,âˆ’w) =uâˆ’(âˆ’w) =u+w=v
shows vâˆˆImg(L) and thus U+WâŠ†Img(L). Therefore Img( L) =U+W.
LetuâˆˆUandwâˆˆW, and suppose ( u,w)âˆˆNul(L). Then L(u,w) =uâˆ’w=0, which
implies that w=uand thus ( u,w) = (u,u) with uâˆˆUâˆ©W. From this we conclude that
Nul(L)âŠ† {(v,v) :vâˆˆUâˆ©W}, and since the reverse containment is easy to verify we obtain
Nul(L) ={(v,v) :vâˆˆUâˆ©W}. (4.24)
Let{v1, . . . ,vr}be a basis for Uâˆ©W. We wish to show the set
B={(vi,vi) : 1â‰¤iâ‰¤r}
is a basis for Nul(L). Let ( u,w)âˆˆNul(L). By (4.24) , (u,w) = (v,v) for some vâˆˆUâˆ©W, and
since there exist scalars c1, . . . , c rsuch that v=c1v1+Â·Â·Â·+crvr, we find that
(u,v) = rX
i=1civi,rX
i=1civi!
=rX
i=1(civi, civi) =rX
i=1ci(vi,vi)
and thus
(u,v)âˆˆSpan{(vi,vi) : 1â‰¤iâ‰¤r}= Span( B). (4.25)
On the other hand if we suppose that (4.25) is true, so that ( u,w) =Pr
i=1ci(vi,vi) for some
scalars c1, . . . , c r, then
L(u,w) =L rX
i=1ci(vi,vi)!
=rX
i=1ciL(vi,vi) =rX
i=1ci(viâˆ’vi) =0142
demonstrates that ( u,w)âˆˆNul(L) and so
Nul(L) = Span {(vi,vi) : 1â‰¤iâ‰¤r}= Span( B).
Next, set
rX
i=1ci(vi,vi) = (0,0).
Then
(0,0) =rX
i=1(civi, civi) = rX
i=1civi,rX
i=1civi!
,
which gives
rX
i=1civi=0
and hence c1=Â·Â·Â·=cr= 0 since v1, . . . ,vrare linearly independent. Therefore Bis a linearly
independent set and Span( B) = Nul( L), which shows that Bis a basis for Nul( L) and then
dim(Nul( L)) =|B|=r= dim( Uâˆ©W).
Because L:UÃ—Wâ†’Vis a linear mapping,
dim(UÃ—W) = dim(Nul( L)) + dim(Img( L))
by Theorem 4.37. But Img( L) =U+Wand dim(Nul( L)) = dim( Uâˆ©W), so that
dim(UÃ—W) = dim( Uâˆ©W) + dim( U+W).
InÂ§3.5 we established that dim( UÃ—W) = dim( U) + dim( W), and thus
dim(U) + dim( W) = dim( Uâˆ©W) + dim( U+W)
obtains and the proof is done. â– 
Recall the concept of a direct sum introduced in section Â§3.3. The dimension formula
furnished by Proposition 4.43 becomes especially nice if a vector space Vhappens to be the
direct sum of two subspaces UandW.
Proposition 4.44. LetVbe a vector space. If UandWare subspaces such that V=UâŠ•W,
then dim(V) = dim( U) + dim( W).
Proof. From Uâˆ©W={0}we have dim( Uâˆ©W) = 0, so that
dim(U+W) = dim( U) + dim( W)
by Proposition 4.43. The conclusion follows from U+W=V. â– 
Theorem 4.45. LetVbe a vector space, and let U1, . . . , U nbe subspaces of V. Then
V=nM
k=1Ukâ‡’dim(V) =nX
k=1dim(Uk).143
Proof. The statement of the proposition is trivially true when n= 1. Let nâˆˆNbe arbitrary,
and suppose the proposition is true for n. Let U1, . . . , U n+1be subspaces of a vector space V
such that
V=n+1M
k=1Uk.
Define U=U1+Â·Â·Â·+UnandW=Un+1, so that V=U+W. Note that Uis a subspace of V
by Proposition 3.20. By Definition 3.21 it is immediate that
Uâˆ©W=Un+1âˆ©nX
k=1Uk={0},
and so in fact V=UâŠ•W.
LetvâˆˆU, so that for 1 â‰¤kâ‰¤nthere exist vectors ukâˆˆUksuch that
nX
k=1uk=v.
Suppose that for 1 â‰¤kâ‰¤nthe vectors uâ€²
kâˆˆUkare such that
nX
k=1uâ€²
k=v
also. Setting un+1=uâ€²
n+1=0, we obtain
v=n+1X
k=1uk=n+1X
k=1uâ€²
kâˆˆV=n+1M
k=1Uk,
and so by Theorem 3.23 we must have uk=uâ€²
kfor all 1 â‰¤kâ‰¤n+ 1. Since vâˆˆUis arbitrary,
we conclude that for each vâˆˆUthere exist unique vectors u1âˆˆU1, . . . ,unâˆˆUnsuch that
v=u1+Â·Â·Â·+un, and therefore
U=nM
k=1Uk
by Theorem 3.23. Now, by Proposition 4.44 and our inductive hypothesis,
dim(V) = dim( U) + dim( W) =nX
k=1dim(Uk) + dim( Un+1) =n+1X
k=1dim(Uk)
as desired. â– 
Proposition 4.46. IfVis a subspace of Rn, then dim(V) + dim( VâŠ¥) =n.
Proof. Suppose that Vis a subspace of Rn. Setting r= dim( V), so that râ‰¤n, let
BV={b1, . . . ,br}
be a basis for V, where
bi=ï£®
ï£°bi1...
binï£¹
ï£»144
for each 1 â‰¤iâ‰¤r. Let Abe the nÃ—nmatrix given by
A=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°bâŠ¤
1...
bâŠ¤
r
0
...
0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°b11Â·Â·Â· b1n.........
br1Â·Â·Â· brn
0Â·Â·Â· 0
.........
0Â·Â·Â· 0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
and observe that
Row(A) = Span {b1, . . . ,br,0}= Span {b1, . . . ,br}=V.
Now, define L:Rnâ†’Rnto be the linear mapping given by L(x) =Axfor all xâˆˆRn. Since
Img(L) = Col( A) by Proposition 4.35, we have
dim(Img( L)) = dim(Col( A)) = rank( A) = dim(Row( A)) = dim( V).
Suppose xâˆˆNul(L), so that Ax=0and we obtain bâŠ¤
ix= 0 for all 1 â‰¤iâ‰¤r. Let vâˆˆV.
Then v=a1b1+Â·Â·Â·+arbrfor some a1, . . . , a râˆˆR, and since
xÂ·v=vâŠ¤x= (a1bâŠ¤
1+Â·Â·Â·+arbâŠ¤
r)x=a1bâŠ¤
1x+Â·Â·Â·+arbâŠ¤
rx=a1(0) +Â·Â·Â·+an(0) = 0
we conclude that xâˆˆVâŠ¥and so Nul( L)âŠ†VâŠ¥.
Now suppose that xâˆˆVâŠ¥. Then xÂ·v= 0 for all vâˆˆV, and in particular xÂ·bi= 0 for
each 1 â‰¤iâ‰¤r. Thus
L(x) =Ax=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°bâŠ¤
1x
...
bâŠ¤
rx
0
...
0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°xÂ·b1...
xÂ·br
0
...
0ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ï£®
ï£°0
...
0ï£¹
ï£»=0,
which shows that xâˆˆNul(L) and so VâŠ¥âŠ†Nul(L).
We now have Nul( L) =VâŠ¥, and so of course dim(Nul( L)) = dim( VâŠ¥). By Theorem 4.37
dim(Rn) = dim(Nul( L)) + dim(Img( L)),
and from this we obtain n= dim( VâŠ¥) + dim( V). â– 
For the remainder of this section we develop a few formulas involving the ranks of matrices
that will be useful later on.
Theorem 4.47.
1.IfAâˆˆFnÃ—nis invertible, then rank(A) = rank( Aâˆ’1).
2.IfAâˆˆFmÃ—mis invertible and BâˆˆFmÃ—n, then rank(AB) = rank( B).
3.IfBâˆˆFnÃ—nis invertible and AâˆˆFmÃ—n, then rank(AB) = rank( A).
4.IfA,CâˆˆFnÃ—nare invertible and BâˆˆFnÃ—n, then rank(ABC ) = rank( B).145
Proof.
Proof of Part (1). Suppose AâˆˆFnÃ—nis invertible. Since Aâˆ’1is also invertible, both AandAâˆ’1
are row-equivalent to Inby Theorem 2.30, and then by Theorem 3.66 we have rank(A) =rank(In)
and rank( Aâˆ’1) = rank( In). Therefore rank( A) = rank( Aâˆ’1) =n.
Proof of Part (2). Suppose AâˆˆFmÃ—mis invertible and BâˆˆFmÃ—n. By the Rank-Nullity
Theorem for Matrices,
rank(AB) + nullity( AB) =nand rank( B) + nullity( B) =n,
and hence
rank(AB) + nullity( AB) = rank( B) + nullity( B). (4.26)
Now, since Ais invertible,
A(Bx) =0â‡’Aâˆ’1[A(Bx)] =Aâˆ’10â‡’Bx=0,
and so
xâˆˆNul(B)â‡”Bx=0â‡”A(Bx) =0â‡”(AB)x=0â‡”xâˆˆNul(AB),
Hence Nul(B) =Nul(AB), so that nullity (B) =nullity (AB), and then (4.26) gives rank(AB) =
rank(B).
Proof of Part (3). Suppose BâˆˆFnÃ—nis invertible and AâˆˆFmÃ—n. Since BâŠ¤is invertible by
Proposition 2.32, we use Problem 3.8.2 and Part (2) to obtain
rank(AB) = rank 
(AB)âŠ¤
= rank( BâŠ¤AâŠ¤) = rank( AâŠ¤) = rank( A).
Proof of Part (4). Suppose A,CâˆˆFnÃ—nare invertible and BâˆˆFnÃ—n. We have
rank(ABC ) = rank 
A(BC)
= rank( BC) = rank( B),
where the second equality follows from Part (2), and the third equality follows from Part (3). â– 146
4.7 â€“ Compositions of Mappings
Definition 4.48. Given mappings S:Xâ†’YandT:Yâ†’Z, thecomposition ofTwithS
is the mapping Tâ—¦S:Xâ†’Zgiven by
(Tâ—¦S)(x) =T(S(x))
for all xâˆˆX.
The composition operation â—¦is not commutative in general (i.e. Tâ—¦Sis generally not the
same function as Sâ—¦T), but it does have associative and distributive properties as the next two
theorems establish.
Theorem 4.49. LetX1,X2,X3,X4be sets. If T1:X1â†’X2,T2:X2â†’X3, and T3:X3â†’X4
are mappings, then
T3â—¦(T2â—¦T1) = (T3â—¦T2)â—¦T1.
Proof. For any xâˆˆX1,
(T3â—¦(T2â—¦T1))(x) =T3((T2â—¦T1)(x)) =T3(T2(T1(x)))
= (T3â—¦T2)(T1(x)) = (( T3â—¦T2)â—¦T1)(x).
Therefore T3â—¦(T2â—¦T1) = (T3â—¦T2)â—¦T1. â– 
Given mappings
T1:X1â†’X2, T 2:X2â†’X3, T 3:X3â†’X4,
it is routine to write the composition as simply T3â—¦T2â—¦T1without fear of ambiguity. Whether
we interpret T3â—¦T2â—¦T1as signifying T3â—¦(T2â—¦T1) or (T3â—¦T2)â—¦T1makes no difference according to
Theorem 4.49. This idea extends naturally to the composition of any finite number of mappings.
Theorem 4.50. LetV1,V2,V3be vector spaces over F. Let S1, S2:V1â†’V2andT1, T2:V2â†’V3
be mappings, and let câˆˆF. Then
1. (T1Â±T2)â—¦S1=T1â—¦S1Â±T2â—¦S1
2.T1â—¦(S1Â±S2) =T1â—¦S1Â±T1â—¦S2ifT1is linear.
3. (cT1)â—¦S1=c(T1â—¦S1)
4.T1â—¦(cS1) =c(T1â—¦S1)ifT1is linear.
Proof.
Proof of Part (1). For any uâˆˆV1
((T1+T2)â—¦S1)(u) = (T1+T2)(S1(u)) =T1(S1(u)) +T2(S1(u))
= (T1â—¦S1)(u) + (T2â—¦S1)(u) = (T1â—¦S1+T2â—¦S1)(u),
and therefore ( T1+T2)â—¦S1=T1â—¦S1+T2â—¦S1. The proof that ( T1âˆ’T2)â—¦S1=T1â—¦S1âˆ’T2â—¦S1
is similar.147
Proof of Part (2). For any uâˆˆV1
(T1â—¦(S1+S2))(u) =T1((S1+S2)(u)) =T1(S1(u)) +S2(u))
=T1(S1(u)) +T1(S2(u)) = ( T1â—¦S1)(u) + (T1â—¦S2)(u)
= (T1â—¦S1+T1â—¦S2)(u),
where the third equality obtains from the linearity of T1. Therefore
T1â—¦(S1+S2) =T1â—¦S1+T1â—¦S2
ifT1is linear. The proof that T1â—¦(S1âˆ’S2) =T1â—¦S1âˆ’T1â—¦S2ifT1is linear is similar.
Proof of Part (3). For any uâˆˆV1
((cT1)â—¦S1)(u) = (cT1)(S1(u)) =cT1(S1(u)) =c(T1â—¦S1)(u),
and therefore ( cT1)â—¦S1=c(T1â—¦S1).
Proof of Part (4). Suppose that T1is a linear mapping. For any uâˆˆV1
(T1â—¦(cS1))(u) =T1((cS1)(u)) =T1(cS1(u)) =cT1(S1(u)) =c(T1â—¦S1)(u),
where the third equality obtains from the linearity of T1. Therefore T1â—¦(cS1) =c(T1â—¦S1) ifT1
is linear. â– 
Proposition 4.51. LetV1,V2,V3be vector spaces over F. IfL1:V1â†’V2andL2:V2â†’V3
are linear mappings, then the composition L2â—¦L1:V1â†’V3is linear.
Proof. For any u,vâˆˆV1we have
(L2â—¦L1)(u+v) =L2(L1(u+v)) =L2(L1(u) +L1(v))
=L2(L1(u)) +L2(L1(v)) = ( L2â—¦L1)(u) + (L2â—¦L1)(v),
and for any câˆˆFanduâˆˆV1we have
(L2â—¦L1)(cu) =L2(L1(cu)) =L2(cL1(u)) =cL2(L1(u)) =c(L2â—¦L1)(u).
Therefore L2â—¦L1is linear. â– 
IfL:Vâ†’Vis a linear operator on a vector space V, then Lâ—¦Lis likewise a linear operator
onV, as is Lâ—¦Lâ—¦Land so on. A useful notation is to let L2denote Lâ—¦L,L3denote Lâ—¦Lâ—¦L,
and in general
Ln=Lâ—¦Lâ—¦ Â·Â·Â· â—¦ L|{z }
n Lâ€™s
for any nâˆˆN. We also define L0=IV, the identity operator on V.
A linear operator Î  : Vâ†’Vfor which Î 2= Î  is called a projection and is of special
theoretical importance. We have
Î (Î (v)) = (Î  â—¦Î )(v) = Î 2(v) = Î ( v)148
for any vâˆˆV.
Example 4.52. LetVbe a vector space, and let Î  : Vâ†’Vbe a projection.
(a) Show that V= Nul(Î ) + Img(Î ).
(b) Show that Nul(Î ) âˆ©Img(Î ) = {0}.
Therefore V= Nul(Î ) âŠ•Img(Î ).
Solution.
(a)LetvâˆˆV, and let IV:Vâ†’Vbe the identity operator on Vso that IV(v) =v. By
Theorem 4.50(2) we have
Î (vâˆ’Î (v)) = Î ( IV(v)âˆ’Î (v)) = Î (( IVâˆ’Î )(v)) = (Î  â—¦(IVâˆ’Î ))(v)
= (Î â—¦IVâˆ’Î â—¦Î )(v) = (Î  â—¦IV)(v)âˆ’(Î â—¦Î )(v)
= Î ( IV(v))âˆ’Î 2(v) = Î ( v)âˆ’Î (v) =0,
and so vâˆ’Î (v)âˆˆNul(Î ). Noting that Î ( v)âˆˆImg(Î ), we readily obtain
v= (vâˆ’Î (v)) + Î ( v)âˆˆNul(Î ) + Img(Î ) .
Thus VâŠ†Nul(Î ) + Img(Î ), and since the reverse containment follows from the closure
properties of a vector space, we conclude that V= Nul(Î ) + Img(Î ).
(b)LetvâˆˆNul(Î )âˆ©Img(Î ). Then Î ( v) =0and there exists some uâˆˆVsuch that Î ( u) =v.
With these results and the hypothesis Î 2= Î , we have
0= Î (v) = Î (Î ( u)) = Î 2(u) = Î ( u) =v,
implying vâˆˆ {0}and so Nul(Î )âˆ©Img(Î )âŠ† {0}. The reverse containment holds
since Nul(Î ) and Img(Î ) are subspaces of Vand so must both contain 0. Therefore
Nul(Î ) âˆ©Img(Î ) = {0}. â– 
We found in Â§4.4 (Theorem 4.24) that every linear mapping L:Vâ†’Whas a unique
corresponding matrix [ L]BCwith respect to chosen bases BandCfor the vector spaces Vand
W, respectively. Let U,V, and Wbe vector spaces with bases A,B, andC, respectively. Let
L1:Uâ†’Vhave corresponding matrix [ L1]ABwith respect to AandB, and let L2:Vâ†’W
have corresponding matrix [ L2]BCwith respect to BandC, so that
[L1(u)]B= [L1]AB[u]Aand [ L2(v)]C= [L2]BC[v]B.
Thus for any uâˆˆUwe have
[(L2â—¦L1)(u)]C= [L2(L1(u))]C= [L2]BC[L1(u)]B= [L2]BC[L1]AB[u]A
Thus we see that the matrix Acorresponding to L2â—¦L1:Uâ†’Wwith respect to AandCis
given by A= [L2]BC[L1]AB. That is,
[L2â—¦L1]AC= [L2]BC[L1]AB
and we have proven the following.149
Proposition 4.53. LetL1:V1â†’V2andL2:V2â†’V3be linear mappings, and let Bibe a basis
forVi. Then
[L2â—¦L1]B1B3= [L2]B2B3[L1]B1B2.150
4.8 â€“ The Inverse of a Mapping
Definition 4.54. LetT:Xâ†’Ybe a mapping. We say Tisinvertible if there exists a
mapping S:Yâ†’Xsuch that Sâ—¦T=IXandTâ—¦S=IY, in which case Sis called the inverse
ofTand we write S=Tâˆ’1.
Proposition 4.55. IfT:Xâ†’Yis an invertible mapping, then
Img(Tâˆ’1) = Dom( T) =X and Dom( Tâˆ’1) = Img( T) =Y,
and for all xâˆˆX,yâˆˆY,
T(x) =yâ‡”Tâˆ’1(y) =x.
Proof. Suppose that T:Xâ†’Yis invertible, so that there is a mapping Tâˆ’1:Yâ†’Xsuch
thatTâˆ’1â—¦T=IXandTâ—¦Tâˆ’1=IY. From this it is immediate that
Img(Tâˆ’1)âŠ†X= Dom( T) and Img( T)âŠ†Y= Dom( Tâˆ’1).
LetxâˆˆX, so that T(x) =yfor some yâˆˆY. Then
Tâˆ’1(y) =Tâˆ’1(T(x)) = ( Tâˆ’1â—¦T)(x) =IX(x) =x
shows that xâˆˆImg(Tâˆ’1), and so Img( Tâˆ’1) =Xand
T(x) =yâ‡’Tâˆ’1(y) =x
for all xâˆˆX.
Next, for any yâˆˆYwe have Tâˆ’1(y) =xfor some xâˆˆX, whence
T(x) =T(Tâˆ’1(y)) = ( Tâ—¦Tâˆ’1)(y) =IY(y) =y
shows that yâˆˆImg(T), and so Img( T) =Yand
Tâˆ’1(y) =xâ‡’T(x) =y
for all yâˆˆY. â– 
Proposition 4.56. IfS:Xâ†’YandT:Yâ†’Zare invertible mappings, then
(Tâ—¦S)âˆ’1=Sâˆ’1â—¦Tâˆ’1.
Proof. Suppose that S:Xâ†’YandT:Yâ†’Zare invertible mappings. Then SandTare
bijective, from which it follows that Tâ—¦Sis likewise bijective and so ( Tâ—¦S)âˆ’1:Zâ†’Xexists.
That is, Tâ—¦Sis invertible.
LetzâˆˆZ. Then ( Tâ—¦S)âˆ’1(z) =xfor some xâˆˆX, and by repeated use of Proposition 4.55
we obtain
(Tâ—¦S)âˆ’1(z) =xâ‡”(Tâ—¦S)(x) =zâ‡”T(S(x)) =z
â‡”S(x) =Tâˆ’1(z)â‡”x=Sâˆ’1(Tâˆ’1(z)).
â‡”(Sâˆ’1â—¦Tâˆ’1)(z) =x151
Hence
(Tâ—¦S)âˆ’1(z) = (Sâˆ’1â—¦Tâˆ’1)(z)
for all zâˆˆZ, and we conclude that ( Tâ—¦S)âˆ’1=Sâˆ’1â—¦Tâˆ’1. â– 
Proposition 4.57. LetVandWbe vector spaces over F. IfL:Vâ†’Wis an invertible linear
mapping, then its inverse Lâˆ’1:Wâ†’Vis also linear.
Proof. Suppose that L:Vâ†’Wis an invertible linear mapping, and let Lâˆ’1:Wâ†’Vbe its
inverse. Let w1,w2âˆˆW. Then Lâˆ’1(w1) and Lâˆ’1(w2) are vectors in V, and by the linearity of
Lwe obtain
L(Lâˆ’1(w1) +Lâˆ’1(w2)) =L(Lâˆ’1(w1)) +L(Lâˆ’1(w2))
= (Lâ—¦Lâˆ’1)(w1) + (Lâ—¦Lâˆ’1)(w2)
=IW(w1) +IW(w2) =w1+w2,
and hence
Lâˆ’1(w1+w2) =Lâˆ’1(w1) +Lâˆ’1(w2)
by Proposition 4.55.
Next, let wâˆˆWandcâˆˆF. Then cLâˆ’1(w) is a vector in V, and from
L(cLâˆ’1(w)) =cL(Lâˆ’1(w)) =c(Lâ—¦Lâˆ’1)(w) =cIW(w) =cw
we obtain
Lâˆ’1(cw) =cLâˆ’1(w)
by Proposition 4.55. â– 
There is a close connection between the idea of an invertible linear mapping and that of an
invertible matrix which the following theorem makes clear.
Theorem 4.58. LetVandWbe vector spaces with ordered bases BandC, respectively, and
suppose that dim(V) =dim(W) =nandLâˆˆ L(V, W ). Then Lis invertible if and only if [L]BC
is invertible, in which case
[L]âˆ’1
BC= [Lâˆ’1]CB.
Proof. Suppose that Lis invertible. Then there exists a mapping Lâˆ’1:Wâ†’Vsuch that
Lâˆ’1â—¦L=IVandLâ—¦Lâˆ’1=IW, and since Lâˆ’1is linear by Proposition 4.57 it has a corresponding
matrix [ Lâˆ’1]CBâˆˆFnÃ—nwith respect to the bases CandB. For all vâˆˆVwe have
[L(v)]C= [L]BC[v]B,
and for all wâˆˆW
[Lâˆ’1(w)]B= [Lâˆ’1]CB[w]C.
Now, for all wâˆˆW,
 
[L]BC[Lâˆ’1]CB
[w]C= [L]BC 
[Lâˆ’1]CB[w]C
= [L]BC[Lâˆ’1(w)]B
= [L(Lâˆ’1(w))]C= [(Lâ—¦Lâˆ’1)(w)]C= [IW(w)]C= [w]C,152
which shows that [ L]BC[Lâˆ’1]CB=Inby Proposition 2.12(1). Similarly, for all vâˆˆV,
 
[Lâˆ’1]CB[L]BC
[v]B= [Lâˆ’1]CB 
[L]BC[v]B
= [Lâˆ’1]CB[L(v)]C
= [Lâˆ’1(L(v))]B= [(Lâˆ’1â—¦L)(v)]B= [IV(v)]B= [v]B,
and so [ Lâˆ’1]CB[L]BC=In. Thus [ Lâˆ’1]CBis the inverse for [ L]BC, which is to say [ L]BCis invertible
and
[L]âˆ’1
BC= [Lâˆ’1]CB.
For the converse, suppose that [ L]BCis invertible. Then there exists a matrix [ L]âˆ’1
BCâˆˆFnÃ—n
such that
[L]BC[L]âˆ’1
BC= [L]âˆ’1
BC[L]BC=In.
Let Î› : Wâ†’Vbe the linear mapping with corresponding matrix [ L]âˆ’1
BCwith respect to Cand
B, so that
[Î›(w)]B= [L]âˆ’1
BC[w]C
for each wâˆˆW. For each wâˆˆWwe have
[(Lâ—¦Î›)(w)]C= [L(Î›(w))]C= [L]BC[Î›(w)]B= [L]BC[L]âˆ’1
BC[w]C= [w]C,
and since the coordinate map w7â†’[w]Cis an isomorphismâ€”and hence injectiveâ€”by Theorem
4.11, it follows that ( Lâ—¦Î›)(w) =w. Next, for each vâˆˆVwe have
[(Î›â—¦L)(v)]B= [Î›( L(v))]B= [L]âˆ’1
BC[L(v)]C= [L]âˆ’1
BC[L]BC[v]B= [v]B,
and since the coordinate map v7â†’[v]Bis an isomorphism it follows that (Î› â—¦L)(v) =v. Since
Lâ—¦Î› =IWand Î› â—¦L=IV, we conclude that Î› is the inverse of L, and therefore Lis invertible.
Finally, since Î› = Lâˆ’1and [Î›] CB= [L]âˆ’1
BC, we find that
[L]âˆ’1
BC= [Lâˆ’1]CB
once again. â– 
The result [ Lâˆ’1]CB= [L]âˆ’1
BCgiven in the theorem reduces the task of finding the inverse of
an invertible linear mapping Lâˆˆ L(V, W ) to an exercise in finding the inverse of the matrix
corresponding to Lwith respect to BandC. Indeed, once a linear mappingâ€™s corresponding
matrix is known, the mapping itself is effectively known.
Corollary 4.59. LetVbe a vector space with ordered basis B, and let Lâˆˆ L(V). Then Lis
invertible if and only if [L]Bis invertible, in which case
[L]âˆ’1
B= [Lâˆ’1]B.
Theorem 4.60. A mapping T:Xâ†’Yis invertible if and only if it is a bijection.
Theorem 4.61. LetVandWbe finite-dimensional vector spaces such that dim(V) =dim(W),
and let L:Vâ†’Wbe a linear mapping.
1.IfLis injective, then Lis invertible.
2.IfLis surjective, then Lis invertible.153
Proof.
Proof of Part (1). Suppose that Lis injective. By Proposition 4.15 Nul( L) ={0}, and so
dim(W) = dim( V) = dim(Nul( L)) + dim(Img( L)) = 0 + dim(Img( L)) = dim(Img( L))
by the Rank-Nullity Theorem for Mappings. Now, since Img(L) is a subspace of Wand
dim(Img(L)) = dim(W), by Theorem 3.56(3) Img(L) =Wand so Lis surjective. Since Lis
injective and surjective, it follows by Theorem 4.60 that Lis invertible.
Proof of Part(2). Suppose that Lis surjective, so that Img(L) =W. By the Rank-Nullity
Theorem for Mappings
dim(V) = dim(Nul( L)) + dim(Img( L)) = dim(Nul( L)) + dim( W) = dim(Nul( L)) + dim( V),
whence dim(Nul(L)) = 0 and so Nul(L) ={0}. Now, by Proposition 4.15 we conclude that Lis
injective, and therefore Lis invertible by Theorem 4.60. â– 
Proposition 4.62. Leta1,a2, . . . ,anâˆˆFn. The nÃ—nmatrix
A=a1a2Â·Â·Â·an
is invertible if and only if a1,a2, . . . ,anare linearly independent.
Proof. Suppose that Ais invertible. Let L:Fnâ†’Fnbe the linear mapping with associated
matrix A, so that L(x) =Axfor all xâˆˆFn. Then Lis invertible by Theorem 4.58, and so by
Theorem 4.60 Lis bijective and we have Img(L) =Fn. But by Proposition 4.35 we also have
Img(L) = Col( A) = Span {a1, . . . ,an}, whence
dim(Span {a1, . . . ,an}) = dim(Img( L)) = dim( Fn).
Since Span{a1, . . . ,an}is a subspace of Fnwith dimension equal to dim(Fn), by Theorem 3.56(3)
we conclude that Span{a1, . . . ,an}=Fn, and thus {a1, . . . ,an}is a basis for Fnby Theorem
3.54(2). That is, the vectors a1, . . . ,anare linearly independent.
Next, suppose that a1, . . . ,anare linearly independent. Then {a1, . . . ,an}is a basis for Fn
by Theorem 3.54(1), so that Span{a1, . . . ,an}=Fn. Let L:Fnâ†’Fnbe the linear mapping
given by L(x) =Axfor all xâˆˆFn. By Proposition 4.35
Img(L) = Col( A) = Span {a1, . . . ,an}=Fn,
and thus Lis surjective and it follows by Theorem 4.61(2) that Lis invertible. Therefore Ais
invertible by Theorem 4.58. â– 
We can employ Proposition 4.62 to show that a change of basis matrix is always invertibleâ€”a
fact already established in Â§4.5 by quite different means. Let B= (v1, . . . ,vn) andBâ€²be ordered
bases for a vector space V. By Theorem 4.27 the change of basis matrix IBBâ€²is given by
IBBâ€²=h
[v1]Bâ€²Â·Â·Â·[vn]Bâ€²i
=h
Ï†Bâ€²(v1)Â·Â·Â·Ï†Bâ€²(vn)i
.
The coordinate map Ï†Bâ€²:Vâ†’Fnis an isomorphism by Theorem 4.11, and so in particular
is an injective linear mapping. Thus Nul(Ï†Bâ€²) ={0}by Proposition 4.15, and since the basis
vectors v1, . . . ,vnare linearly independent, it follows by Proposition 4.16 that the column154
vectors Ï†Bâ€²(v1), . . . , Ï† Bâ€²(vn) ofIBBâ€²are likewise linearly independent. Therefore IBBâ€²is invertible
by Proposition 4.62.
We finish this section with a theorem that establishes that, in a certain sense, there is only
â€œone kindâ€ of vector space for each dimension value nâ‰¥0.
Theorem 4.63. LetVandWbe finite-dimensional vector spaces. Then Vâˆ¼=Wif and only if
dim(V) = dim( W).
Proof. Suppose that Vâˆ¼=W, so there exists an isomorphism L:Vâ†’W. Since Lis injective,
Nul(L) ={0}by Proposition 4.15, and then
nullity( L) = dim(Nul( L)) = 0 .
Since Lis surjective, Img( L) =W, and then
rank( L) = dim(Img( L)) = dim( W).
Now, by the Rank-Nullity Theorem for Mappings,
dim(V) = rank( L) + nullity( L) = dim( W) + 0 = dim( W)
as desired.
Now suppose that dim(V) =dim(W) =n. LetBbe a basis for VandCa basis for W. By
Theorem 4.11 the coordinate maps Ï†B:Vâ†’FnandÏ†C:Wâ†’Fnare isomorphisms. Since Ï†C
is a bijection, by Theorem 4.60 it is invertible, with the inverse Ï†âˆ’1
C:Fnâ†’Wbeing a linear
mapping by Proposition 4.57. Of course Ï†âˆ’1
Cis itself invertible with inverse Ï†C, so that Theorem
4.60 implies that Ï†âˆ’1
Cis bijective and hence an isomorphism. Now, by Proposition 4.51 the
composition Ï†âˆ’1
Câ—¦Ï†B:Vâ†’Wis a linear mapping that is easily verified to be an isomorphism,
and therefore Vâˆ¼=W. â– 
Example 4.64. Given vector spaces VandWoverF, with dim(V) =nanddim(W) =m, by
Theorem 4.24 we found that L(V, W )âˆ¼=FmÃ—n. Therefore
dim 
L(V, W )
= dim 
FmÃ—n
=mn
by Theorem 4.63. â– 155
4.9 â€“ Properties of Invertible Operators and Matrices
Linear operators play a central role in the more advanced developments of linear algebra,
and so it will be convenient to collect some of their most important general properties into a
single theorem.
Theorem 4.65 (Invertible Operator Theorem ).LetVbe a finite-dimensional vector space,
and let Lâˆˆ L(V). Then the following statements are equivalent.
1.Lis invertible.
2.Lis an isomorphism.
3.Lis injective.
4.Lis surjective.
5. Nul( L) ={0}.
6. [L]Bis invertible for any basis B.
7. [L]Bis invertible for some basis B.
Proof.
(1)â‡’(2):IfLis invertible, then Lis bijective by Theorem 4.60, and hence Lis an isomorphism
by Definition 4.10.
(2)â‡’(3):IfLis an isomorphism, then of course it must be injective.
(3)â‡’(4):IfL:Vâ†’Vis injective, then Lis invertible by Theorem 4.61(1). By Theorem 4.60
it follows that Lis bijective, and therefore Lis surjective.
(4)â‡’(5):IfL:Vâ†’Vis surjective, then Lis invertible by Theorem 4.61(2). By Theorem 4.60
it follows that Lis bijective, which implies that Lis injective. We conclude that Nul(L) ={0}
by Proposition 4.15.
(5)â‡’(6): Suppose that Nul(L) ={0}, and let Bbe any basis for V. Now, Lis injective by
Proposition 4.15, and hence must be invertible by Theorem 4.61(1). The invertibility of [ L]B
now follows from Corollary 4.59.
(6)â‡’(7):This is trivial.
(7)â‡’(1):If [L]Bis invertible for some basis B, then Lis invertible by Corollary 4.59. â– 
The following proposition will be improved on in the next chapter, at which point it will be
promoted to a theorem.
Proposition 4.66 (Invertible Matrix Proposition ).LetAâˆˆFnÃ—n, and let LAbe the linear
operator on Fnhaving corresponding matrix Awith respect to the standard basis EofFn. Then
the following statements are equivalent.
1.Ais invertible.
2.AâŠ¤is invertible.156
3.Ais row-equivalent to In.
4.The row vectors of Aare linearly independent.
5.Ais column-equivalent to In.
6.The column vectors of Aare linearly independent.
7. col-rank( A) =n.
8. row-rank( A) =n.
9. rank( A) =n.
10.The system Ax=bhas a unique solution for each bâˆˆFn.
11.The system Ax=0has only the trivial solution.
12. Nul( A) ={0}.
13.LAâˆˆ L(Fn)is invertible.
Proof.
(1)â‡’(2):This follows immediately from Proposition 2.32.
(2)â‡’(3): Suppose AâŠ¤is invertible. Then by Proposition 2.32 ( AâŠ¤)âŠ¤is invertible, where of
course ( AâŠ¤)âŠ¤=A. Now, by Theorem 2.30 the invertibility of Aimplies that Ais row-equivalent
toIn.
(3)â‡’(4): Suppose that Ais row-equivalent to In. Then Ais invertible by Theorem 2.30, so
by Proposition 2.32 AâŠ¤is invertible, and then by Proposition 4.62 the column vectors of AâŠ¤
are linearly independent. Since the row vectors of Aare the column vectors of AâŠ¤, we conclude
that the row vectors of Aare linearly independent.
(4)â‡’(5): Suppose the row vectors of Aare linearly independent. Then the column vectors
ofAâŠ¤are linearly independent, whereupon Proposition 4.62 implies that AâŠ¤is invertible.
By Theorem 2.30 AâŠ¤is row-equivalent to In, which is to say there exist elementary matrices
M1, . . . ,Mksuch that
MkÂ·Â·Â·M2M1AâŠ¤=In,
where each left-multiplication by Miis an elementary row operation by Definition 2.15. Taking
the transpose of each side then yields
AMâŠ¤
1MâŠ¤
2Â·Â·Â·MâŠ¤
k=In,
where each right-multiplication by MâŠ¤
iis an elementary column operation by Definition 2.15.
Therefore Ais column-equivalent to In.
(5)â‡’(6):Suppose Ais column-equivalent to In. Then
col-rank( A) = rank( A) = rank( In) =n
by the definition of rank and Theorem 3.66, which implies that the ncolumn vectors of Aare
linearly independent.
(6)â‡’(7): Suppose the column vectors of Aare linearly independent. There are ncolumn
vectors, so col-rank( A) =n.157
(7)â‡’(8):Suppose col-rank( A) =n. Then row-rank( A) =nby Theorem 3.64.
(8)â‡’(9):Suppose row-rank( A) =n. By definition rank( A) = row-rank( A) =n.
(9)â‡’(10): Suppose that rank(A) =n. Then col-rank (A) =n, which is to say the dimension
of the span of the column vectors of Aisn. Since Ahasncolumn vectors in all, it follows that
the column vectors of Aare linearly independent, and so by Proposition 4.62 Ais invertible.
Thus Aâˆ’1exists. Let bâˆˆFnbe arbitrary. Then Aâˆ’1bis a solution to the system, for when we
substitute Aâˆ’1bforxin the equation Ax=b, we obtain
A(Aâˆ’1b) = (AAâˆ’1)b=Inb=b.
This proves the existence of a solution. As for uniqueness, suppose x1andx2are solutions to
the system, so that Ax 1=bandAx 2=b. Now, for iâˆˆ {1,2},
Axi=bâ‡’Aâˆ’1(Axi) =Aâˆ’1bâ‡’(Aâˆ’1A)xi=Aâˆ’1bâ‡’xi=Aâˆ’1b.
That is, x1=x2=Aâˆ’1b, which proves the uniqueness of a solution.
(10)â‡’(11): Suppose that the system Ax=bhas a unique solution for each bâˆˆFn. Then if
we choose b=0, it follows that the system Ax=0has a unique solution, and clearly that
solution must be the trivial solution 0.
(11)â‡’(12): IfAx=0admits only the trivial solution, then
Nul(A) ={xâˆˆFn:Ax=0}={0}
obtains immediately.
(12)â‡’(13): Suppose Nul( A) ={0}, and suppose xâˆˆFnis such that LA(x) =0. Since
LA(x) =0â‡’Ax=0â‡’xâˆˆNul(A)â‡’x=0,
it follows that Nul(LA) ={0}. Therefore LAmust be invertible by the Invertible Operator
Theorem.
(13)â‡’(1):Suppose that LAâˆˆ L(Fn) is invertible. Then [ LA]Eis invertible by Corollary 4.59,
and since [ LA]E=Awe conclude that Ais invertible. â– 
With the help of the Invertible Matrix Proposition we now prove that any square matrix
with either a left-inverse or a right-inverse must be invertible,
Proposition 4.67. LetAâˆˆFnÃ—n. Then the following statements are equivalent:
1.Ais invertible.
2.There exists some DâˆˆFnÃ—nsuch that AD=In.
3.There exists some CâˆˆFnÃ—nsuch that CA=In.158
Proof.
(1)â‡’(2): Suppose that Ais invertible. Then by definition there exists some DâˆˆFnÃ—nsuch
thatAD=DA=In.
(2)â‡’(1):Suppose that
D=d1Â·Â·Â·dn
âˆˆFnÃ—n
is such that AD=In. Ifa1, . . . ,anâˆˆFnare such that aâŠ¤
1, . . . ,aâŠ¤
nare the row vectors for A,
then we haveï£®
ï£¯ï£°aâŠ¤
1...
aâŠ¤
nï£¹
ï£ºï£»d1Â·Â·Â·dn
=In,
and thus
aâŠ¤
idj=(
1,ifi=j
0,ifiÌ¸=j(4.27)
Now, let
b=ï£®
ï£°b1...
bnï£¹
ï£»âˆˆFn
be arbitrary and consider the system Ax=b. Choose
x=nX
i=1bidi.
Then we obtain
Ax=ï£®
ï£¯ï£°aâŠ¤
1...
aâŠ¤
nï£¹
ï£ºï£»x=ï£®
ï£¯ï£°aâŠ¤
1x
...
aâŠ¤
nxï£¹
ï£ºï£»=ï£®
ï£¯ï£°aâŠ¤
1(Pn
i=1bidi)
...
aâŠ¤
n(Pn
i=1bidi)ï£¹
ï£ºï£»=ï£®
ï£¯ï£°Pn
i=1bi(aâŠ¤
1di)
...Pn
i=1bi(aâŠ¤
ndi)ï£¹
ï£ºï£»=ï£®
ï£°b1...
bnï£¹
ï£»=b,
where the penultimate equality follows from (4.27) . This shows that Ax=bhas a solution for
anybâˆˆFn.
LetLAâˆˆ L(Fn) be the linear operator with corresponding matrix Awith respect to the
standard basis E. For each bâˆˆFnthere exists some xâˆˆFnsuch that Ax=b, and hence
LA(x) =b. This shows that LAis surjective, so LAis invertible by the Invertible Operator
Theorem, and hence Ais invertible by the Invertible Matrix Proposition.
(1)â‡’(3): Suppose that Ais invertible. Then by definition there exists some CâˆˆFnÃ—nsuch
thatCA=AC=In.
(3)â‡’(1):Suppose there exists some CâˆˆFnÃ—nsuch that CA=In. Then Ais a right-inverse
forC, and by the equivalency of parts (1) and (2) it follows that Cis invertible. Thus Câˆ’1
exists (and is invertible), and since
CA=Inâ‡’Câˆ’1(CA) =Câˆ’1Inâ‡’(Câˆ’1C)A=Câˆ’1â‡’A=Câˆ’1,159
we conclude that Ais invertible. â– 
An immediate application of Proposition 4.67 provides something of a converse to Theorem
2.26.
Proposition 4.68. LetA,BâˆˆFnÃ—n. IfABis invertible, then AandBare invertible.
Proof. Suppose that ABis invertible. Then there exists some DâˆˆFnÃ—nsuch that ( AB)D=In,
and so by associativity of matrix multiplication we obtain A(BD) =In. Therefore Ais invertible
by Proposition 4.67.
Now, the invertibility of Ameans that Aâˆ’1exists, and since Aâˆ’1andABare invertible, by
Theorem 2.26 Aâˆ’1(AB) is invertible. But
Aâˆ’1(AB) = (Aâˆ’1A)B=InB=B,
and therefore Bis invertible. â– 
The following proposition (and its corollary) could have been proved at the end of the
previous chapter and has wide application in the calculus of manifolds, among other fields.
Proposition 4.69. ForAâˆˆFmÃ—nlet1â‰¤k < min{m, n}. Then there is an invertible
(k+ 1)Ã—(k+ 1) submatrix of Aif and only if rank(A)â‰¥k+ 1.
Proof. Suppose A= [a1Â·Â·Â·an] has an invertible ( k+ 1)Ã—(k+ 1) submatrix. If the submatrix
is formed by the entries that are in rows i1, . . . , i k+1and columns j1, . . . , j k+1ofA, and we
designate the ordered index sets Î±= (i1, . . . , i k+1) and Î²= (j1, . . . , j k+1), then we may denote
the submatrix by A[Î±, Î²]. Let A[Â·, Î²] denote the mÃ—(k+ 1) submatrix formed by the entries
in rows 1 , . . . , m (i.e. all the rows) and columns j1, . . . , j k+1, which is to say
A[Â·, Î²] =aj1Â·Â·Â·ajk+1
.
Then A[Î±, Î²] is a submatrix of A[Â·, Î²], and in particular the k+ 1 row vectors of A[Î±, Î²] are
row vectors of A[Â·, Î²]. Now, since A[Î±, Î²] is invertible, by the Invertible Matrix Proposition
we have rank(A[Î±, Î²]) =k+ 1. Since rank(A[Î±, Î²]) equals the dimension of the row space of
A[Î±, Î²], it follows that the k+ 1 row vectors of A[Î±, Î²] are linearly independent, and therefore
at least k+ 1 row vectors of A[Â·, Î²] are linearly independent. That is, the dimension of the row
space of A[Â·, Î²] is at least k+ 1, and then we find that
col-rank 
A[Â·, Î²]
= row-rank 
A[Â·, Î²]
â‰¥k+ 1.
In fact, since A[Â·, Î²] has precisely k+ 1 column vectors we must have
col-rank 
A[Â·, Î²]
=k+ 1,
which is to say the k+ 1 column vectors of A[Â·, Î²] are linearly independent. However, the
column vectors of A[Â·, Î²] are also column vectors of Aitself, and so now we have
rank(A) = col-rank( A)â‰¥k+ 1> k.160
For the converse, suppose that rank(A)> k. Then at least k+1 column vectors aj1, . . . ,ajk+1
ofAare linearly independent, and with Î²defined as before we construct the nÃ—(k+1) submatrix
A[Â·, Î²]. Since col-rank (A[Â·, Î²]) =k+ 1, it follows that row-rank (A[Â·, Î²]) =k+ 1 also. Thus
there are k+ 1 linearly independent row vectors in A[Â·, Î²], which we number i1, . . . , i k+1. With
Î±defined as before, we obtain the ( k+ 1)Ã—(k+ 1) submatrix A[Î±, Î²] that has k+ 1 linearly
independent row vectors. Now,
rank 
A[Î±, Î²]
= row-rank 
A[Î±, Î²]
=k+ 1,
and the Invertible Matrix Proposition implies that A[Î±, Î²] is invertible. â– 
Applying Proposition 4.69 in the case when m=min{m, n}andk=mâˆ’1, then we conclude
that rank(A)â‰¥miff some mÃ—msubmatrix of Ais invertible, and thus (since the rank of a
matrix cannot exceed its smaller dimension) rank(A) =miff some mÃ—msubmatrix of Ais
invertible. A similar conclusion obtains if n=min{m, n}. Defining a matrix AâˆˆFmÃ—nto have
full rank ifrank(A) =min{m, n}(i.e.Ahas the greatest possible rank), we have proved the
following.
Corollary 4.70. ForAâˆˆFmÃ—nletk=min{m, n}. Then Ahas full rank if and only if Ahas
an invertible kÃ—ksubmatrix.
A good exercise is to prove Corollary 4.70 from established principles, and then use it to
prove Proposition 4.68. Is the argument any easier than that above?
Proposition 4.71. LetVandWbe finite-dimensional vector spaces over Fwith bases Band
C, letLâˆˆ L(V, W )be a linear mapping, and let [L]be itsBC-matrix.
1.IfLis injective, then [L]has full rank.
2.If[L]has full rank and dim(V)â‰¤dim(W), then Lis injective.
Proof.
Proof of Part (1). Setn=dim(V) and m=dim(W), so that [ L]âˆˆFmÃ—n. Suppose that Lis
injective. Proposition 4.15 implies that Nul(L) ={0}, and thus Nul([L]) ={0}as well. This
gives nullity ([L]) = 0, and so rank([L]) =nby the Rank-Nullity Theorem for Matrices. Since n
is a dimension of [ L], it must in fact be the smaller dimension (see remark below) and so we
conclude that [ L] has full rank.
Proof of Part (2). For the converse, suppose that Lis not injective. Then Nul(L)Ì¸={0}
implies Nul([L])Ì¸={0}, so that nullity ([L])>0 and therefore rank([L])< nby the Rank-Nullity
Theorem for Matrices. If n=dim(V)â‰¤dim(W) =m, then it follows that [ L] does not have
full rank and we are done. â– 
Remark. In the proof of the first part of Proposition 4.71, note that L:Vâ†’L(V) is an
isomorphism, which is to say Vâˆ¼=L(V), and so dim(L(V)) = dim(V) =nby Theorem 4.63.
ButL(V) is a vector subspace of W, and so n= dim( L(V))â‰¤dim(W) =mby Theorem 3.56.
In short, if Lâˆˆ L(V, W ) is injective, then dim(V)â‰¤dim(W). A similar truth, left as a problem,
states that if Lâˆˆ L(V, W ) is surjective then dim( V)â‰¥dim(W).161
5
Determinants
5.1 â€“ Determinants of Low Order
Definition 5.1. The1Ã—1determinant function det1:F1Ã—1â†’Fis given by
det1([a]) =a
for each [a]âˆˆF1Ã—1.
The2Ã—2determinant function det2:F2Ã—2â†’Fis given by
det2
a b
c d
=adâˆ’bc.
Generally the scalar det n(A) is called the determinant of the matrix A, and may also be
denoted more simply by det( A) or|A|.
The 1 Ã—1 determinant function has little practical value and tends to arise only in inductive
arguments as in the proof of Theorem 5.4. The 2 Ã—2 determinant function, on the other
hand, is highly important, and so it will be the focus of study for the remainder of this section.
Henceforth we will denote det 2(A) simply as det( A).162
5.2 â€“ Determinants of Arbitrary Order
The general definition we will give here for the determinant of an nÃ—nmatrix is recursive
in nature. That is, for nâ‰¥2, the determinant of an nÃ—nmatrix will be defined in terms of
determinants of ( nâˆ’1)Ã—(nâˆ’1) matrices. Thus determinants of nÃ—nmatrices are ultimately
defined in terms of determinants of 1 Ã—1 matrices, and since the determinant of a 1 Ã—1 matrix
is defined to equal the sole scalar entry of the matrix, we can see that the definition rests on a
firm foundation.
Before stating the definition a bit of notation needs to be established. If A= [aij] is an
nÃ—nmatrix, then we define Aijto be the submatrix that results when the ith row and jth
column of Aare deleted. That is,
Aij=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a11Â·Â·Â· a1(jâˆ’1) a1(j+1) Â·Â·Â· a1n..................
a(iâˆ’1)1Â·Â·Â· a(iâˆ’1)(jâˆ’1)a(iâˆ’1)(j+1)Â·Â·Â· a(iâˆ’1)n
a(i+1)1Â·Â·Â· a(i+1)(jâˆ’1)a(i+1)(j+1)Â·Â·Â· a(i+1)n..................
an1Â·Â·Â· an(jâˆ’1) an(j+1) Â·Â·Â· annï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».
We now have what we need to give the general definition for the determinant function.
Definition 5.2. Letnâ‰¥2. The nÃ—ndeterminant function detn:FnÃ—nâ†’Fis given by
detn(A) =nX
j=1(âˆ’1)1+ja1jdetnâˆ’1(A1j) (5.1)
for each nÃ—nmatrix Awith entries in F. The scalar detn(A)is called an nÃ—ndeterminant .
As is our custom we will take the field Fto beRunless otherwise indicated. Often we will
write det n(A) as simply det( A). Other symbols for the determinant of Aare
|A|,detï£«
ï£­ï£®
ï£°a11Â·Â·Â·a1n.........
an1Â·Â·Â·annï£¹
ï£»ï£¶
ï£¸,anda11Â·Â·Â·a1n.........
an1Â·Â·Â·ann.
Example 5.3. Given that
A=ï£®
ï£°âˆ’2 3 âˆ’1
0 2 5
0âˆ’6 4ï£¹
ï£»,
evaluate det( A).
Solution. We have
det(A) = (âˆ’1)1+1(âˆ’2)2 5
âˆ’6 4+ (âˆ’1)1+2(3)0 5
0 4+ (âˆ’1)1+3(âˆ’1)0 2
0âˆ’6
=âˆ’22 5
âˆ’6 4âˆ’30 5
0 4âˆ’0 2
0âˆ’6163
=âˆ’2[(2)(4) âˆ’(5)(âˆ’6)]âˆ’3[(0)(4) âˆ’(5)(0)] âˆ’[(0)(âˆ’6)âˆ’(2)(0)]
=âˆ’76,
using Definitions 5.2 and 5.1. â– 
It is frequently convenient to regard detn:FnÃ—nâ†’Fas being a function of the column
vectors of a matrix AâˆˆFnÃ—n. Thus, if
A=a1Â·Â·Â·an
,
where ajâˆˆFnis a column vector for each 1 â‰¤jâ‰¤n, then we define
detn(a1, . . . ,an) = det n a1Â·Â·Â·an
so that in fact we have detn:Qn
j=1Fnâ†’F. This leads to no ambiguity since there is a natural
isomorphism between the vector spaces
FnÃ—n=x1Â·Â·Â·xn
:xkâˆˆFnfor 1â‰¤kâ‰¤n	
and
nY
j=1Fn={(x1, . . . ,xn) :xkâˆˆFnfor 1â‰¤kâ‰¤n}
that enables us to identify, in particular, the column vectors of any matrix A= [aij]ninFnÃ—n
with a unique n-tuple ( a1, . . . ,an) of vectors in Fn. We use this natural identification to express
certain properties of determinants.
Theorem 5.4. For all nâˆˆN, the determinant function detn:FnÃ—nâ†’Fhas the following
properties, where all vectors represent column vectors.
DP1. Multilinearity. For any 1â‰¤jâ‰¤n, ifaj=u+vthen
detn(a1, . . . ,u+v, . . . ,an) = det n(a1, . . . ,u, . . . ,an) + det n(a1, . . . ,v, . . . ,an),
and if aj=xuthen
detn(a1, . . . , x u, . . . ,an) =xdetn(a1, . . . ,u, . . . ,an).
DP2. Alternating. For any 1â‰¤j < kâ‰¤n,
detn(a1, . . . ,aj, . . . ,ak, . . . ,an) =âˆ’detn(a1, . . . ,ak
j, . . . ,aj
k, . . . ,an).
DP3. Normalization.
detn(In) = 1 .
DP4. IfA= [a1Â·Â·Â·an]withaj=akfor some jÌ¸=k, then detn(A) = 0 .
DP5. For any xâˆˆFandjÌ¸=k,
detn(a1, . . . ,aj, . . . ,an) = det n(a1, . . . ,aj+xak, . . . ,an).
DP6. For any 1â‰¤jâ‰¤n, ifaj=0then
detn(a1, . . . ,0, . . . ,an) = 0 .164
Proof.
Proof of DP1. Given any u, vâˆˆF, we have [ u+v]âˆˆF1Ã—1with
det([u+v]) =u+v= det([ u]) + det([ v])
by Definition 5.1. Thus DP1 holds in the case when n= 1. Suppose that DP1 holds for some
arbitrary nâˆˆN. Let
A= [aij] = [a1Â·Â·Â·an+1]âˆˆF(n+1)Ã—(n+1),
fixkâˆˆ {1, . . . , n + 1}, and suppose ak=u+v. For each 1 â‰¤jâ‰¤n+ 1 define
aâ€²
j=ï£®
ï£°a2j...
a(n+1)jï£¹
ï£»âˆˆFn,
and also
uâ€²=ï£®
ï£°u2...
un+1ï£¹
ï£»and vâ€²=ï£®
ï£°v2...
vn+1ï£¹
ï£».
By Definition 5.2
det(A) =n+1X
j=1(âˆ’1)1+ja1jdet(A1j) =n+1X
j=1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,an+1),(5.2)
where itâ€™s understood that
det(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,an+1) = det( aâ€²
2, . . . ,an+1)
ifj= 1, and
det(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,an+1) = det( aâ€²
1, . . . ,an)
ifj=n+ 1.
Now, if j < k , then
det(Aij) = det( aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
k, . . . ,aâ€²
n+1)
= det( aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,uâ€²+vâ€², . . . ,aâ€²
n+1)
= det( . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,uâ€², . . .) + det( . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,vâ€², . . .)
by the inductive hypothesis, since Aijis an nÃ—nmatrix. Similarly, if j > k then
det(Aij) = det( aâ€²
1, . . . ,aâ€²
k, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1)
= det( aâ€²
1, . . . ,uâ€²+vâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1)
= det( . . . ,uâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .) + det( . . . ,vâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .)
These results, together with equation (5.2), yields
det(A) =kâˆ’1X
j=1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,uâ€²+vâ€², . . . ,aâ€²
n+1)165
+ (âˆ’1)1+ka1kdet(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1)
+n+1X
j=k+1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,uâ€²+vâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1)
=kâˆ’1X
j=1(âˆ’1)1+ja1j[det(. . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,uâ€², . . .) + det( . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,vâ€², . . .)]
+ (âˆ’1)1+k(u1+v1) det(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1)
+n+1X
j=k+1(âˆ’1)1+ja1j[det(. . . ,uâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .) + det( . . . ,vâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .)],
where we use the fact that a1k=u1+v1. Observing that
det(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1) = det( A1k),
we finally obtain
det(A) ="kâˆ’1X
j=1(âˆ’1)1+ja1jdet(. . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,uâ€², . . .) + (âˆ’1)1+ku1det(A1k)
+n+1X
j=k+1(âˆ’1)1+ja1jdet(. . . ,uâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .)#
+"kâˆ’1X
j=1(âˆ’1)1+ja1jdet(. . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,vâ€², . . .) + (âˆ’1)1+kv1det(A1k)
+n+1X
j=k+1(âˆ’1)1+ja1jdet(. . . ,vâ€², . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . .)#
= det( a1, . . . ,u, . . . ,an+1) + det( a1, . . . ,v, . . . ,an+1)
That is,
det(a1, . . . ,u+v, . . . ,an+1) = det( a1, . . . ,u, . . . ,an+1) + det( a1, . . . ,v, . . . ,an+1),
and so the first multilinearity property holds for all nâ‰¥1 by induction.
We now prove the second multilinearity property. We have det([xa]) =xa=xdet([a]) for
anyxâˆˆFand [ a]âˆˆF1Ã—1, so the property holds in the case when n= 1. Suppose it holds for
some arbitrary nâˆˆN. For AâˆˆF(n+1)Ã—(n+1),kâˆˆ {1, . . . , n + 1}andxâˆˆFwe have
det(a1, . . . , x ak, . . . ,an+1) =kâˆ’1X
j=1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . , x aâ€²
k, . . . ,aâ€²
n+1)
+ (âˆ’1)1+kxa1kdet(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1)
+n+1X
j=k+1(âˆ’1)1+ja1jdet(aâ€²
1, . . . , x aâ€²
k, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1).166
Since the determinants in the summations are nÃ—n, we use the inductive hypothesis to obtain
det(a1, . . . , x ak, . . . ,an+1) =kâˆ’1X
j=1(âˆ’1)1+jxa1jdet(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
k, . . . ,aâ€²
n+1)
+ (âˆ’1)1+kxa1kdet(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1)
+n+1X
j=k+1(âˆ’1)1+jxa1jdet(aâ€²
1, . . . ,aâ€²
k, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1),
and hence
det(a1, . . . , x ak, . . . ,an+1) =x"kâˆ’1X
j=1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
k, . . . ,aâ€²
n+1)
+ (âˆ’1)1+ka1kdet(aâ€²
1, . . . ,aâ€²
kâˆ’1,aâ€²
k+1, . . . ,aâ€²
n+1)
+n+1X
j=k+1(âˆ’1)1+ja1jdet(aâ€²
1, . . . ,aâ€²
k, . . . ,aâ€²
jâˆ’1,aâ€²
j+1, . . . ,aâ€²
n+1)#
=xdet(a1, . . . ,an+1).
Therefore the second multilinearity property holds for all nâ‰¥1 by induction.
Proof of DP2. This is done using induction and careful bookkeeping much as with the proofs of
the previous two properties, and so is left as a problem.
Proof of DP3. Certainly det([1]) = 1, so normalization holds when n= 1. Suppose it holds for
some nâˆˆN. Let I=In+1, with ij-entry denoted by eij. We have e11= 1 and e1j= 0 for all
2â‰¤jâ‰¤n+ 1, and so
det(I) =n+1X
j=1(âˆ’1)1+je1jdet(I1j) = det( I11) = det( In) = 1 .
Therefore the normalization property holds for all nâˆˆNby induction.
Proof of DP4. LetAâˆˆFnÃ—n, and fix 1 â‰¤j < kâ‰¤n. By the alternating property DP2,
det(A) = det( a1, . . . ,aj, . . . ,ak, . . . ,an) =âˆ’det(a1, . . . ,ak, . . . ,aj, . . . ,an),
and so if aj=akwe obtain
det(A) =âˆ’det(a1, . . . ,aj, . . . ,ak, . . . ,an) =âˆ’det(A).
That is, 2 det( A) = 0, and therefore det( A) = 0.
Proof of DP5. LetAâˆˆFnÃ—n, and fix 1 â‰¤j, kâ‰¤nwith jÌ¸=k. For any xâˆˆFwe have by DP1,
det(a1, . . . ,aj+xak
j, . . . ,an) = det( a1, . . . ,aj, . . . ,an) + det( a1, . . . , x ak
j, . . . ,an)167
= det( a1, . . . ,aj, . . . ,an) +xdet(a1, . . . ,ak
j, . . . ,an)
The matrix
a1Â·Â·Â·ak
jÂ·Â·Â·an
hasjth and kth column both equal to ak, so that
det(a1, . . . ,ak
j, . . . ,an) = 0
by DP4, and we obtain
det(a1, . . . ,aj+xak, . . . ,an) = det( a1, . . . ,aj, . . . ,an)
as desired.
Proof of DP6. LetA= [a1Â·Â·Â·0Â·Â·Â·an], soaj=0for some 1 â‰¤jâ‰¤n. By DP1,
det(A) = det( a1, . . . ,0+0, . . . ,an)
= det( a1, . . . ,0, . . . ,an) + det( a1, . . . ,0, . . . ,an)
= det( A) + det( A),
which immediately implies that det( A) = 0. â– 
Proposition 5.5. IfAâˆˆFnÃ—nis an upper-triangular or lower-triangular matrix, then
det(A) =nY
i=1aii.
Proof. The statement of the proposition is vacuously true in the case when n= 1. Let nâˆˆN
be arbitrary and suppose whenever A= [aij]nis an upper-triangular or lower-triangular matrix,
then det( A) =a11a22Â·Â·Â·ann.
Suppose that AâˆˆFnÃ—nis an upper-triangular matrix, so that A= [aij] such that aij= 0
whenever i > j . Now, for all 2 â‰¤jâ‰¤n+ 1 the matrix A1jhas0in its first column, so that
det(A1j) = 0 by DP6 and we obtain
det(A) =n+1X
j=1(âˆ’1)1+ja1jdet(A1j) =a11det(A11). (5.3)
Now, A11is an nÃ—nupper-triangular matrix,
A11=ï£®
ï£°a22Â·Â·Â· a2(n+1).........
0Â·Â·Â·a(n+1)(n+1)ï£¹
ï£»,
and so by the inductive hypothesis det(A11) =a22Â·Â·Â·a(n+1)(n+1). Then from (5.3) we conclude
that
det(A) =a11a22Â·Â·Â·a(n+1)(n+1).168
Next, suppose that Ais a lower-triangular matrix, so that aij= 0 whenever i < j . Then
a1j= 0 for all 2 â‰¤jâ‰¤n, and since A11is an nÃ—nlower-triangular matrix, we once again we
obtain
det(A) =a11det(A11) =a11a22Â·Â·Â·a(n+1)(n+1)
as desired. â– 
Lemma 5.6. Define the function detâ€²
n:FnÃ—nâ†’Fby
detâ€²
n(A) =nX
i=1(âˆ’1)i+1ai1detâ€²
nâˆ’1(Ai1), (5.4)
with detâ€²
1([a]) =ain particular. Then detâ€²
n(A) = det n(A)for all nâˆˆNandAâˆˆFnÃ—n.
Proof. First, it can be shown via analogous arguments that the function detâ€²
npossesses the
same six properties listed in Theorem 5.4 that detnpossesses. Also Proposition 5.5 applies to
detâ€²
n, with the proof being symmetric to the one given for det n.
FixnâˆˆNand let AâˆˆFnÃ—n. Recall the elementary row and column operations R1,R2,
C1, and C2 from Definition 2.15. If Aâ€²is obtained from Aby an application of C1, then by
Proposition 2.17(1) and DP5 we have det(A) =det(Aâ€²); and if Aâ€²is obtained from Aby an
application of C2, then det(A) =âˆ’det(Aâ€²) by Proposition 2.14(2) and DP2. By Proposition
2.20 and the particulars of its proof, row operations R1 and R2 may be applied to AâŠ¤to obtain
an upper-triangular matrix U, which corresponds to employing a succession of C1 and C2
operations to Ato obtain a lower-triangular matrix
L=ï£®
ï£°â„“11Â·Â·Â· 0
.........
â„“n1Â·Â·Â·â„“nnï£¹
ï£»=UâŠ¤;
that is, L= [â„“ij]nwith â„“ij= 0 for i < j . If a total of kC2 operations are performed in doing
this, then det( A) = (âˆ’1)kdet(L). Now
det(A) = (âˆ’1)kdet(L) = (âˆ’1)kâ„“11â„“22Â·Â·Â·â„“nn
by Proposition 5.5.
On the other hand, because Theorem 5.4 applies to detâ€², we have detâ€²(A) = (âˆ’1)kdetâ€²(L).
And then because Proposition 5.5 also applies to detâ€², we easily obtain
detâ€²(A) = (âˆ’1)kâ„“11â„“22Â·Â·Â·â„“nn= det( A)
as claimed. â– 
Theorem 5.7. For any AâˆˆFnÃ—n,detn(A) = det n(AâŠ¤).
Proof. LetAâˆˆFnÃ—n. Let aâŠ¤
ijdenote the ij-entry for AâŠ¤. Since aâŠ¤
1j=aj1and (AâŠ¤)1j= (Aj1)âŠ¤
for all j,
detn(AâŠ¤) =nX
j=1(âˆ’1)1+jaâŠ¤
1jdetnâˆ’1[(AâŠ¤)1j] =nX
j=1(âˆ’1)1+jaj1detnâˆ’1[(Aj1)âŠ¤].169
Now, by Lemma 5.6 we have det nâˆ’1[(Aj1)âŠ¤] = detâ€²
nâˆ’1[(Aj1)âŠ¤] so that
detn(AâŠ¤) =nX
j=1(âˆ’1)1+jaj1detâ€²
nâˆ’1[(Aj1)âŠ¤] = detâ€²
n(A),
and therefore
detn(AâŠ¤) = det n(A)
by another application of Lemma 5.6. â– 
Lemma 5.8. For all nâˆˆNand1â‰¤jâ‰¤n, define detâ€²
n,j:FnÃ—nâ†’Fby
detâ€²
n,j(A) =nX
i=1(âˆ’1)i+jaijdetâ€²
nâˆ’1,j(Aij),
withdetâ€²
1,1([a]) =ain particular. Then, for every nâˆˆN,detâ€²
n,j(A) =detâ€²
n(A)for all 1â‰¤jâ‰¤n
andAâˆˆFnÃ—n.
Proof. The conclusion is trivially true in the case when n= 1, so suppose the conclusion is
true for some nâˆˆN. Since detâ€²
n+1,1= detâ€²
n+1by definition, consider detâ€²
n+1,jfor some jâ‰¥2.
LetA= [a1Â·Â·Â·an+1]âˆˆF(n+1)Ã—(n+1), and let
B=ajÂ·Â·Â·ajâˆ’1a1aj+1Â·Â·Â·an+1
.
Since Theorem 5.4â€”and in particular DP2â€”applies to detâ€²
n+1, we have
detâ€²
n+1(A) =âˆ’detâ€²
n+1(B) =âˆ’n+1X
i=1(âˆ’1)i+1aijdetâ€²
n(Bi1), (5.5)
where
Bi1=aâ€²
2Â·Â·Â·aâ€²
jâˆ’1aâ€²
1aâ€²
j+1Â·Â·Â·aâ€²
n+1
,
eachaâ€²
krepresenting akwith its ith component deleted. A succession of jâˆ’2 transpositions
of the column vectors of Bi1will bring aâ€²
1to the position of the column without altering the
relative positions of the other vectors:
aâ€²
1Â·Â·Â·aâ€²
jâˆ’1aâ€²
j+1Â·Â·Â·aâ€²
n+1
.
This matrix is precisely Aij, and since Aijobtains from Bi1viajâˆ’2 column transpositions, by
DP2 and the inductive hypothesis we have
detâ€²
n(Bi1) = (âˆ’1)jâˆ’2detâ€²
n(Aij) = (âˆ’1)jâˆ’2detâ€²
n,j(Aij).
Substituting this result into (5.5) yields
detâ€²
n+1(A) =âˆ’n+1X
i=1(âˆ’1)i+1(âˆ’1)jâˆ’2aijdetâ€²
n,j(Aij) =n+1X
i=1(âˆ’1)i+jaijdetâ€²
n,j(Aij) = detâ€²
n+1,j(A)
as desired. Therefore detâ€²
n+1,j= detâ€²
n+1for all 1 â‰¤jâ‰¤n+ 1. â– 170
Lemma 5.9. For all nâˆˆNand1â‰¤iâ‰¤n, define detn,i:FnÃ—nâ†’Fby
detn,i(A) =nX
j=1(âˆ’1)i+jaijdetnâˆ’1,i(Aij),
with det1,1([a]) =ain particular. Then, for every nâˆˆN,detn,i(A) =detn(A)for all 1â‰¤iâ‰¤n
andAâˆˆFnÃ—n.
Proof. The conclusion is trivially true in the case when n= 1, so suppose the conclusion is
true for some nâˆˆN. Since detn+1,1= det n+1by definition, consider detn+1,ifor some iâ‰¥2. Let
AâˆˆF(n+1)Ã—(n+1). We have
detn+1(A) = det n+1(AâŠ¤) = detâ€²
n+1(AâŠ¤) = detâ€²
n+1,i(AâŠ¤) (5.6)
by Theorem 5.7, Lemma 5.6, and Lemma 5.8, respectively. Letting AâŠ¤=B= [bjk]n, where
bjk=akj, we have
detâ€²
n+1,i(AâŠ¤) = detâ€²
n+1,i(B) =n+1X
j=1(âˆ’1)j+ibjidetâ€²
n,i(Bji). (5.7)
However, since Bji= (AâŠ¤)ji= (Aij)âŠ¤, it follows that
detâ€²
n,i(Bji) = detâ€²
n,i 
(Aij)âŠ¤
= detâ€²
n 
(Aij)âŠ¤
= det n 
(Aij)âŠ¤
= det n(Aij) = det n,i(Aij),
making use of Lemma 5.8, Lemma 5.6, Theorem 5.7, and the inductive hypothesis, in turn.
This result, along with bji=aijand (5.6), turns (5.7) into
detn+1(A) =n+1X
j=1(âˆ’1)i+jaijdetn,i(Aij),
and therefore det n+1(A) = det n+1,i(A) as desired. â– 
All of the functions detn,ianddetâ€²
n,jare rightly called determinant functions; however
Lemmas 5.6, 5.8 and 5.9, taken together, show that
detn,i= det n= detâ€²
n= detâ€²
n,j
for any nâˆˆNand 1â‰¤i, jâ‰¤n. That is, all of the determinant functions defined thus far in this
section turn out to be the same function, even though they are given by different formulas! For
each i, the formula given for detn,i(A) is called â€œexpansion of the determinant of Aalong the
ith rowâ€; and for each j, the formula given for detâ€²
n,j(A)is called â€œexpansion of the determinant
ofAalong the jth column.â€ Since all of the functions detn,ianddetâ€²
n,jare the same, and since
in practice it is not generally necessary or desirable to specify which way the determinant of a
square matrix is being expanded, from now on we shall denote all expansions of the determinant
ofAby the symbol det n(A) or det( A). We summarize as follows.171
Definition 5.10. Given AâˆˆFnÃ—n, the sum
detn(A) =nX
j=1(âˆ’1)i+jaijdetnâˆ’1(Aij)
is called the expansion of the determinant of Aalong the ith row , and the sum
detn(A) =nX
i=1(âˆ’1)i+jaijdetnâˆ’1(Aij)
is called the expansion of the determinant of Aalong the jth column .
Given column vectors a1, . . . ,an, we define
detn(aâŠ¤
1, . . . ,aâŠ¤
n) = det nï£«
ï£­ï£®
ï£°aâŠ¤
1...
aâŠ¤
nï£¹
ï£»ï£¶
ï£¸;
that is, we take detn(aâŠ¤
1, . . . ,aâŠ¤
n) to be the determinant of the matrix with row vectors
aâŠ¤
1, . . . ,aâŠ¤
n. (It is important to bear in mind that, notational conventions aside, detnis by
definition strictly a function with domain FnÃ—nâ€”which is to say the allowed â€œinputsâ€ are nÃ—n
matrices, and not n-tuples of vectors in Fn.) In light of Theorem 5.7 we readily obtain the
following result.
Proposition 5.11. The properties DP1 â€“ DP6 given in Theorem 5.4 remain valid if a1, . . . ,an
represent the row vectors of a matrix AâˆˆFnÃ—ninstead of the column vectors.
Proof. The proof for DP1 should suffice to convey the general strategy. Given row vectors
a1, . . . ,u+v, . . . ,an, we have
detn(a1, . . . ,u+v, . . . ,an) = det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
u+v
...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸= det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
u+v
...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»âŠ¤ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
= det n aâŠ¤
1Â·Â·Â·uâŠ¤+vâŠ¤Â·Â·Â·aâŠ¤
n
= det n aâŠ¤
1Â·Â·Â·uâŠ¤Â·Â·Â·aâŠ¤
n
+ det n aâŠ¤
1Â·Â·Â·vâŠ¤Â·Â·Â·aâŠ¤
n
= det naâŠ¤
1Â·Â·Â·uâŠ¤Â·Â·Â·aâŠ¤
nâŠ¤
+ det naâŠ¤
1Â·Â·Â·vâŠ¤Â·Â·Â·aâŠ¤
nâŠ¤
= det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
u
...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸+ det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
v
...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸
= det n(a1, . . . ,u, . . . ,an) + det n(a1, . . . ,v, . . . ,an)172
by our notational convention and repeated use of Theorem 5.7. â– 
Example 5.12. Evaluate the determinant
3 0âˆ’6
âˆ’2 4 7
1 0 10
Solution. Since the second column of the determinant has two zero entries, our labors will be
lessened if we expand the determinant along the second column:
3 0âˆ’6
âˆ’2 4 7
1 0 10= (âˆ’1)1+2(0)âˆ’2 7
1 10+ (âˆ’1)2+2(4)3âˆ’6
1 10+ (âˆ’1)3+2(0)3âˆ’6
âˆ’2 7
= 43âˆ’6
1 10= 4
(3)(10) âˆ’(âˆ’6)(1)
= 144 .
Expanding along any other column or row will yield the same result. â– 
Example 5.13. Given that
A=ï£®
ï£¯ï£¯ï£°3 1 âˆ’5 9
âˆ’6 4 10 âˆ’18
0âˆ’2 8 âˆ’7
5 1 âˆ’1 3ï£¹
ï£ºï£ºï£»,
evaluate det( A).
Solution. Applying DP5 together with Proposition 5.11, we add twice the first row of the
determinant to the second row, obtaining a new determinant having the same value as the old
one:
det(A) =3 1 âˆ’5 9
âˆ’6 4 10 âˆ’18
0âˆ’2 8 âˆ’7
5 1 âˆ’1 32r1+r2â†’r2= = = = = = = =3 1 âˆ’5 9
0 6 0 0
0âˆ’2 8 âˆ’7
5 1 âˆ’1 3
Now we find it convenient to expand the determinant of Aalong the second row, since that row
contains three zero entries:
det(A) = (âˆ’1)2+2(6) =3âˆ’5 9
0 8 âˆ’7
5âˆ’1 3= 63âˆ’5 9
0 8 âˆ’7
5âˆ’1 3
Expanding the 3 Ã—3 determinant along the first column, we finally obtain
det(A) = 6
38âˆ’7
âˆ’1 3+ 5âˆ’5 9
8âˆ’7
= 6
3(17) + 5( âˆ’37)
=âˆ’804
and weâ€™re done. â– 173
Example 5.14. ThenÃ—nVandermonde determinant is
Vn= det 
[xjâˆ’1
i]nÃ—n
=1x1Â·Â·Â· xnâˆ’1
1
1x2Â·Â·Â· xnâˆ’1
2............
1xnÂ·Â·Â· xnâˆ’1
n
The claim is that
Vn+1=Y
1â‰¤i<jâ‰¤n+1(xjâˆ’xi) (5.8)
for all nâ‰¥1. This clearly holds when n= 1 and n= 2:
V2=1x1
1x2=x2âˆ’x1and V3=1x1x2
1
1x2x2
2
1x3x3
3= (x2âˆ’x1)(x3âˆ’x1)(x3âˆ’x2).
Letnâ‰¥1 be arbitrary, and suppose that (5.8) is true. Now, by DP5,
Vn+2=1x1Â·Â·Â· xn+1
1
1x2Â·Â·Â· xn+1
2............
1xn+2Â·Â·Â· xn+1
n+2âˆ’x1cj+cj+1â†’cj+1= = = = = = = = = = = = =
forj=n+ 1, . . . , 11 0 Â·Â·Â· 0
1x2âˆ’x1Â·Â·Â· xn+1
2âˆ’x1xn
2............
1xn+2âˆ’x1Â·Â·Â· xn+1
n+2âˆ’x1xn
n+2.
Expanding the determinant along the first row and then employing Proposition 5.11 to DP1
yields
Vn+2=x2âˆ’x1 x2
2âˆ’x1x2Â·Â·Â· xn+1
2âˆ’x1xn
2............
xn+2âˆ’x1x2
n+2âˆ’x1xn+2Â·Â·Â· xn+1
n+2âˆ’x1xn
n+2
= (x2âˆ’x1)Â·Â·Â·(xn+2âˆ’x1)1x2Â·Â·Â· xn
2............
1xn+2Â·Â·Â· xn
n+2
The last determinant is an ( n+ 1)Ã—(n+ 1) Vandermonde determinant, and so by (5.8) we have
1x2Â·Â·Â· xn
2............
1xn+2Â·Â·Â· xn
n+2=Y
2â‰¤i<jâ‰¤n+2(xjâˆ’xi).
Hence
Vn+2= (x2âˆ’x1)Â·Â·Â·(xn+2âˆ’x1)Y
2â‰¤i<jâ‰¤n+2(xjâˆ’xi) =Y
1â‰¤i<jâ‰¤n+2(xjâˆ’xi),
and so by the principle of induction we conclude that (5.8) holds for all nâ‰¥1. â– 174
5.3 â€“ Applications of Determinants
As a first application, we establish a few results that will enable us to significantly extend
the Invertible Matrix Proposition of Â§4.9.
Proposition 5.15. LetA= [a1Â·Â·Â·an]âˆˆFnÃ—n. The vectors a1, . . . ,anare linearly dependent
if and only if det(A) = 0 .
Proof. Suppose that a1, . . . ,anare linearly dependent, so there exist c1, . . . , c nâˆˆFsuch that
nX
j=1cjaj=0,
andckÌ¸= 0 for some 1 â‰¤kâ‰¤n. Now
ckak+X
jÌ¸=kcjaj=0â‡’ak=âˆ’X
jÌ¸=kcj
ckaj,
and so
det(A) = det( a1, . . . ,ak, . . . ,an) = det 
a1, . . . ,âˆ’X
jÌ¸=kcj
ckaj, . . . ,an!
=âˆ’X
jÌ¸=kcj
ckdet(a1, . . . ,aj, . . . ,an) (5.9)
by the multilinearity properties of the determinant function. By DP4 we have
det(a1, . . . , aj|{z}
kth col., . . . ,an) = 0
for each 1 â‰¤jâ‰¤nsuch that jÌ¸=k, and so from (5.9) we obtain det( A) = 0.
For the converse, suppose that a1, . . . ,anare linearly independent, so col-rank (A) =n.
Recall the elementary row and column operations R1,R2,C1, and C2 from Definition 2.15. The
proof of Theorem 3.64 shows that Ais equivalent via the operations R1,R2,C1, and C2 to a
diagonal matrix
B=ï£®
ï£°b11Â·Â·Â· 0
.........
0Â·Â·Â·bnnï£¹
ï£»,
and since by Theorem 3.66
col-rank( B) = col-rank( A) =n,
it follows that bjjÌ¸= 0 for all 1 â‰¤jâ‰¤n.
Now, if pis the number of R2 and C2 operations performed (which by Propositions 2.16(2)
and 2.17(2) correspond to swapping rows and columns) in passing from AtoB, then by DP2
and 5.4(5), together with Proposition 5.11, we have
det(A) = (âˆ’1)pdet(B).175
Of course, Bis an upper-triangular matrix, and so
det(A) = (âˆ’1)pb11b22Â·Â·Â·bnnÌ¸= 0
by Proposition 5.5. â– 
Proposition 5.16. A âˆˆFnÃ—nis invertible if and only if detn(A)Ì¸= 0.
Proof. By the Invertible Matrix Proposition (Proposition 4.66),
A=a1Â·Â·Â·an
is invertible if and only if a1, . . . ,anare linearly independent, and by Proposition 5.15 the
vectors a1, . . . ,anare linearly independent if and only if detn(A)Ì¸= 0. The conclusion is now
self-evident. â– 
We now improve on the Invertible Matrix Proposition given in Â§4.9 to obtain what we shall
call the Invertible Matrix Theorem, incorporating also the results of Proposition 4.67 as well as
observing that nullity( A) = 0 is equivalent to Nul( A) ={0}.
Theorem 5.17 (Invertible Matrix Theorem ).LetAâˆˆFnÃ—n, and let LAbe the linear
operator on Fnhaving corresponding matrix Awith respect to the standard basis EofFn. Then
the following statements are equivalent.
1.Ais invertible.
2.AâŠ¤is invertible.
3.Ais row-equivalent to In.
4.The row vectors of Aare linearly independent.
5.Ais column-equivalent to In.
6.The column vectors of Aare linearly independent.
7. col-rank( A) =n.
8. row-rank( A) =n.
9. rank( A) =n.
10.The system Ax=bhas a unique solution for each bâˆˆFn.
11.The system Ax=0has only the trivial solution.
12. Nul( A) ={0}.
13. nullity( A) = 0 .
14.LAâˆˆ L(Fn)is invertible.
15.There exists some DâˆˆFnÃ—nsuch that AD=In.
16.There exists some CâˆˆFnÃ—nsuch that CA=In.
17. det n(A)Ì¸= 0.
Determinants can be applied to find the solution to a nonhomogeneous system of nequations
with nunknowns, provided that a unique solution exists.
Theorem 5.18 (Cramerâ€™s Rule ).Leta1, . . . ,anâˆˆFnsuch that detn(a1, . . . ,an)Ì¸= 0. If
bâˆˆFnandx1, . . . , x nare scalars such that
x1a1+Â·Â·Â·+xnan=b, (5.10)176
then
xj=detn(a1, . . . ,ajâˆ’1,b,aj+1, . . . ,an)
detn(a1, . . . ,an)
for each 1â‰¤jâ‰¤n.
Proof. Suppose that bâˆˆFn. Since detn(a1, . . . ,an)Ì¸= 0 it follows from the Invertible Matrix
Theorem that there exist unique scalars x1, . . . , x nsuch that equation (5.10) holds. Fix 1 â‰¤jâ‰¤n.
Letting
detn(a1, . . . ,b, . . . ,an) = det n(a1, . . . ,ajâˆ’1,b,aj+1, . . . ,an)
for brevity, we obtain
detn(a1, . . . ,b, . . . ,an) = det n 
a1, . . . ,nX
k=1xkak, . . . ,an!
=nX
k=1xkdetn(a1, . . . ,ak, . . . ,an) (5.11)
by DP1. Now, for each kÌ¸=jwe have detn(a1, . . . ,ak, . . . ,an) = 0 by DP4, since both the jth
andkth column of the matrix
a1Â·Â·Â·ak|{z}
jth col.Â·Â·Â·an
is equal to aj. Hence from (5.11) comes
detn(a1, . . . ,b, . . . ,an) =xjdetn(a1, . . . , aj|{z}
jth col., . . . ,an) =xjdetn(a1, . . . ,an),
and therefore
xj=detn(a1, . . . ,b, . . . ,an)
detn(a1, . . . ,an)
as desired. â– 
If we let
A=a1a2Â·Â·Â·an
and
x=ï£®
ï£°x1...
xnï£¹
ï£»,
then Cramerâ€™s Rule may be given as
Ax=bâ‡’xj=detn(a1, . . . ,ajâˆ’1,b,aj+1, . . . ,an)
detn(A)
for each 1 â‰¤jâ‰¤n, so long as det( A)Ì¸= 0.177
Example 5.19. Solve the system
ï£±
ï£²
ï£³2xâˆ’y+z= 1
x+ 3yâˆ’2z= 0
4xâˆ’3y+z= 2
using Cramerâ€™s Rule.
Solution. HereAx=bwith
A=ï£®
ï£°2âˆ’1 1
1 3 âˆ’2
4âˆ’3 1ï£¹
ï£»,x=ï£®
ï£°x
y
zï£¹
ï£»,b=ï£®
ï£°1
0
2ï£¹
ï£».
We have
det(A) = 23âˆ’2
âˆ’3 1âˆ’âˆ’1 1
âˆ’3 1+ 4âˆ’1 1
3âˆ’2= 2(âˆ’3)âˆ’2 + 4(âˆ’1) =âˆ’12,
so det( A)Ì¸= 0 and by Cramerâ€™s Rule
x=1
det(A)1âˆ’1 1
0 3 âˆ’2
2âˆ’3 1=âˆ’1
12(âˆ’5) =5
12
y=1
det(A)2 1 1
1 0âˆ’2
4 2 1=âˆ’1
12(1) =âˆ’1
12
z=1
det(A)2âˆ’1 1
1 3 0
4âˆ’3 2=âˆ’1
12(âˆ’1) =1
12
Therefore the solution to the system, which is unique, is (5 /12,âˆ’1/12,1/12). â– 
Next, we construct a method for finding the inverse of a square matrix using determinants,
provided the matrix is invertible.
Theorem 5.20. LetA= [aij]n. Ifdetn(A)Ì¸= 0, then X= [xij]ngiven by
xij=(âˆ’1)i+jdetnâˆ’1(Aji)
detn(A)
for all 1â‰¤i, jâ‰¤nis the inverse for A.
Proof. Suppose that det n(A)Ì¸= 0. For any jâˆˆ {1, . . . , n }, let
xj=ï£®
ï£°x1j...
xnjï£¹
ï£»,
and recall the jth standard unit vector ejofFn. By Cramerâ€™s Rule the system of equations
corresponding to the matrix equation
Axj=ej178
has a unique solution given by
xij=detn(a1, . . . ,aiâˆ’1,ej,ai+1, . . . ,an)
detn(A)
for each 1 â‰¤iâ‰¤n. Since the jth coordinate of ejis 1 and all other coordinates are 0, we obtain
detn(a1, . . . ,aiâˆ’1,ej,ai+1, . . . ,an) = (âˆ’1)i+jdetnâˆ’1(Aji)
by expanding the determinant along the ith column. Therefore
xij=(âˆ’1)i+jdetnâˆ’1(Aji)
detn(A)
for each 1 â‰¤iâ‰¤nand 1 â‰¤jâ‰¤n, and if we define X= [xij]n, then we readily obtain
AX=In. (5.12)
It remains to show that XA=In. Since detn(AâŠ¤) =detn(A)Ì¸= 0, we can find a matrix Y
such that AâŠ¤Y=In, and then
AâŠ¤Y=Inâ‡’(AâŠ¤Y)âŠ¤=IâŠ¤
nâ‡’YâŠ¤A=In. (5.13)
Now, using (5.12) we obtain
YâŠ¤A=Inâ‡’(YâŠ¤A)X=InXâ‡’YâŠ¤(AX) =Xâ‡’YâŠ¤In=Xâ‡’X=YâŠ¤,
and hence
XA=In
by the rightmost equation in (5.13).
Since XA=AX=In, we conclude that
X=(âˆ’1)i+jdetnâˆ’1(Aji)
detn(A)
n
is the inverse for A. â– 
Put another way, Theorem 5.20 states that if detn(A)Ì¸= 0 then Ais invertible, and the
inverse Aâˆ’1is given by
Aâˆ’1=(âˆ’1)i+jdetnâˆ’1(Aji)
detn(A)
n. (5.14)
Example 5.21. Show that if DâˆˆFnÃ—nis given as a block matrix by
D=
A B
O C
,
where A= [aij]â„“andC= [cij]mare square matrices, then
detn(D) = det â„“(A) det m(C).179
Solution. We must show that, for all â„“, mâˆˆN,
detâ„“+m(D) = det â„“+m[aij]â„“B
O [cij]m
= det â„“([aij]â„“) det m([cij]m), (5.15)
where of course O= [0] mÃ—â„“andB= [bij]â„“Ã—m.
First consider the case when â„“= 1 and mâˆˆNis arbitrary. Letting D= [dij]m+1denote the
block matrix and expanding along the first column, we have
detm+1aB
O[cij]m
=m+1X
i=1(âˆ’1)i+1di1detm(Di1).
Since D11= [cij]m,d11=aanddi1= 0 for i >1, let a11=ato obtain
detm+1aB
O[cij]m
= (âˆ’1)1+1d11detm(D11) =adetm([cij]m)
= det 1([aij]1) det m([cij]m).
This establishes the base case of an inductive argument on â„“.
Next, fix â„“âˆˆN, and assume that (5.15) is true for â„“and all mâˆˆN. We must show that
(5.15) is true for â„“+ 1 and all m. Let mâˆˆNbe arbitrary, and define
D= [dij]â„“+m+1=[aij]â„“+1 B
O [cij]m
.
Letting Bidenote Bwith ith row deleted, and also setting A= [aij]â„“+1, we have
detâ„“+m+1(D) = det â„“+m+1[aij]â„“+1 B
O [cij]m
=â„“+m+1X
i=1(âˆ’1)i+1di1detâ„“+m(Di1)
=â„“+1X
i=1(âˆ’1)i+1ai1detâ„“+m(Di1).
Since Ai1is an â„“Ã—â„“matrix for each 1 â‰¤iâ‰¤â„“+ 1, by the inductive hypothesis we find that
detâ„“+m(Di1) = det â„“+m
Ai1Bi
O C
= det â„“(Ai1) det m(C)
for each 1 â‰¤iâ‰¤â„“+ 1, and hence
detâ„“+m+1(D) =â„“+1X
i=1(âˆ’1)i+1ai1detâ„“(Ai1) det m(C) = det â„“+1(A) det m(C).
By induction we conclude that (5.15) holds for â„“, mâˆˆN, and therefore
det
A B
O C
= det( A) det(C)
for any square matrices AandC. â– 
For the next example we define a minor of a matrix AâˆˆFmÃ—nto be the determinant of
any square submatrix of A. We have encountered minors already: each Aijthat appears in
Definition 5.10 is an ( nâˆ’1)Ã—(nâˆ’1) minor of the nÃ—nmatrix A.180
Example 5.22. ForAâˆˆFmÃ—nlet 1â‰¤k <min{m, n}. Show that rank(A)â‰¤kif and only if
every ( k+ 1)Ã—(k+ 1) minor of Aequals 0.
Solution. By Proposition 4.69, rank(A)â‰¤kif and only if every ( k+ 1)Ã—(k+ 1) submatrix of
Ais noninvertible. By the Invertible Matrix Theorem a ( k+ 1)Ã—(k+ 1) submatrix of Ais
noninvertible if and only if the determinant of the submatrix equals 0. Therefore rank(A)â‰¤k
if and only if every ( k+ 1)Ã—(k+ 1) minor of Aequals 0. â– 
Problems
1. Solve the systemï£±
ï£²
ï£³x+y+ 2z= 1
2x + 4z= 2
3y+z= 3
using Cramerâ€™s Rule.181
5.4 â€“ Determinant Formulas
Recall the elementary matrices Mi,j(c) and Mi,jdefined in section 2.3. Given a scalar xand
annÃ—nmatrix Awith row vectors a1, . . . ,an, by Proposition 2.16 we have
Mi,j(x)A=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
aj+xai...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»	
jth row,
and so by Proposition 5.11 (recalling DP5 in Theorem 5.4) we find that
detn(Mi,j(x)A) = det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
aj+xai...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸= det nï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°a1...
aj...
anï£¹
ï£ºï£ºï£ºï£ºï£ºï£»ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸= det n(A). (5.16)
By Proposition 2.16 the matrix Mi,jAis obtained from Aby interchanging the ith and jth
rows, and so by Proposition 5.11 (recalling DP2 in Theorem 5.4) we find that
detn(Mi,jA) =âˆ’detn(A). (5.17)
We use these facts to prove the following.
Theorem 5.23. For any A,BâˆˆFnÃ—n,
detn(AB) = det n(A) det n(B).
Proof. IfAis not invertible, then ABis not invertible by Proposition 4.68 and we obtain
detn(A) det n(B) = 0Â·detn(B) = 0 = det n(AB)
by the Invertible Matrix Theorem. If Bis not invertible we obtain a similar result since
detn(AB) = 0 and det n(B) = 0.
Suppose that AandBare both invertible, so that ABis also invertible by Theorem 2.26.
By the proof of Theorem 2.30 the matrix Ais row-equivalent via R1 and R2 operations to a
diagonal matrix
D=ï£®
ï£°d1 0
...
0 dnï£¹
ï£»=ï£®
ï£°d1e1...
dnenï£¹
ï£»
that is, there exists a sequence of elementary matrices M1, . . . ,Mk, of which â„“are of the R2
variety and the rest of the R1 variety, such that
A=MkÂ·Â·Â·M1D.182
Now, if b1, . . . ,bnare the row vectors of B, then
DB=ï£®
ï£°d1b1...
dnbnï£¹
ï£»
and so, recalling (5.16) and (5.17) as well as Theorem 5.7,
detn(AB) = det n 
(MkÂ·Â·Â·M1D)B
= det n 
MkÂ·Â·Â·M1(DB)
= (âˆ’1)â„“detn(DB)
= (âˆ’1)â„“detn 
(DB)âŠ¤
= (âˆ’1)â„“detn 
d1bâŠ¤
1, . . . , d nbâŠ¤
n
= (âˆ’1)â„“d1Â·Â·Â·dndetn 
bâŠ¤
1, . . . ,bâŠ¤
n
= (âˆ’1)â„“d1Â·Â·Â·dndetn 
BâŠ¤
= (âˆ’1)â„“d1Â·Â·Â·dndetn(B) = (âˆ’1)â„“detn(D) det n(B)
= det n(MkÂ·Â·Â·M1D) det n(B) = det n(A) det n(B).
Here we use the fact that Dis an upper-triangular matrix and so by Proposition 5.5 has
determinant equal to the product of its diagonal entries. â– 
Theorem 5.24. IfAâˆˆFnÃ—nis invertible, then
detn(Aâˆ’1) =1
detn(A).
Proof. Suppose that AâˆˆFnÃ—nis invertible. Then there exists some Aâˆ’1âˆˆFnÃ—nsuch that
AAâˆ’1=In, and thus
detn(A) det n(Aâˆ’1) = det n(AAâˆ’1) = det n(In) = 1 . (5.18)
by Theorems 5.23 and 5.4(7) . Now, the invertibility of Aimplies that detn(A)Ì¸= 0 by the
Invertible Matrix Theorem, and so from (5.18) we readily obtain
detn(Aâˆ’1) =1
detn(A).
as desired. â– 
Another way to write the statement of Theorem 5.24 that is particularly elegant is:
detn(Aâˆ’1) = det n(A)âˆ’1
ifAâˆˆFnÃ—nis invertible.
Recall Corollary 4.33: given a linear operator L:Vâ†’V, bases BandBâ€²forV, and
corresponding matrices [ L]Band [ L]Bâ€², we have
[L]Bâ€²=IBBâ€²[L]BIâˆ’1
BBâ€², (5.19)
From this matrix equation we obtain an interesting result involving determinants.183
Theorem 5.25. Letdim(V) =n, letLbe a linear operator on V, and let BandBâ€²be bases
forV. If [L]Bis the matrix corresponding to Lwith respect to Band[L]Bâ€²is the matrix
corresponding to Lwith respect to Bâ€², then
detn([L]Bâ€²) = det n([L]B). (5.20)
Proof. From equation (5.19) we obtain
detn([L]Bâ€²) = det n(IBBâ€²[L]BIâˆ’1
BBâ€²).
Now, by Theorems 5.23 and 5.24,
detn([L]Bâ€²) = det n(IBBâ€²) det n([L]B) det n(Iâˆ’1
BBâ€²)
= det n(IBBâ€²) det n([L]B)1
detn(IBBâ€²)
= det n([L]B),
which affirms (5.20) and finishes the proof. â– 
Thus the determinant of the matrix corresponding to a linear operator on Vis invariant in
value under change of bases, so that we can meaningfully speak of the â€œdeterminantâ€ of a linear
operator.
Definition 5.26. Letdim(V) =n, and let Lbe a linear operator on V. The determinant of
Lis defined to be
detn(L) = det n([L]),
where [L]is the matrix corresponding to Lwith respect to any basis for V.184
5.5 â€“ Permutations and the Symmetric Group
Definition 5.27. LetnâˆˆN, and let In={1,2, . . . , n }. The symmetric group Snis the
group consisting of all bijections
Ïƒ:Inâ†’In
under the operation of function composition â—¦. Each Ïƒâˆˆ S nis called a permutation .
By definition every group must have an identity element. We denote by Îµtheidentity
permutation in Snthat is given by Îµ(k) =kfor each kâˆˆIn.
A special matrix notation, known as the two-line notation , is often used to define a
permutation Ïƒâˆˆ S nexplicitly. We write
Ïƒ=
1 2 Â·Â·Â· n
Ïƒ(1)Ïƒ(2)Â·Â·Â·Ïƒ(n)
to indicate that Ïƒmaps 1 to the value Ïƒ(1), 2 to the value Ïƒ(2), and so on. Thus the first row
of the matrix lists the â€œinputsâ€ for the function Ïƒ, and the second row lists the corresponding
â€œoutputs.â€
Since Ïƒâˆˆ S nis a bijection, it has an inverse which we denote (as usual) by Ïƒâˆ’1, and it is
easy to see that Ïƒâˆ’1âˆˆ S nalso. We also define Ïƒ0=Îµ,Ïƒ1=Ïƒ,Ïƒ2=Ïƒâ—¦Ïƒ, and so on.
Example 5.28. One permutation belonging to the group S5isÏƒ:I5â†’I5given by
Ïƒ(1) = 4 , Ïƒ(2) = 2 , Ïƒ(3) = 1 , Ïƒ(4) = 5 , Ïƒ(5) = 3 ,
which we denote by
1 2 3 4 5
4 2 1 5 3
in the two-line notation. â– 
Example 5.29. Just as there are 6 possible permutations (i.e. ordered arrangements) of a set
of 3 distinct objects {a, b, c}, namely
(a, b, c ),(a, c, b ),(b, a, c ),(b, c, a ),(c, a, b ),(c, b, a ),
so too are there six permutations in the group S3. These are
1 2 3
1 2 3
,
1 2 3
1 3 2
,
1 2 3
2 1 3
,
1 2 3
2 3 1
,
1 2 3
3 1 2
,
1 2 3
3 2 1
.
The first permutation in the list is the identity permutation Îµ. â– 
IfÏƒ, Ï„âˆˆ S n, then Ï„â—¦Ïƒâˆˆ S nis given by
(Ï„â—¦Ïƒ)(i) =Ï„(Ïƒ(i))
for each iâˆˆInin the usual manner of function composition. Thus
Ï„â—¦Ïƒ=
1 2 Â·Â·Â· n
Ï„(1)Ï„(2)Â·Â·Â·Ï„(n)
â—¦
1 2 Â·Â·Â· n
Ïƒ(1)Ïƒ(2)Â·Â·Â·Ïƒ(n)185
=
1 2 Â·Â·Â· n
Ï„(Ïƒ(1)) Ï„(Ïƒ(2))Â·Â·Â·Ï„(Ïƒ(n))
.
Example 5.30. InS3we have
1 2 3
1 3 2
â—¦
1 2 3
3 2 1
=
1 2 3
2 3 1
, (5.21)
Note that the matrix immediately to the right of the symbol â—¦in(5.21) takes the input first, so
1â†’
1 2 3
3 2 1
â†’3â†’
1 2 3
1 3 2
â†’2,
which gives the first column of the matrix to the right of the = symbol in (5.21). â– 
Assuming nâ‰¥2, atransposition is a permutation Ï„âˆˆ S nfor which there exist k, â„“âˆˆIn
with kÌ¸=â„“such that
Ï„(i) =ï£±
ï£´ï£²
ï£´ï£³i,ifiâˆˆIn\ {k, â„“}
k,ifi=â„“
â„“,ifi=k.
Thus a transposition interchanges precisely two distinct elements of Inwhile leaving all other
elements fixed. The classic example in S2is
1 2
2 1
,
and an example in S4is
1 2 3 4
1 4 3 2
.
Any permutation Ïƒâˆˆ S nis uniquely determined by the arrangement of the elements of Inin
the second row of its corresponding matrix. Since the nelements in Inhave n! possible distinct
arrangements, it follows that Snitself has n! elements. This proves the following.
Proposition 5.31. |Sn|=n!for all nâˆˆN.
We now introduce another notation for elements of Sncalled cycle notation . For mâ‰¤n
letJ={j1, j2, j3, . . . , j m}be a set of distinct elements of In. Then the symbol
(j1, j2, j3, . . . , j m), (5.22)
denotes a permutation in Snthat performs the mappings
j17â†’j27â†’j37â†’ Â·Â·Â· 7â†’ jmâˆ’17â†’jm7â†’j1,
and also i7â†’ifor any iâˆˆIn\J. Using function notation, if Ïƒâˆˆ S nis such that
Ïƒ= (j1, j2, j3, . . . , j m),
then
Ïƒ(j1) =j2, Ïƒ(j2) =j3, . . . , Ïƒ (jmâˆ’1) =jm, Ïƒ(jm) =j1,
with Ïƒ(i) =ifor any iâˆˆIn\J.186
Any permutation expressible in the form (5.22) is called a cycle . The entries in (5.22) are
ideally envisioned as being written in a circular arrangement, like the numbers on a clock, so
that the â€œlastâ€ entry jmis naturally seen to be followed by j1. In this way
(jm, j1, j2, . . . , j mâˆ’1)
is easily recognized as being the same permutation as that given by (5.22).
Example 5.32. InS3the cycle (1 ,3,2) is the permutation

1 2 3
3 1 2
.
InS5the cycle (1 ,3,2) is the permutation

1 2 3 4 5
3 1 2 4 5
.
Since (1 ,3,2)âˆˆ S 5does not feature 4 or 5 among its entries, we see that (1 ,3,2) maps 4 7â†’4
and 5 7â†’5.
InSnfor any nâ‰¥3 we have
(1,3,2) = (2 ,1,3) = (3 ,2,1).
That is, moving the last entry in a cycle to the first position does not change the corresponding
permutation. â– 
As with permutations in general, two cycles ÏƒandÏ„inSnmay be composed. If
Ïƒ= (j1, j2, . . . , j m) and Ï„= (i1, i2, . . . , i â„“), (5.23)
then
(j1, j2, . . . , j m)â—¦(i1, i2, . . . , i â„“)
is the permutation Ïƒâ—¦Ï„. Typically the symbol â—¦is omitted in the cycle notation, and we write
Ïƒâ—¦Ï„= (j1, j2, . . . , j m)(i1, i2, . . . , i â„“).
Thelength of a cycle is simply the number of entries it contains. For instance the cycles Ïƒ
andÏ„in(5.23) have lengths mandâ„“, respectively. We will say a cycle is an m-cycle if it has
length m. We now gather a few facts about transpositions.
Proposition 5.33. Letnâ‰¥2.
1.S1has no transpositions.
2.Ï„âˆˆ S nis a transposition if and only if Ï„is a 2-cycle.
3.IfÏ„âˆˆ S nis a transposition, then Ï„â—¦Ï„=Îµ.
4.IfÏ„1, Ï„2âˆˆ S nare transpositions, then Ï„1â—¦Ï„2=Ï„2â—¦Ï„1.187
Proof.
Proof of (3). Suppose Ï„âˆˆ S nis a transposition, so Ï„= (a, b) for some a, bâˆˆInwith aÌ¸=bby
part (2). Then
(Ï„â—¦Ï„)(a) =Ï„(Ï„(a)) =Ï„(b) =aand ( Ï„â—¦Ï„)(b) =Ï„(Ï„(b)) =Ï„(a) =b,
and furthermore
(Ï„â—¦Ï„)(i) =Ï„(Ï„(i)) =Ï„(i) =i
for any iâˆˆIn\ {a, b}. Therefore Ï„â—¦Ï„=Îµ. â– 
Proofs of the other parts of Proposition 5.33 are left as exercises.
Two cycles ( j1, . . . , j m) and ( i1, . . . , i â„“) inSnaredisjoint if
{j1, . . . , j m} âˆ© {i1, . . . , i â„“}=âˆ…,
which is to say the cycles have no entries in common. Thus (1 ,6,3) and (4 ,2,5,8) are disjoint since
{1,6,3}âˆ©{4,2,5,8}=âˆ…, but (5 ,2,1) and (3 ,1,9,2) are not disjoint since {5,2,1}âˆ©{3,1,9,2}=
{1,2}.
Proposition 5.34. If(j1, . . . , j m)and(i1, . . . , i â„“)are disjoint cycles in Sn, then
(j1, . . . , j m)(i1, . . . , i â„“) = (i1, . . . , i â„“)(j1, . . . , j m).
The proof of Proposition 5.34 is left as an exercise. Another way to state Proposition 5.34 is to
say that disjoint cycles commute. Parts (2) and (4) of Proposition 5.33 imply that commutativity
always holds in the special case of 2-cycles, even if the 2-cycles under consideration are not
disjoint.
The process of expressing a permutation as a composition of two or more cycles is known
ascycle decomposition . Even a permutation that is itself a cycle we may be interested in
expressing anew as a composition of two cycles of lesser length. Indeed, of particular importance
to us along our path to a new formulation for determinants in the next section is the process of
decomposing a permutation into 2-cycles (i.e. transpositions).
Example 5.35. Consider the permutation
Ïƒ=
1 2 3 4 5 6 7
3 6 1 2 5 7 4
inS7. We see that Ïƒmaps 1 to 3, and also 3 back to 1. We may write this as 1 7â†’37â†’1. We
also have the chain of mappings
27â†’67â†’77â†’47â†’2.
The only mapping left is 5 7â†’5. Thus Ïƒhas the cycle decomposition
(1,3)(2,6,7,4),
or equivalently (2 ,6,7,4)(1,3). Recall that if a value is absent from a cycleâ€™s list of entries, then
the cycle returns that value unchanged. Thus
5â†’(1,3)â†’5â†’(2,6,7,4)â†’5,188
whereas
1â†’(1,3)â†’3â†’(2,6,7,4)â†’3.
To decompose Ïƒinto transpositions it is only necessary to decompose (2 ,6,7,4) into trans-
positions. In fact we have
(2,6,7,4) = (2 ,6)(2,7)(2,4),
where the three transpositions on the right-hand side may be written in any order, and so
Ïƒ= (1,3)(2,6,7,4) = (1 ,3)(2,6)(2,7)(2,4),
where again any order is permissible. â– 
It was not mere luck that the permutation Ïƒin Example 5.35 was able to be decomposed
into transpositions. As the next proposition makes clear, this is true of any permutation in Sn
fornâ‰¥2.
Proposition 5.36. Letnâ‰¥2. IfÏƒâˆˆ S n, then for some kâˆˆNthere exist transpositions
Ï„1, . . . , Ï„ kâˆˆ S nsuch that
Ïƒ=Ï„1â—¦ Â·Â·Â· â—¦ Ï„k.
Proof. The proof will employ induction, so we start by showing the n= 2 case is true. The
symmetric group S2has only two elements: Îµand (1 ,2). Since (1 ,2) is already a transposition,
we need only show that
Îµ=
1 2
1 2
can be expressed as a composition of transpositions. But by Proposition 5.33(3) we immediately
have Îµ= (1,2)(1,2), and so weâ€™re done.
Now let nâ‰¥2 be arbitrary, and suppose the statement of the proposition holds for this value
n. Let Ïƒâˆˆ S n+1, so that
Ïƒ=
1 2 Â·Â·Â·n+ 1
i1i2Â·Â·Â· in+1
.
Since Ïƒ:In+1â†’In+1is a bijection there exists some mâˆˆIn+1such that Ïƒ(m) =n+ 1. There
are two cases to consider: either m=n+ 1 or m < n + 1.
Ifm=n+ 1, so that Ïƒ(n+ 1) = n+ 1, then Ïƒ(i)âˆˆInfor each iâˆˆIn. If we define
Ë†Ïƒ:Inâ†’InbyË†Ïƒ(i) =Ïƒ(i) for each iâˆˆIn, then Ë†Ïƒâˆˆ S n, and by our inductive hypothesis there
exist transpositions Ï„1, . . . , Ï„ kâˆˆ S nsuch that Ë†Ïƒ=Ï„1â—¦ Â·Â·Â· â—¦ Ï„k. By Proposition 5.33(2) each
transposition Ï„jis a 2-cycle ( aj, bj), and since aj, bjâˆˆInandInâŠ†In+1, it follows that ( aj, bj)
also defines a 2-cycle in Sn+1. Taking Ï„j= (aj, bj) to be in Sn+1for each 1 â‰¤jâ‰¤k, we find
thatÏƒ=Ï„1â—¦ Â·Â·Â· â—¦ Ï„k, and so Ïƒis expressible as a composition of transpositions.
Suppose next that m < n + 1, so Ïƒ(m) =n+ 1 for mâˆˆIn. Defining Ïƒ0âˆˆ S n+1by
Ïƒ0=Ïƒâ—¦(m, n+ 1), Proposition 5.33(3) and the known associativity of the function composition
operation imply that
Ïƒ=Ïƒâ—¦Îµ=Ïƒâ—¦((m, n + 1)â—¦(m, n + 1))
= (Ïƒâ—¦(m, n + 1))â—¦(m, n + 1) = Ïƒ0â—¦(m, n + 1). (5.24)189
Now, since
Ïƒ0(n+ 1) = ( Ïƒâ—¦(m, n + 1))( n+ 1) = Ïƒ(m) =n+ 1,
we see that Ïƒ0has the property treated in the m=n+ 1 case, and so by the same argument
used in that case there exist transpositions Ï„1, . . . , Ï„ kâˆˆ S n+1such that Ïƒ0=Ï„1â—¦ Â·Â·Â· â—¦ Ï„k. Then
by (5.24) we find that
Ïƒ=Ï„1â—¦ Â·Â·Â· â—¦ Ï„kâ—¦(m, n + 1),
which shows that Ïƒis again expressible as a composition of transpositions. â– 
What Proposition 5.36 does not say is that the cycle decomposition of a permutation into
transpositions is necessarily unique, and thatâ€™s because it never is. Even for (1 ,2)âˆˆ S 2we have
(1,2) = (1 ,2)(1,2)(1,2) = (1 ,2)(1,2)(1,2)(1,2)(1,2),
and in general (1 ,2) = (1 ,2)2kâˆ’1for any kâˆˆN.
Is there anything more that can be said about the decomposition of a permutation into
transpositions, beyond its mere existence? Recall that any integer has a parity, which is to
say the integer is either even (divisible by 2) or odd (not divisible by 2). Now we define the
parity of a particular decomposition of Ïƒâˆˆ S ninto transpositions Ï„1, . . . , Ï„ kas being odd ifk
is odd, and even ifkis even. The next proposition states that no one permutation can have
two decompositions of opposite parity.
Proposition 5.37. Letnâ‰¥2. IfÏƒâˆˆ S n, then the decompositions of Ïƒinto transpositions are
either all odd or all even.
Proof. The proof will employ induction, so we start by showing the n= 2 case is true. The
symmetric group S2has only two elements, Îµand (1 ,2), with (1 ,2) in particular being the only
transposition available. Now, for any kâ‰¥0 Proposition 5.33(3) implies that
(1,2)2k= [(1 ,2)(1,2)]k=Îµk=Îµ,
and
(1,2)2k+1= (1,2)(1,2)2k= (1,2)â—¦Îµ= (1,2).
Thus all the possible even decompositions equal Îµ, and all the possible odd decompositions equal
(1,2). It follows that Îµhas only even decompositions, and (1 ,2) has only odd decompositions.
Now let nâ‰¥2 be arbitrary, and suppose the statement of the proposition holds for this value
n. The remainder of the proof we leave as an exercise. â– 
It is because of Proposition 5.37 that the following definition is meaningful.
Definition 5.38. Letnâ‰¥2. A permutation Ïƒâˆˆ S niseven if it can be expressed as a
composition of an even number of transpositions, and oddif it can be expressed as a composition
of an odd number of transpositions. By definition Îµâˆˆ S 1we take to be even.
Thesign function on Snis the function sgn :Snâ†’ {âˆ’ 1,1}given by
sgn(Ïƒ) =(
1,ifÏƒis even
âˆ’1,ifÏƒis odd.190
The only element of S1is
Îµ=
1
1
,
which cannot be expressed as a composition of transpositions since there are no transpositions
inS1. Nonetheless it will be convenient to define Îµâˆˆ S 1to be even, and therefore sgn( Îµ) = 1.
Remark. Since ( âˆ’1)mis 1 if mis even and âˆ’1 ifmis odd, we see from Definition 5.38 that if
a permutation Ïƒcan be expressed as a composition of mtranspositions, then sgn( Ïƒ) = (âˆ’1)m.
It is straightforward to check that the composition of two even permutations is again even,
and so if Anis the set of all even permutations in Sn, then Anis in fact a subgroup of Sncalled
theantisymmetric group . In contrast the composition of two odd permutations is even, and
so the set Sn\ Anof all odd permutations in Snis not a group since it is not closed under the
operation â—¦of function composition.191
5.6 â€“ The Leibniz Formula
InÂ§5.2 we found that, for each nâˆˆN, the functions detn,ianddetâ€²
n,jwere equal for all
1â‰¤i, jâ‰¤n; that is,
detn,1=Â·Â·Â·= det n,n= detâ€²
n,1=Â·Â·Â·= detâ€²
n,n.
That all these functions are the same ultimately derives from the fact that they all possess the
six properties given in Theorem 5.4. A close look at these properties, however, reveals that not
all of them are fundamental. That is, some of the properties are an immediate consequence of
one or more of the others. In particular, analyzing the details of the theoremâ€™s proof, it can be
seen that properties DP1, DP2, and DP3 are independent (i.e. no two can be used to derive the
third), and yet taken together they readily imply DP4, DP5, and DP6.
While all the â€œdifferentâ€ determinants defined in Â§5.2 turned out to be the same, it is
reasonable to wonder whether there is some way to define the determinant of a square matrix A
so that it possesses the properties in Theorem 5.4 and yet is genuinely different. Put another
way, if the minimum qualifications that a function must satisfy in order for it to be called a
â€œdeterminantâ€ are that it possess the multilinearity, alternating, and normalization properties in
Theorem 5.4, does that uniquely characterize the function? The answer is yes.
Theorem 5.39 (Uniqueness of the Determinant ).FornâˆˆNsuppose D:FnÃ—nâ†’Fhas
the following properties:
DP1. Multilinearity. For any 1â‰¤jâ‰¤nandxâˆˆF,
D(. . . ,aj, . . .) +D(. . . ,bj, . . .) =D(. . . ,aj+bj, . . .),
and
D(. . . , xaj, . . .) =xD(. . . ,aj, . . .).
DP2. Alternating. For any 1â‰¤j < kâ‰¤n,
D(. . . ,aj, . . . ,ak, . . .) =âˆ’D(. . . ,ak
j, . . . ,aj
k, . . .).
DP3. Normalization.
D(In) = 1 .
Then D= det n.
Proof. Applying DP2 in the case when aj=ak=ugives
D(. . . ,u, . . . ,u, . . .) =âˆ’D(. . . ,u, . . . ,u, . . .),
and hence
D(. . . ,u, . . . ,u, . . .) = 0 .
That is, D(A) = 0 whenever AâˆˆFnÃ—nhas two identical columns.
LetA= [a1Â·Â·Â·an]âˆˆFnÃ—nbe arbitrary. By DP1,
D(A) =D(a1, . . . ,an) =D nX
i1=1ai11ei1, . . . ,nX
in=1ainnein!192
=nX
i1=1Â·Â·Â·nX
in=1D 
ai11ei1, . . . , a innein
=nX
i1=1Â·Â·Â·nX
in=1ai11Â·Â·Â·ainnD(ei1, . . . ,ein). (5.25)
It remains to evaluate D(ei1, . . . ,ein) in(5.25) . In fact we have D(ei1, . . . ,ein) = 0 whenever
ik=iâ„“for some kÌ¸=â„“, since the matrix [ ei1Â·Â·Â·ein] then has two identical columns, and it
follows that only those terms in the sum (5.25) for which the list of values i1, . . . , i nrepresents a
permutation Ïƒâˆˆ S nare all thatâ€™s left. In particular, for each such term we take Ïƒto be given by
Ïƒ(k) =ikfor 1â‰¤kâ‰¤n, and since there is a one-to-one correspondence between the remaining
terms in (5.25) and the elements of Sn, we obtain
D(A) =X
ÏƒâˆˆSnaÏƒ(1),1Â·Â·Â·aÏƒ(n),nD(eÏƒ(1), . . . ,eÏƒ(n)). (5.26)
Now, for any Ïƒâˆˆ S nthere exist, by Proposition 5.36, transpositions Ï„1, . . . , Ï„ msuch that
Ïƒ=Ï„mâ—¦ Â·Â·Â· â—¦ Ï„1. By DP2,
D(e1, . . . ,en) =âˆ’D(eÏ„1(1), . . . ,eÏ„1(n)) = (âˆ’1)2D(eÏ„2(Ï„1(1)), . . . ,eÏ„2(Ï„1(n)))
= (âˆ’1)3D(eÏ„3(Ï„2(Ï„1(1))), . . . ,eÏ„3(Ï„2(Ï„1(n))))
...
= (âˆ’1)mD(e(Ï„mâ—¦Â·Â·Â·â—¦Ï„1)(1), . . . ,e(Ï„mâ—¦Â·Â·Â·â—¦Ï„1)(n))
= sgn( Ïƒ)D(eÏƒ(1), . . . ,eÏƒ(n)),
and so by DP3, noting that 1 /sgn(Ïƒ) = sgn( Ïƒ),
D(eÏƒ(1), . . . ,eÏƒ(n)) = sgn( Ïƒ)D(e1, . . . ,en) = sgn( Ïƒ)D(In) = sgn( Ïƒ).
Putting this result into (5.26) gives
D(A) =X
ÏƒâˆˆSnaÏƒ(1),1Â·Â·Â·aÏƒ(n),nsgn(Ïƒ). (5.27)
The expression at right in (5.27) is entirely independent of D. Indeed, if we assume Ë†Dis
another function on FnÃ—nthat satisfies the properties DP1, DP2, and DP3, then an identical
argument will lead to Ë†D(A) equalling the same expression, and hence Ë†D(A) =D(A). Since
detnhas the properties DP1, DP2, and DP3, we conclude that det n(A) =D(A). â– 
The proof of Theorem 5.39 immediately gives the following result, which is a formula for the
determinant function that is explicit rather than recursive.
Theorem 5.40 (Leibniz Formula ).For any nâˆˆNandAâˆˆFnÃ—n,
detn(A) =X
ÏƒâˆˆSnsgn(Ïƒ)aÏƒ(1),1aÏƒ(2),2Â·Â·Â·aÏƒ(n),n.193
Proposition 5.41. Letnâ‰¥2. For any 1â‰¤k < â„“â‰¤n,
X
ÏƒâˆˆSnsgn(Ïƒ) 
aÏƒ(k),kaÏƒ(â„“),kY
iâˆˆIn\{k,â„“}aÏƒ(i),i!
= 0. (5.28)
Proof. Fix 1â‰¤k < â„“â‰¤n, and let Î£ Ïƒdenote the sum in (5.28). Then
Î£Ïƒ=X
Ï€âˆˆAn 
aÏ€(k),kaÏ€(â„“),kY
iâˆˆIn\{k,â„“}aÏ€(i),i!
âˆ’X
Î½âˆˆSn\An 
aÎ½(k),kaÎ½(â„“),kY
iâˆˆIn\{k,â„“}aÎ½(i),i!
. (5.29)
FixÏ€0âˆˆ A n. Then Î½0=Ï€0â—¦(k, â„“)âˆˆ S n\ Anis given by
Î½0(i) =ï£±
ï£´ï£²
ï£´ï£³Ï€0(i),ifiâˆˆIn\ {k, n}
Ï€0(â„“),ifi=k
Ï€0(k),ifi=â„“,
so that
aÎ½0(â„“),kaÎ½0(k),kY
iâˆˆIn\{k,â„“}aÎ½0(i),i=aÏ€0(k),kaÏ€0(â„“),kY
iâˆˆIn\{k,â„“}aÏ€0(i),i.
This shows that the term in the sumP
Ï€âˆˆAnthat corresponds to Ï€0is canceled by the term inP
Î½âˆˆSn\Anthat corresponds to Î½0at right in (5.29) . In a similar way, for any Î½1âˆˆ S n\ Anwe
have Ï€1=Î½1â—¦(k, â„“)âˆˆ A n, and the terms inP
Ï€âˆˆAnandP
Î½âˆˆSn\Ancorresponding to Ï€1andÎ½1
will cancel in (5.29). Therefore the sumP
Ïƒmust equal zero. â– 194
6
Eigen Theory
6.1 â€“ Eigenvectors and Eigenvalues
Throughout this chapter we assume that all vector spaces are finite-dimensional with
dimension at least 1 unless otherwise specified.
Definition 6.1. LetVbe a vector space over FandL:Vâ†’Va linear operator. An
eigenvector ofLis a nonzero vector vâˆˆVsuch that
L(v) =Î»v
for some Î»âˆˆF. The scalar Î»is an eigenvalue ofL, and vis said to be an eigenvector
corresponding to Î». The set
EL(Î») ={vâˆˆV:L(v) =Î»v}
is the eigenspace ofLcorresponding to Î».
The symbol Ïƒ(L) will occasionally be used to denote the set of eigenvalues possessed by a
linear operator L, so that |Ïƒ(L)|denotes the number of distinct eigenvalues of L.
A careful examination of Definition 6.1 should make it clear that, while the zero vector
0âˆˆVcannot be an eigenvector, the zero scalar 0 âˆˆFcan be an eigenvalue. Despite not being
an eigenvector, however, it is always true that 0is an element of EL(Î») since
L(0) =0=Î»0
holds for any linear operator L
Proposition 6.2. LetVbe a vector space. If L:Vâ†’Vis a linear operator with eigenvalue Î»,
then EL(Î»)is a subspace of V.
Proof. Suppose L:Vâ†’Vis linear with eigenvalue Î». It has already been established that
0âˆˆEL(Î»). Given u,vâˆˆEL(Î») and scalar cwe have
L(u+v) =L(u) +L(v) =Î»u+Î»v=Î»(u+v)195
and
L(cv) =cL(v) =c(Î»v) =Î»(cv),
which shows that u+vâˆˆEL(Î») and cvâˆˆEL(Î»). â– 
Example 6.3. LetVbe a vector space and consider the identity operator IV:Vâ†’Vgiven
byIV(v) =vfor all vâˆˆV. It is clear that Î»= 1 is the only eigenvalue of IV, and all nonzero
vectors in Vare corresponding eigenvectors. Indeed,
EIV(1) ={vâˆˆV:IV(v) =v}=V
is the corresponding eigenspace. â– 
Example 6.4. LetVbe a vector space and consider the zero operator OV:Vâ†’Vgiven by
OV(v) =0for all vâˆˆV. For any vÌ¸=0, then, we have
OV(v) =0= 0v,
which shows that 0 is an eigenvalue of OV. Moreover
EOV(0) ={vâˆˆV:OV(v) = 0v}=V
is the corresponding eigenspace. There are no other eigenvalues. â– 
In addition to eigenvectors, eigenvalues, and eigenspaces of linear mappings, there are related
notions for square matrices.
Definition 6.5. LetAâˆˆFnÃ—n. Aneigenvector ofAis a nonzero vector xâˆˆFnsuch that
Ax=Î»x
for some Î»âˆˆF. The scalar Î»is an eigenvalue ofA, and xis said to be an eigenvector
corresponding to Î». The set
EA(Î») ={xâˆˆFn:Ax=Î»x}
is the eigenspace ofAcorresponding to Î».
The symbol Ïƒ(A) will occasionally be used to denote the set of eigenvalues possessed by a
square matrix A, so that |Ïƒ(A)|denotes the number of distinct eigenvalues of A.
Remark. A careful reading of Definition 6.5 should make it clear that any eigenvector corre-
sponding to an eigenvalue of AâˆˆFnÃ—nmust be an element of Fn. Thus, if we are given that
AâˆˆRnÃ—n, then we would discount any zâˆˆCn\Rnfor which Az=Î»zfor some Î»âˆˆR.
Proposition 6.6. IfÎ»is an eigenvalue of AâˆˆFnÃ—n, then EA(Î»)is a subspace of Fn.196
Proof. Suppose that Î»is an eigenvalue of A. By Definitions 6.5 and 3.15,
xâˆˆEA(Î»)â‡”Ax=Î»xâ‡”Axâˆ’Î»x=0â‡”Axâˆ’Î»Inx=0
â‡”(Aâˆ’Î»In)x=0â‡”xâˆˆNul(Aâˆ’Î»In).
That is,
EA(Î») = Nul( Aâˆ’Î»In), (6.1)
the null space of Aâˆ’Î»In. By Proposition 3.16 Nul(Aâˆ’Î»In) is a subspace of Fn, and hence so
too is EA(Î»). â– 
Proposition 6.7. LetVbe a vector space over F, and suppose Lâˆˆ L(V)has eigenvalues Î»1, Î»2
with corresponding eigenvectors v1,v2, respectively.
1.IfÎ»1Ì¸=Î»2, then v1Ì¸=v2.
2.EL(Î»1)âˆ©EL(Î»2) ={0}if and only if Î»1Ì¸=Î»2.
Proof.
Proof of Part (1). We will prove the contrapositive: â€œIf v1=v2, then Î»1=Î»2.â€ Suppose that
v1=v2=v, so
Î»1v=Î»1v1=L(v1) =L(v2) =Î»2v2=Î»2v,
and then
(Î»1âˆ’Î»2)v=Î»1vâˆ’Î»2v=0.
By Proposition 3.2(3) either Î»1âˆ’Î»2= 0 or v=0. But vÌ¸=0since an eigenvector is nonzero
by definition, and so it must be that Î»1âˆ’Î»2. Therefore Î»1=Î»2.
Proof of Part (2). Suppose Î»1=Î»2=Î», so that Î»is an eigenvalue of Lwith corresponding
eigenvectors v1andv2. In particular
v1âˆˆEL(Î») =EL(Î»1) =EL(Î»2),
and thus
v1âˆˆEL(Î»1)âˆ©EL(Î»2).
Since v1Ì¸=0, it follows that EL(Î»1)âˆ©EL(Î»2)Ì¸={0}.
For the converse, suppose Î»1Ì¸=Î»2. Let vâˆˆEL(Î»1)âˆ©EL(Î»2). Then L(v) =Î»1vand
L(v) =Î»2v, and thus Î»1v=Î»2v. Now,
Î»1v=Î»2vâ‡’(Î»1âˆ’Î»2)v=0,
and since Î»1âˆ’Î»2Ì¸= 0, Proposition 3.2(3) implies that v=0. Therefore EL(Î»1)âˆ©EL(Î»2) =
{0}. â– 
The converse of Proposition 6.7(1) is not true in general; that is, if Lâˆˆ L(V) has eigenvalues
Î»1, Î»2with corresponding eigenvectors v1,v2, then v1Ì¸=v2does not necessarily imply that
Î»1Ì¸=Î»2. Consider for example v2= 2v1: certainly v1Ì¸=v2since we know v1Ì¸=0, but
L(v2) =L(2v1) = 2 L(v1) = 2( Î»1v1) =Î»1(2v1) =Î»1v2
shows that Î»1=Î»2.197
Theorem 6.8. LetVbe a vector space over F, and let Lâˆˆ L(V)have distinct eigenvalues
Î»1, . . . , Î» nâˆˆF. Ifv1, . . . ,vnare eigenvectors corresponding to Î»1, . . . , Î» n, respectively, then the
set{v1, . . . ,vn}is linearly independent.
Proof. An eigenvector is nonzero by definition, so if n= 1 then certainly the set {v1}is linearly
independent. This establishes the base case of an inductive argument.
Suppose the theorem is true when n=m, where mis some arbitrary positive integer (this
is our â€œinductive hypothesisâ€). Let Lbe a linear operator on Vwith distinct eigenvalues
Î»1, . . . , Î» m+1and corresponding eigenvectors v1, . . . ,vm+1, so that L(vk) = Î»kvkfor each
1â‰¤kâ‰¤m+ 1. Suppose c1, . . . , c m+1âˆˆFare such that
m+1X
k=1ckvk=0. (6.2)
Since the eigenvalues Î»1, . . . , Î» m+1are distinct, there exists some 1 â‰¤k0â‰¤m+ 1 such that
Î»k0Ì¸= 0. Since the eigenvalues may be indexed in any convenient way, we can assume k0=m+ 1
so that Î»m+1Ì¸= 0. Multiplying (6.2) by Î»m+1gives
m+1X
k=1ckÎ»m+1vk=0, (6.3)
and we also have
L m+1X
k=1ckvk!
=L(0)â‡’m+1X
k=1ckL(vk) =0â‡’m+1X
k=1ckÎ»kvk=0. (6.4)
Subtracting (6.3) from the rightmost equation in (6.4), we obtain
m+1X
k=1ckÎ»kvkâˆ’m+1X
k=1ckÎ»m+1vk=0,
so that
m+1X
k=1ck(Î»kâˆ’Î»m+1)vk=0. (6.5)
Of course
ck(Î»kâˆ’Î»m+1)vk=0
ifk=m+ 1, and so (6.5) becomes
mX
k=1ck(Î»kâˆ’Î»m+1)vk=0. (6.6)
Now, v1, . . . ,vmare the eigenvectors corresponding to the distinct eigenvalues Î»1, . . . , Î» m, and
so by our inductive hypothesis the set {v1, . . . ,vm}is linearly independent. From (6.6) it follows
that
ck(Î»kâˆ’Î»m+1) = 0198
for 1â‰¤kâ‰¤m, which in turn implies that ck= 0 for 1 â‰¤kâ‰¤msince Î»1, . . . , Î» mdo not equal
Î»m+1. Now (6.2) becomes cm+1vm+1=0, which immediately yields cm+1= 0. Since (6.2) results
only in the trivial solution
c1=Â·Â·Â·=cm+1= 0
we conclude that v1, . . . ,vm+1are linearly independent.
We see now that the theorem holds for n=m+ 1 when we assume that it holds for n=m,
and therefore by induction it holds for all nâˆˆN. â– 
Corollary 6.9. IfVis a finite-dimensional vector space and Lâˆˆ L(V), then Lhas at most
dim(V)distinct eigenvalues.
Proof. Suppose that Vis an n-dimensional vector space and Lâˆˆ L(V). Suppose Î»1, . . . , Î» n+1
are distinct eigenvalues of Lwith corresponding eigenvectors v1, . . . ,vn+1. Then {v1, . . . ,vn+1}
is a basis for Vby Theorem 6.8 and we are led to conclude that the dimension of Visn+ 1,
which is a contradiction. Therefore Lhas at most ndistinct eigenvalues. â– 
Example 6.10. LetAâˆˆFnÃ—nbe the diagonal matrix
A=ï£®
ï£°Î»1Â·Â·Â· 0
.........
0Â·Â·Â·Î»nï£¹
ï£»
having distinct diagonal entries Î»1, . . . , Î» n(i.e.Î»iÌ¸=Î»jwhenever iÌ¸=j). Ife1, . . . ,enare the
standard basis vectors for Fn, so that
e1=ï£®
ï£¯ï£¯ï£°1
0
...
0ï£¹
ï£ºï£ºï£»,e2=ï£®
ï£¯ï£¯ï£°0
1
...
0ï£¹
ï£ºï£ºï£», . . . , en=ï£®
ï£¯ï£¯ï£°0
0
...
1ï£¹
ï£ºï£ºï£»,
then for each 1 â‰¤kâ‰¤nwe find that Aek=Î»kek, and so Î»kis an eigenvalue of A.
IfLis the linear operator on Fnhaving Aas its corresponding matrix with respect to
the standard basis E={e1, . . . ,en}, then clearly Î»1, . . . , Î» nare distinct eigenvalues of L, with
e1, . . . ,enbeing corresponding eigenvectors:
L(ek) =Aek=Î»kek.
Of course the eigenvectors e1, . . . ,enare linearly independent as predicted by Theorem 6.8. â– 
Proposition 6.11. An operator Lâˆˆ L(V)is not invertible if and only if 0is an eigenvalue of
L. A matrix AâˆˆFnÃ—nis not invertible if and only if 0is an eigenvalue of A.
Proof. By the Invertible Operator Theorem (Theorem 4.65), Lis not invertible if and only if
Nul(L)Ì¸={0}, and Nul(L)Ì¸={0}if and only if there exists some vÌ¸=0such that L(v) =0,
which is to say 0 is an eigenvalue of Lsince 0= 0v.
By the Invertible Matrix Theorem (Theorem 5.17), Ais not invertible if and only if
Nul(A)Ì¸={0}, and Nul(A)Ì¸={0}if and only if there exists some xÌ¸=0such that Ax=0,
which is to say 0 is an eigenvalue of Asince 0= 0x. â– 199
Proposition 6.12. LetLâˆˆ L(V)andAâˆˆFnÃ—n.
1.LetnâˆˆN. IfÎ»is an eigenvalue of L(resp. A) with corresponding eigenvector v, then Î»nis
an eigenvalue of Ln(resp. An) with eigenvector v.
2.Suppose LandAare invertible. If Î»is an eigenvalue of L(resp. A) with corresponding
eigenvector v, then Î»âˆ’1is an eigenvalue of Lâˆ’1(resp. Aâˆ’1) with eigenvector v.
The proof will consider only the statements about an operator L:Vâ†’V, since the
arguments are much the same for a square matrix A.
Proof.
Proof of Part (1): Then= 1 case is trivially true. Suppose the statement of Part (1) is true for
some arbitrary nâˆˆN. Let Î»be an eigenvalue of Lwith corresponding eigenvector v. Then
L(v) =Î»v, and by our inductive hypothesis Ln(v) =Î»nv. Now,
Ln+1(v) =Ln(L(v)) =Ln(Î»v) =Î»Ln(v) =Î»(Î»nv) =Î»n+1v,
and Part (1) is proven for all nâˆˆNby the principle of induction.
Proof of Part (2): Suppose that Î»is an eigenvalue of Lwith corresponding eigenvector v, so
thatL(v) =Î»v. By Proposition 4.55 we obtain Lâˆ’1(Î»v) =v, and since Î»Ì¸= 0 by Proposition
6.11, it follows that
Lâˆ’1(Î»v) =vâ‡’Î»Lâˆ’1(v) =vâ‡’Lâˆ’1(v) =Î»âˆ’1v.
Hence Î»âˆ’1is an eigenvalue of Lâˆ’1. â– 200
6.2 â€“ The Characteristic Polynomial
Definition 6.13. LetAâˆˆFnÃ—n. The characteristic polynomial ofAis the polynomial
function PA:Fâ†’Fgiven by
PA(t) = det n(Aâˆ’tIn).
Some books define the characteristic polynomial of Ato be det(tInâˆ’A) instead of
det(Aâˆ’tIn), but whichever way it is done will have no impact on either the theory of
characteristic polynomials or any application involving them. This is because only the zeros of
the characteristic polynomial will be of any concern. Setting QA(t) =det(tInâˆ’A), observe
thatPA=QAifnis even, and PA=âˆ’QAifnis odd. In either case PAandQAwill have the
same zeros.
Proposition 6.14. LetVbe a vector space over Fwith dim(V) =n, and let Lâˆˆ L(V). Then
the following statements are equivalent.
1.Î»is an eigenvalue of Lwith corresponding eigenvector u.
2.There exists some basis BforVsuch that Î»is an eigenvalue of [L]Bwith corresponding
eigenvector [u]B.
3.For all bases BforV,Î»is an eigenvalue of [L]Bwith corresponding eigenvector [u]B.
Proof.
(1)â‡’(3):Suppose that Î»is an eigenvalue of L, so there exists some uÌ¸=0such that L(u) =Î»u.
LetBbe any basis for V, and let [ L]BâˆˆFnÃ—nbe the matrix corresponding to Lwith respect to
B, so that
[L]B[v]B= [L(v)]B
for all vâˆˆV. Recall that by Theorem 4.11 the coordinate map Vâ†’Fngiven by v7â†’[v]Bis
an isomorphism, so [ u]BâˆˆFnis not the zero vector since Nul([ Â·]B) ={0}. Thus, from
[L]B[u]B= [L(u)]B= [Î»u]B=Î»[u]B
we conclude that Î»is an eigenvalue of [ L]Bwith corresponding eigenvector [ u]B.
(3)â‡’(2):This is obvious.
(2)â‡’(1): Suppose there exists some basis BforVsuch that Î»is an eigenvalue of [ L]Bwith
corresponding eigenvector [ u]B. Again, the coordinate map [Â·]B:Vâ†’Fnis an isomorphism, so
[L]B[u]B=Î»[u]Bâ‡’[L(u)]B= [Î»u]Bâ‡’L(u) =Î»u,
where the last implication follows from the fact that [Â·]B:Vâ†’Fnis injective. Therefore Î»is
an eigenvalue of Lwith corresponding eigenvector u. â– 
We see from the proposition that if we consider two different bases BandBâ€²forV, then
the matrices corresponding to Lwith respect to these bases, [ L]Band [ L]Bâ€², have the same
eigenvalues as L. The only thing that changes is the corresponding eigenvector: [ v]Bis an201
eigenvector of [ L]Bcorresponding to eigenvalue Î»if and only if [ v]Bâ€²is an eigenvector of [ L]Bâ€²
corresponding to eigenvalue Î».
But thereâ€™s something more: the characteristic polynomials of [ L]Band [ L]Bâ€²will be found
to be the same! To see this, recall IBBâ€², the change of basis matrix from BtoBâ€². By Corollary
4.33 we have
[L]Bâ€²=IBBâ€²[L]BIâˆ’1
BBâ€².
Now, noting that In=IBBâ€²InIâˆ’1
BBâ€²and
det(IBBâ€²) det(Iâˆ’1
BBâ€²) = det( IBBâ€²)Â·1
det(IBBâ€²)= 1
by Theorem 5.24, for any tâˆˆFwe have
P[L]Bâ€²(t) = det([ L]Bâ€²âˆ’tIn) = det 
IBBâ€²[L]BIâˆ’1
BBâ€²âˆ’t(IBBâ€²InIâˆ’1
BBâ€²)
= det 
(IBBâ€²[L]Bâˆ’t(IBBâ€²In))Iâˆ’1
BBâ€²
= det 
(IBBâ€²[L]Bâˆ’IBBâ€²(tIn))Iâˆ’1
BBâ€²
= det 
IBBâ€²([L]Bâˆ’tIn)Iâˆ’1
BBâ€²
= det( IBBâ€²) det([ L]Bâˆ’tIn) det(Iâˆ’1
BBâ€²)
= det([ L]Bâˆ’tIn) =P[L]B(t)
by Theorem 5.23. That is, P[L]Bâ€²=P[L]B, which is to say that the characteristic polynomial of
a linear operatorâ€™s associated matrix is invariant under change of basis. We have proven the
following.
Proposition 6.15. LetL:Vâ†’Vbe a linear operator, and let BandBâ€²be bases for V. If
[L]Band[L]Bâ€²are the matrices corresponding to Lwith respect to BandBâ€², then P[L]B=P[L]Bâ€².
Because of Proposition 6.15, it makes sense to speak of the â€œcharacteristic polynomialâ€ of a
linear operator on Vwithout reference to any specific basis for V.
Definition 6.16. LetLbe a linear operator on V. The characteristic polynomial of Lis
the polynomial function PL:Fâ†’Fgiven by
PL(t) =P[L](t),
where [L]is the matrix corresponding to Lwith respect to any basis for V.
While the idea of an eigenvalue is simple, it can be quite difficult to find eigenvalues of either
a linear operator or a matrix by direct means. To help find the eigenvalue of a linear operator
we have the following.
Theorem 6.17. LetL:Vâ†’Vbe a linear operator. Then Î»is an eigenvalue of Lif and only
ifLâˆ’Î»IVis not invertible.
Proof. Suppose that Î»is an eigenvalue of L. Then there exists some vâˆˆVsuch that vÌ¸=0
andL(v) =Î»v. Now,
(Lâˆ’Î»IV)(v) =L(v)âˆ’(Î»IV)(v) =Î»vâˆ’Î»IV(v) =Î»vâˆ’Î»v=0,202
which shows that vâˆˆNul(Lâˆ’Î»IV) and so Nul(Lâˆ’Î»IV)Ì¸={0}. Hence Lâˆ’Î»IVis not invertible
by the Invertible Operator Theorem.
For the converse, suppose that Lâˆ’Î»IVis not invertible. Then Nul(Lâˆ’Î»IV)Ì¸={0}
by the Invertible Operator Theorem, and it follows that there exists some vÌ¸=0such that
(Lâˆ’Î»IV)(v) =0. Now,
(Lâˆ’Î»IV)(v) =0â‡”L(v)âˆ’Î»IV(v) =0â‡”L(v)âˆ’Î»v=0â‡”L(v) =Î»v,
and therefore Î»is an eigenvalue of L. â– 
The next theorem plainly reduces the problem of finding eigenvalues of an nÃ—nmatrix to
that of finding the zeros of an nth-degree polynomial function. We begin to see the utility of
characteristic polynomials at this point.
Theorem 6.18. LetAâˆˆFnÃ—n. Then Î»is an eigenvalue of Aif and only if PA(Î») = 0 .
Proof. Suppose that Î»is an eigenvalue of A, so that Ax0=Î»x0for some x0âˆˆFn. Define the
linear mapping L:Fnâ†’FnbyL(x) =Ax. Then
L(x0) =Ax 0=Î»x0
shows that Î»is an eigenvalue of L, and so by Theorem 6.17 the mapping
Lâˆ’Î»IFn:Fnâ†’Fn
is not invertible. Let I=IFn, and observe that the matrix associated with Lâˆ’Î»IisAâˆ’Î»In:
(Lâˆ’Î»I)(x) =L(x)âˆ’Î»I(x) =Axâˆ’Î»x=Axâˆ’Î»Inx= (Aâˆ’Î»In)x
for all xâˆˆFn. Thus, since the operator Lâˆ’Î»Iis not invertible, by Corollary 4.59 its associated
square matrix Aâˆ’Î»Inis also not invertible, and so det(Aâˆ’Î»In) = 0 by the Invertible Matrix
Theorem. That is, PA(Î») = 0.
Conversely, suppose that PA(Î») = 0. Then det(Aâˆ’Î»In) = 0, so by the Invertible Matrix
Theorem Aâˆ’Î»Inis not invertible. Define L:Fnâ†’FnbyL(x) =Ax. Then Aâˆ’Î»Inis the
matrix corresponding to the linear operator Lâˆ’Î»I:Fnâ†’Fn, and by Corollary 4.59 Lâˆ’Î»Iis
not invertible. So Î»is an eigenvalue of Lby Theorem 6.17, which is to say there exists some
nonzero x0âˆˆFnsuch that L(x0) =Î»x0. Hence Ax 0=Î»x0and we conclude that Î»is an
eigenvalue of A. â– 
Example 6.19. Find the characteristic polynomial PA:Râ†’Rof
A=ï£®
ï£°1âˆ’3 3
3âˆ’5 3
6âˆ’6 4ï£¹
ï£»,
find the eigenvalues of A, and find a basis for each eigenspace as a subspace of R3.203
Solution. We have
PA(t) = det( Aâˆ’tI3) = detï£«
ï£­ï£®
ï£°1âˆ’3 3
3âˆ’5 3
6âˆ’6 4ï£¹
ï£»âˆ’ï£®
ï£°t0 0
0t0
0 0 tï£¹
ï£»ï£¶
ï£¸=1âˆ’tâˆ’3 3
3âˆ’5âˆ’t3
6 âˆ’6 4 âˆ’t
c1+c2â†’c2= = = = = = =1âˆ’tâˆ’2âˆ’t3
3âˆ’2âˆ’t3
6 0 4 âˆ’tâˆ’r1+r2â†’r2= = = = = = = =1âˆ’tâˆ’2âˆ’t3
t+ 2 0 0
6 0 4 âˆ’t.
Expanding the determinant according to the 2nd row then gives
PA(t) = (âˆ’1)2+1(t+ 2)âˆ’2âˆ’t3
0 4 âˆ’t= (t+ 2)2(tâˆ’4),
and so we see that PA(t) = 0 for t=âˆ’2,4. Thus by Theorem 6.18 the eigenvalues of Aare
Î»=âˆ’2,4.
By (6.1) the eigenspace of Acorresponding to Î»=âˆ’2 is
EA(âˆ’2) = Nul( A+ 2I3) ={xâˆˆR3: (A+ 2I3)x=0}
=ï£±
ï£²
ï£³ï£®
ï£°x1
x2
x3ï£¹
ï£»âˆˆR3:ï£®
ï£°3âˆ’3 3
3âˆ’3 3
6âˆ’6 6ï£¹
ï£»ï£®
ï£°x1
x2
x3ï£¹
ï£»=ï£®
ï£°0
0
0ï£¹
ï£»ï£¼
ï£½
ï£¾.
Writing the matrix equationâ€”which is a homogeneous system of equationsâ€”as an augmented
matrix, we haveï£®
ï£°3âˆ’3 3 0
3âˆ’3 3 0
6âˆ’6 6 0ï£¹
ï£»âˆ’r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’2r1+r3â†’r3ï£®
ï£°3âˆ’3 3 0
0 0 0 0
0 0 0 0ï£¹
ï£»1
3r1â†’r1âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°1âˆ’1 1 0
0 0 0 0
0 0 0 0ï£¹
ï£». (6.7)
Hence x1âˆ’x2+x3= 0, which implies that x3=x2âˆ’x1and so
EA(âˆ’2) =ï£±
ï£²
ï£³ï£®
ï£°x1
x2
x3ï£¹
ï£»âˆˆR3:x1âˆ’x2+x3= 0ï£¼
ï£½
ï£¾=ï£±
ï£²
ï£³ï£®
ï£°x1
x2
x2âˆ’x1ï£¹
ï£»:x1, x2âˆˆRï£¼
ï£½
ï£¾.
Observing thatï£®
ï£°x1
x2
x2âˆ’x1ï£¹
ï£»=ï£®
ï£°x1
0
âˆ’x1ï£¹
ï£»+ï£®
ï£°0
x2
x2ï£¹
ï£»=ï£®
ï£°1
0
âˆ’1ï£¹
ï£»x1+ï£®
ï£°0
1
1ï£¹
ï£»x2,
we have
EA(âˆ’2) =ï£±
ï£²
ï£³ï£®
ï£°1
0
âˆ’1ï£¹
ï£»x1+ï£®
ï£°0
1
1ï£¹
ï£»x2:x1, x2âˆˆRï£¼
ï£½
ï£¾
and so it is clear that
Bâˆ’2=ï£±
ï£²
ï£³ï£®
ï£°1
0
âˆ’1ï£¹
ï£»,ï£®
ï£°0
1
1ï£¹
ï£»ï£¼
ï£½
ï£¾
is a linearly independent set of vectors that spans EA(âˆ’2) and therefore must be a basis for
EA(âˆ’2). Notice that the elements of Bâˆ’2are in fact eigenvectors of Acorresponding to the
eigenvalue âˆ’2, as are all the vectors belonging to EA(âˆ’2).204
Next, the eigenspace of Acorresponding to Î»= 4 is
EA(4) = Nul( Aâˆ’4I3) ={xâˆˆR3: (Aâˆ’4I3)x=0}
=ï£±
ï£²
ï£³ï£®
ï£°x1
x2
x3ï£¹
ï£»âˆˆR3:ï£®
ï£°âˆ’3âˆ’3 3
3âˆ’9 3
6âˆ’6 0ï£¹
ï£»ï£®
ï£°x1
x2
x3ï£¹
ï£»=ï£®
ï£°0
0
0ï£¹
ï£»ï£¼
ï£½
ï£¾.
Applying Gaussian Elimination to the corresponding augmented matrix yields
ï£®
ï£°âˆ’3âˆ’3 3 0
3âˆ’9 3 0
6âˆ’6 0 0ï£¹
ï£»r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
2r1+r3â†’r3ï£®
ï£°âˆ’3âˆ’3 3 0
0âˆ’12 6 0
0âˆ’12 6 0ï£¹
ï£»âˆ’r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’1
2r2+r1â†’r1ï£®
ï£°âˆ’3 3 0 0
0âˆ’12 6 0
0 0 0 0ï£¹
ï£».
From the top row we obtain x2=x1, and from the middle row we obtain x3= 2x2and thus
x3= 2x1. Now,
EA(4) =ï£±
ï£²
ï£³ï£®
ï£°x1
x1
2x1ï£¹
ï£»:x1âˆˆRï£¼
ï£½
ï£¾=ï£±
ï£²
ï£³ï£®
ï£°1
1
2ï£¹
ï£»x1:x1âˆˆRï£¼
ï£½
ï£¾.
Clearly
B4=ï£±
ï£²
ï£³ï£®
ï£°1
1
2ï£¹
ï£»ï£¼
ï£½
ï£¾
is a linearly independent set that spans EA(4) and so qualifies as a basis for EA(4). The vector
belonging to B4is an eigenvector of Acorresponding to the eigenvalue 4, as is any real scalar
multiple of the vector. â– 
In Example 6.19 we found in (6.7) that A+ 2I3is row-equivalent to
B=ï£®
ï£°1âˆ’1 1
0 0 0
0 0 0ï£¹
ï£»,
which clearly has rank 1, and so
rank(A+ 2I3) = rank( B) = 1
by Theorem 3.66. Then by the Rank-Nullity Theorem for Matrices we have
dim(EA(âˆ’2)) = nullity( A+ 2I3) = dim( R3)âˆ’rank(A+ 2I3) = 3âˆ’1 = 2 . (6.8)
Then, employing the equation x1âˆ’x2+x3= 0 obtained at right in (6.7), we could have easily
obtained the two solutions ï£®
ï£°1
1
0ï£¹
ï£»andï£®
ï£°0
1
1ï£¹
ï£».
Since these two vectors in EA(âˆ’2) are linearly independent and we know from (6.8) thatEA(âˆ’2)
has dimension 2, we can conclude by Theorem 3.54(1) that the two vectors must be a basis for
EA(âˆ’2). We mention this here in order to suggest an alternative means of finding a basis for
an eigenspace which makes use of earlier theoretical developments.205
In the next example, for varietyâ€™s sake, eigenspaces will be found using Definition 6.5 directly,
rather than equation (6.1).
Example 6.20. Find the eigenvalues of the matrix
A=ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»,
and also find a basis for each eigenspace as a subspace of R3.
Solution. Expanding the determinant according to the second row, we have
PA(t) = det( Aâˆ’tI3) =âˆ’1âˆ’t4âˆ’2
âˆ’3 4 âˆ’t0
âˆ’3 1 3 âˆ’t
= (âˆ’1)2+1(âˆ’3)4âˆ’2
1 3âˆ’t+ (âˆ’1)2+2(4âˆ’t)âˆ’1âˆ’tâˆ’2
âˆ’3 3 âˆ’t
=âˆ’t3+ 6t2âˆ’11t+ 6,
and so
PA(t) = 0 â‡”t3âˆ’6t2+ 11tâˆ’6 = 0 .
By the Rational Zeros Theorem of algebra, the only rational numbers that may be zeros of PA
areÂ±1,Â±2,Â±3 and Â±6. Itâ€™s an easy matter to verify that 1 is in fact a zero, and so by the
Factor Theorem of algebra tâˆ’1 must be a factor of PA(t). Now,
t3âˆ’6t2+ 11tâˆ’6
tâˆ’1=t2âˆ’5t+ 6,
whence we obtain
t3âˆ’6t2+ 11tâˆ’6 = 0 â‡”(tâˆ’1)(t2âˆ’5t+ 6) = 0 â‡”(tâˆ’1)(tâˆ’2)(tâˆ’3) = 0 ,
and therefore PA(t) = 0 if and only if t= 1,2,3. By Theorem 6.18 the eigenvalues of Aare
Î»= 1,2,3.
The eigenspace of Acorresponding to Î»= 1 is
EA(1) ={xâˆˆR3:Ax=x}=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3:ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»ï£®
ï£°x
y
zï£¹
ï£»=ï£®
ï£°x
y
zï£¹
ï£»ï£¼
ï£½
ï£¾.
The matrix equation yields the system of equations
ï£±
ï£²
ï£³âˆ’x+ 4yâˆ’2z=x
âˆ’3x+ 4y =y
âˆ’3x+y+ 3z=z206
or equivalently
ï£±
ï£²
ï£³âˆ’x+ 2yâˆ’1z= 0
âˆ’x+y = 0
âˆ’3x+y+ 2z= 0
Apply Gaussian elimination on the corresponding augmented matrix:
ï£®
ï£°âˆ’1 2âˆ’10
âˆ’1 1 0 0
âˆ’3 1 2 0ï£¹
ï£»âˆ’r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’3r1+r3â†’r3ï£®
ï£°âˆ’1 2 âˆ’10
0âˆ’1 1 0
0âˆ’5 5 0ï£¹
ï£»âˆ’5r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°âˆ’1 2 âˆ’10
0âˆ’1 1 0
0 0 0 0ï£¹
ï£»,
so from the second row we have y=z, and from the first row we have x= 2yâˆ’z= 2zâˆ’z=z.
Replacing zwith t, so that x=y=z=t, we have
EA(1) =ï£±
ï£²
ï£³ï£®
ï£°t
t
tï£¹
ï£»:tâˆˆRï£¼
ï£½
ï£¾=ï£±
ï£²
ï£³ï£®
ï£°1
1
1ï£¹
ï£»t:tâˆˆRï£¼
ï£½
ï£¾.
From this we see that the set
B1=ï£±
ï£²
ï£³ï£®
ï£°1
1
1ï£¹
ï£»ï£¼
ï£½
ï£¾
is a basis for EA(1).
The eigenspace of Acorresponding to Î»= 2 is
EA(2) ={xâˆˆR3:Ax= 2x}=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3:ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»ï£®
ï£°x
y
zï£¹
ï£»=ï£®
ï£°2x
2y
2zï£¹
ï£»ï£¼
ï£½
ï£¾.
The matrix equation yields the system of equations
ï£±
ï£²
ï£³âˆ’3x+ 4yâˆ’2z= 0
âˆ’3x+ 2y = 0
âˆ’3x+y+z= 0
Apply Gaussian elimination on the corresponding augmented matrix:
ï£®
ï£°âˆ’3 4âˆ’20
âˆ’3 2 0 0
âˆ’3 1 1 0ï£¹
ï£»âˆ’r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’r1+r3â†’r3ï£®
ï£°âˆ’3 4 âˆ’20
0âˆ’2 2 0
0âˆ’3 3 0ï£¹
ï£»âˆ’3
2r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’ï£®
ï£°âˆ’3 4 âˆ’20
0âˆ’2 2 0
0 0 0 0ï£¹
ï£»,
so from the second row we have y=z, and from the first row we have
x=4
3yâˆ’2
3z=4
3zâˆ’2
3z=2
3z.
Hence
EA(2) =ï£±
ï£²
ï£³ï£®
ï£°2z/3
z
zï£¹
ï£»:zâˆˆRï£¼
ï£½
ï£¾=ï£±
ï£²
ï£³ï£®
ï£°2/3
1
1ï£¹
ï£»z:zâˆˆRï£¼
ï£½
ï£¾.207
If we replace zwith 3 t, we obtain an equivalent rendition of E2that features no fractions:
EA(2) =ï£±
ï£²
ï£³ï£®
ï£°2
3
3ï£¹
ï£»t:tâˆˆRï£¼
ï£½
ï£¾.
The set
B2=ï£±
ï£²
ï£³ï£®
ï£°2
3
3ï£¹
ï£»ï£¼
ï£½
ï£¾
is a basis for E2.
Finally, the eigenspace of Acorresponding to Î»= 3 is
EA(3) ={xâˆˆR3:Ax= 3x}=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»âˆˆR3:ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»ï£®
ï£°x
y
zï£¹
ï£»=ï£®
ï£°3x
3y
3zï£¹
ï£»ï£¼
ï£½
ï£¾.
The matrix equation yields the system of equations
ï£±
ï£²
ï£³âˆ’4x+ 4yâˆ’2z= 0
âˆ’3x+y = 0
âˆ’3x+y = 0
Once more we apply Gaussian elimination to the augmented matrix:
ï£®
ï£°âˆ’4 4âˆ’20
âˆ’3 1 0 0
âˆ’3 1 0 0ï£¹
ï£»âˆ’r2+r3â†’r3âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
1
2r1â†’r1ï£®
ï£°âˆ’2 2âˆ’10
âˆ’3 1 0 0
0 0 0 0ï£¹
ï£»,
soy= 3xandâˆ’2x+ 2yâˆ’z= 0, where
âˆ’2x+ 2yâˆ’z= 0â‡’z=âˆ’2x+ 2yâ‡’z= 4x.
Therefore, replacing xwith tso that y= 3tandz= 4t, we have
EA(3) =ï£±
ï£²
ï£³ï£®
ï£°1
3
4ï£¹
ï£»t:tâˆˆRï£¼
ï£½
ï£¾.
The set
B3=ï£±
ï£²
ï£³ï£®
ï£°1
3
4ï£¹
ï£»ï£¼
ï£½
ï£¾
is a basis for EA(3). â– 
Example 6.21. Find the eigenvalues of the matrix
A=
2 3
âˆ’1 4
,
and also find a basis for each eigenspace as a subspace of C2.208
Solution. We have
PA(t) = det( Aâˆ’tI2) =2âˆ’t3
âˆ’1 4âˆ’t=t2âˆ’6t+ 11,
and so
PA(t) = 0 â‡”t2âˆ’6t+ 11 = 0 â‡”t= 3Â±iâˆš
2.
That is, Ahas two complex-valued eigenvalues. Let Î»= 3âˆ’iâˆš
2. The eigenspace corresponding
toÎ»is
EA(Î») ={zâˆˆC2:Az=Î»z}=
z1
z2
âˆˆC2:
2 3
âˆ’1 4
z1
z2
=
Î»z1
Î»z2
.
Now,
2 3
âˆ’1 4
z1
z2
=
Î»z1
Î»z2
corresponds to the system of equations
(2âˆ’Î»)z1+ 3 z2= 0
âˆ’z1+ (4âˆ’Î»)z2= 0
We apply Gaussian elimination to the augmented matrix,
2âˆ’Î» 3 0
âˆ’1 4âˆ’Î»0
r1â†”r2âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’1 4âˆ’Î»0
2âˆ’Î» 3 0
(2âˆ’Î»)r1+r2â†’r2âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’
âˆ’1 4âˆ’Î»0
0 0 0
,
observing that
(2âˆ’Î»)(4âˆ’Î») + 3 = ( Î»2âˆ’6Î»+ 8) + 3 = 
3âˆ’iâˆš
22âˆ’6 
3âˆ’iâˆš
2
+ 11 = 0 .
Thus
z1= 
âˆ’1âˆ’iâˆš
2
z2,
and so we obtain
EA(Î») = 
âˆ’1âˆ’iâˆš
2
z2
z2
:z2âˆˆC
=
âˆ’1âˆ’iâˆš
2
1
z:zâˆˆC
,
where for simplicity we replace z2with zin the end. Hence the eigenvector
âˆ’1âˆ’iâˆš
2
1
corresponding to the eigenvalue 3 âˆ’iâˆš
2 constitutes a basis for the eigenspace EA(Î»).
The analysis of the other eigenvalue 3 + iâˆš
2is quite similar (with eigenspace also of
dimension 1) and so is left as a problem. â– 
Example 6.22. We will show that, for all nâˆˆN, ifAâˆˆFis given by
A=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0 0 Â·Â·Â· 0âˆ’a0
1 0 0 Â·Â·Â· 0âˆ’a1
0 1 0 Â·Â·Â· 0âˆ’a2..................
0 0 0 Â·Â·Â· 0âˆ’anâˆ’2
0 0 0 Â·Â·Â· 1âˆ’anâˆ’1ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£», (6.9)209
then
PA(t) = (âˆ’1)n(a0+a1t+Â·Â·Â·+anâˆ’1tnâˆ’1+tn). (6.10)
In the case when n= 1 we take A= [âˆ’a0], whereupon we obtain
PA(t) = det 1(Aâˆ’tI1) = det 1([âˆ’a0âˆ’t]) =âˆ’a0âˆ’t= (âˆ’1)(a0+t).
This establishes the base case of an inductive argument. Fix nâˆˆN, and suppose any matrix of
the form (6.9) has characteristic polynomial (6.10); that is, det n(Aâˆ’tIn) is given by
âˆ’t0 0 Â·Â·Â· 0âˆ’a0
1âˆ’t0Â·Â·Â· 0âˆ’a1
0 1 âˆ’tÂ·Â·Â· 0âˆ’a2..................
0 0 0 Â·Â·Â· âˆ’ tâˆ’anâˆ’2
0 0 0 Â·Â·Â· 1âˆ’anâˆ’1âˆ’t= (âˆ’1)n(a0+a1t+Â·Â·Â·+anâˆ’1tnâˆ’1+tn).
Now, define AâˆˆF(n+1)Ã—(n+1)by
A=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0 0 Â·Â·Â· 0âˆ’a0
1 0 0 Â·Â·Â· 0âˆ’a1
0 1 0 Â·Â·Â· 0âˆ’a2..................
0 0 0 Â·Â·Â· 0âˆ’anâˆ’1
0 0 0 Â·Â·Â· 1âˆ’anï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»,
so
Aâˆ’tIn+1=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’t0 0 Â·Â·Â· 0âˆ’a0
1âˆ’t0Â·Â·Â· 0âˆ’a1
0 1 âˆ’tÂ·Â·Â· 0âˆ’a2..................
0 0 0 Â·Â·Â· âˆ’ tâˆ’anâˆ’1
0 0 0 Â·Â·Â· 1âˆ’anâˆ’tï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£».
Letting B=Aâˆ’tIn+1,
PA(t) = det n+1(B) =n+1X
j=1(âˆ’1)1+ja1jdetn(B1j)
=âˆ’tdetn(B11) + (âˆ’1)n+2(âˆ’a0) det n(B1(n+1)),
where
B11=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’t0 0 Â·Â·Â· 0âˆ’a1
1âˆ’t0Â·Â·Â· 0âˆ’a2
0 1 âˆ’tÂ·Â·Â· 0âˆ’a3..................
0 0 0 Â·Â·Â· âˆ’ tâˆ’anâˆ’1
0 0 0 Â·Â·Â· 1âˆ’anâˆ’tï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»and B1(n+1)=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1âˆ’t0Â·Â·Â· 0 0
0 1 âˆ’tÂ·Â·Â· 0 0
0 0 1 Â·Â·Â· 0 0
..................
0 0 0 Â·Â·Â· 1âˆ’t
0 0 0 Â·Â·Â· 0 1ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£».210
Clearly det n(B1(n+1)) = 1, and by the inductive hypothesis we have
detn(B11) = (âˆ’1)n(a1+a2t+Â·Â·Â·+antnâˆ’1+tn),
so that
PA(t) =âˆ’t(âˆ’1)n(a1+a2t+Â·Â·Â·+antnâˆ’1+tn)âˆ’(âˆ’1)na0
= (âˆ’1)n+1(a1t+a2t2+Â·Â·Â·+antn+tn+1) + (âˆ’1)n+1a0
= (âˆ’1)n+1(a0+a1t+Â·Â·Â·+antn+tn+1),
as desired. â– 
Problems
1. For each of the 2 Ã—2 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)
3 0
8âˆ’1
(b)
0 3
4 0
(c)
âˆ’2âˆ’7
1 2
(d)
0 0
0 0
2. For each of the 3 Ã—3 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)ï£®
ï£°4 0 1
âˆ’2 1 0
âˆ’2 0 1ï£¹
ï£» (b)ï£®
ï£°3 0 âˆ’5
1
5âˆ’1 0
1 1 âˆ’2ï£¹
ï£» (c)ï£®
ï£°5 6 2
0âˆ’1âˆ’8
1 0 âˆ’2ï£¹
ï£»
3. For each of the 4 Ã—4 matrices below, do the following:
(i) Find the characteristic equation.
(ii) Find all real eigenvalues.
(iii) Find a basis for the eigenspace corresponding to each real eigenvalue.
(a)ï£®
ï£¯ï£¯ï£°0 0 2 0
1 0 1 0
0 1âˆ’2 0
0 0 0 1ï£¹
ï£ºï£ºï£»(b)ï£®
ï£¯ï£¯ï£°10âˆ’9 0 0
4âˆ’2 0 0
0 0 âˆ’2âˆ’7
0 0 1 2ï£¹
ï£ºï£ºï£»211
6.3 â€“ Applications of the Characteristic Polynomial
Recall that Pn(F) denotes the set of polynomials of degree nwith coefficients in F. That is,
fornâˆˆW,
Pn(F) =(nX
k=0akxk:a0, . . . , a nâˆˆFandanÌ¸= 0)
.
We regard 0 to be the polynomial of degree âˆ’1 and define Pâˆ’1(F) ={0}. IfFis an infinite field
such as RorC, it is common to treat a polynomial as a function f:Fâ†’Fgiven by
f(x) =nX
k=0akxk
for all xâˆˆF, in which case it is called a polynomial function. For nâ‰¥ âˆ’1, a polynomial function
pis said to have degree niff(x)âˆˆ P n(F), in which case we write deg(p) =n. Thus, we may
just as well regard Pn(F) as the set of all polynomial functions of degree n, so that it makes as
much sense to write fâˆˆ P n(F) asf(x)âˆˆ P n(F). Finally, we define
P(F) =âˆ[
n=0Pnâˆ’1(F).
In what follows we will have need of the following theorem, which is proven in Â§5.1 of the
Complex Analysis Notes.
Theorem 6.23 (Fundamental Theorem of Algebra ).If
p(z) =anzn+anâˆ’1znâˆ’1+Â·Â·Â·+a1z+a0
is a polynomial function of degree nâ‰¥1with coefficients a0, . . . , a nâˆˆC, then there exists some
z0âˆˆCsuch that p(z0) = 0 .
Theorem 6.24 (Division Algorithm for Polynomials ).Letfâˆˆ P n(F), and let gâˆˆ P m(F)
for some mâ‰¥0. Then there exist unique polynomial functions qandrsuch that
f(x) =q(x)g(x) +r(x)
for all xâˆˆF, where deg(r)â‰¤m.
Theorem 6.25 (Factor Theorem ).Letfâˆˆ P n(F)for some nâ‰¥1, and let câˆˆF. Then
f(c) = 0 if and only if xâˆ’cis a factor of f(x).
Lemma 6.26. Suppose P(t) = [pij(t)]nâˆˆFnÃ—nis such that pijis a polynomial function for all
1â‰¤i, jâ‰¤n. If
deg(pij)â‰¤(
0, iÌ¸=j
1, i=j
then deg(det n(P(t)))â‰¤n.212
Proof. The statement of the lemma is clearly true in the case when n= 1. Suppose that it is
true for some arbitrary nâˆˆN. Let P(t) = [pij(t)]n+1. Then, expanding along the first row, we
have
detn+1(P(t)) =n+1X
j=1(âˆ’1)1+jp1j(t) det n(P1j(t)).
For each 1 â‰¤jâ‰¤n+ 1 we find that the nÃ—nsubmatrix P1jis such that all non-diagonal
entries are degree 0 polynomial functions (i.e. constants), and all diagonal entries are polynomial
functions of either degree 0 or degree 1. Thus deg(detn(P1j))â‰¤nby our inductive hypothesis,
and since p1j(t) is a constant for 2 â‰¤jâ‰¤n+ 1, it follows that
deg 
(âˆ’1)1+jp1j(t) det n(P1j(t))
â‰¤n
for 2â‰¤jâ‰¤n+ 1. In the case when j= 1 we have
(âˆ’1)1+jp1j(t) det n(P1j(t)) =p11(t) det n(P11(t)),
where p11(t) has degree at most 1, and det n(P11(t)) has degree at most n. Hence
deg 
p11(t) det n(P11(t))
â‰¤n+ 1,
and therefore deg(detn(P(t)))â‰¤n+ 1 since detn+1(P(t)) is the sum of polynomials of degree at
most n+ 1. Thus the lemma holds true in the n+ 1 case, and so it must hold for all nâˆˆNby
induction. â– 
Proposition 6.27. IfAâˆˆFnÃ—n, then deg(PA) =nand the lead coefficient of PAis(âˆ’1)n.
Proof. In the case when n= 1 we have A= [a], so that
PA(t) = det 1([a]âˆ’[t]) = det 1([aâˆ’t]) =aâˆ’t=âˆ’t+a,
and we clearly that deg( PA) = 1 and the lead coefficient of PAis (âˆ’1)1.
Suppose the proposition is true for some nâˆˆN. Let A= [aij]n+1âˆˆF(n+1)Ã—(n+1), and define
P(t) =Aâˆ’tIn+1so that P(t) = [pij(t)]n+1with
deg(pij) =(
0, iÌ¸=j
1, i=j
Now,
PA(t) = det n+1(P(t)) =n+1X
k=1(âˆ’1)1+kp1k(t) det n(P1k(t)),
where for each kwe have P1k(t) = [pk,ij(t)]nsuch that
deg(pk,ij)â‰¤(
0, iÌ¸=j
1, i=j
and so deg(detn(P1k(t)))â‰¤nby Lemma 6.26. Since p1k(t) is a constant for 2 â‰¤kâ‰¤n+ 1, it
follows that
deg 
(âˆ’1)1+kp1k(t) det n(P1k(t))
â‰¤n213
for 2â‰¤kâ‰¤n+ 1. In the case when k= 1 we have
(âˆ’1)1+kp1k(t) det n(P1k(t)) =p11(t) det n(P11(t)),
where
detn(P11(t)) = det n(A11âˆ’tIn) =PA11(t)
has degree nand lead coefficient ( âˆ’1)nby our inductive hypothesis. That is,
detn(P11(t)) = (âˆ’1)ntn+bnâˆ’1tnâˆ’1+Â·Â·Â·+b1t+b0
for some bnâˆ’1, . . . , b 0âˆˆF, and since p11(t) =a11âˆ’twe obtain
p11(t) det n(P11(t)) = (âˆ’t+a11) 
(âˆ’1)ntn+bnâˆ’1tnâˆ’1+Â·Â·Â·+b1t+b0
= (âˆ’1)n+1tn+1+cntn+Â·Â·Â·+c1t+c0
for some cn, . . . , c 0âˆˆF. Hence Q(t) =p11(t)detn(P11(t)) has degree n+ 1 with lead coefficient
(âˆ’1)n+1. Since PA(t) =detn+1(P(t)) is the sum of Q(t) with other polynomials of degree at
most n, it follows that PAlikewise has degree n+ 1 with lead coefficient ( âˆ’1)n+1.
We conclude by the principle of induction that the proposition holds for all nâˆˆN, which
finishes the proof. â– 
Corollary 6.28. IfVis a nontrivial finite-dimensional vector space over FandLâˆˆ L(V), then
deg(PL) = dim( V)and the lead coefficient of PLis(âˆ’1)dim(V).
Proof. Suppose Vis a nontrivial finite-dimensional vector space over FandLâˆˆ L(V). Let Bbe
any basis for V. Since [ L]BâˆˆFdim(V)Ã—dim(V), by Proposition 6.27 we have deg(P[L]B) =dim(V)
and the lead coefficient of P[L]Bis (âˆ’1)n. Now, PL=P[L]Bby Definition 6.16, and so the proof
is done. â– 
Proposition 6.29. LetnâˆˆN.
1.IfAâˆˆCnÃ—n, then 1â‰¤ |Ïƒ(A)| â‰¤n.
2.LetVbe an n-dimensional vector space over C. IfLâˆˆ L(V), then 1â‰¤ |Ïƒ(L)| â‰¤n.
Proof.
Proof of Part (1): LetAâˆˆCnÃ—n. By Proposition 6.27 the polynomial function PAis of degree
nâˆˆN, so by the Fundamental Theorem of Algebra PAhas at least one zero in C, and by the
Factor Theorem PAhas at most nzeros in C. Since, by Theorem 6.18, Î»is an eigenvalue of
Aif and only if PA(Î») = 0, it follows that Apossesses at least one and at most ndistinct
eigenvalues. That is, 1 â‰¤ |Ïƒ(A)| â‰¤n.
Proof of Part (2): Suppose Lâˆˆ L(V), and let Bbe an ordered basis for V. Then [ L]BâˆˆCnÃ—n,
and by Part (1) we have 1 â‰¤ |Ïƒ([L]B)| â‰¤n. Now, because Î»is an eigenvalue of Lif and only if
it is an eigenvalue of [ L]Bby Proposition 6.14, we conclude that 1 â‰¤ |Ïƒ(L)| â‰¤n. â– 214
Definition 6.30. IfAâˆˆFnÃ—n, then the algebraic multiplicity Î±A(Î»)of an eigenvalue
Î»âˆˆÏƒ(A)is given by
Î±A(Î») = max {j: (tâˆ’Î»)jis a factor of PA(t)}. (6.11)
Thegeometric multiplicity ofÎ»isÎ³A(Î») = dim( EA(Î»)).
IfLâˆˆ L(V), then the algebraic multiplicity Î±L(Î»)of an eigenvalue Î»âˆˆÏƒ(L)is given by
Î±L(Î») =Î±[L](Î»),
where [L]denotes the matrix corresponding to Lwith respect to any basis for V. The geometric
multiplicity ofÎ»isÎ³L(Î») = dim( EL(Î»)).
Proposition 6.15 ensures that the algebraic multiplicity of any eigenvalue Î»of an operator
Lâˆˆ L(V) is independent of the choice of basis for V. That is, Î±L(Î») is invariant under change
of bases.
It must be stressed that if a matrix Ais regarded as being an element of FnÃ—n, then in
general we consider only eigenvalues that are elements of F. Thus, if AâˆˆRnÃ—n, then Ïƒ(A)âŠ†R,
and we discount any value in C\Ras being an eigenvalue. A similar convention is observed in
the case when Lâˆˆ L(V), where Vis given to be a vector space over the field F; that is, we take
Ïƒ(L)âŠ†F.
An easy consequence of the Factor Theorem is that the multiplicities of the distinct complex
zeros of an nth-degree polynomial function must sum to n. Thus, since the characteristic
polynomial of a matrix AâˆˆCnÃ—nhas degree nby Proposition 6.27, it readily follows from
Theorem 6.18 that the sum of the algebraic multiplicities of the distinct complex eigenvalues
Î»1, . . . , Î» mofAmust be n:
mX
k=1Î±A(Î»k) =n. (6.12)
It is in this sense (i.e. counting multiplicities) that it can be said that an nÃ—nmatrix Awith
complex-valued entries has â€œ neigenvalues,â€ which we may sometimes denote by Î»1, . . . , Î» n. The
same applies to any linear operator Lon an n-dimensional vector space over C.
Theorem 6.31. IfAâˆˆCnÃ—nhas distinct complex eigenvalues Î»1, . . . , Î» m, then
detn(A) =mY
k=1Î»Î±A(Î»k)
k
Proof. Suppose AâˆˆCnÃ—nhas distinct complex eigenvalues Î»1, . . . , Î» m. Then Î»1, . . . , Î» mare
precisely the zeros of PAby Theorem 6.18, and so
PA(t) = (âˆ’1)n(tâˆ’Î»1)Î±A(Î»1)Â·Â·Â·(tâˆ’Î»m)Î±A(Î»m)
by the Factor Theorem and (6.11) , along with Proposition 6.27 which tells us that the lead
coefficient of PAis (âˆ’1)n. Now, since
detn(A) = det n(Aâˆ’0In) =PA(0),
from (6.12) we obtain
detn(A) = (âˆ’1)n(âˆ’Î»1)Î±A(Î»1)Â·Â·Â·(âˆ’Î»m)Î±A(Î»m)215
= (âˆ’1)n(âˆ’1)Î±A(Î»1)+Â·Â·Â·+Î±A(Î»m)Î»Î±A(Î»1)
1Â·Â·Â·Î»Î±A(Î»m)
m
= (âˆ’1)n(âˆ’1)nÎ»Î±A(Î»1)
1Â·Â·Â·Î»Î±A(Î»m)
m
=Î»Î±A(Î»1)
1Â·Â·Â·Î»Î±A(Î»m)
m
as desired. â– 
Proposition 6.32. LetAâˆˆFnÃ—n. IfÎ»is an eigenvalue of A, then it is also an eigenvalue of
AâŠ¤
Proof. Suppose that Î»âˆˆFis an eigenvalue of A. Then PA(Î») = 0 by Theorem 6.18, and thus
detn(Aâˆ’Î»In) = 0 .
Now, by Theorem 5.7
detn 
(Aâˆ’Î»In)âŠ¤
= det n(Aâˆ’Î»In),
and since
(Aâˆ’Î»In)âŠ¤=AâŠ¤âˆ’Î»IâŠ¤
n=AâŠ¤âˆ’Î»In,
it follows that
detn(AâŠ¤âˆ’Î»In) = 0 .
That is, PAâŠ¤(Î») = 0, and so by Theorem 6.18 we conclude that Î»is an eigenvalue of AâŠ¤.â– 216
6.4 â€“ Similar Matrices
Definition 6.33. LetA,BâˆˆFnÃ—n. We say Aissimilar toB, written Asâˆ¼B, if there exists
an invertible matrix QâˆˆFnÃ—nsuch that B=QAQâˆ’1.
Theorem 6.34. The similarity relationsâˆ¼is an equivalence relation on the class of square
matrices over F.
Proof. For any AâˆˆFnÃ—nwe have A=InAIâˆ’1
n, so that Asâˆ¼Aand hencesâˆ¼is reflexive.
Suppose that Asâˆ¼B. Then B=QAQâˆ’1for some invertible matrix Q, and since
B=QAQâˆ’1â‡’A=Qâˆ’1BQâ‡’A=Qâˆ’1B(Qâˆ’1)âˆ’1,
it follows that Bsâˆ¼Aand thereforesâˆ¼is symmetric.
Suppose Asâˆ¼BandBsâˆ¼C, so that
B=QAQâˆ’1and C=PBPâˆ’1
for some invertible matrices QandP. Then by the associativity of matrix multiplication and
Theorem 2.26 we obtain
C=P(QAQâˆ’1)Pâˆ’1= (PQ)A(Qâˆ’1Pâˆ’1) = (PQ)A(PQ)âˆ’1,
which shows that Asâˆ¼Cand thereforesâˆ¼is transitive. â– 
Remark. Because the relationsâˆ¼is symmetric, when two matrices AandBare said to be
similar it does not matter whether we take that to mean Asâˆ¼B(i.e.B=QAQâˆ’1) orBsâˆ¼A
(i.e.A=QBQâˆ’1).
Proposition 6.35. Suppose that AandBare similar matrices.
1.Ais invertible if and only if Bis invertible.
2. det( A) = det( B).
3.PA=PB.
4.Ïƒ(A) =Ïƒ(B).
5. rank( A) = rank( B).
Proof.
Proof of Part (1): IfAis invertible, then there exists an invertible matrix Qsuch that
B=QAQâˆ’1, and therefore Bis invertible by Theorem 2.26. The converse follows from the
symmetric property ofsâˆ¼.
Proof of Part (2): There exists an invertible matrix Qsuch that B=QAQâˆ’1. Now,
det(B) = det( QAQâˆ’1) = det( Q) det(A) det(Qâˆ’1) = det( Q) det(A)1
det(Q)= det( A)
by Theorems 5.23 and 5.24.217
Proof of Part (3): There exists an invertible matrix Qsuch that B=QAQâˆ’1, and so
PB(t) = det( Bâˆ’tI) = det( QAQâˆ’1âˆ’tQIQâˆ’1) = det 
Q(Aâˆ’tI)Qâˆ’1
= det( Q) det(Aâˆ’tI) det(Qâˆ’1) = det( Q) det(Aâˆ’tI) det(Q)âˆ’1
= det( Aâˆ’tI) =PA(t)
for any tâˆˆF. Therefore PA=PB.
Proof of Part (4): Applying Theorem 6.18 and Part (3), we have
Î»âˆˆÏƒ(A)â‡”PA(Î») = 0 â‡”PB(Î») = 0 â‡”Î»âˆˆÏƒ(B),
and therefore Ïƒ(A) =Ïƒ(B).
Proof of Part (5): This is an immediate consequence of Theorem 4.47(4). â– 
The following proposition is a direct consequence of Corollary 4.33 and will prove useful
later on.
Proposition 6.36. Suppose Vis a finite-dimensional vector space and Lâˆˆ L(V). IfBandBâ€²
are ordered bases for V, then [L]Band[L]Bâ€²are similar matrices.
Proposition 6.37. Suppose that Vis a finite-dimensional vector space, Lâˆˆ L(V), and
AâˆˆFnÃ—n. If there is an ordered basis BforVsuch that [L]Bsâˆ¼A, then there exists a basis Bâ€²
such that [L]Bâ€²=A.
Proof. Suppose B= (v1, . . . ,vn) is an ordered basis for Vsuch that [ L]Bsâˆ¼A. Thus there
exists an invertible matrix
Q= [qij]n=q1Â·Â·Â·qn
such that [ L]B=QAQâˆ’1. LetBâ€²={vâ€²
1, . . . ,vâ€²
n}be the set of vectors for which
vâ€²
k=q1kv1+Â·Â·Â·+qnkvn
for each 1 â‰¤kâ‰¤n, so that
[vâ€²
k]B=ï£®
ï£°q1k...
qnkï£¹
ï£»=qk.
Since Qis invertible, by the Invertible Matrix Theorem the column vectors q1, . . . ,qnofQ
are linearly independent, which is to say [ vâ€²
1]B, . . . , [vâ€²
n]Bare linearly independent vectors in Fn.
Thus, since the mapping Ï†âˆ’1
B:Fnâ†’V(the inverse of the B-coordinate map) is an isomorphism
and
Ï†âˆ’1
B 
[vâ€²
k]B
=vâ€²
k
for 1â‰¤kâ‰¤n, it follows by Proposition 4.16 that vâ€²
1, . . . ,vâ€²
nare linearly independent and
therefore Bâ€²is a basis for V. We give it the natural order: Bâ€²= (vâ€²
1, . . . ,vâ€²
n).218
Now, by Theorem 4.27,
IBâ€²B=h
[vâ€²
1]BÂ·Â·Â·[vâ€²
n]Bi
=q1Â·Â·Â·qn
=Q,
and so
[L]B=QAQâˆ’1=IBâ€²BAIâˆ’1
Bâ€²B=Iâˆ’1
BBâ€²AIBBâ€²
by Proposition 4.31. Finally, by Corollary 4.33 we obtain
A=IBBâ€²[L]BIâˆ’1
BBâ€²= [L]Bâ€²
as desired. â– 
We will often have need to raise matrix expressions of the form QAQâˆ’1andQâˆ’1AQto an
arbitrary positive integer power, for which the following proposition will prove invaluable.
Proposition 6.38. IfA,QâˆˆFnÃ—nandQis invertible, then
(Qâˆ’1AQ)k=Qâˆ’1AkQ (6.13)
and
(QAQâˆ’1)k=QAkQâˆ’1(6.14)
for all kâˆˆN.
Proof. First we prove that (6.13) holds for all kâ‰¥1. Certainly the equation holds when
k= 1. Suppose it holds for some arbitrary kâ‰¥1. Then, exploiting the associativity of matrix
multiplication, we obtain
(Qâˆ’1AQ)k+1= (Qâˆ’1AQ)k(Qâˆ’1AQ) = (Qâˆ’1AkQ)(Qâˆ’1AQ)
=Qâˆ’1Ak(QQâˆ’1)AQ=Qâˆ’1Ak(Ik)AQ=Qâˆ’1AkAQ
=Qâˆ’1(AkA)Q=Qâˆ’1Ak+1Q,
which shows the equation holds for k+ 1. By the Principle of Induction we conclude that (6.13)
holds for all kâˆˆN.
Equation (6.14) is a symmetrical result that is easily derived from (6.13) merely by replacing
QwithQâˆ’1. â– 219
6.5 â€“ The Theory of Diagonalization
Definition 6.39. Suppose Vis a nontrivial finite-dimensional vector space over F, and let
Lâˆˆ L(V). An ordered basis for Vconsisting of the eigenvectors of Lis called a spectral basis
forL. We say Lisdiagonalizable if there exists a spectral basis for L. Any procedure that
finds a spectral basis for Lis called diagonalization .
A matrix AâˆˆFnÃ—nisdiagonalizable inFif it is similar to a diagonal matrix
DâˆˆFnÃ—n.
Theorem 6.40. Suppose Vis a finite-dimensional vector space over F,Lâˆˆ L(V), and
Î»1, . . . , Î» mare the distinct eigenvalues of L. Then the following statements are equivalent.
1.Lis diagonalizable.
2.There exists some ordered basis BforVsuch that [L]Bis a diagonal matrix.
3.There exists some ordered basis BforVsuch that [L]Bis diagonalizable in F.
4.Vdecomposes as
V=EL(Î»1)âŠ• Â·Â·Â· âŠ• EL(Î»m).
5.The dimension of Vis
dim(V) = dim( EL(Î»1)) +Â·Â·Â·+ dim( EL(Î»m)).
Proof.
(1)â‡’(2):Suppose Lis diagonalizable. Then there exists some ordered basis B= (v1, . . . ,vn)
consisting of eigenvectors of L, so that L(vk) =Î»kvkfor each 1 â‰¤kâ‰¤n. By Corollary 4.21 the
matrix corresponding to Lwith respect to Bis
[L]B=h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
=h
Î»1v1
BÂ·Â·Â·
Î»nvn
Bi
=h
Î»1
v1
BÂ·Â·Â·Î»n
vn
Bi
=ï£®
ï£¯ï£¯ï£¯ï£°Î»1ï£®
ï£¯ï£¯ï£°1
0
...
0ï£¹
ï£ºï£ºï£»Â·Â·Â· Î»nï£®
ï£¯ï£¯ï£°0
0
...
1ï£¹
ï£ºï£ºï£»ï£¹
ï£ºï£ºï£ºï£»=ï£®
ï£°Î»1 0
...
0 Î»nï£¹
ï£»,
and so we see that [ L]Bis a diagonal matrix as desired.
(2)â‡’(1):Suppose there exists some ordered basis B= (v1, . . . ,vn) such that [ L]BâˆˆFnÃ—nis a
diagonal matrix:
[L]B=ï£®
ï£°d1 0
...
0 dnï£¹
ï£».
Since [ vk]B= [Î´ik]nÃ—1for each 1 â‰¤kâ‰¤n, we have
[L]B[vk]B=dk[vk]B,
and so dkis an eigenvalue of [ L]Bwith corresponding eigenvector [ vk]B. By Proposition 6.14 we
conclude that, for each 1 â‰¤kâ‰¤n,dkis an eigenvalue of Lwith corresponding eigenvector vk,
and therefore Bis an ordered basis for Vconsisting of eigenvectors of L.220
(2)â‡’(3):This is trivial since equal matrices are similar matrices.
(3)â‡’(2):If there is an ordered basis Bsuch that [ L]Bis similar to a diagonal matrix D, then
by Proposition 6.37 there is an ordered basis Bâ€²such that [ L]Bâ€²=D.
(1)â‡’(4):Suppose Lis diagonalizable, so there is an ordered basis B= (v1, . . . ,vn) consisting
of eigenvectors of L. We may take the order to be such that v1, . . . ,vmhave the distinct
eigenvalues Î»1, . . . , Î» m. Let Î»m+1, . . . , Î» nbe the eigenvalues corresponding to vm+1, . . . ,vn. For
anyuâˆˆVthere exist c1, . . . , c nâˆˆFsuch that u=c1v1+Â·Â·Â·+cnvn, and so
L(u) =nX
k=1ckL(vk) =nX
k=1ckÎ»kvk. (6.15)
Now,
EL(Î»k) ={vâˆˆV:L(v) =Î»kv}
is the eigenspace of Lcorresponding to Î»k, and since
Î»m+1, . . . , Î» nâˆˆ {Î»1, . . . , Î» m},
it is clear that we may recast (6.15) as
L(u) =mX
k=1câ€²
kÎ»kvâ€²
k
by combining terms with matching eigenvalues. For each 1 â‰¤kâ‰¤mwe have
L(câ€²
kvâ€²
k) =câ€²
kL(vâ€²
k) =câ€²
kÎ»kvâ€²
k=Î»k(câ€²
kvâ€²
k),
so that câ€²
kvâ€²
kâˆˆEL(Î»k), and thus
u=nX
k=1ckvk=mX
k=1câ€²
kvâ€²
kâˆˆmX
k=1EL(Î»k).
This establishes that V=EL(Î»1) +Â·Â·Â·+EL(Î»m).
Next, suppose that
mX
k=1uk=uandmX
k=1uâ€²
k=u
foruk,uâ€²
kâˆˆEL(Î»k). Then
mX
k=1(ukâˆ’uâ€²
k) =0, (6.16)
where ukâˆ’uâ€²
kâˆˆEL(Î»k) for each 1 â‰¤kâ‰¤m. Suppose that ukjâˆ’uâ€²
kjÌ¸=0for some values
1â‰¤k1< k 2<Â·Â·Â·< k â„“â‰¤m,221
withukâˆ’uâ€²
k=0for all k /âˆˆ {k1, . . . , k â„“}. Then (6.16) becomes
â„“X
j=1(ukjâˆ’uâ€²
kj) =0. (6.17)
However, each ukjâˆ’uâ€²
kj(being nonzero) is an eigenvalue of Lwith corresponding eigenvalue
Î»kj, and since the eigenvalues Î»k1, . . . , Î» kâ„“are distinct, it follows by Theorem 6.8 that the set

uk1âˆ’uâ€²
k1, . . . ,ukâ„“âˆ’uâ€²
kâ„“	
is linearly independent. Now (6.17) forces us to conclude that ukjâˆ’uâ€²
kj=0for some 1 â‰¤jâ‰¤â„“,
which is a contradiction. We must conclude that ukâˆ’uâ€²
k=0for all 1 â‰¤kâ‰¤m, or equivalently
u1=uâ€²
1, . . .um=uâ€²
m.
Hence any uâˆˆVhas a unique representation u1+Â·Â·Â·+umsuch that each ukis an element of
EL(Î»k), and therefore V=EL(Î»1)âŠ• Â·Â·Â· âŠ• EL(Î»m).
(4)â‡’(5):That
V=mM
k=1EL(Î»k)â‡’dim(V) =mX
k=1dim(EL(Î»k))
is an immediate consequence of Theorem 4.45.
(5)â‡’(1):Suppose that
dim(V) = dim( EL(Î»1)) +Â·Â·Â·+ dim( EL(Î»m)),
with dim( EL(Î»i)) =nifor each 1 â‰¤iâ‰¤m. Let
Bi={vi1, . . . ,vini}
be a basis for EL(Î»i). Suppose
mX
i=1niX
j=1aijvij=n1X
j=1a1jv1j+Â·Â·Â·+nmX
j=1amjvmj=0, (6.18)
where
vi=niX
j=1aijvijâˆˆEL(Î»i)
and so
v1+Â·Â·Â·+vm=0. (6.19)
For each 1 â‰¤iâ‰¤mthe nonzero elements of EL(Î»i) are eigenvectors of Lwith corresponding
eigenvalue Î»i, and since Î»1, . . . , Î» mare distinct we conclude by Theorem 6.8 that if v1, . . . ,vmÌ¸=0,
thenv1, . . . ,vmare linearly independent. However, (6.19) implies that v1, . . . ,vmare not linearly
independent, and so at least one of the vectors must be the zero vector. In fact, if we suppose
that
vk1, . . . ,vkâ„“âˆˆ {v1, . . . ,vm}222
are the nonzero vectors, then (6.19) becomes
vk1+Â·Â·Â·+vkâ„“=0
and we are compelled to concludeâ€”just as beforeâ€”that at least one term on the left-hand side
must be 0! HenceniX
j=1aijvij=vi=0
for all 1 â‰¤iâ‰¤m, and since vi1, . . . ,viniare linearly independent it follows that
ai1= 0, . . . , a ini= 0
for all 1 â‰¤iâ‰¤m. It is now clear that (6.18) admits only the trivial solution, so that the set
B=m[
i=1Bi
of eigenvectors of Lis linearly independent; and because
|B|=mX
i=1|Bi|=mX
i=1ni=mX
i=1dim(EL(Î»i)) = dim( V)
(Proposition 6.7 ensures that Biâˆ© B j=âˆ…for any iÌ¸=j), Theorem 3.51(1) implies that Bmust
in fact be a basis for Vconsisting of eigenvectors of L. Assigning any order to Bthat we wish,
we conclude that Lis diagonalizable. â– 
From the details of the proof of Theorem 6.40 (specifically that the first statement implies
the second statement) we immediately obtain the following result.
Corollary 6.41. Suppose Vis a finite-dimensional vector space over F. If Lâˆˆ L(V)is
diagonalizable, B= (v1, . . . ,vn)is a spectral basis for L, and Î»kis the eigenvalue corresponding
to eigenvector vk, then [L]BâˆˆFnÃ—nis a diagonal matrix with kk-entry Î»kfor1â‰¤kâ‰¤n. That
is,[L]B= diag
Î»1, . . . , Î» n
.
Definition 6.42. A polynomial function pâˆˆ P n(F)splits over Fif there exist c, a1, . . . , a nâˆˆF
such that
p(t) =cnY
k=1(tâˆ’ak)
for all tâˆˆF.
Proposition 6.43. Suppose Vis a finite-dimensional vector space over F. IfLâˆˆ L(V)is
diagonalizable, then PLsplits over F.
Proof. Suppose Lâˆˆ L(V) is diagonalizable, with dim(V) =n. LetBbe a spectral basis for L,
so that [ L]Bis a diagonal matrix
[L]B=ï£®
ï£°d1 0
...
0 dnï£¹
ï£»223
for some d1, . . . , d nâˆˆFby Theorem 6.40. Now,
PL(t) =P[L]B(t) = det n([L]Bâˆ’tIn) =d1âˆ’t 0
...
0 dnâˆ’t= (âˆ’1)nnY
k=1(tâˆ’dk),
and therefore PLsplits over F. â– 
The first part of the following theorem tells us that the algebraic multiplicity of an eigenvalue
of a diagonalizable linear operator on a finite-dimensional vector space is always equal to its
geometric multiplicity.
The following theorem will, in the next section, show itself to be the workhorse that yields a
practical method for diagonalizing linear operators and square matrices alike.
Theorem 6.44. Suppose Vis a finite-dimensional vector space over F,Lâˆˆ L(V), and
Î»1, . . . , Î» mare the distinct eigenvalues of L. Assuming that PLsplits over F, then:
1.Lis diagonalizable if and only if Î±L(Î»k) =Î³L(Î»k)for all 1â‰¤kâ‰¤m.
2.IfLis diagonalizable and Bkis a basis for EL(Î»k)for each 1â‰¤kâ‰¤m, thenSm
k=1Bkis a
spectral basis for L.
Proof.
Proof of Part (1). Letn= dim( V). Suppose that
max{j: (tâˆ’Î»k)jis a factor of PL(t)}=Î±L(Î»k) =Î³L(Î»k) = dim( EL(Î»k))
for each 1 â‰¤kâ‰¤m. Then
PL(t) =p(t)mY
k=1(tâˆ’Î»k)dim(EL(Î»k))
for some polynomial function pfor which Î»1, . . . , Î» mare not zeros. However, PLsplits over F
by hypothesis, and so deg(p) is either 0 or 1. If deg(p) = 1, so that p(t) =c(tâˆ’Î») for some
Î», câˆˆF, then PL(Î») = 0 and we conclude that Î»Ì¸=Î»1, . . . , Î» mmust be an eigenvalue of L. This
is a contradiction since Î»1, . . . , Î» mrepresent all the distinct eigenvalues of L. Hence deg(p) = 0,
which is to say p(t) =cfor some câˆˆFand we have
PL(t) =cmY
k=1(tâˆ’Î»k)dim(EL(Î»k)). (6.20)
Now, since deg( PL) =nby Corollary 6.28, it follows from (6.20) that
mX
k=1dim(EL(Î»k)) =n= dim( V),
and therefore Lis diagonalizable by Theorem 6.40.
Suppose that Lis diagonalizable, and let B= (v1, . . . ,vn) be a spectral basis for Lsuch
that L(vk) =Î»kvkfor each 1 â‰¤kâ‰¤n. By Corollary 6.41, [ L]BâˆˆFnÃ—nis a diagonal matrix224
with kk-entry Î»k:
[L]B=ï£®
ï£°Î»1 0
...
0 Î»nï£¹
ï£».
Letrk=Î±L(Î»k) for each 1 â‰¤kâ‰¤m; that is,
rk= max {i: (tâˆ’Î»k)iis a factor of PL(t)},
and so
PL(t) = det n([L]Bâˆ’tIn) =Î»1âˆ’t 0
...
0 Î»nâˆ’t
=nY
k=1(Î»kâˆ’t) = (âˆ’1)nnY
k=1(tâˆ’Î»k) = (âˆ’1)nmY
k=1(tâˆ’Î»k)rk. (6.21)
The last equality holds since Î»kâˆˆ {Î»1, . . . , Î» m}for all 1 â‰¤kâ‰¤n, so there can be no factor of
PL(t) of the form tâˆ’Î»such that Î»Ì¸=Î»1, . . . , Î» m. Corollary 6.28 and (6.21) now imply that
dim(V) = deg( PL) =mX
k=1rk. (6.22)
From (6.21) we also see that, for each 1 â‰¤kâ‰¤m, the scalar Î»kmust occur precisely rktimes
on the diagonal of [ L]B; that is, for each 1 â‰¤kâ‰¤mthere exist
1â‰¤i1< i2<Â·Â·Â·< irkâ‰¤n
such that
Î»i1=Î»i2=Â·Â·Â·=Î»irk=Î»k,
and therefore
S={vi1,vi2, . . . ,virk} âŠ†EL(Î»k).
Now Theorem 3.56(2) implies that
dim(EL(Î»k))â‰¥rk (6.23)
since Span( S) is a subspace of EL(Î»k) of dimension rk.
Since Lis diagonalizable,
dim(V) =mX
k=1dim(EL(Î»k)) (6.24)
by Theorem 6.40. If we suppose that dim(EÎ»j(L))> rjfor some 1 â‰¤jâ‰¤m, then by equations
(6.24), (6.23), and (6.22), in turn, we obtain
dim(V) =mX
k=1dim(EL(Î»k))>mX
k=1rk= dim( V),
which is an egregious contradiction. Hence dim(EL(Î»k))â‰¤rkfor all 1 â‰¤kâ‰¤m, which together
with (6.23) leads to the conclusion that
Î±L(Î»k) =rk= dim( EL(Î»k)) =Î³L(Î»k)225
for all 1 â‰¤kâ‰¤m.
Proof of Part (2). Suppose that Lis diagonalizable and Bkis a basis for EL(Î»k) for each
1â‰¤kâ‰¤m. Statement (5) of Theorem 6.40 is true, and in the proof that statement (5) implies
statement (1) we immediately see that B=Sm
k=1Bkis a basis for Vconsisting of eigenvectors
ofL. That is, Bis a spectral basis for L. â– 226
6.6 â€“ Diagonalization Methods and Applications
In general, if a square matrix Ais given to be in FnÃ—n, then to say Ais â€œdiagonalizableâ€
means in particular â€œdiagonalizable in F.â€
Theorem 6.45 (Matrix Diagonalization Procedure ).LetAâˆˆFnÃ—nhave distinct eigen-
values Î»1, . . . , Î» m, with Bka basis for EA(Î»k)for each 1â‰¤kâ‰¤m. IfPAsplits over Fand
Î±A(Î»k) =Î³A(Î»k)for each k, then Ais diagonalizable in Fwith diagonal matrix DâˆˆFnÃ—ngiven
by
D=IEBAIâˆ’1
EB,
where Eis the standard basis for FnandB= (v1, . . . ,vn)is an ordered basis formed from the
elements ofSm
k=1Bk. Therefore
A=v1Â·Â·Â·vn
diag
Âµ1, . . . , Âµ nv1Â·Â·Â·vnâˆ’1, (6.25)
where Âµkis an eigenvalue corresponding to vkfor each 1â‰¤kâ‰¤n.
Proof. Suppose PAsplits over F, and Î±A(Î»k) =Î³A(Î»k) for each 1 â‰¤kâ‰¤m. Define Lâˆˆ L(Fn)
byL(x) =Axin the standard basis E, which is to say [ L]E=A. It is immediate that Î»1, . . . , Î» m
are the distinct eigenvalues of L, and since PL=PAby Definition 6.16, it follows that PLsplits
overF. By Definition 6.30 Î±L(Î»k) =Î±A(Î»k) for each k, and since EL(Î»k) =EA(Î»k),
Î³L(Î»k) = dim( EL(Î»k)) = dim( EA(Î»k)) =Î³A(Î»k).
Hence Î±L(Î»k) =Î³L(Î»k) for all 1 â‰¤kâ‰¤m, and so Lis diagonalizable by Theorem 6.44(1). Since
eachBkthat is a basis for EA(Î»k) is also a basis for EL(Î»k), by Theorem 6.44(2) the set
B=m[
k=1Bk
is a spectral basis for L. We order the elements of B, where |B|=nsinceBis a basis for Fn,
so that B= (v1, . . . ,vn) is an ordered basis for Fn. Then D= [L]Bis a diagonal matrix by
Corollary 6.41, and by Corollary 4.33
IEBAIâˆ’1
EB=IEB[L]EIâˆ’1
EB= [L]B=D
as desired.
To obtain (6.25) , observe that if Âµkis the eigenvalue corresponding to eigenvector vkfor
each 1 â‰¤kâ‰¤n, then
D= [L]B= diag
Âµ1, . . . , Âµ n
by Corollary 6.41, and so from D=IEBAIâˆ’1
EBwe having, recalling Proposition 4.31 and Theorem
4.27,
A=Iâˆ’1
EBDIEB=IBEdiag
Âµ1, . . . , Âµ n
Iâˆ’1
BE
=h
[v1]EÂ·Â·Â·[vn]Ei
diag
Âµ1, . . . , Âµ nh
[v1]EÂ·Â·Â·[vn]Eiâˆ’1
=v1Â·Â·Â·vn
diag
Âµ1, . . . , Âµ nv1Â·Â·Â·vnâˆ’1,227
where the last equality is due to the simple fact that each symbol vkalready represents the
E-coordinates of a vector in Fn, so that [ vk]E=vk. â– 
One particularly appealing feature of diagonal matrices is that, for any nâˆˆN, their nth
powers are found simply by taking the nth powers of their entries.
Proposition 6.46. IfD= [dij]nis a diagonal matrix, then Dk= [dk
ij]nfor all kâˆˆN.
Proof. The statement of the proposition is certainly true in the case when k= 1. Suppose
it is true for some arbitrary kâˆˆN, so that Dk= [dk
ij]n. Since Dis diagonal we have dij= 0
whenever iÌ¸=j. Fix 1 â‰¤i, jâ‰¤n. By Definition 2.4,

Dk+1
ij=
DkD
ij=nX
â„“=1dk
iâ„“dâ„“j= 0 = dk+1
ij
ifiÌ¸=j, and

Dk+1
ij=
DkD
jj=nX
â„“=1dk
jâ„“dâ„“j=dk
jjdjj=dk+1
jj
ifi=j. In either case we see that the ij-entry of Dk+1isdk+1
ij, and so Dk+1= [dk+1
ij]n.
Therefore the statement of the proposition holds for all kâˆˆNby the Principle of Induction
and the proof is done. â– 
Example 6.47. Determine whether
A=ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»
is diagonalizable in R. If it is, then find an invertible matrix Pand a diagonal matrix Dsuch
thatA=PDPâˆ’1.
Solution. In Example 6.20 we found that the characteristic polynomial PAsplits over Rby
direct factorization:
PA(t) =âˆ’t3+ 6t2âˆ’11t+ 6 = âˆ’(tâˆ’1)(tâˆ’2)(tâˆ’3).
In this way we determined that the eigenvalues of Aare 1, 2, and 3, and by inspection we see
that
Î±A(1) = Î±A(2) = Î±A(3) = 1 .
We also determined a basis for the eigenspace corresponding to each eigenvalue: for eigenspaces
EA(1),EA(2), and EA(3) we found bases
B1=ï£±
ï£²
ï£³ï£®
ï£°1
1
1ï£¹
ï£»ï£¼
ï£½
ï£¾,B2=ï£±
ï£²
ï£³ï£®
ï£°2
3
3ï£¹
ï£»ï£¼
ï£½
ï£¾,and B3=ï£±
ï£²
ï£³ï£®
ï£°1
3
4ï£¹
ï£»ï£¼
ï£½
ï£¾,
respectively. Since |B1|=|B2|=|B3|= 1, we see that
Î³A(1) = Î³A(2) = Î³A(3) = 1 .228
Hence Î±A(Î») =Î³A(Î») for every eigenvalue Î»ofA. Therefore Ais diagonalizable in Rby
Theorem 6.45.
Letting
v1=ï£®
ï£°1
1
1ï£¹
ï£»,v2=ï£®
ï£°2
3
3ï£¹
ï£»,and v3=ï£®
ï£°1
3
4ï£¹
ï£»,
thenB= (v1,v2,v3) is an ordered set formed from the elements of B1âˆª B 2âˆª B 3which Theorem
6.45 implies is an ordered basis for R3. Now, since eigenvalues 1, 2, and 3 correspond to
eigenvectors v1,v2, and v3, respectively, by (6.25) we easily find that
A=v1v2v3
diag
1,2,3v1v2v3âˆ’1.
Thus if we let
P=v1v2v3
=ï£®
ï£°1 2 1
1 3 3
1 3 4ï£¹
ï£»and D= diag
1,2,3
=ï£®
ï£°1 0 0
0 2 0
0 0 3ï£¹
ï£»,
then we have A=PDPâˆ’1as desired. â– 
In Example 6.47 there are other possible solutions. If we had chosen the ordered basis
(v3,v2,v1) instead of ( v1,v2,v3), then we would have
A=v3v2v1
diag
3,2,1v3v2v1âˆ’1,
which is to say A=PDPâˆ’1for
P=ï£®
ï£°1 2 1
3 3 1
4 3 1ï£¹
ï£»and D=ï£®
ï£°3 0 0
0 2 0
0 0 1ï£¹
ï£».
One great use for diagonalization is that it makes it possible to calculate high powers of
square matrices with relative ease, as illustrated in the following example.
Example 6.48. Given
A=ï£®
ï£°âˆ’1 4âˆ’2
âˆ’3 4 0
âˆ’3 1 3ï£¹
ï£»,
Find a formula for An, and use it to calculate A10.
Solution. From Example 6.47 we have A=PDPâˆ’1, with
P=ï£®
ï£°1 2 1
1 3 3
1 3 4ï£¹
ï£»,D=ï£®
ï£°1 0 0
0 2 0
0 0 3ï£¹
ï£»,Pâˆ’1=ï£®
ï£°3âˆ’5 3
âˆ’1 3 âˆ’2
0âˆ’1 1ï£¹
ï£»,
and so by Propositions 6.38 and 6.46, respectively,
An= (PDPâˆ’1)n=PDnPâˆ’1=ï£®
ï£°1 2 1
1 3 3
1 3 4ï£¹
ï£»ï£®
ï£°1 0 0
0 2n0
0 0 3nï£¹
ï£»ï£®
ï£°3âˆ’5 3
âˆ’1 3 âˆ’2
0âˆ’1 1ï£¹
ï£»229
=ï£®
ï£°3âˆ’2n+1âˆ’5 + 3Â·2n+1âˆ’3n3âˆ’2n+2+ 3n
3âˆ’3Â·2nâˆ’5 + 9Â·2nâˆ’3n+13âˆ’3Â·2n+1+ 3n+1
3âˆ’3Â·2nâˆ’5 + 9Â·2nâˆ’4Â·3n3âˆ’3Â·2n+1+ 4Â·3nï£¹
ï£».
Therefore
A10=ï£®
ï£°3âˆ’211âˆ’5 + 3Â·211âˆ’3103âˆ’212+ 310
3âˆ’3Â·210âˆ’5 + 9Â·210âˆ’3113âˆ’3Â·211+ 311
3âˆ’3Â·210âˆ’5 + 9Â·210âˆ’4Â·3103âˆ’3Â·211+ 4Â·310ï£¹
ï£»
=ï£®
ï£°âˆ’2045 âˆ’52,910 54 ,956
âˆ’3069 âˆ’167,936 171 ,006
âˆ’3069 âˆ’226,985 230 ,055ï£¹
ï£»,
a result far more easily obtained than calculating A10directly! â– 
Example 6.49. Determine whether the linear operator Lâˆˆ L(R2Ã—2) given by L(A) =AâŠ¤is
diagonalizable. If it is, then find a spectral basis for L, and find the matrix corresponding to L
with respect to the spectral basis.
Solution. In Example 4.23 we found that the matrix corresponding to Lwith respect to the
standard basis E= (E11,E12,E21,E22) is
[L]E=ï£®
ï£¯ï£¯ï£°1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1ï£¹
ï£ºï£ºï£».
The characteristic polynomial of Lis thus
PL(t) =P[L]E(t) = det 4 
[L]Eâˆ’tI4
=1âˆ’t0 0 0
0âˆ’t1 0
0 1 âˆ’t0
0 0 0 1 âˆ’t= (tâˆ’1)3(t+ 1),
which makes clear that PLsplits over R, and the eigenvalues of LareÂ±1 with Î±L(âˆ’1) = 1 and
Î±L(1) = 3.
Next we find bases for the eigenspaces of L. For the eigenvalue 1 we have
EL(1) =
XâˆˆR2Ã—2:L(X) =X	
,
where
X=L(X)â‡”X=XâŠ¤â‡”
x y
z w
=
x z
y w
forx, y, z, w âˆˆR, implying that y=zand thus
EL(1) =
x y
z w
âˆˆR2Ã—2:y=z
=
x y
y w
:x, y, w âˆˆR
.
Letting x=s1,y=s2, and w=s3, we finally obtain
EL(1) =
1 0
0 0
s1+
0 1
1 0
s2+
0 0
0 1
s3:s1, s2, s3âˆˆR
,230
which shows that EL(1) has basis
B1=
1 0
0 0
,
0 1
1 0
,
0 0
0 1
={E11,E12+E21,E22}
and therefore Î³L(1) = 3.
For the eigenvalue âˆ’1,
EL(âˆ’1) =
XâˆˆR2Ã—2:L(X) =âˆ’X	
,
where
âˆ’X=L(X)â‡” âˆ’ X=XâŠ¤â‡”
âˆ’xâˆ’y
âˆ’zâˆ’w
=
x z
y w
forx, y, z, w âˆˆR, implying that x=âˆ’x,z=âˆ’y,âˆ’z=y, and w=âˆ’w. Thus x=w= 0, and
z=âˆ’y, so that
EL(âˆ’1) =
0y
âˆ’y0
:yâˆˆR
.
Letting y=s, we obtain
EL(âˆ’1) =
0 1
âˆ’1 0
s:sâˆˆR
,
which shows that EL(âˆ’1) has basis
B2=
0 1
âˆ’1 0
={E12âˆ’E21},
and therefore Î³L(âˆ’1) = 1.
By Theorem 6.44(1), since PLsplits over R,Î±L(1) = Î³L(1), and Î±L(âˆ’1) = Î³L(âˆ’1), the
operator Lis diagonalizable. By Theorem 6.44(2) the ordered set
B=B1âˆª B 2= (v1,v2,v3,v4) = (E11,E12+E21,E22,E12âˆ’E21)
is a spectral basis for L. By Corollary 6.41 the B-matrix of Lis a diagonal matrix with kk-entry
the eigenvalue corresponding to the kth vector vkinB. Since v1=E11,v2=E12+E21, and
v3=E22are eigenvectors corresponding to 1, and v4=E12âˆ’E21is an eigenvector corresponding
toâˆ’1, we have
[L]B=ï£®
ï£¯ï£¯ï£°1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 âˆ’1ï£¹
ï£ºï£ºï£».
It is in this sense that Lis â€œdiagonalizedâ€ by finding a spectral basis. â– 231
Problems
1. The matrix
A=
3 2
2 3
is diagonalizable.
(a) Find the characteristic polynomial of A, and use it to find the eigenvalues of A.
(b) For each eigenvalue of Afind the basis for the corresponding eigenspace.
(c) Find an invertible matrix Pand a diagonal matrix Dsuch that A=PDPâˆ’1.
(d) Find A50andA1/2.
2. Determine whether the matrix
A=
7âˆ’15
2âˆ’4
is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDPâˆ’1.
3. Determine whether the matrix
A=ï£®
ï£°4 0 4
0 4 4
4 4 8ï£¹
ï£»
is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDPâˆ’1.
4. Determine whether the matrix
A=ï£®
ï£°2 0âˆ’2
0 3 0
0 0 3ï£¹
ï£»
is diagonalizable in R. If it is, then find an invertible matrix Pand diagonal matrix Dsuch
thatA=PDPâˆ’1.
6.7 â€“ Matrix Limits and Markov Chains232
6.8 â€“ The Cayley-Hamilton Theorem
Proposition 6.50. LetVbe a finite-dimensional vector space over Fwith subspace W. IfWis
invariant under Lâˆˆ L(V), then the characteristic polynomial of L|Wdivides the characteristic
polynomial of L.
Proof. LetC={v1, . . . ,vm}be a basis for W. By Theorem 3.55 we can extend Cto a basis
B={v1, . . . ,vm,vm+1, . . . ,vn}
forV. Since WisL-invariant, for each vjwith 1 â‰¤jâ‰¤mwe have L(vj)âˆˆW, and so there
exist a1j, . . . , a mjâˆˆFsuch that
L(vj) =mX
i=1aijvi.
Form+ 1â‰¤jâ‰¤nwe have
L(vj) =nX
i=1aijvi.
Defining
A=ï£®
ï£°a11Â·Â·Â· a1m.........
am1Â·Â·Â·ammï£¹
ï£»,B=ï£®
ï£°a1(m+1)Â·Â·Â· a1n.........
am(m+1)Â·Â·Â·amnï£¹
ï£»,C=ï£®
ï£°a(m+1)(m+1)Â·Â·Â·a(m+1)n.........
an(m+1) Â·Â·Â· annï£¹
ï£»,
by Corollary 4.21 the B-matrix for Lis
[L]B=h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
=A B
O C
and the C-matrix for L|Wis
[L|W]C=h
L|W(v1)
CÂ·Â·Â·
L|W(vm)
Ci
=h
L(v1)
CÂ·Â·Â·
L(vm)
Ci
=A.
Now by Example 5.21,
PL(t) = det n 
[L]Bâˆ’tIn
= det n
A B
O C
âˆ’
tImO
OtInâˆ’m
= det n
Aâˆ’tIm B
O C âˆ’tInâˆ’m
= det m(Aâˆ’tIm) det nâˆ’m(Câˆ’tInâˆ’m)
= det m 
[L|W]Câˆ’tIm
detnâˆ’m(Câˆ’tInâˆ’m) =PL|W(t) det nâˆ’m(Câˆ’tInâˆ’m),
and since det nâˆ’m(Câˆ’tInâˆ’m) is a polynomial we conclude that PL|W(t) divides PL(t). â– 
Definition 6.51. Suppose Vis a vector space, and vâˆˆVsuch that vÌ¸=0. The L-cyclic
subspace of Vgenerated by Lis the subspace
Span{Lk(v) :kâ‰¥0}.233
As usual we take it as understood that L0=IV, the identity operator on V, so that
L0(v) =IV(v) =v.
Proposition 6.52. Suppose Vis a finite-dimensional vector space, vâˆˆVis a nonzero vector,
Lâˆˆ L(V), and Wis the L-cyclic subspace of Vgenerated by v. Ifdim(W) =m, then the
following hold.
1.The set {v, L(v), . . . , Lmâˆ’1(v)}is a basis for W.
2.Ifa0, . . . , a mâˆ’1âˆˆFare such that
mâˆ’1X
k=0akLk(v) +Lm(v) =0,
then
PL|W(t) = (âˆ’1)m mâˆ’1X
k=0aktk+tm!
.
Proof.
Proof of Part (1). Since vÌ¸=0the set S0={v}is linearly independent. For each kâ‰¥0 let
Sk={v, L(v), . . . , Lk(v)},
and define
n= max {k:Skis a linearly independent set }.
Then Snis a linearly independent set and Sn+1=Snâˆª {Ln+1(v)}is linearly dependent, and by
Proposition 3.39 it follows that Ln+1(v)âˆˆSpan( Sn).
Fixjâ‰¥1 and suppose Ln+j(v)âˆˆSpan( Sn), so that there exist a0, . . . , a nâˆˆFsuch that
Ln+j(v) =nX
k=0akLk(v).
Now,
Ln+j+1(v) =L(Ln+j(v)) =L nX
k=0akLk(v)!
=nX
k=0akLk+1(v)
=a0L(v) +a1L2(v) +Â·Â·Â·+anâˆ’1Ln(v) +anLn+1(v),
and since akâˆ’1Lk(v)âˆˆSpan (Sn) for all 1 â‰¤kâ‰¤n+ 1, we conclude that Ln+j+1(v)âˆˆSpan (S)
as well. Therefore Lk(v)âˆˆSpan( Sn) for all kâ‰¥0 by the principle of induction.
It is clear that
Span( Sn)âŠ†W= Span {Lk(v) :kâ‰¥0}.
FixwâˆˆW. Then there exist
a0, . . . , a râˆˆFand 0 â‰¤k0< k 1<Â·Â·Â·< k r
such that
w=rX
j=0ajLkj(v)234
for some râˆˆW, and since ajLkj(v)âˆˆSpan (Sn) for each j, we have wâˆˆSpan (Sn) also, and
thus WâŠ†Span (Sn). It is now established that Span (Sn) =W, and since Snis a linearly
independent set, it follows that Snis a basis for Wand hence |Sn|= dim( W) =m. Therefore
Sn={v, L(v), . . . , Ln(v)}={v, L(v), . . . , Lmâˆ’1(v)},
as was to be shown.
Proof of Part (2) Suppose a0, . . . , a mâˆ’1âˆˆFare such that
Lm(v) =âˆ’a0vâˆ’a1L(v)âˆ’ Â·Â·Â· âˆ’ amâˆ’1Lmâˆ’1(v).
By Part (1) the ordered set C={v, L(v), . . . , Lmâˆ’1(v)}is a basis for W, and so
[L|W]C=h
L|W(v)
C
L|W(L(v))
CÂ·Â·Â·
L|W(Lmâˆ’1(v))
Ci
=h
L(v)
C
L2(v)
CÂ·Â·Â·
Lmâˆ’1(v)
C
Lm(v)
Ci
=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0 0 0 Â·Â·Â· 0âˆ’a0
1 0 0 Â·Â·Â· 0âˆ’a1
0 1 0 Â·Â·Â· 0âˆ’a2..................
0 0 0 Â·Â·Â· 0âˆ’amâˆ’2
0 0 0 Â·Â·Â· 1âˆ’amâˆ’1ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£».
It follows by Example 6.22 that the characteristic polynomial of [ L|W]Cis
(âˆ’1)m(a0+a1t+Â·Â·Â·+amâˆ’1tmâˆ’1+tm),
and therefore
PL|W(t) = (âˆ’1)m mâˆ’1X
k=0aktk+tm!
by Definition 6.16. â– 
Definition 6.53. Letfâˆˆ P n(F)be a polynomial function Fâ†’Fgiven by
f(t) =nX
k=0aktk.
IfVis a vector space over F,Lâˆˆ L(V), and AâˆˆFnÃ—n, we define the mapping f(L)and matrix
f(A)by
f(L) =nX
k=0akLkand f(A) =nX
k=0akAk.
Some needed basic properties of mappings of the form f(L) and matrices of the form f(A)
which are routine to verify are the following.
Proposition 6.54. Suppose that Vis a vector space over F,Lâˆˆ L(V),AâˆˆFnÃ—n, and
f, g, h âˆˆ P(F). Then235
1.f(L)âˆˆ L(V)andf(A)âˆˆFnÃ—n.
2.f(L)â—¦g(L) =g(L)â—¦f(L)andf(A)g(A) =g(A)f(A).
3.Ifh(t) =f(t)g(t), then h(L) =f(L)â—¦g(L)andh(A) =f(A)g(A).
Theorem 6.55 (Cayley-Hamilton Theorem ).LetVbe a finite-dimensional vector space. If
Lâˆˆ L(V), then PL(L) =OV.
Proof. Suppose Lâˆˆ L(V), and fix vâˆˆVsuch that vÌ¸=0. Let Wbe the L-cyclic subspace of
Vgenerated by v, with m= dim( W). By Proposition 6.52(1) the set
B={v, L(v), . . . , Lmâˆ’1(v)}
is a basis for W, so that Lm(v)âˆˆSpan(B) and there exist scalars a0, . . . , a mâˆ’1âˆˆFsuch that
Lm(v) =âˆ’a0vâˆ’a1L(v)âˆ’ Â·Â·Â· âˆ’ amâˆ’1Lmâˆ’1(v).
By Proposition 6.52(2) it follows that
PL|W(t) = (âˆ’1)m(a0IV+a1t+a2t2+Â·Â·Â·+amâˆ’1tmâˆ’1+tm).
Now, by Proposition 6.50, the polynomial PL|Wdivides PL, which is to say there exists some
fâˆˆ P(F) such that
PL(t) =f(t)PL|W(t),
and hence by Proposition 6.54(3)
PL(L) =f(L)â—¦PL|W(L).
However,
PL|W(L)(v) = 
(âˆ’1)m(a0IV+a1L+Â·Â·Â·+amâˆ’1Lmâˆ’1+Lm)
(v)
= (âˆ’1)m(a0v+a1L(v) +Â·Â·Â·+amâˆ’1Lmâˆ’1(v) +Lm(v))
= (âˆ’1)m(âˆ’Lm(v) +Lm(v)) = (âˆ’1)m0=0,
and so
PL(L)(v) = 
f(L)â—¦PL|W(L)
(v) =f(L) 
PL|W(L)(v)
=f(L)(0) =0,
where the last equality follows from the observation that f(L) is a linear operator by Proposition
6.54(1). Therefore PL(L)(v) =0for all nonzero vâˆˆV, and since PL(L)(0) =0also, we
conclude that PL(L) =OV. â– 
Corollary 6.56. IfAâˆˆFnÃ—n, then PA(A) =On.
Proof. Suppose AâˆˆFnÃ—n. Let Lâˆˆ L(Fn) be such that [ L]E=A, soL(x) =Axfor all xâˆˆFn.
By Definition 6.16 we have PL=PA, where deg( PA) =nby Proposition 6.27 and so
PA(t) =PL(t) =a0+a1t+Â·Â·Â·+antn
for some a0, . . . , a nâˆˆF. By the Cayley-Hamilton Theorem PL(L) =O, the zero operator on Fn,
which is to say
PL(L)(x) = (a0I+a1L+Â·Â·Â·+anLn)(x) =O(x) =0236
for all xâˆˆFn, where Iis the identity operator on Fn. Now, PA(A)âˆˆFnÃ—nis given by
PA(A) =a0In+a1A+Â·Â·Â·+anAn,
so that
PA(A)(x) = (a0In+a1A+Â·Â·Â·+anAn)(x) =a0x+a1Ax+Â·Â·Â·+anAnx
=a0I(x) +a1L(x) +Â·Â·Â·+anLn(x) = (a0I+a1L+Â·Â·Â·+anLn)(x)
=PL(L)(x) =O(x) =0
for all xâˆˆFn, and therefore PA(A) =Onby Proposition 2.12(2). â– 237
7
Inner Product Spaces
7.1 â€“ Inner Products
Recall that if zis a complex number, then Â¯zdenotes the conjugate of z,Re(z) denotes the
real part of z, and Im( z) denotes the imaginary part of z. By definition,
a+bi=aâˆ’bi, Re(a+bi) =a,Im(a+bi) =b
for any a, bâˆˆR. Throughout this chapter we take Fto represent any field that is a subfield of
the complex numbers C, which is to say Fis a field consisting of objects on which the operation
of conjugation may be done. This of course includes Citself, as well as the field of real numbers
R, rational numbers Q, and others.
Definition 7.1. Aninner product on a vector space VoverFis a function âŸ¨ âŸ©:VÃ—Vâ†’F
that associates each pair of vectors (u,v)âˆˆVÃ—Vwith a scalar âŸ¨u,vâŸ© âˆˆFin accordance with
the following axioms:
IP1.âŸ¨u,vâŸ©=âŸ¨v,uâŸ©for all u,vâˆˆV
IP2.âŸ¨u+v,wâŸ©=âŸ¨u,wâŸ©+âŸ¨v,wâŸ©for all u,v,wâˆˆV
IP3.âŸ¨au,vâŸ©=aâŸ¨u,vâŸ©for all u,vâˆˆVandaâˆˆF.
IP4.âŸ¨u,uâŸ©>0for all uÌ¸=0.
A vector space Vtogether with an associated inner product âŸ¨ âŸ©is called an inner product
space and denoted by (V,âŸ¨ âŸ©).
Remark. Care must be taken to not confuse the symbol for the inner product of two vectors
âŸ¨u,vâŸ©with, say, the symbol for a Euclidean vector âŸ¨x, yâŸ© âˆˆR2that is used in some textbooks
(particularly calculus books). One features a pair of vectors between angle brackets, while the
other features a pair of scalars .
An inner product âŸ¨ âŸ©associated with a vector space VoverCis generally complex-valued
and called a hermitian inner product or simply a hermitian product , in which case the
pair ( V,âŸ¨ âŸ©) is called a hermitian inner product space .238
Axiom IP1 is the conjugate symmetry property. If Vis a vector space over R(or some
subfield of R), then this axiom becomes
âŸ¨u,vâŸ©=âŸ¨v,uâŸ©for all u,vâˆˆV
and is called the symmetry property.
Axioms IP2 and IP3 taken together are the linearity properties, and using them we easily
obtain
âŸ¨uâˆ’v,wâŸ©=âŸ¨u+ (âˆ’v),wâŸ©=âŸ¨u,wâŸ©+âŸ¨âˆ’v,wâŸ©=âŸ¨u,wâŸ© âˆ’ âŸ¨v,wâŸ©.
Axiom IP4 is the positive-definiteness property. Products which satisfy all axioms save
IP4 (or which satisfy a modified version of IP4) are also of theoretical interest, but will not be
entertained in this chapter.
Theorem 7.2. Let(V,âŸ¨ âŸ©)be an inner product space over F. For u,v,wâˆˆVandaâˆˆF, the
following properties hold:
1.âŸ¨0,uâŸ©=âŸ¨u,0âŸ©= 0.
2.âŸ¨u,v+wâŸ©=âŸ¨u,vâŸ©+âŸ¨u,wâŸ©.
3.âŸ¨u, avâŸ©= Â¯aâŸ¨u,vâŸ©.
4.âŸ¨u,uâŸ©= 0if and only if u=0.
5.IfâŸ¨u,vâŸ©=âŸ¨u,wâŸ©for all uâˆˆV, then v=w.
Proof.
Proof of Part (1): LetuâˆˆV. By Axiom IP2 we have
âŸ¨0,uâŸ©=âŸ¨0+0,uâŸ©=âŸ¨0,uâŸ©+âŸ¨0,uâŸ©.
Subtracting âŸ¨0,uâŸ©from the leftmost and rightmost expressions yields âŸ¨0,uâŸ©= 0 as desired.
Then
âŸ¨u,0âŸ©=âŸ¨0,uâŸ©=Â¯0 = 0
completes the proof.
Proof of Part (2): For any u,v,wâˆˆVwe have
âŸ¨u,v+wâŸ©=âŸ¨v+w,uâŸ© Axiom IP1
=âŸ¨v,uâŸ©+âŸ¨w,uâŸ© Axiom IP2
=âŸ¨v,uâŸ©+âŸ¨w,uâŸ© Property of complex conjugates
=âŸ¨u,vâŸ©+âŸ¨u,wâŸ© Axiom IP1
Proof of Part (3): For any u,vâˆˆVandaâˆˆFwe have
âŸ¨u, avâŸ©=âŸ¨av,uâŸ© Axiom IP1
=aâŸ¨v,uâŸ© Axiom IP3
= Â¯aâŸ¨v,uâŸ© Property of complex conjugates239
= Â¯aâŸ¨u,vâŸ© Axiom IP1
Proof of Part (4): The contrapositive of Axiom IP4 states that if âŸ¨u,uâŸ© â‰¤0, then u=0. Thus,
in particular, âŸ¨u,uâŸ©= 0 implies that u=0.
For the converse, suppose that u=0. Then, applying Axiom IP2,
âŸ¨u,uâŸ©=âŸ¨0,0âŸ©=âŸ¨0+0,0âŸ©=âŸ¨0,0âŸ©+âŸ¨0,0âŸ©;
that is,
âŸ¨0,0âŸ©+âŸ¨0,0âŸ©=âŸ¨0,0âŸ©,
from which we obtain âŸ¨0,0âŸ©= 0. We conclude that u=0implies that âŸ¨u,uâŸ©= 0.
Proof of Part (5): Suppose that âŸ¨u,vâŸ©=âŸ¨u,wâŸ©for all uâˆˆV. Then
âŸ¨u,vâˆ’wâŸ©=âŸ¨u,v+ (âˆ’1)wâŸ©=âŸ¨u,vâŸ©+âŸ¨u,(âˆ’1)wâŸ©
=âŸ¨u,vâŸ©+ (âˆ’1)âŸ¨u,wâŸ©=âŸ¨u,vâŸ© âˆ’ âŸ¨u,wâŸ©
=âŸ¨u,vâŸ© âˆ’ âŸ¨u,vâŸ©= 0
for all uâˆˆV, making use of Proposition 3.3, parts (2) and (3), and the property
x+ (âˆ’1)y=xâˆ’y
forx, yâˆˆF. Letting u=vâˆ’wsubsequently yields
âŸ¨vâˆ’w,vâˆ’wâŸ©= 0,
so that vâˆ’w=0by part (4), and therefore v=w. â– 
One sure result that obtains from Axiom IP4 and Theorem 7.2(4) is that âŸ¨u,uâŸ© â‰¥0 for all
uâˆˆV. This will be important when the discussion turns to norms in the next section.
Recall the Euclidean dot product as defined for vectors
x=ï£®
ï£°x1...
xnï£¹
ï£»and y=ï£®
ï£°y1...
ynï£¹
ï£»
inRn:
xÂ·y=yâŠ¤x=y1Â·Â·Â·ynï£®
ï£°x1...
xnï£¹
ï£»=nX
k=1xkyk.
It is easily verified that the Euclidean dot product applied to Rnsatisfies the four axioms of an
inner product, and so ( Rn,Â·) is an inner product space.
It might be assumed that ( Cn,Â·) is also an inner product space (where as usual Cnis taken
to have underlying field C), but this is not the case. Consider for instance the vector z= [ 1 i]âŠ¤
inC2. We have
zÂ·z=zâŠ¤z=1i
1
i
= 12+i2= 1 + ( âˆ’1) = 0;240
that is, zÂ·z= 0 even though zÌ¸=0, and so Axiom IP4 fails! Or consider z= [i0 0 ]âŠ¤inC3,
for which we find that
zÂ·z=zâŠ¤z=i0 0ï£®
ï£°i
0
0ï£¹
ï£»=i2=âˆ’1<0
and again Axiom IP4 fails. To remedy the situation only requires a modest modification of the
dot product definition. For the definition we need the conjugate transpose matrix operation:
IfA= [aij]âˆˆCmÃ—n, then set
Aâˆ—=AâŠ¤= [aij]âŠ¤.
Thus, in particular, if
z=ï£®
ï£°z1...
znï£¹
ï£»âˆˆCn,
then
zâˆ—=z1Â·Â·Â·zn
.
Definition 7.3. Ifw,zâˆˆCn, then the hermitian dot product ofwandzis
wÂ·z=zâˆ—w=z1Â·Â·Â·znï£®
ï£°w1...
wnï£¹
ï£»=nX
k=1wkzk. (7.1)
The natural isomorphism [ a]1Ã—17â†’ais an implicit part of the definition, so that the hermitian
dot product produces a scalar value as expected.
Letting Â·denote the hermitian dot product, we return to the vector [ 1 i]âŠ¤âˆˆC2and find
that 
1
i
Â·
1
i
=
1i
1
i
=1âˆ’i
1
i
= 1Â·1 +i(âˆ’i) = 1âˆ’i2= 1âˆ’(âˆ’1) = 2 ,
which is an outcome that does not run afoul of Axiom IP4 and so corrects the problem [ 1 i]âŠ¤
presented for the Euclidean dot product above.
The hermitian dot product becomes the Euclidean dot product when applied to vectors in
Rn: letting x,yâˆˆRnwe have
xÂ·y=yâˆ—x=yâŠ¤x=y1Â·Â·Â·ynï£®
ï£°x1...
xnï£¹
ï£»=y1Â·Â·Â·ynï£®
ï£°x1...
xnï£¹
ï£»=yâŠ¤x,
since ykâˆˆRimplies that yk=ykfor each 1 â‰¤kâ‰¤n. For this reason we will henceforth always
assume (unless stated otherwise) that Â·denotes the hermitian dot product, and call it simply
thedot product .
Example 7.4. Leta, bâˆˆRsuch that a < b , and let Vbe the vector space over Rconsisting of
all continuous functions f: [a, b]â†’R. Given f, gâˆˆV, define
âŸ¨f, gâŸ©=Zb
afg. (7.2)241
We verify that ( V,âŸ¨ âŸ©) is an inner product space. Since âŸ¨f, gâŸ©is real-valued for any f, gâˆˆV, we
have
âŸ¨f, gâŸ©=Zb
afg=Zb
agf=âŸ¨g, fâŸ©=âŸ¨g, fâŸ©
and thus Axiom IP1 is confirmed.
Next, for any f, g, h âˆˆVwe have
âŸ¨f+g, hâŸ©=Zb
a(f+g)h=Zb
a(fh+fg) =Zb
afh+Zb
afg=âŸ¨f, hâŸ©+âŸ¨f, gâŸ©,
confirming Axiom IP2.
Axiom IP3 obtains readily:
âŸ¨af, gâŸ©=Zb
a(af)g=Zb
aa(fg) =aZb
afg=aâŸ¨f, gâŸ©.
Next, for any fâˆˆVwe have f2(x)â‰¥0 for all xâˆˆ[a, b], and so
âŸ¨f, fâŸ©=Zb
af2â‰¥0
follows from an established property of the definite integral. Finally, if
Zb
af2= 0
it follows from another property of definite integrals that f(x) = 0 for all xâˆˆ[a, b], which is to
sayf= 0 and therefore Axiom IP4 holds. â– 
Example 7.5. Recall the notion of the trace of a square matrix, which is a linear mapping
tr :FnÃ—nâ†’Fgiven by
tr(A) =nX
i=1aii
for each A= [aij]âˆˆFnÃ—n. Letting F=R, define âŸ¨ âŸ©: Symn(R)Ã—Symn(R)â†’Rby
âŸ¨A,BâŸ©= tr(AB).
The claim is that ( Symn(R),âŸ¨ âŸ©) is an inner product space. To substantiate the claim we must
verify that the four axioms of an inner product are satisfied.
LetA= [aij] and B= [bij] be elements of Symn(R). The ii-entry of ABisPn
j=1aijbji, and
so
tr(AB) =nX
i=1nX
j=1aijbji. (7.3)
Theii-entry of BAisPn
j=1bijaji, from which we obtain
tr(BA) =nX
i=1nX
j=1bijaji
=nX
j=1nX
i=1bjiaij (Interchange iandj)242
=nX
i=1nX
j=1aijbji (Interchange summations)
= tr(AB). (Equation (7.3))
Hence
âŸ¨A,BâŸ©= tr(AB) = tr( BA) =âŸ¨B,AâŸ©
and Axiom IP1 is confirmed to hold.
In Chapter 4 it was found that the trace operation is a linear mapping, and so for any
A,B,CâˆˆSymn(R) and xâˆˆRwe have
âŸ¨A+B,CâŸ©= tr(( A+B)C) = tr( AC+BC) = tr( AC) + tr( BC) =âŸ¨A,CâŸ©+âŸ¨B,CâŸ©
and
âŸ¨xA,BâŸ©= tr(( xA)B) = tr( x(AB)) =xtr(AB) =xâŸ¨A,BâŸ©,
which confirms Axioms IP2 and IP3.
Next, observing that A= [aij]âˆˆSymn(R) if and only if aij=ajifor all 1 â‰¤i, jâ‰¤n, we
have
âŸ¨A,AâŸ©= tr(A2) =nX
i=1nX
j=1aijaji=nX
i=1nX
j=1aijaij=nX
i=1nX
j=1a2
ijâ‰¥0.
It is easy to see that if tr(A2) = 0, then we must have aij= 0 for all 1 â‰¤i, jâ‰¤n, and thus
A=On. Axiom IP4 is confirmed. â– 243
7.2 â€“ Norms
Given an inner product space ( V,âŸ¨ âŸ©) and a vector uâˆˆV, we define the norm ofuto be
the scalar
âˆ¥uâˆ¥=p
âŸ¨u,uâŸ©.
Ifâˆ¥uâˆ¥= 1 we say that uis aunit vector . Notice that, by Axiom IP4, âˆ¥uâˆ¥is always a
nonnegative real number. The distance d(u,v) between two vectors u,vâˆˆVis given by
d(u,v) =âˆ¥uâˆ’vâˆ¥,
also always a nonnegative real number. If
âŸ¨u,vâŸ©= 0
we say that uandvareorthogonal and write uâŠ¥v.
Proposition 7.6. Let(V,âŸ¨ âŸ©)be an inner product space. If WâŠ†Vis a subspace of V, then
WâŠ¥={vâˆˆV:âŸ¨v,wâŸ©= 0 for all wâˆˆW} (7.4)
is also a subspace of V.
Proof. Suppose u,vâˆˆWâŠ¥. Then for any wâˆˆWwe have
âŸ¨u+v,wâŸ©=âŸ¨u,wâŸ©+âŸ¨v,wâŸ©= 0 + 0 = 0 ,
which shows that u+vâˆˆWâŠ¥. Moreover, for any aâˆˆFwe have
âŸ¨au,wâŸ©=aâŸ¨u,wâŸ©=a(0) = 0
for any wâˆˆW, which shows that auâˆˆWâŠ¥. Since WâŠ¥âŠ†Vis closed under scalar multiplication
and vector addition, we conclude that it is a subspace of V. â– 
The subspace WâŠ¥defined by (7.4) is called the orthogonal complement ofW.6IfvâˆˆWâŠ¥,
then we say visorthogonal toWand write vâŠ¥W.
Proposition 7.7. Let(V,âŸ¨ âŸ©)be an inner product space. Let w1, . . . ,wmâˆˆV, and define the
subspace
U={vâˆˆV:vâŠ¥wifor all 1â‰¤iâ‰¤m}.
IfW= Span {w1, . . . ,wm}, then U=WâŠ¥.
Proof. It is a routine matter to verify that Uis indeed a subspace of V. Let vâˆˆU. For any
wâˆˆWwe have
w=c1w1+Â·Â·Â·+cmwm
for some c1, . . . , c mâˆˆF, and then since vâŠ¥wiimplies âŸ¨wi,vâŸ©= 0 we obtain
âŸ¨w,vâŸ©=DXm
i=1ciwi,vE
=Xm
i=1ciâŸ¨wi,vâŸ©=Xm
i=1ci(0) = 0
6The symbol WâŠ¥is often read as â€œ Wperp.â€244
by Axioms IP2 and IP3. Hence vâŠ¥wfor all wâˆˆW, so that vâˆˆWâŠ¥and therefore UâŠ†WâŠ¥.
Next, let vâˆˆWâŠ¥. Then âŸ¨w,vâŸ©= 0 for all wâˆˆW, or equivalently
DXm
i=1ciwi,vE
= 0 (7.5)
for any c1, . . . , c mâˆˆF. If for any 1 â‰¤iâ‰¤mwe choose ci= 1 and cj= 0 for jÌ¸=i, then (7.5)
givesâŸ¨wi,vâŸ©= 0. Thus vâŠ¥wifor all 1 â‰¤iâ‰¤m, implying that vâˆˆUand so WâŠ¥âŠ†U.
Therefore U=WâŠ¥. â– 
Letvâˆˆ(V,âŸ¨ âŸ©) such that âˆ¥vâˆ¥ Ì¸= 0. Given any uâˆˆ(V,âŸ¨ âŸ©) there can be found some câˆˆF
such that
âŸ¨v,uâˆ’cvâŸ©= 0.
Indeed
âŸ¨v,uâˆ’cvâŸ©= 0â‡” âŸ¨v,uâŸ© âˆ’ âŸ¨v, cvâŸ©= 0â‡” âŸ¨v,uâŸ© âˆ’Â¯câŸ¨v,vâŸ©= 0
â‡”Â¯câŸ¨v,vâŸ©=âŸ¨v,uâŸ© â‡” Â¯câŸ¨v,vâŸ©=âŸ¨v,uâŸ©
â‡”câŸ¨v,vâŸ©=âŸ¨u,vâŸ© â‡” c=âŸ¨u,vâŸ©
âŸ¨v,vâŸ©, (7.6)
where âŸ¨v,vâŸ© Ì¸= 0 since âˆ¥vâˆ¥ Ì¸= 0.
Definition 7.8. Letâˆ¥vâˆ¥ Ì¸= 0. The orthogonal projection of uonto vis given by
projvu=âŸ¨u,vâŸ©
âŸ¨v,vâŸ©v.
Theorem 7.9. Letu,vâˆˆ(V,âŸ¨ âŸ©).
1.Pythagorean Theorem: If uâŠ¥v, then
âˆ¥u+vâˆ¥2=âˆ¥uâˆ¥2+âˆ¥vâˆ¥2.
2.Parallelogram Law:
âˆ¥u+vâˆ¥2+âˆ¥uâˆ’vâˆ¥2= 2âˆ¥uâˆ¥2+ 2âˆ¥vâˆ¥2.
3.Schwarz Inequality:
|âŸ¨u,vâŸ©| â‰¤ âˆ¥ uâˆ¥âˆ¥vâˆ¥.
4.Triangle Inequality:
âˆ¥u+vâˆ¥ â‰¤ âˆ¥uâˆ¥+âˆ¥vâˆ¥.
5.Cauchy Inequality:
âˆ¥uâˆ¥âˆ¥vâˆ¥ â‰¤1
2âˆ¥uâˆ¥2+1
2âˆ¥vâˆ¥2.
Proof.
Pythagorean Theorem: Suppose uâŠ¥v, so that âŸ¨u,vâŸ©=âŸ¨v,uâŸ©= 0. By direct calculation we
have
âˆ¥u+vâˆ¥2=âŸ¨u+v,u+vâŸ©=âŸ¨u,u+vâŸ©+âŸ¨v,u+vâŸ© Axiom IP2
=âŸ¨u,uâŸ©+âŸ¨u,vâŸ©+âŸ¨v,uâŸ©+âŸ¨v,vâŸ© Theorem 7.2(2)245
=âŸ¨u,uâŸ©+âŸ¨v,vâŸ©=âˆ¥uâˆ¥2+âˆ¥vâˆ¥2
Parallelogram Law: We have
âˆ¥u+vâˆ¥2=âŸ¨u,uâŸ©+âŸ¨u,vâŸ©+âŸ¨v,uâŸ©+âŸ¨v,vâŸ©=âˆ¥uâˆ¥2+âŸ¨u,vâŸ©+âŸ¨v,uâŸ©+âˆ¥vâˆ¥2(7.7)
from the proof of the Pythagorean Theorem, and
âˆ¥uâˆ’vâˆ¥2=âŸ¨uâˆ’v,uâˆ’vâŸ©=âˆ¥uâˆ¥2âˆ’ âŸ¨u,vâŸ© âˆ’ âŸ¨v,uâŸ©+âˆ¥vâˆ¥2. (7.8)
Adding equations (7.7) and (7.8) completes the proof.
Schwarz Inequality: Ifu=0orv=0, then by Theorem 7.2(1) we obtain
âˆ¥âŸ¨u,vâŸ©âˆ¥=|0|= 0 = âˆ¥uâˆ¥âˆ¥vâˆ¥,
which affirms the theoremâ€™s conclusion.
Suppose u,vÌ¸=0, and let
c=âŸ¨u,vâŸ©
âŸ¨v,vâŸ©=âŸ¨u,vâŸ©
âˆ¥vâˆ¥2.
Now, by (7.6),
âŸ¨uâˆ’cv, cvâŸ©=câŸ¨uâˆ’cv,vâŸ©=câŸ¨v,uâˆ’cvâŸ©=c(Â¯0) = c(0) = 0 .
Thus uâˆ’cvandcvare orthogonal, and by the Pythagorean Theorem
âˆ¥uâˆ¥2=âˆ¥(uâˆ’cv) +cvâˆ¥2=âˆ¥uâˆ’cvâˆ¥2+âˆ¥cvâˆ¥2.
Hence âˆ¥cvâˆ¥2â‰¤ âˆ¥uâˆ¥2sinceâˆ¥uâˆ’cvâˆ¥2â‰¥0. However, recalling that zÂ¯z=|z|2for any zâˆˆF, we
obtain
âˆ¥cvâˆ¥2=âŸ¨cv, cvâŸ©=cÂ¯câŸ¨v,vâŸ©=|c|2âˆ¥vâˆ¥2=|âŸ¨u,vâŸ©|2
âˆ¥vâˆ¥4âˆ¥vâˆ¥2=|âŸ¨u,vâŸ©|2
âˆ¥vâˆ¥2,
and so âˆ¥cvâˆ¥2â‰¤ âˆ¥uâˆ¥2implies that
|âŸ¨u,vâŸ©|2
âˆ¥vâˆ¥2â‰¤ âˆ¥uâˆ¥2.
Therefore we have
|âŸ¨u,vâŸ©|2â‰¤ âˆ¥uâˆ¥2âˆ¥vâˆ¥2,
and taking the square root of both sides completes the proof.
Triangle Inequality: For any u,vâˆˆVwe have âŸ¨u,vâŸ©=a+bifor some a, bâˆˆR, so that the real
part of âŸ¨u,vâŸ©isRe(âŸ¨u,vâŸ©) =a. (IfVis a vector field over Rthen b= 0, but this will not affect
our analysis.) By the Schwarz Inequality we have
âˆš
a2+b2=|a+bi|=|âŸ¨u,vâŸ©| â‰¤ âˆ¥ uâˆ¥âˆ¥vâˆ¥,
and since
Re 
âŸ¨u,vâŸ©
=aâ‰¤ |a|=âˆš
a2â‰¤âˆš
a2+b2,
it follows that
Re 
âŸ¨u,vâŸ©
â‰¤ âˆ¥uâˆ¥âˆ¥vâˆ¥. (7.9)246
Recalling the property of complex numbers z+ Â¯z= 2 Re( z), we have
âŸ¨u,vâŸ©+âŸ¨v,uâŸ©=âŸ¨u,vâŸ©+âŸ¨u,vâŸ©= 2 Re 
âŸ¨u,vâŸ©
. (7.10)
Now,
âˆ¥u+vâˆ¥2=âˆ¥uâˆ¥2+âŸ¨u,vâŸ©+âŸ¨v,uâŸ©+âˆ¥vâˆ¥2Equation (7.7)
=âˆ¥uâˆ¥2+ 2 Re 
âŸ¨u,vâŸ©
+âˆ¥vâˆ¥2Equation (7.10)
â‰¤ âˆ¥uâˆ¥2+ 2âˆ¥uâˆ¥âˆ¥vâˆ¥+âˆ¥vâˆ¥2, Inequality (7.9)
and so
âˆ¥u+vâˆ¥2â‰¤ 
âˆ¥uâˆ¥+âˆ¥vâˆ¥2.
Taking the square root of both sides completes the proof.
Cauchy Inequality: This inequality in fact holds for all real numbers: if a, bâˆˆR, then
0â‰¤(aâˆ’b)2=a2âˆ’2ab+b2â‡’2abâ‰¤a2+b2â‡’abâ‰¤1
2a2+1
2b2,
and weâ€™re done. â– 
Proposition 7.10. Let(V,âŸ¨ âŸ©)be an inner product space, and let v1, . . . ,vnâˆˆVbe such that
viÌ¸=0for each 1â‰¤iâ‰¤nandviâŠ¥vjwhenever iÌ¸=j. IfvâˆˆVand
ci=âŸ¨v,viâŸ©
âŸ¨vi,viâŸ©
for each 1â‰¤iâ‰¤n, then
vâˆ’nX
i=1civi
is orthogonal to v1, . . . ,vn.
Proof. Fix 1â‰¤kâ‰¤n. Since vkÌ¸= 0 we have âŸ¨vk,vkâŸ© Ì¸= 0. Also âŸ¨vi,vjâŸ©= 0 whenever iÌ¸=j.
Now,*
vâˆ’nX
i=1civi,vk+
=âŸ¨v,vkâŸ© âˆ’*nX
i=1civi,vk+
=âŸ¨v,vkâŸ© âˆ’nX
i=1ciâŸ¨vi,vkâŸ©
=âŸ¨v,vkâŸ© âˆ’ckâŸ¨vk,vkâŸ©=âŸ¨v,vkâŸ© âˆ’âŸ¨v,vkâŸ©
âŸ¨vk,vkâŸ©âŸ¨vk,vkâŸ©
=âŸ¨v,vkâŸ© âˆ’ âŸ¨v,vkâŸ©= 0,
and therefore vâˆ’Pn
k=1ckvkâŠ¥vkfor any 1 â‰¤kâ‰¤n. â– 
Proposition 7.11. Let(V,âŸ¨ âŸ©)be an inner product space, and let v1, . . . ,vnâˆˆVbe such that
viÌ¸=0for each 1â‰¤iâ‰¤nandviâŠ¥vjwhenever iÌ¸=j. IfvâˆˆVandci=âŸ¨v,viâŸ©/âŸ¨vi,viâŸ©for
each 1â‰¤iâ‰¤n, then vâˆ’Xn
i=1civiâ‰¤vâˆ’Xn
i=1aivi
for any a1, . . . , a nâˆˆF.247
Proof. FixvâˆˆVanda1, . . . , a nâˆˆF, and let ci=âŸ¨v,viâŸ©/âŸ¨vi,viâŸ©for each 1 â‰¤iâ‰¤n. First we
observe that for any scalars x1, . . . , x nwe haveD
vâˆ’Xn
k=1ckvk,Xn
i=1xiviE
=Xn
i=1D
vâˆ’Xn
k=1ckvk, xiviE
Theorem 7.2(2)
=Xn
i=1Â¯xiD
vâˆ’Xn
k=1ckvk,viE
Theorem 7.2(3)
=Xn
i=1Â¯xi(0) = 0 , Proposition 7.10
which is to say that vâˆ’Pn
k=1ckvkis orthogonal to any linear combination of the vectors
v1, . . . ,vn. In particular
vâˆ’nX
i=1civiâŠ¥nX
i=1(ciâˆ’ai)vi,
and so by the Pythagorean Theorem
vâˆ’Xn
i=1aivi2
=vâˆ’Xn
i=1civi+Xn
i=1(ciâˆ’ai)vi2
=vâˆ’Xn
i=1civi2
+Xn
i=1(ciâˆ’ai)vi2
â‰¥vâˆ’Xn
i=1civi2
.
Taking square roots completes the proof. â– 
Problems
1. Let ( V,âŸ¨ âŸ©) be an inner product space, and let SâŠ†Vwith SÌ¸=âˆ…. Show that
SâŠ¥={vâˆˆV:âŸ¨v,sâŸ©= 0 for all sâˆˆS}
is a subspace of Veven if Sis not a subspace.248
7.3 â€“ Orthogonal Bases
IfB={v1, . . . ,vn}is a basis for a vector space VandâŸ¨Â·,Â·âŸ©:VÃ—Vâ†’Fis an inner product,
then we refer to Bas a basis for the inner product space ( V,âŸ¨ âŸ©).
Definition 7.12. LetB={v1, . . . ,vn}be a basis for an inner product space (V,âŸ¨ âŸ©). IfviâŠ¥vj
whenever iÌ¸=j, then Bis anorthogonal basis . IfBis an orthogonal basis such that âˆ¥viâˆ¥= 1
for all i, then Bis called an orthonormal basis .
Lemma 7.13. Letv1, . . . ,vnâˆˆ(V,âŸ¨ âŸ©)be nonzero vectors. If viâŠ¥vjwhenever iÌ¸=j, then
v1, . . . ,vnare linearly independent.
Proof. Suppose that viâŠ¥vjwhenever iÌ¸=j. Let x1, . . . , x nâˆˆFand set
x1v1+Â·Â·Â·+xnvn=0. (7.11)
Now, for each 1 â‰¤iâ‰¤n,*nX
k=1xkvk,vi+
=âŸ¨0,viâŸ©= 0.
On the other hand,*nX
k=1xkvk,vi+
=nX
k=1xkâŸ¨vk,viâŸ©=xiâŸ¨vi,viâŸ©.
Hence
xiâŸ¨vi,viâŸ©= 0,
and since viÌ¸=0implies âŸ¨vi,viâŸ© Ì¸= 0, it follows that xi= 0. Therefore (7.11) leads to the
conclusion that x1=Â·Â·Â·=xn= 0, and so v1, . . . ,vnare linearly independent. â– 
Theorem 7.14 (Gram-Schmidt Orthogonalization Process ).LetmâˆˆN. For any nâˆˆN,
if(V,âŸ¨ âŸ©)is an inner product space over Fwith dim(V) =m+n,Wis a subspace of Vwith
orthogonal basis (wi)m
i=1, and
(w1, . . . ,wm,um+1, . . . ,um+n) (7.12)
is a basis for V, then an orthogonal basis for Vis(wi)m+n
i=1, where
wi=uiâˆ’iâˆ’1X
k=1âŸ¨ui,wkâŸ©
âŸ¨wk,wkâŸ©wk (7.13)
for each m+ 1â‰¤iâ‰¤m+n. Moreover,
Span( wi)m+k
i=1= Span( w1, . . . ,wm,um+1, . . . ,um+k) (7.14)
for all 1â‰¤kâ‰¤n.
Note that the existence of vectors um+1, . . . ,um+nâˆˆVsuch that (7.12) is a basis for Vis
assured by Theorem 3.55. Also observe that, since m, nâˆˆNimplies m+nâ‰¥2, the theorem
does not address one-dimensional vector spaces. This is because one-dimensional vector spaces
are not of much interest: any nonzero vector serves as an orthogonal basis!249
Proof. We carry out an argument by induction on nby first considering the case when
n= 1. That is, we let mâˆˆNbe arbitrary, and suppose ( V,âŸ¨ âŸ©) is an inner product space with
dim(V) =m+1,Wis a subspace of Vwith orthogonal basis ( wi)m
i=1, andB= (w1, . . . ,wm,um+1)
is a basis for V. Let
wm+1=um+1âˆ’mX
k=1âŸ¨um+1,wkâŸ©
âŸ¨wk,wkâŸ©wk.
Ifwm+1=0, then
um+1=mX
k=1âŸ¨um+1,wkâŸ©
âŸ¨wk,wkâŸ©wk
obtains, so that um+1âˆˆSpan (wi)m
i=1and by Proposition 3.39 it follows that Bis a linearly
dependent setâ€”a contradiction. Hence wm+1Ì¸=0is assured. Moreover wm+1is orthogonal
tow1, . . . ,wmby Proposition 7.10, implying that wiâŠ¥wjfor all 1 â‰¤i, jâ‰¤m+ 1 such that
iÌ¸=j. Since {w1, . . . ,wm+1}is an orthogonal set of nonzero vectors, by Lemma 7.13 it is also a
linearly independent set. Therefore, by Theorem 3.54, ( wi)m+1
i=1is a basis for Vthat is also an
orthogonal basis. We have proven that the theorem is true in the base case when n= 1.
Next, suppose the theorem is true for some particular nâˆˆN. Fix mâˆˆN, suppose ( V,âŸ¨ âŸ©) is
an inner product space with dim(V) =m+n+ 1,Wis a subspace of Vwith orthogonal basis
(wi)m
i=1, and
B= (w1, . . . ,wm,um+1, . . . ,um+n+1)
is a basis for V. Let Vâ€²=Span (B \ {um+n+1}), which is to say ( Vâ€²,âŸ¨ âŸ©) is an inner product
space with basis
Bâ€²= (w1, . . . ,wm,um+1, . . . ,um+n),
andWis a subspace of Vâ€². Since dim(Vâ€²) =m+n, by our inductive hypothesis we conclude
that ( wi)m+n
i=1, where
wi=uiâˆ’iâˆ’1X
k=1âŸ¨ui,wkâŸ©
âŸ¨wk,wkâŸ©wk
for each m+ 1â‰¤iâ‰¤m+n, is an orthogonal basis for Vâ€².
Now, Vâ€²is a subspace of Vwith orthogonal basis ( wi)m+n
i=1, and
C= (w1, . . . ,wm+n,um+n+1)
is a basis for V. (To substantiate the latter claim use Proposition 3.39 twice: first to find that
um+n+1/âˆˆSpan(Bâ€²) =Vâ€²= Span( wi)m+n
i=1,
and then to find that Cis a linearly independent set. Now invoke Theorem 3.54.) Applying
the base case proven above, only with mreplaced by m+n, we conclude that ( wi)m+n+1
i=1 is an
orthogonal basis for V, where
wm+n+1=um+n+1âˆ’m+nX
k=1âŸ¨um+n+1,wkâŸ©
âŸ¨wk,wkâŸ©wk.
We have now shown that if the theorem holds when mâˆˆNis arbitrary and dim(V) =m+n,
then it holds when mâˆˆNis arbitrary and dim(V) =m+n+ 1. All but the last statement of
the theorem is now proven by the Principle of Induction.250
Finally, to see that (7.14) holds for each 1 â‰¤kâ‰¤n, simply note from (7.13) that each vector
in (wi)m+k
i=1lies in
Span( w1, . . . ,wm,um+1, . . . ,um+k),
and also each vector in
(w1, . . . ,wm,um+1, . . . ,um+k)
lies in Span( wi)m+k
i=1. â– 
Corollary 7.15. If(V,âŸ¨ âŸ©)is an inner product space over Fof dimension nâˆˆN, then it has an
orthonormal basis.
Example 7.16. Give the vector space R3the customary dot product, thereby producing the
inner product space ( R3,Â·). Let
u1=ï£®
ï£°1
1
1ï£¹
ï£»,u2=ï£®
ï£°âˆ’1
1
0ï£¹
ï£»,u3=ï£®
ï£°1
2
1ï£¹
ï£».
ThenB={u1,u2,u3}is a basis for ( R3,Â·). Use the Gram-Schmidt Process to transform Binto
an orthogonal basis for ( R3,Â·), and then find an orthonormal basis for ( R3,Â·).
Solution. Letw1=u1. Then {w1}is an orthogonal basis for the subspace W=Span{w1}.
Certainly WÌ¸=R3, and we already know that {w1,u2,u3}is a basis for R3. Hence we have the
essential ingredients to commence the Gram-Schmidt Process and find vectors w2andw3so
that{w1,w2,w3}constitutes an orthogonal basis for ( R3,Â·). The formula for finding wi(where
i= 2,3) is
wi=uiâˆ’iâˆ’1X
k=1uiÂ·wk
wkÂ·wkwk.
Hence
w2=u2âˆ’u2Â·w1
w1Â·w1w1=ï£®
ï£°âˆ’1
1
0ï£¹
ï£»âˆ’[âˆ’1,1,0]âŠ¤Â·[1,1,1]âŠ¤
[1,1,1]âŠ¤Â·[1,1,1]âŠ¤ï£®
ï£°1
1
1ï£¹
ï£»=ï£®
ï£°âˆ’1
1
0ï£¹
ï£»,
and
w3=u3âˆ’2X
k=1u3Â·wk
wkÂ·wkwk=u3âˆ’u3Â·w1
w1Â·w1w1âˆ’u3Â·w2
w2Â·w2w2
=ï£®
ï£°1
2
1ï£¹
ï£»âˆ’4
3ï£®
ï£°1
1
1ï£¹
ï£»âˆ’1
2ï£®
ï£°âˆ’1
1
0ï£¹
ï£»=ï£®
ï£°1/6
1/6
âˆ’1/3ï£¹
ï£».
(Note: it should not be surprising that w2=u2since u2is in fact already orthogonal to w1.)
We have obtained
{w1,w2,w3}=ï£±
ï£²
ï£³ï£®
ï£°1
1
1ï£¹
ï£»,ï£®
ï£°âˆ’1
1
0ï£¹
ï£»,ï£®
ï£°1/6
1/6
âˆ’1/3ï£¹
ï£»ï£¼
ï£½
ï£¾
as an orthogonal basis for ( R3,Â·).251
To find an orthonormal basis all we need do is normalize the vectors w1,w2andw3. We
have
Ë†w1=w1
âˆ¥w1âˆ¥=1âˆš
3,1âˆš
3,1âˆš
3âŠ¤
,Ë†w2=w2
âˆ¥w2âˆ¥=
âˆ’1âˆš
2,1âˆš
2,0âŠ¤
,
and
Ë†w3=w3
âˆ¥w3âˆ¥=1âˆš
6,1âˆš
6,âˆ’2âˆš
6âŠ¤
.
The set {Ë†w1,Ë†w2,Ë†w3}is an orthonormal basis for ( R3,Â·). â– 
Example 7.17. Recall the vector space P2(R) of polynomial functions of degree at most 2 with
coefficients in R, which here we shall denote simply by P2. Define
âŸ¨p, qâŸ©=Z1
âˆ’1pq
for all p, qâˆˆ P 2. The verification that ( P2,âŸ¨ âŸ©) is an inner product space proceeds in much the
same way as Example 7.4. Apply the Gram-Schmidt Process to transform the standard basis
E={1, x, x2}into an orthonormal basis for ( P2,âŸ¨ âŸ©).
Solution. Letw1= 1, the polynomial function with constant value 1. If W=Span{w1}, then
Wis a subspace of P2such that WÌ¸=P2, and{w1}is an orthogonal basis for W. Starting
withw1, we employ the Gram-Schmidt Process to obtain w2andw3fromu2=xandu3=x2,
respectively. We have
w2=u2âˆ’âŸ¨u2,w1âŸ©
âŸ¨w1,w1âŸ©w1=xâˆ’âŸ¨x,1âŸ©
âŸ¨1,1âŸ©=xâˆ’R1
âˆ’1x dx
R1
âˆ’11dx=xâˆ’0
2=x,
and
w3=u3âˆ’âŸ¨u3,w1âŸ©
âŸ¨w1,w1âŸ©w1âˆ’âŸ¨u3,w2âŸ©
âŸ¨w2,w2âŸ©w2=x2âˆ’âŸ¨x2,1âŸ©
âŸ¨1,1âŸ©âˆ’âŸ¨x2, xâŸ©
âŸ¨x, xâŸ©x
=x2âˆ’R1
âˆ’1x2dx
R1
âˆ’11dxâˆ’R1
âˆ’1x3dx
R1
âˆ’1x2dxx=x2âˆ’1
3,
and so
{w1,w2,w3}=
1, x, x2âˆ’1
3	
is an orthogonal basis for P2.
To find an orthonormal basis we need only normalize the vectors w1,w2andw3. From
âˆ¥w1âˆ¥=p
âŸ¨w1,w1âŸ©=p
âŸ¨1,1âŸ©=qR1
âˆ’11dx=âˆš
2,
âˆ¥w2âˆ¥=p
âŸ¨w2,w2âŸ©=p
âŸ¨x, xâŸ©=qR1
âˆ’1x2dx=q
2
3,
and
âˆ¥w3âˆ¥=p
âŸ¨w3,w3âŸ©=q
x2âˆ’1
3, x2âˆ’1
3
=qR1
âˆ’1 
x2âˆ’1
32dx=q
8
45,
we obtain
Ë†w1=w1
âˆ¥w1âˆ¥=1âˆš
2,Ë†w2=w2
âˆ¥w2âˆ¥=âˆš
6
2x, Ë†w3=w3
âˆ¥w3âˆ¥=âˆš
10
4(3x2âˆ’1).252
The set {Ë†w1,Ë†w2,Ë†w3}, which consists of the first three of what are known as normalized Legendre
polynomials, is an orthonormal basis for ( P2,âŸ¨ âŸ©). â– 
Proposition 7.18. Let(V,âŸ¨ âŸ©)be an inner product space over Fof dimension nâˆˆN, let
B={w1, . . . ,wr,u1, . . . ,us}
be an orthogonal basis for V, and let
W= Span {w1, . . . ,wr}and U= Span {u1, . . . ,us}.
Then U=WâŠ¥,W=UâŠ¥, and
dim(W) + dim( WâŠ¥) = dim( V).
Proof. LetuâˆˆU. Then there exist scalars x1, . . . , x sâˆˆFsuch that
u=sX
i=1xiui.
LetwâˆˆWbe arbitrary, so that
w=rX
j=1yjwj
for scalars y1, . . . , y râˆˆF. Now,
âŸ¨u,wâŸ©=DXs
i=1xiui,wE
=Xs
i=1xiâŸ¨ui,wâŸ© (Axiom IP2)
=Xs
i=1
xiD
ui,Xr
j=1yjwjE
=Xs
i=1
xiXr
j=1âŸ¨ui, yjwjâŸ©
(Theorem 7.2(2))
=Xs
i=1
xiXr
j=1Â¯yjâŸ¨ui,wjâŸ©
(Theorem 7.2(3))
SinceBis an orthogonal basis we have âŸ¨ui,wjâŸ©= 0 for all 1 â‰¤iâ‰¤sand 1 â‰¤jâ‰¤r, so that
âŸ¨u,wâŸ©=sX
i=1rX
j=1xiÂ¯yjâŸ¨ui,wjâŸ©= 0
and therefore uâŠ¥w. Since wâˆˆWis arbitrary, we conclude that uâˆˆWâŠ¥and hence UâŠ†WâŠ¥.
Next, let vâˆˆWâŠ¥. Since Bis a basis for V, there exist scalars x1, . . . , x s, y1, . . . , y râˆˆFsuch
that
v=sX
i=1xiui+rX
j=1yjwj.
Fix 1â‰¤kâ‰¤r. Since ykwkâˆˆWwe have
âŸ¨v, ykwkâŸ©= 0. (7.15)253
On the other hand, since âŸ¨ui,wkâŸ©= 0 for all 1 â‰¤iâ‰¤s, andâŸ¨wj,wkâŸ©= 0 for all jÌ¸=k, we have
âŸ¨v, ykwkâŸ©=sX
i=1xiÂ¯ykâŸ¨ui,wkâŸ©+rX
j=1yjÂ¯ykâŸ¨wj,wkâŸ©=ykÂ¯ykâŸ¨wk,wkâŸ©=|yk|2âŸ¨wk,wkâŸ©.(7.16)
Combining (7.15) and (7.16) yields
|yk|2âŸ¨wk,wkâŸ©= 0,
and since wkÌ¸=0implies that âŸ¨wk,wkâŸ© Ì¸= 0 by Axiom IP4, it follows that yk= 0. We conclude,
then, that
v=sX
i=1xiuiâˆˆU,
and so WâŠ¥âŠ†U. Therefore U=WâŠ¥, and by symmetry W=UâŠ¥.
Finally, since {u1, . . . ,us}is a basis for Uand{w1, . . . ,wr}is a basis for W, we obtain
dim(V) =n=r+s= dim( W) + dim( U) = dim( W) + dim( WâŠ¥),
which completes the proof. â– 
The conclusions of Proposition 7.18 in fact apply to any arbitrary subspace of an inner
product space, as the next theorem establishes.
Theorem 7.19. LetWbe a subspace of an inner product space (V,âŸ¨ âŸ©)overFwithdim(V)âˆˆN.
Then
(WâŠ¥)âŠ¥=W
and
dim(W) + dim( WâŠ¥) = dim( V).
Proof. The proof is trivial in the case when dim(V) = 0, since the only possible subspace is
then{0}. So suppose henceforth that n= dim( V)>0.
IfW={0}, then WâŠ¥=V. Now,
(WâŠ¥)âŠ¥=VâŠ¥={0}=W,
and since dim( {0}) = 0 we have
dim(V) = dim( {0}) + dim( V) = dim( W) + dim( WâŠ¥)
IfW=V, then WâŠ¥={0}and a symmetrical argument to the one above leads to the same
conclusions.
Setm=dim(W), and suppose WÌ¸={0}andWÌ¸=V. Then mâ‰¤nby Theorem 3.56(2),
andmÌ¸=nby Theorem 3.56(3), so that 0 < m < n . Since Wis a nontrivial vector space
in its own right, by Corollary 7.15 it has an orthogonal basis {w1, . . . ,wm}. Since WÌ¸=Vit
follows by Theorem 7.14 that there exist wm+1, . . . ,wnâˆˆVsuch that B={w1, . . . ,wn}is an
orthogonal basis for V. Observing that W= Span {w1, . . . ,wm}and defining
U= Span {wm+1, . . . ,wn},254
by Proposition 7.18 we have U=WâŠ¥,W=UâŠ¥, and
dim(W) + dim( WâŠ¥) = dim( V).
Finally, observe that
(WâŠ¥)âŠ¥=UâŠ¥=W,
which finishes the proof. â– 
The dimension equation in Theorem 7.19 amounts to a generalization of Proposition 4.46
from the setting of real Euclidean vector spaces (equipped specifically with the Euclidean dot
product) to that of abstract inner product spaces over an arbitrary field F.
Example 7.20. As a compelling application of some of the developments thus far, we give a
proof that the row rank of a matrix equals its column rank that is quite different (and shorter)
than the proof given in Â§3.6. Let A= [aij]âˆˆRmÃ—n.
Define the linear mapping L:Rnâ†’RmbyL(x) =Ax, and let a1, . . . ,amâˆˆRnbe such
thataâŠ¤
1, . . . ,aâŠ¤
mare the row vectors of A. Then Nul(L) is a subspace of the inner product space
(Rn,Â·) by Proposition 4.14, and so too is Row( A) = Span {a1, . . . ,am}. Now,
xâˆˆNul(L)â‡”Ax=0â‡”ï£®
ï£¯ï£°aâŠ¤
1x
...
aâŠ¤
mxï£¹
ï£ºï£»=ï£®
ï£°xÂ·a1...
xÂ·amï£¹
ï£»=ï£®
ï£°0
...
0ï£¹
ï£»â‡”xâŠ¥a1, . . . ,xâŠ¥am,
so that
Nul(L) ={xâˆˆRn:xâŠ¥aifor all 1 â‰¤iâ‰¤m}
and by Proposition 7.7 we have Nul( L) = Row( A)âŠ¥. By Theorem 7.19
dim(Row( A)) + dim(Row( A)âŠ¥) = dim( Rn),
whence
row-rank( A) + dim(Nul( L)) =n
and finally
row-rank( A) =nâˆ’dim(Nul( L)).
Next, by Theorem 4.37,
dim(Nul( L)) + dim(Img( L)) = dim( Rn),
and since Img( L) = Col( A) by Proposition 4.35, it follows that
n= dim( Rn) = dim(Nul( L)) + dim(Col( A)) = dim(Nul( L)) + col-rank( A)
and finally
col-rank( A) =nâˆ’dim(Nul( L)).
Therefore
row-rank( A) = col-rank( A) =nâˆ’dim(Nul( L)),
and weâ€™re done. â– 
Proposition 7.21. IfWis a subspace of an inner product space (V,âŸ¨ âŸ©)overF, then
V=WâŠ•WâŠ¥.255
Proof. The situation is trivial in the cases when W={0}orW=V, so suppose Wis a
subspace such that WÌ¸={0}, V. Let dim(W) =manddim(V) =n, and note that 0 < m < n .
Since ( W,âŸ¨ âŸ©) is a nontrivial inner product space, by Corollary 7.15 is has an orthogonal basis
{w1, . . . ,wm}. By Theorem 7.14 there exist wm+1, . . . ,wnâˆˆVsuch that B={w1, . . . ,wn}is
an orthogonal basis for V, and WâŠ¥= Span {wm+1, . . . ,wn}by Proposition 7.18.
LetvâˆˆV. Since Span( B) =V, there exist scalars c1, . . . , c nâˆˆFsuch that
v=nX
k=1ckwk=mX
k=1ckwk+nX
k=m+1ckwk,
and so vâˆˆW+WâŠ¥. Hence VâŠ†W+WâŠ¥, and since the reverse containment is obvious we
have V=W+WâŠ¥.
Suppose that vâˆˆWâˆ©WâŠ¥. From vâˆˆWâŠ¥we have vâŠ¥wfor all wâˆˆW, and since vâˆˆW
it follows that vâŠ¥v. Thus âŸ¨v,vâŸ©= 0, and so v=0by Theorem 7.2(4). Hence Wâˆ©WâŠ¥âŠ† {0},
and since the reverse containment is obvious we have Wâˆ©WâŠ¥={0}.
Since V=W+WâŠ¥andWâˆ©WâŠ¥={0}, we conclude that V=WâŠ•WâŠ¥. â– 
Corollary 7.22. IfWis a subspace of an inner product space (V,âŸ¨ âŸ©)overF, then
dim(WâŠ•WâŠ¥) = dim( W) + dim( WâŠ¥).
Proof. By Proposition 7.21 we have V=WâŠ•WâŠ¥, and thus dim(V) =dim(WâŠ•WâŠ¥). The
conclusion then follows from Theorem 7.19. â– 
The corollary could also be proved quite easily by utilizing Proposition 4.36, which applies
to abstract vector spaces over F.
For the following theorem we take all vectors in Fnto be, as ever, nÃ—1 column matrices
(i.e. column vectors).
Theorem 7.23. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space over F. IfOis an
ordered orthonormal basis for V, then
âŸ¨u,vâŸ©= [v]âˆ—
O[u]O (7.17)
for all u,vâˆˆV.
Proof. The statement of the theorem is clearly true if V={0}, so assume dim(V) =nâˆˆN
and set O= (w1, . . . ,wn). Let u,vâˆˆV, so there exist u1, . . . , u n, v1, . . . , v nâˆˆFsuch that
u=u1w1+Â·Â·Â·+unwnand v=v1w1+Â·Â·Â·+vnwn,
and hence
[u]O=ï£®
ï£°u1...
unï£¹
ï£»and [ v]O=ï£®
ï£°v1...
vnï£¹
ï£».
Now, because Ois orthonormal, âŸ¨wi,wjâŸ©= 0 whenever iÌ¸=j, andâŸ¨wi,wiâŸ©=âˆ¥wiâˆ¥2= 1 for all
i= 1, . . . , n . By Definition 7.1 and Theorem 7.2 we obtain
âŸ¨u,vâŸ©=*nX
i=1uiwi,nX
j=1vjwj+
=nX
i=1nX
j=1uiÂ¯vjâŸ¨wi,wjâŸ©256
=nX
i=1uiÂ¯viâŸ¨wi,wiâŸ©=nX
i=1uiÂ¯vi= [v]âˆ—
O[u]O,
as desired. â– 
In the case when F=Rwe find that [ v]âˆ—
O= [v]âŠ¤
O, since the components of [ v]Oare all real
numbers, and thus we readily obtain the following.
Corollary 7.24. If(V,âŸ¨ âŸ©)is an inner product space over R, and O= (w1, . . . ,wn)is an
ordered orthonormal basis for V, then
âŸ¨u,vâŸ©= [v]âŠ¤
O[u]O
for all u,vâˆˆV.
In Theorem 7.23, let Ï†O:Vâ†’Fndenote the O-coordinate map, so that
Ï†O(v) = [v]O
for all vâˆˆV, and then (7.17) may be written as
âŸ¨u,vâŸ©=Ï†O(u)Â·Ï†O(v),
recalling Definition 7.3. Now, if âˆ¥ Â· âˆ¥ Vdenotes the norm in Vandâˆ¥ Â· âˆ¥Fnthe norm in Fn, then
âˆ¥vâˆ¥V=p
âŸ¨v,vâŸ©=p
Ï†O(v)Â·Ï†O(v) =âˆ¥Ï†O(v)âˆ¥Fn (7.18)
for all vâˆˆV. In fact, if dVanddFnare the distance functions on VandFn, respectively, so
that for any u,vâˆˆVandx,yâˆˆFnwe have
dV(u,v) =âˆ¥uâˆ’vâˆ¥Vand dFn(x,y) =âˆ¥xâˆ’yâˆ¥Fn,
then it follows from (7.18) that
dV(u,v) =âˆ¥uâˆ’vâˆ¥V=âˆ¥Ï†O(uâˆ’v)âˆ¥Fn=âˆ¥Ï†O(u)âˆ’Ï†O(v)âˆ¥Fn=dFn(Ï†O(u), Ï†O(v)),(7.19)
recalling that Ï†Ois an isomorphism.
Equation (7.18) exhibits a property of the mapping Ï†Othat is called norm-preserving ,
and equation (7.19) exhibits the distance-preserving property of Ï†O.
Definition 7.25. Let(U,âŸ¨ âŸ©U)and(V,âŸ¨ âŸ©V)be inner product spaces, and let âˆ¥ Â· âˆ¥ Uandâˆ¥ Â· âˆ¥ V
denote the norms on UandVinduced by the inner products âŸ¨ âŸ©UandâŸ¨ âŸ©V, respectively. A linear
mapping L:Uâ†’Vis an isometry if it is norm-preserving; that is,
âˆ¥uâˆ¥U=âˆ¥L(u)âˆ¥V
for all uâˆˆU. IfLis also an isomorphism, then (U,âŸ¨ âŸ©U)and(V,âŸ¨ âŸ©V)are said to be isomet-
rically isomorphic .
Thus we see that the mapping Ï†Ois an isometry as well as an isomorphism, where it must
not be forgotten that Orepresents an orthonormal basis for an inner product space ( V,âŸ¨ âŸ©) over
Fof dimension nâ‰¥1. By Corollary 7.15 every such inner product space admits an orthonormal
basis, and so must be isometrically isomorphic to ( Fn,Â·).257
Problems
1.LetR2have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2}into an orthonormal basis.
(a)u1= [1,âˆ’3]âŠ¤,u2= [2,2]âŠ¤.
(b)u1= [1,0]âŠ¤,u2= [3,âˆ’5]âŠ¤.
2.LetR3have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2,u3}into an orthonormal basis.
(a)u1= [1,1,1]âŠ¤,u2= [âˆ’1,1,0]âŠ¤,u3= [1,2,1]âŠ¤.
(b)u1= [1,0,0]âŠ¤,u2= [3,7,âˆ’2]âŠ¤,u3= [0,4,1]âŠ¤.
3.LetR4have the Euclidean inner product. Use the Gram-Schmidt Process to transform the
basis{u1,u2,u3,u4}into an orthonormal basis:
u1=ï£®
ï£¯ï£¯ï£°0
2
1
0ï£¹
ï£ºï£ºï£»,u2=ï£®
ï£¯ï£¯ï£°1
âˆ’1
0
0ï£¹
ï£ºï£ºï£»,u3=ï£®
ï£¯ï£¯ï£°1
2
0
âˆ’1ï£¹
ï£ºï£ºï£»,u4=ï£®
ï£¯ï£¯ï£°1
0
0
1ï£¹
ï£ºï£ºï£».
4. Let Wbe the subspace of R4spanned by the vectors
v1=ï£®
ï£¯ï£¯ï£°1
0
1
0ï£¹
ï£ºï£ºï£»,v2=ï£®
ï£¯ï£¯ï£°3
0
2
0ï£¹
ï£ºï£ºï£»,v3=ï£®
ï£¯ï£¯ï£°2
1
âˆ’1
3ï£¹
ï£ºï£ºï£».
(a)Beginning with the vector v1, use the Gram-Schmidt Orthogonalization Process to obtain
an orthogonal basis for W.
(b) Find an orthonormal basis for W.
5. Consider the matrix
A=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°1 2 5
âˆ’1 1 âˆ’4
âˆ’1 4 âˆ’3
1âˆ’4 7
1 2 1ï£¹
ï£ºï£ºï£ºï£ºï£».
Letu1,u2,u3denote the column vectors of A.
(a) Show that {u1,u2,u3}is a basis for Col( A).
(b) Find an orthogonal basis for Col( A).
(c) Find an orthonormal basis for Col( A).258
(d)Letting r1, . . . ,r5âˆˆR3denote the row vectors of A, find a basis for Row(A) of the form
R={râŠ¤
1,râŠ¤
i,râŠ¤
j}, where 1 < i < j â‰¤5 are such that iandjare as small as possible.7
(e) Use the basis Rfound in part (d) to obtain an orthogonal basis for Row( A).
(f) Find an orthonormal basis for Row( A).
7This ensures that there is only one possible answer.259
7.4 â€“ Quadratic Forms
Recall from Â§7.1 that the vector space Cntogether with the operation given by
wÂ·z=zâˆ—w
forw,zâˆˆCnis an inner product space over C(and the product itself is called the hermitian
inner product). For the conjugate transpose operation zâˆ—=zâŠ¤we find that if zhas only
real-valued entries (so that zâˆˆRn) then zâˆ—=zâŠ¤. The norm of zis
âˆ¥zâˆ¥=âˆšzÂ·z=âˆš
zâˆ—z. (7.20)
For the statement and proof of the next theorem recall that the standard form for elements
ofCnisx+iy, where x,yâˆˆRn. In particular if z=x+iyâˆˆCforx, yâˆˆR, then z=zimplies
zis real:
z=zâ‡’xâˆ’iy=x+iyâ‡’2iy= 0â‡’y= 0â‡’z=x.
Theorem 7.26. All eigenvalues of a real symmetric matrix Aare real, and if x+iyâˆˆCnis a
complex eigenvector corresponding to Î», then either xoryis a real eigenvector corresponding to
Î».
Proof. Suppose AâˆˆSymn(R), soA=Asince Ais real and AâŠ¤=Asince Ais symmetric,
and thus Aâˆ—=A. Let Î»be an eigenvalue of Awith corresponding eigenvector zâˆˆCn, sozÌ¸=0
is such that Az=Î»z. Now,
zâˆ—Az=zâˆ—Î»z=Î»(zâˆ—z) =Î»âˆ¥zâˆ¥2,
and since âˆ¥zâˆ¥>0 by Axiom IP4, we may write
Î»=zâˆ—Az
âˆ¥zâˆ¥2.
As a 1 Ã—1 matrix Î»is symmetric, so that
Î»= (Î»)âŠ¤=Î»âˆ—=zâˆ—Az
âˆ¥zâˆ¥2âˆ—
=zâˆ—Aâˆ—(zâˆ—)âˆ—
âˆ¥zâˆ¥2=zâˆ—Az
âˆ¥zâˆ¥2=Î»,
and hence Î»is real.
Next, zâˆˆCnimplies z=x+iyforx,yâˆˆRn, and then from Az=Î»zwe obtain
Ax+iAy=Î»x+iÎ»y.
Since the entries of Aare real and Î»is real, it follows that
Ax=Î»xand Ay=Î»y.
Now, because zÌ¸=0, either xÌ¸=0oryÌ¸=0. Therefore either xoryis a real eigenvector of A
corresponding to Î». â– 260
LetaijâˆˆRfor all 1 â‰¤i, jâ‰¤n. A function f:Rnâ†’Rgiven by
f(x) =nX
i=1nX
j=1aijxixj. (7.21)
for each x= [x1, . . . , x n]âŠ¤is called a quadratic form onRn. An example of a quadratic form
onR2is
f(x, y) = 2 x2+ 10xyâˆ’2y2.
Letting
x=
x
y
and A=
2 5
5âˆ’2
,
it is easy to check that f(x) =xâŠ¤Axif we identify the 1 Ã—1 matrix xâŠ¤Axwith its scalar entry.
The fact that Ais a symmetric real matrix here is not an accident: any quadratic form on Rn
may be written in the form xâŠ¤Axfor some AâˆˆSymn(R).
Definition 7.27. IfAâˆˆSymn(R), then the quadratic form associated with Ais the
function QA:Rnâ†’Rgiven by
QA(x) =xâŠ¤Ax
for all xâˆˆRn.
Again we note that, formally, xâŠ¤Axis a 1Ã—1 matrix, but the natural isomorphism [ c]7â†’c
is implicitly in play in Definition 7.27 so that QA(x) is a real number.
Example 7.28. Any quadratic form in R2may be written as
f(x, y) =ax2+ 2bxy+cy2
fora, b, câˆˆR. We wish to find a real symmetric 2 Ã—2 matrix Asuch that QA=fonR2. We
have
f(x, y) = (ax2+bxy) + (bxy+cy2) = (ax+by)x+ (bx+cy)y
=ax+by bx +cy
x
y
=x y
a b
b c
x
y
,
which shows that fis the quadratic form associated with
A=
a b
b c
.
â– 
Example 7.29. Let
A=ï£®
ï£°3âˆ’1 2
âˆ’1 1 4
2 4 âˆ’2ï£¹
ï£»and x=ï£®
ï£°x
y
zï£¹
ï£».
Then
QA(x) =x y zï£®
ï£°3âˆ’1 2
âˆ’1 1 4
2 4 âˆ’2ï£¹
ï£»ï£®
ï£°x
y
zï£¹
ï£»=x y zï£®
ï£°3xâˆ’y+ 2z
âˆ’x+y+ 4z
2x+ 4yâˆ’2zï£¹
ï£»261
=x(3xâˆ’y+ 2z) +y(âˆ’x+y+ 4z) +z(2x+ 4yâˆ’2z)
= 3x2âˆ’2xy+ 4xz+y2+ 8yzâˆ’2z2
is the quadratic form associated with A.
More generally, if
A=ï£®
ï£°a b c
b d e
c e fï£¹
ï£»
then
QA(x) =ax2+ 2bxy+ 2cxz+dy2+ 2eyz+fz2(7.22)
is the associated quadratic form. â– 
FornâˆˆNdefine Snto be the set of all unit vectors in the vector space Rn+1with respect to
the Euclidean dot product:
Sn={xâˆˆRn+1:âˆ¥xâˆ¥= 1}=ï£±
ï£²
ï£³ï£®
ï£°x1...
xn+1ï£¹
ï£»âˆˆRn+1:n+1X
k=1x2
k= 1ï£¼
ï£½
ï£¾.
The set Snmay be referred to as the n-sphere or the ( n-dimensional) unit sphere .8Ifn= 1
we obtain a circle centered at âŸ¨0,0âŸ©,
S1=
x
y
âˆˆR2:x2+y2= 1
,
and if n= 2 we obtain a sphere with center âŸ¨0,0,0âŸ©,
S2=ï£±
ï£²
ï£³ï£®
ï£°x
y
zï£¹
ï£»âˆˆR2:x2+y2+z2= 1ï£¼
ï£½
ï£¾.
The next proposition establishes an important property of the quadratic forms of symmetric
matrices that have, in particular, real-valued entries. It depends on a fact from analysis, not
proven here, that if f:SâŠ†Rnâ†’Ris a continuous function and Sis a closed and bounded set,
then fattains a maximum value on S. That is, there exists some x0âˆˆSsuch that
f(x0) = max {f(x) :xâˆˆS}.
Certainly Snâˆ’1, as a subset of Rn, is closed and bounded with respect to the Euclidean dot
product. Also a cursory examination of (7.21) should make it clear that, for any AâˆˆRnÃ—n,
the function QAis a polynomial function. Hence QAis continuous on Rnwith respect to the
Euclidean dot product, which easily implies that QAis continuous on Snâˆ’1âŠ†Rn.
8It makes no difference whether we regard the elements of Snas vectors or points. For consistencyâ€™s sake we
keep on with the â€œvector interpretationâ€ here, but later will make occasional use of the â€œpoint interpretationâ€ to
aid intuitive understanding.262
Definition 7.30. LetUâŠ†Rbe an open set, and let f:Uâ†’Rnbe given by
f(t) =ï£®
ï£°f1(t)
...
fn(t)ï£¹
ï£»,
where fk:Uâ†’Rfor each 1â‰¤kâ‰¤n. If the derivatives fâ€²
1(t0), . . . , fâ€²
n(t0)are defined at t0âˆˆU,
then the derivative of the vector-valued function fatt0is
fâ€²(t0) =ï£®
ï£°fâ€²
1(t0)
...
fâ€²
n(t0)ï£¹
ï£».
Since all the eigenvalues of a symmetric real matrix Aare real by Theorem 7.26, it makes
sense to speak of the â€œsmallestâ€ and â€œlargestâ€ eigenvalue of A, as in the next theorem.
Theorem 7.31. Suppose AâˆˆSymn(R), and let Î»minandÎ»maxbe the smallest and largest
eigenvalues of A, respectively.
1.If
QA(v1) = max {QA(x) :xâˆˆSnâˆ’1}and QA(v2) = min {QA(x) :xâˆˆSnâˆ’1},
thenv1andv2are eigenvectors of A.
2.For all xâˆˆSnâˆ’1,
Î»minâ‰¤xâŠ¤Axâ‰¤Î»max.
3.ForxâˆˆRnsuch that âˆ¥xâˆ¥= 1,xâŠ¤Ax=Î»max(resp. Î»min) iffxis an eigenvector of A
corresponding to Î»max(resp. Î»min).
Proof.
Proof of (1). Define UâŠ†Rnto be the set
U={uâˆˆRn:uÂ·v1= 0}.
Sinceâˆ¥v1âˆ¥= 1 implies that v1Ì¸=0, by Example 4.39 we find that Uis a subspace of Rnand
dim(U) =nâˆ’1. By Proposition 4.46
dim(UâŠ¥) = dim( Rn)âˆ’dim(U) =nâˆ’(nâˆ’1) = 1 ,
and since clearly v1âˆˆUâŠ¥and{v1}is a linearly independent set, it follows by Theorem 3.54(1)
that{v1}is a basis for UâŠ¥. Hence
UâŠ¥= Span( v1) ={cv1:câˆˆR}.
FixuâˆˆUsuch that âˆ¥uâˆ¥= 1, and define the vector-valued function f:Râ†’Rnby
f(t) = sin( t)u+ cos( t)v1.
Since v1Â·v1=âˆ¥v1âˆ¥2= 1,uÂ·u=âˆ¥uâˆ¥2= 1, and uÂ·v1= 0, we find that
âˆ¥f(t)âˆ¥2=f(t)Â·f(t) = 
sin(t)u+ cos( t)v1
Â· 
sin(t)u+ cos( t)v1
= sin2(t)uÂ·u+ 2 cos( t) sin(t)uÂ·v1+ cos2(t)v1Â·v1263
= sin2(t) + cos2(t) = 1 ,
and so f(t)âˆˆSnâˆ’1for all tâˆˆR. That is, the function fcan be regarded as defining a curve on
the unit sphere Snâˆ’1, and f(0) = v1shows that the curve passes through the point v1. Letting
u=ï£®
ï£°u1...
unï£¹
ï£»and v1=ï£®
ï£°v1...
vnï£¹
ï£»
we have
f(t) =ï£®
ï£°u1sin(t) +v1cos(t)
...
unsin(t) +vncos(t)ï£¹
ï£»,
and so by definition
fâ€²(t) =ï£®
ï£°u1cos(t)âˆ’v1sin(t)
...
uncos(t)âˆ’vnsin(t)ï£¹
ï£»= cos( t)uâˆ’sin(t)v1.
Now, letting g=QAâ—¦fand defining the function Afby (Af)(t) =Af(t) for tâˆˆR, we have
g(t) =QA(f(t)) =f(t)âŠ¤Af(t) =f(t)Â·Af(t) =f(t)Â·(Af)(t).
By the Product Rule of dot product differentiation,
gâ€²(t) =fâ€²(t)Â·(Af)(t) +f(t)Â·(Af)â€²(t) =fâ€²(t)Â·Af(t) +f(t)Â·Afâ€²(t)
=fâ€²(t)âŠ¤Af(t) +f(t)âŠ¤Afâ€²(t). (7.23)
Since f(t)âŠ¤Afâ€²(t) is a scalar it equals its own transpose, and so by Proposition 2.13 and the fact
thatAâŠ¤=Awe obtain
f(t)âŠ¤Afâ€²(t) = 
f(t)âŠ¤Afâ€²(t)âŠ¤=fâ€²(t)âŠ¤AâŠ¤f(t) =fâ€²(t)âŠ¤Af(t).
Combining this result with (7.23) yields
gâ€²(t) = 2fâ€²(t)âŠ¤Af(t). (7.24)
Because the function fmaps from RtoSnâˆ’1, the function QA:Snâˆ’1â†’Rhas a maximum at
v1âˆˆSnâˆ’1, and
g(0) = QA(f(0)) = QA(v1),
it follows that the function g:Râ†’Rhas a local maximum at t= 0. Thus, since gâ€²(0) exists, it
further follows by Fermatâ€™s Theorem in Â§4.1 of the Calculus Notes thatgâ€²(0) = 0. From (7.24)
we have
uÂ·Av 1=uâŠ¤Av 1=fâ€²(0)âŠ¤Af(0) = 0 ,
and since uâˆˆUis arbitrary we conclude that Av 1âŠ¥ufor all uâˆˆU. Therefore
Av 1âˆˆUâŠ¥={xâˆˆRn:xâŠ¥ufor all uâˆˆU}= Span( v1),
and so there must exist some Î»âˆˆRsuch that Av 1=Î»v1. Since v1âˆˆRnis nonzero, we
conclude that v1is an eigenvector of A. The proof that v2âˆˆSnâˆ’1is also an eigenvector of Ais264
much the same.
Proof of (2). By the previous result, letting Î»1(resp. Î»2) be the eigenvalue of Acorresponding
tov1(resp. v2), we have
xâŠ¤Ax=QA(x)â‰¤QA(v1) =vâŠ¤
1Av 1=vâŠ¤
1(Î»1v1) =Î»1(vâŠ¤
1v1) =Î»1â‰¤Î»max
and
xâŠ¤Ax=QA(x)â‰¥QA(v2) =vâŠ¤
2Av 2=vâŠ¤
2(Î»2v2) =Î»2(vâŠ¤
2v2) =Î»2â‰¥Î»min
for any xâˆˆSnâˆ’1.
Proof of (3). We provide only the proof of the statement concerning Î»max, since the proof of the
other statement is similar. Let xâˆˆRnbe such that âˆ¥xâˆ¥= 1.
Suppose xâŠ¤Ax=Î»max. Then
QA(x) = max {QA(u) :uâˆˆSnâˆ’1}
by part (2), and it follows by part (1) that xis an eigenvector of A. Let Î»be the eigenvalue of
Acorresponding to x. We now have
Î»max=xâŠ¤Ax=xâŠ¤(Î»x) =Î»xâŠ¤x=Î»,
and so xis an eigenvector of Acorresponding to Î»max.
For the converse, suppose xis an eigenvector of Acorresponding to Î»max. Then
xâŠ¤Ax=xâŠ¤(Î»maxx) =Î»maxxâŠ¤x=Î»max,
and the proof is done. â– 
Example 7.32. Find the maximum and minimum value of the function Ï†:R3â†’Rgiven by
Ï†(x, y, z ) =x2âˆ’4xy+ 4y2âˆ’4yz+z2(7.25)
on the unit sphere S2.
Solution. Comparing (7.25) to equation (7.22) in Example 7.29, we see we have a= 1,b=âˆ’2,
c= 0,d= 4,e=âˆ’2, and f= 1. Thus the function Ï†is the quadratic form associated with the
matrix
A=ï£®
ï£°a b c
b d e
c e fï£¹
ï£»=ï£®
ï£°1âˆ’2 0
âˆ’2 4 âˆ’2
0âˆ’2 1ï£¹
ï£».
The characteristic polynomial of Ais
PA(t) = det( Aâˆ’tI3) =1âˆ’tâˆ’2 0
âˆ’2 4âˆ’tâˆ’2
0âˆ’2 1âˆ’t
= (âˆ’1)1+1(1âˆ’t)4âˆ’tâˆ’2
âˆ’2 1âˆ’t+ (âˆ’1)1+2(âˆ’2)âˆ’2âˆ’2
0 1âˆ’t
=âˆ’t3+ 6t2âˆ’tâˆ’4,265
and so
PA(t) = 0 â‡”t3âˆ’6t2+t+ 4 = 0 .
By the Rational Zeros Theorem of algebra, the only rational numbers that may be zeros of
PAareÂ±1,Â±2, and Â±4. It happens that 1 is in fact a zero, and so by the Factor Theorem of
algebra tâˆ’1 must be a factor of PA(t). Now,
t3âˆ’6t2+t+ 4
tâˆ’1=t2âˆ’5tâˆ’4,
whence we obtain
PA(t) = 0 â‡’(tâˆ’1)(t2âˆ’5tâˆ’4) = 0 â‡’t= 1 or t2âˆ’5tâˆ’4 = 0 ,
and so PA(t) = 0 implies that
tâˆˆ(
5 +âˆš
41
2,5âˆ’âˆš
41
2,1)
.
By Theorem 6.18 the eigenvalues of Aare
Î»1=5 +âˆš
41
2, Î» 2=5âˆ’âˆš
41
2, Î» 3= 1,
so by Theorem 7.31 the maximum value of Ï†onS2isÎ»1(approximately 5 .702) and the minimum
value is Î»2(approximately âˆ’0.702). â– 
Example 7.33. Find the maximum and minimum value of the function
f(x, y) =x2+xy+ 2y2
on the ellipse x2+ 3y2= 16.
Solution. We effect a change of variables so that, in terms of the new variables, the ellipse
becomes a unit circle. In particular we declare uandvto be such that 4 u=xand 4 v/âˆš
3=y.266
8
Operator Theory
8.1 â€“ The Adjoint of a Linear Operator
Many of the results developed in this chapter are of a technical nature which will be pressed
into service in due course to uncover some of the most wondrous and practical properties of
finite-dimensional vector spaces and the linear mappings between them.
Definition 8.1. Let(V,âŸ¨ âŸ©V)and(W,âŸ¨ âŸ©W)be inner product spaces over the field F, and let
Lâˆˆ L(V, W ). The adjoint ofLis the mapping Lâˆ—âˆˆ L(W, V )satisfying
âŸ¨L(v),wâŸ©W=âŸ¨v, Lâˆ—(w)âŸ©V
for all vâˆˆVandwâˆˆW.
Theorem 8.2. Let(V,âŸ¨ âŸ©V)and(W,âŸ¨ âŸ©W)be inner product spaces over F. For every Lâˆˆ L(V, W )
there exists a unique adjoint Lâˆ—âˆˆ L(W, V ).
Given an inner product space ( V,âŸ¨ âŸ©) and an operator Lâˆˆ L(V), the adjoint of Lis the
unique operator Lâˆ—âˆˆ L(V) satisfying
âŸ¨L(u),vâŸ©=âŸ¨u, Lâˆ—(v)âŸ© (8.1)
for all u,vâˆˆV.
Proposition 8.3. Let(V,âŸ¨ âŸ©)be an inner product space over F. IfL,Ë†Lâˆˆ L(V)andcâˆˆF,
then
1. (cL)âˆ—= Â¯cLâˆ—
2. (Lâˆ—)âˆ—=L
3. (L+Ë†L)âˆ—=Lâˆ—+Ë†Lâˆ—
4. (Lâ—¦Ë†L)âˆ—=Ë†Lâˆ—â—¦Lâˆ—
Proof.
Proof of Part (2). Letu,vâˆˆVbe arbitrary. By definition we have
âŸ¨L(v),uâŸ©=âŸ¨v, Lâˆ—(u)âŸ©,267
and thus
âŸ¨Lâˆ—(u),vâŸ©=âŸ¨u, L(v)âŸ©.
This shows that Lis the adjoint of Lâˆ—; that is, L= (Lâˆ—)âˆ—.
Proof of Part (4). Letu,vâˆˆV. By definition

(Lâ—¦Ë†L)(u),v
=
u,(Lâ—¦Ë†L)âˆ—(v)
(8.2)
and
âŸ¨Ë†L(u),vâŸ©=âŸ¨u,Ë†Lâˆ—(v)âŸ©. (8.3)
Substituting Ë†L(u) foruin (8.1), and Lâˆ—(v) forvin (8.3), we obtain

L(Ë†L(u)),v
=âŸ¨Ë†L(u), Lâˆ—(v)âŸ©and âŸ¨Ë†L(u), Lâˆ—(v)âŸ©=
u,Ë†Lâˆ—(Lâˆ—(v))
,
and hence
(Lâ—¦Ë†L)(u),v
=
u,(Ë†Lâˆ—â—¦Lâˆ—)(v)
.
Comparing this equation with (8.2), and recalling that u,vâˆˆVare arbitrary, we see that both
(Lâ—¦Ë†L)âˆ—andË†Lâˆ—â—¦Lâˆ—are adjoints of Lâ—¦Ë†L. Since the adjoint of a linear operator is unique, we
conclude that
(Lâ—¦Ë†L)âˆ—=Ë†Lâˆ—â—¦Lâˆ—
as desired.
Proofs of the other parts of the proposition are left as exercises. â– 
Definition 8.4. LetAâˆˆFmÃ—n. The adjoint (orconjugate transpose ) ofAis the matrix
Aâˆ—âˆˆFnÃ—mgiven by Aâˆ—= 
AâŠ¤.
IfA= [aij]mÃ—n, then the ij-entry of Aâˆ—is [Aâˆ—]ij=aji. It is an easy matter to verify that
 
AâŠ¤=(AâŠ¤)
(that is, the transpose of Ais the same as the conjugate of AâŠ¤), so there would be no ambiguity
if we were to write simply AT. Hence,
Aâˆ—=AâŠ¤=(AâŠ¤) = 
AâŠ¤. (8.4)
Also we define
Aâˆ—âˆ—= (Aâˆ—)âˆ—.
Proposition 8.5. IfA,BâˆˆFnÃ—nandcâˆˆF, then
1. (cA)âˆ—= Â¯cAâˆ—
2.Aâˆ—âˆ—=A
3. (A+B)âˆ—=Aâˆ—+Bâˆ—
4. (AB)âˆ—=Bâˆ—Aâˆ—268
Proof.
Proof of Part (2). Liberal use of equation (8.4) is prescribed here, as well as properties of the
transpose and conjugation operations established in chapters 2 and 6, respectively. We have
Aâˆ—=(AâŠ¤), and so Aâˆ—=AâŠ¤. Now,
Aâˆ—âˆ—= (Aâˆ—)âˆ—= 
Aâˆ—âŠ¤= (AâŠ¤)âŠ¤=A,
as desired.
Proof of Part (4). For this we must recall Proposition 2.13 as well as equation (8.4):
(AB)âˆ—= 
ABâŠ¤=(AB)âŠ¤=BâŠ¤AâŠ¤=BâŠ¤AâŠ¤= 
BâŠ¤ 
AâŠ¤=Bâˆ—Aâˆ—.
Proofs of the other parts of the proposition are left as exercises. â– 
Theorem 8.6. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space with ordered orthonormal
basisO, and let Î›, Lâˆˆ L(V). Then Î› =Lâˆ—if and only if [Î›]O= [L]âˆ—
O.
Proof. Let [ ] represent [ ] Ofor simplicity. Suppose that Î› âˆˆ L(V) is such that [Î›] = [ L]âˆ—,
where
[Î›] = [ L]âˆ—â‡”[Î›] = [L]âŠ¤â‡”[Î›] = [ L]âŠ¤.
Now, for any u,vâˆˆVwe have, by Theorem 7.23,
âŸ¨L(u),vâŸ©= [v]âˆ—[L(u)] = [v]âˆ— 
[L][u]
= 
[v]âˆ—[L]
[u]
= 
[v]âˆ—[Î›]âˆ—
[u] = 
[Î›][v]âˆ—[u] = [Î›( v)]âˆ—[u] =âŸ¨u,Î›(v)âŸ©,
and therefore Î› = Lâˆ—. â– 
With this theorem we have a way of finding the adjoint of a linear operator: given an
operator Lâˆˆ L(V), find an orthonormal basis OforV(perhaps using the Gram-Schmidt
Orthogonalization Process), then determine [ L]O(the matrix corresponding to Lwith respect
toO) using Corollary 4.21, and then obtain [ L]âˆ—
Oby taking the conjugate of the transpose of
[L]O. The matrix [ L]âˆ—
Odefines a new operator Lâˆ—âˆˆ L(V) that will in fact be the adjunct of L.269
8.2 â€“ Self-Adjoint and Unitary Operators
Definition 8.7. Let(V,âŸ¨ âŸ©)be an inner product space over F. A linear operator Lâˆˆ L(V)is
self-adjoint with respect to the inner product âŸ¨ âŸ©ifLâˆ—=L. A matrix AâˆˆFnÃ—nisself-adjoint
ifAâˆ—=A.
Observe that if AâˆˆRnÃ—n, then Ais self-adjoint if and only if A=AâŠ¤, since
A=Aâˆ—=AâŠ¤=AâŠ¤.
That is, â€œself-adjointâ€ and â€œsymmetricâ€ mean the same thing in the context of matrices with
real-valued entries. It is for this reason that a self-adjoint operator on an inner product space
over specifically the field Rmay also be called a symmetric operator. (Meanwhile, physicists
especially are fond of calling a self-adjoint operator on an inner product space over Cahermitian
operator.)
Theorem 8.8. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V). Then Lis
self-adjoint if and only if
âŸ¨L(u),vâŸ©=âŸ¨u, L(v)âŸ©
for all u,vâˆˆV.
Proof. Suppose that Lis self-adjoint, so that Lâˆ—=L. From (8.1) we have
âŸ¨L(u),vâŸ©=âŸ¨u, Lâˆ—(v)âŸ©=âŸ¨u, L(v)âŸ©
for all u,vâˆˆV, as desired.
Now suppose that
âŸ¨L(u),vâŸ©=âŸ¨u, L(v)âŸ© (8.5)
for all u,vâˆˆV. By Theorem 8.2, Lâˆ—is the unique linear operator on Vfor which
âŸ¨L(u),vâŸ©=âŸ¨u, Lâˆ—(v)âŸ© (8.6)
holds for all u,vâˆˆV. Comparing (8.5) and(8.6), it is clear that Lâˆ—=L, and therefore Lis
self-adjoint. â– 
The following proposition more firmly establishes the connection between the concepts of
self-adjoint operators and self-adjoint matrices.
Theorem 8.9. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space over Fwith ordered
orthonormal basis O, and let Lâˆˆ L(V). Then Lis a self-adjoint operator if and only if
[L]âˆ—
O= [L]O.
Proof. LetO= (w1, . . . ,wn), and let [ ] represent [ ] Ofor simplicity. Suppose that Lis
self-adjoint. By definition [ L]âˆˆFnÃ—n, the matrix corresponding to Lwith respect to O, satisfies
[L][v] = [L(v)] (8.7)270
for all vâˆˆV. By Corollary 4.21
[L] =h
L(w1)
Â·Â·Â·
L(wn)i
,
and so for any vâˆˆV
[L]âŠ¤[v] =ï£®
ï£¯ï£°[L(w1)]âŠ¤
...
[L(wn)]âŠ¤ï£¹
ï£ºï£»[v] =ï£®
ï£¯ï£¯ï£°[L(w1)]âŠ¤[v]
...
[L(wn)]âŠ¤[v]ï£¹
ï£ºï£ºï£»=ï£®
ï£¯ï£°âŸ¨L(w1),vâŸ©
...
âŸ¨L(wn),vâŸ©ï£¹
ï£ºï£»
=ï£®
ï£¯ï£°âŸ¨w1, L(v)âŸ©
...
âŸ¨wn, L(v)âŸ©ï£¹
ï£ºï£»=ï£®
ï£¯ï£¯ï£°[w1]âŠ¤[L(v)]
...
[wn]âŠ¤[L(v)]ï£¹
ï£ºï£ºï£»=ï£®
ï£¯ï£°[w1]âŠ¤
...
[wn]âŠ¤ï£¹
ï£ºï£»[L(v)],
where the third and fifth equalities follow from Theorem 7.23, and the fourth equality is owing
toLbeing self-adjoint. But the nÃ—nmatrixï£®
ï£¯ï£°[w1]âŠ¤
...
[wn]âŠ¤ï£¹
ï£ºï£»âˆˆRnÃ—n
is the identity matrix In, and so we obtain
[L]âŠ¤[v] =[L(v)]. (8.8)
Taking the conjugate of both sides of (8.8) then yields the equation
[L]âŠ¤[v] = [L(v)] (8.9)
for all vâˆˆV. From (8.7) and(8.9) we conclude that [ L] and [L]âŠ¤are matrices corresponding
toLwith respect to O. By Corollary 4.25 the matrix corresponding to Lwith respect to Ois
unique, and therefore it must be that
[L] =[L]âŠ¤= [L]âˆ—
as desired.
For the converse, suppose that [ L] = [L]âˆ—. We have
[L] = [L]âˆ—â‡”[L] =[L]âŠ¤â‡”[L]âŠ¤=[L],
and so by Theorem 7.23 we find that, for all u,vâˆˆV,
âŸ¨L(u),vâŸ©= [L(u)]âŠ¤[v] = 
[L][u]âŠ¤[v] = [u]âŠ¤[L]âŠ¤[v]
= [u]âŠ¤[L][v] = [u]âŠ¤[L(v)] =âŸ¨u, L(v)âŸ©.
Therefore Lis a self-adjoint operator. â– 
In Theorem 8.9, if we let F=Rin particular, then the entries of the matrix [ L]Oare real
valued, in which case
[L]âŠ¤
O= [L]âŠ¤
O271
and we obtain the following quite readily.
Corollary 8.10. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space over Rwith orthonormal
basisO. Then Lâˆˆ L(V)is a self-adjoint operator if and only if [L]O= [L]âŠ¤
O.
A self-adjoint operator Lon an inner product space over Ris called a symmetric operator
precisely because the matrix corresponding to Lwith respect to an orthonormal basis is a
symmetric matrix.
Corollary 8.11. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space over Fwith orthonormal
basisO. IfLâˆˆ L(V)is a self-adjoint operator, then [Lâˆ—]O= [L]âˆ—
O.
Proof. Suppose that Lis self-adjoint. Then L=Lâˆ—and [ L]O= [L]âˆ—
O, whereupon it follows
trivially that [ Lâˆ—]O= [L]âˆ—
O. â– 
Lemma 8.12 (Polarization Identity ).Let(V,âŸ¨ âŸ©)be an inner product space over F. If
Lâˆˆ L(V), then
âŸ¨L(u+v),u+vâŸ© âˆ’ âŸ¨L(uâˆ’v),uâˆ’vâŸ©= 2
âŸ¨L(u),vâŸ©+âŸ¨L(v),uâŸ©
for all u,vâˆˆV.
Proof. Suppose that Lâˆˆ L(V), and let u,vâˆˆV. We have
âŸ¨L(u+v),u+vâŸ©=âŸ¨L(u),uâŸ©+âŸ¨L(u),vâŸ©+âŸ¨L(v),uâŸ©+âŸ¨L(v),vâŸ©
and
âŸ¨L(uâˆ’v),uâˆ’vâŸ©=âŸ¨L(u),uâŸ© âˆ’ âŸ¨L(u),vâŸ© âˆ’ âŸ¨L(v),uâŸ©+âŸ¨L(v),vâŸ©.
Subtraction then yields
âŸ¨L(u+v),u+vâŸ© âˆ’ âŸ¨L(uâˆ’v),uâˆ’vâŸ©= 2âŸ¨L(u),vâŸ©+ 2âŸ¨L(v),uâŸ©,
the desired outcome. â– 
Proposition 8.13. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V).
1.Suppose F=C. IfâŸ¨L(v),vâŸ©= 0for all vâˆˆV, then L=OV.
2.Suppose F=C. Then Lis self-adjoint if and only if âŸ¨L(v),vâŸ© âˆˆRfor all vâˆˆV.
3.IfLis self-adjoint and âŸ¨L(v),vâŸ©= 0for all vâˆˆV, then L=OV.
Proof.
Proof of Part (1). Suppose that âŸ¨L(v),vâŸ©= 0 for all vâˆˆV. From the polarization identity of
Lemma 8.12 we obtain
âŸ¨L(u),vâŸ©+âŸ¨L(v),uâŸ©= 0 (8.10)
for all u,vâˆˆV. Thus we have, for all u,vâˆˆV,
âŸ¨L(u), ivâŸ©+âŸ¨L(iv),uâŸ©=âˆ’iâŸ¨L(u),vâŸ©+iâŸ¨L(v),uâŸ©= 0,
whence
âˆ’ âŸ¨L(u),vâŸ©+âŸ¨L(v),uâŸ©= 0. (8.11)272
Adding equations (8.10) and (8.11) then gives
âŸ¨L(v),uâŸ©= 0
for all u,vâˆˆV. Letting vbe arbitrary and choosing u=L(v), we obtain
âŸ¨L(v), L(v)âŸ©= 0,
and thus L(v) =0. Therefore L=OV.
Proof of Part (2). Suppose that Lis self-adjoint. Let vâˆˆVbe arbitrary. We have
âŸ¨L(v),vâŸ©=âŸ¨v, L(v)âŸ©=âŸ¨L(v),vâŸ©,
which shows that âŸ¨L(v),vâŸ© âˆˆR.
For the converse, suppose that âŸ¨L(v),vâŸ© âˆˆRfor all vâˆˆV. Then
âŸ¨L(v),vâŸ©=âŸ¨v, L(v)âŸ©,
whence we obtain
âŸ¨L(v),vâŸ© âˆ’ âŸ¨v, L(v)âŸ©=âŸ¨v, Lâˆ—(v)âŸ© âˆ’ âŸ¨v, L(v)âŸ©=
v,(Lâˆ—âˆ’L)(v)
= 0.
That is,
(Lâˆ—âˆ’L)(v),v
= 0
for all vâˆˆV, and so by Part (1) we conclude that Lâˆ—âˆ’L=OV. Therefore Lâˆ—=L.
Proof of Part (3). Suppose Lis self-adjoint and âŸ¨L(v),vâŸ©= 0 for all vâˆˆV. The conclusion
follows by Part (1) if F=C, so we can assume that F=R. By the polarization identity we
obtain
âŸ¨L(u),vâŸ©+âŸ¨L(v),uâŸ©= 0,
whereupon commutativity gives
âŸ¨L(u),vâŸ©+âŸ¨u, L(v)âŸ©= 0,
and finally self-adjointness delivers
âŸ¨L(u),vâŸ©+âŸ¨L(u),vâŸ©= 0.
SoâŸ¨L(u),vâŸ©= 0 for all u,vâˆˆV. Letting ube arbitrary and setting v=L(u), we find that
âŸ¨L(u), L(u)âŸ©= 0, and thus L(u) =0. Therefore L=OV. â– 
Definition 8.14. Let(V,âŸ¨ âŸ©)be an inner product space over F. An operator Lâˆˆ L(V)is
unitary with respect to the inner product âŸ¨ âŸ©ifLâˆ—=Lâˆ’1. An invertible matrix AâˆˆFnÃ—nis
unitary ifAâˆ—=Aâˆ’1.
It is common to call a unitary matrix Awith real-valued entries an orthogonal matrix,
and a unitary operator on an inner product space over Ranorthogonal operator. Note that a
unitary operator LonVis invertible: if vâˆˆVis such that L(v) =0, then
âŸ¨v,vâŸ©=âŸ¨L(v), L(v)âŸ©=âŸ¨0,0âŸ©= 0273
implies v=0, so that Nul(L) ={0}and by the Invertible Operator Theorem we conclude that
Lis invertible.
Theorem 8.15. Let(V,âŸ¨ âŸ©)be a finite-dimensional inner product space over Fwith orthonormal
basisO, and let Lâˆˆ L(V). Then Lis a unitary operator if and only if [L]âˆ—
O= [L]âˆ’1
O.
The proof of Theorem 8.15 is much the same as the proof of Theorem 8.9, and so it is left as
a problem.
Theorem 8.16. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V). The following
statements are equivalent:
1.Lis a unitary operator.
2.Ifâˆ¥vâˆ¥= 1, then âˆ¥L(v)âˆ¥= 1.
3.âˆ¥L(v)âˆ¥=âˆ¥vâˆ¥for all vâˆˆV.
4.âŸ¨L(u), L(v)âŸ©=âŸ¨u,vâŸ©for all u,vâˆˆV.
Proof.
(1)â†’(2).Suppose that Lis a unitary operator. Fix vâˆˆVsuch that âˆ¥vâˆ¥= 1. Then
âˆ¥L(v)âˆ¥2=âŸ¨L(v), L(v)âŸ©=âŸ¨v, Lâˆ—(L(v))âŸ©=âŸ¨v, Lâˆ’1(L(v))âŸ©=âŸ¨v,vâŸ©=âˆ¥vâˆ¥2= 1,
and hence âˆ¥L(v)âˆ¥= 1.
(2)â†’(3). Suppose that âˆ¥L(v)âˆ¥= 1 for all vâˆˆVsuch that âˆ¥vâˆ¥= 1. Fix vâˆˆV. Ifv=0,
thenâˆ¥L(0)âˆ¥=âˆ¥0âˆ¥obtains immediately, so suppose that vÌ¸=0. Then Ë†v=v/âˆ¥vâˆ¥is a vector in
Vsuch that âˆ¥Ë†vâˆ¥= 1, and so âˆ¥L(Ë†v)âˆ¥= 1 by hypothesis. Now,
âˆ¥L(v)âˆ¥=L 
âˆ¥vâˆ¥Ë†v=âˆ¥vâˆ¥âˆ¥L(Ë†v)âˆ¥=âˆ¥vâˆ¥.
Therefore âˆ¥L(v)âˆ¥=âˆ¥vâˆ¥for all vâˆˆV.
(3)â†’(4). Suppose that âˆ¥L(v)âˆ¥=âˆ¥vâˆ¥, or equivalently âŸ¨L(v), L(v)âŸ©=âŸ¨v,vâŸ©, for all vâˆˆV.
Fixu,vâˆˆV. By the Parallelogram Law given in Theorem 7.9,
âˆ¥L(u) +L(v)âˆ¥2+âˆ¥L(u)âˆ’L(v)âˆ¥2= 2âˆ¥L(u)âˆ¥2+ 2âˆ¥L(v)âˆ¥2= 2âˆ¥uâˆ¥2+ 2âˆ¥vâˆ¥2,
whence
âˆ¥L(u) +L(v)âˆ¥2+âˆ¥L(uâˆ’v)âˆ¥2=âˆ¥L(u) +L(v)âˆ¥2+âˆ¥uâˆ’vâˆ¥2= 2âˆ¥uâˆ¥2+ 2âˆ¥vâˆ¥2,
and then
âŸ¨L(u) +L(v), L(u) +L(v)âŸ©+âŸ¨uâˆ’v,uâˆ’vâŸ©= 2âŸ¨u,uâŸ©+ 2âŸ¨v,vâŸ©. (8.12)
Since
âŸ¨L(u) +L(v), L(u) +L(v)âŸ©=âŸ¨L(u), L(u)âŸ©+âŸ¨L(u), L(v)âŸ©+âŸ¨L(v), L(u)âŸ©+âŸ¨L(v), L(v)âŸ©
=âŸ¨u,uâŸ©+âŸ¨L(u), L(v)âŸ©+âŸ¨L(v), L(u)âŸ©+âŸ¨v,vâŸ©
and
âŸ¨uâˆ’v,uâˆ’vâŸ©=âŸ¨u,uâŸ© âˆ’ âŸ¨u,vâŸ© âˆ’ âŸ¨v,uâŸ©+âŸ¨v,vâŸ©,274
from (8.12) we obtain
âŸ¨L(u), L(v)âŸ©+âŸ¨L(v), L(u)âŸ©=âŸ¨u,vâŸ©+âŸ¨v,uâŸ©. (8.13)
IfF=R, then the inner product is commutative and (8.13) givesâŸ¨L(u), L(v)âŸ©=âŸ¨u,vâŸ©as
desired. If F=C, then substitute iuforuin (8.13) to obtain
âŸ¨L(iu), L(v)âŸ©+âŸ¨L(v), L(iu)âŸ©=âŸ¨iu,vâŸ©+âŸ¨v, iuâŸ©,
so by the linearity of L, Axiom IP2 in Definition 7.1, and Theorem 7.2(3),
iâŸ¨L(u), L(v)âŸ© âˆ’iâŸ¨L(v), L(u)âŸ©=iâŸ¨u,vâŸ© âˆ’iâŸ¨v,uâŸ©,
and thus
âŸ¨L(u), L(v)âŸ© âˆ’ âŸ¨L(v), L(u)âŸ©=âŸ¨u,vâŸ© âˆ’ âŸ¨v,uâŸ©. (8.14)
Finally, adding (8.13) and (8.14) gives âŸ¨L(u), L(v)âŸ©=âŸ¨u,vâŸ©once again.
(4)â†’(1).Suppose that âŸ¨L(u), L(v)âŸ©=âŸ¨u,vâŸ©for all u,vâˆˆV. Thus, for any u,vâˆˆV,
âŸ¨L(u),vâŸ©=âŸ¨L(u), L(Lâˆ’1(v))âŸ©=âŸ¨u, Lâˆ’1(v)âŸ©,
which shows that Lâˆ’1=Lâˆ—and therefore Lis a unitary operator. â– 
Proposition 8.17. IfA,BâˆˆFnÃ—nare unitary and câˆˆF, then Aâˆ’1,cA,A+B, and ABare
unitary.
Proof. Suppose that A,BâˆˆFnÃ—nare unitary and câˆˆF. By Proposition 8.5(2) we have
(Aâˆ’1)âˆ—= (Aâˆ—)âˆ—=Aâˆ—âˆ—=A= (Aâˆ’1)âˆ’1;
that is, the adjoint of Aâˆ’1equals the inverse of Aâˆ’1, and therefore Aâˆ’1is unitary. â– 
Proposition 8.18. IfOandOâ€²are two ordered orthonormal bases for an inner product space
(V,âŸ¨ âŸ©)overF, then the change of basis matrix IOOâ€²is a unitary matrix.
Proof. Suppose that O= (w1, . . . ,wn) andOâ€²= (wâ€²
1, . . . ,wâ€²
n) are each orthonormal bases for
an inner product space ( V,âŸ¨ âŸ©) over F. Then by Theorem 7.23
âŸ¨u,vâŸ©= [u]âŠ¤
Oâ€²[v]Oâ€²
for all u,vâˆˆV, and also
âŸ¨wi,wjâŸ©=Î´ij=(
1,ifi=j
0,ifiÌ¸=j
By Theorem 4.27
IOOâ€²=h
[w1]Oâ€²Â·Â·Â·[wn]Oâ€²i
,
and so, letting I=IOOâ€²for brevity,
IâŠ¤I=ï£®
ï£¯ï£°[w1]âŠ¤
Oâ€²
...
[wn]âŠ¤
Oâ€²ï£¹
ï£ºï£»h
[w1]Oâ€²Â·Â·Â·[wn]Oâ€²i
.275
Thus the ij-entry of IâŠ¤Iis

IâŠ¤I
ij= [wi]âŠ¤
Oâ€²[wj]Oâ€²=âŸ¨wi,wjâŸ©=Î´ij= [In]ij
for all 1 â‰¤i, jâ‰¤n, and therefore IâŠ¤I=In. Now, since Iis invertible by Proposition 4.31, we
have
IâŠ¤I=Inâ‡”IâŠ¤I=Inâ‡” 
IâŠ¤I
Iâˆ’1=InIâˆ’1â‡”IâŠ¤=Iâˆ’1
and hence Iâˆ—=Iâˆ’1. Therefore IOOâ€²is a unitary matrix. â– 276
8.3 â€“ Normal Operators
Definition 8.19. Let(V,âŸ¨ âŸ©)be an inner product space over F. An operator Lâˆˆ L(V)is
normal if
Lâ—¦Lâˆ—=Lâˆ—â—¦L.
Proposition 8.20. All self-adjoint and unitary operators are normal operators
Proof. Suppose Lis a self-adjoint operator on ( V,âŸ¨ âŸ©). Then Lâˆ—=Lby definition, which
immediately implies that
Lâ—¦Lâˆ—=Lâˆ—â—¦L,
and hence Lis normal.
Now suppose that Lis a unitary operator on ( V,âŸ¨ âŸ©). Then Lâˆ—=Lâˆ’1by definition, so that
Lâ—¦Lâˆ—=Lâ—¦Lâˆ’1=IV=Lâˆ’1â—¦L=Lâˆ—â—¦L
and hence Lis normal. â– 
Proposition 8.21. Let(V,âŸ¨ âŸ©)be an inner product space over F. Then Lâˆˆ L(V)is a normal
operator if and only if âˆ¥L(v)âˆ¥=âˆ¥Lâˆ—(v)âˆ¥for all vâˆˆV.
Proof. Suppose that Lâˆˆ L(V) is a normal operator. Let vâˆˆV. Then
âˆ¥L(v)âˆ¥2=
L(v), L(v)
=
v, Lâˆ—(L(v))
=
v,(Lâˆ—â—¦L)(v))
=
v,(Lâ—¦Lâˆ—)(v)
=
v, L(Lâˆ—(v))
=
L(Lâˆ—(v)),v
=
Lâˆ—(v), Lâˆ—(v)
=âˆ¥Lâˆ—(v)âˆ¥2,
and therefore âˆ¥L(v)âˆ¥=âˆ¥Lâˆ—(v)âˆ¥.
Conversely, suppose that âˆ¥L(v)âˆ¥=âˆ¥Lâˆ—(v)âˆ¥for all vâˆˆV, or equivalently
âŸ¨L(v), L(v)âŸ©=âŸ¨Lâˆ—(v), Lâˆ—(v)âŸ©
for all vâˆˆV. By Proposition 8.3,
(Lâ—¦Lâˆ—âˆ’Lâˆ—â—¦L)âˆ—= (Lâ—¦Lâˆ—)âˆ—âˆ’(Lâˆ—â—¦L)âˆ—=Lâˆ—âˆ—â—¦Lâˆ—âˆ’Lâˆ—â—¦Lâˆ—âˆ—=Lâ—¦Lâˆ—âˆ’Lâˆ—â—¦L,
which shows that Lâ—¦Lâˆ—âˆ’Lâˆ—â—¦Lis self-adjoint. Now, for any vâˆˆV,

v,(Lâˆ—â—¦L)(v)
=
v, Lâˆ—(L(v))
=
L(v), L(v)
=
Lâˆ—(v), Lâˆ—(v)
=
v, L(Lâˆ—(v))
=
v,(Lâ—¦Lâˆ—)(v)
,
and thus
(Lâ—¦Lâˆ—âˆ’Lâˆ—â—¦L)(v),v
=
(Lâ—¦Lâˆ—)(v),v
âˆ’
(Lâˆ—â—¦L)(v),v
= 0.
It follows by Proposition 8.13(3) that
Lâ—¦Lâˆ—âˆ’Lâˆ—â—¦L=OV,
and therefore Lâ—¦Lâˆ—=Lâˆ—â—¦L. â– 277
Proposition 8.22. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V)be a normal
operator. If Uis a subspace of Vthat is invariant under L, then UâŠ¥is also invariant under Lâˆ—.
Proof. Suppose that Uis a subspace of Vthat is invariant under L. Let qâˆˆLâˆ—(UâŠ¥), so there
exists some pâˆˆUâŠ¥such that L(p) =q. Now, pâˆˆUâŠ¥implies that âŸ¨u,pâŸ©= 0 for all uâˆˆU.
On the other hand L(u)âˆˆUfor all uâˆˆU, and so
âŸ¨L(u),pâŸ©= 0
for all uâˆˆU. Now,
âŸ¨L(u),pâŸ©= 0â‡” âŸ¨u, Lâˆ—(p)âŸ©= 0â‡” âŸ¨u,qâŸ©= 0,
which demonstrates that qâŠ¥ufor all uâˆˆU, and hence qâˆˆUâŠ¥. We conclude that
Lâˆ—(UâŠ¥)âŠ†UâŠ¥and therefore UâŠ¥is invariant under Lâˆ—. â– 278
8.4 â€“ The Spectral Theorem
Recall from the previous chapter that if ( V,âŸ¨ âŸ©) is an inner product space over F, then a
linear operator LonVis called self-adjoint if
âŸ¨L(u),vâŸ©=âŸ¨u, L(v)âŸ©
for all u,vâˆˆV. As the first part of the next theorem makes clear, any linear operator on
a nontrivial inner product space ( V,âŸ¨ âŸ©) over the field C, in particular, will always have an
eigenvector. If the underlying field of ( V,âŸ¨ âŸ©) isR, however, then something more is required for
the existence of an eigenvector to be assured: namely, the operator must be self-adjoint.
Theorem 8.23. Let(V,âŸ¨ âŸ©)be a vector space over Fof dimension nâˆˆN, and let Lâˆˆ L(V).
1.IfF=C, then Lhas an eigenvector.
2.IfF=RandLis self-adjoint with respect to some inner product on V, then Lhas an
eigenvector.
Proof.
Proof of Part (1). Suppose F=C. LetBbe an ordered basis for V, and let [ L]Bbe the matrix
corresponding to Lwith respect to B. Then [ L]BâˆˆCnÃ—nsince Vis a vector space over C, and
by Proposition 6.29(1) [ L]Bhas at least one eigenvalue Î»âˆˆC. Now Proposition 6.14 implies
that Î»is an eigenvalue of L, which is to say there exists some vâˆˆVsuch that vÌ¸=0and
L(v) =Î»v. Therefore Lhas an eigenvector.
Proof of Part (2). Suppose F=RandLis self-adjoint with respect to some inner product on V.
By Corollary 7.15 there exists an ordered orthonormal basis OforV, and so [ L]OâˆˆSymn(R)
by Corollary 8.10. It then follows by Theorem7.26 that [ L]Ohas an eigenvalue Î»âˆˆRwith a
corresponding eigenvector in Rn, whereupon Proposition 6.14 implies that Î»is an eigenvalue of
L. Therefore Lhas an eigenvector. â– 
Definition 8.24. LetVbe a vector space over F, letUbe a subspace, and let Lâˆˆ L(V)be a
linear operator. We say that Uisinvariant under L(orL-invariant ) ifL(U)âŠ†U.
Recall that L(U) =Img(L), and notice that a subspace Uof vector space Vis invariant
under Lâˆˆ L(V) if and only if L|Uâˆˆ L(U), where as usual L|Udenotes the restriction of the
function Lto the set U. Many times in proofs, however, we will continue to use the symbol L
to denote L|U, after writing either Lâˆˆ L(U) orL:Uâ†’U, say, to make clear that the domain
ofLis being restricted to U.
Proposition 8.25. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V)be a normal
operator.
1.IfvâˆˆVis an eigenvector of Lwith corresponding eigenvalue Î», then vis an eigenvector of
Lâˆ—with eigenvalue Î».
2.Ifv1,v2âˆˆVare eigenvectors of Lwith corresponding eigenvalues Î»1, Î»2âˆˆFsuch that
Î»1Ì¸=Î»2, then v1âŠ¥v2.279
Proof.
Proof of Part (1). Suppose that vâˆˆVis an eigenvector of Lwith corresponding eigenvalue Î»,
so that L(v) =Î»v. Define Î› = Lâˆ’Î»IV, and note that Î› âˆˆ L(V). In fact Î› just so happens to
be a normal operator: recalling Proposition 8.3 and noting that Iâˆ—
V=IV, for any uâˆˆVwe have
(Î›â—¦Î›âˆ—)(u) = Î›(( Lâˆ—âˆ’Î»IV)(u)) = Î›( Lâˆ—(u)âˆ’Î»u) =L(Lâˆ—(u)âˆ’Î»u)âˆ’Î»(Lâˆ—(u)âˆ’Î»u)
= (Lâ—¦Lâˆ—)(u)âˆ’Î»L(u)âˆ’Î»Lâˆ—(u) +Î»Î»u
and
(Î›âˆ—â—¦Î›)(u) = Î›âˆ—((Lâˆ’Î»IV)(u)) = Î›âˆ—(L(u)âˆ’Î»u) =Lâˆ—(L(u)âˆ’Î»u)âˆ’Î»(L(u)âˆ’Î»u)
= (Lâˆ—â—¦L)(u)âˆ’Î»Lâˆ—(u)âˆ’Î»L(u) +Î»Î»u.
Now, since Lâ—¦Lâˆ—=Lâˆ—â—¦L, we find that
Î›â—¦Î›âˆ—=Lâ—¦Lâˆ—âˆ’Î»Lâˆ’Î»Lâˆ—+Î»Î»IV= Î›âˆ—â—¦Î›
and hence Î› is normal. Forging on, by Proposition 8.21 we obtain
âˆ¥(Lâˆ—âˆ’Î»IV)(v)âˆ¥=âˆ¥(Lâˆ’Î»IV)âˆ—(v)âˆ¥=âˆ¥(Lâˆ’Î»IV)(v)âˆ¥=âˆ¥L(v)âˆ’Î»vâˆ¥=âˆ¥0âˆ¥= 0,
which implies that
(Lâˆ—âˆ’Î»IV)(v) =0.
That is, Lâˆ—(v) =Î»v.
Proof of Part (2). Suppose that v1,v2âˆˆVare eigenvectors of Lwith corresponding eigenvalues
Î»1, Î»2âˆˆFsuch that Î»1Ì¸=Î»2. By Part (1), v1,v2âˆˆVare eigenvectors of Lâˆ—with corresponding
eigenvalues Î»1andÎ»2, respectively. Now,
(Î»1âˆ’Î»2)âŸ¨v1,v2âŸ©=Î»1âŸ¨v1,v2âŸ© âˆ’Î»2âŸ¨v1,v2âŸ©=âŸ¨Î»1v1,v2âŸ© âˆ’ âŸ¨v1,Î»2v2âŸ©
=âŸ¨L(v1),v2âŸ© âˆ’ âŸ¨v1, Lâˆ—(v2)âŸ©= 0,
and since Î»1âˆ’Î»2Ì¸= 0 we obtain âŸ¨v1,v2âŸ©= 0. Therefore v1âŠ¥v2. â– 
Whereas the proposition above establishes some eigen theory concerning normal operators,
the one below performs a similar favor for self-adjoint operators. The latter will be used to
prove the first part of the upcoming Spectral Theorem, the former the second part.
Proposition 8.26. Let(V,âŸ¨ âŸ©)be an inner product space over F, and let Lâˆˆ L(V)be a
self-adjoint operator.
1.All eigenvalues of Lare real.
2.Ifvis an eigenvector of LanduâˆˆVis such that uâŠ¥v, then L(u)âŠ¥valso.
Proof.
Proof of Part (1). LetÎ»be an eigenvalue of L. Then there exists some vâˆˆVsuch that vÌ¸=0
andL(v) =Î»v. Because vÌ¸=0we have âŸ¨v,vâŸ©>0, and because Lis self-adjoint we have
âŸ¨L(v),vâŸ©=âŸ¨v, L(v)âŸ©.280
Now,
âŸ¨L(v),vâŸ©=âŸ¨v, L(v)âŸ© â‡” âŸ¨ Î»v,vâŸ©=âŸ¨v, Î»vâŸ© â‡” Î»âŸ¨v,vâŸ©=Î»âŸ¨v,vâŸ© â‡” Î»=Î»,
where the last equation obtains upon dividing by âŸ¨v,vâŸ©. Since only a real number can equal its
own conjugate, we conclude that Î»âˆˆR.
Proof of Part (2). Suppose that vis an eigenvector of LanduâˆˆVis such that uâŠ¥v. Then
L(v) =Î»vfor some Î»âˆˆF, andâŸ¨u,vâŸ©= 0. By Theorem 7.2(3),
0 =Â¯Î»âŸ¨u,vâŸ©=âŸ¨u, Î»vâŸ©=âŸ¨u, L(v)âŸ©=âŸ¨L(u),vâŸ©,
which demonstrates that L(u)âŠ¥v. â– 
It is a trivial matter to verify that if Lis a normal (resp. self-adjoint) operator on V, and a
subspace UofVis invariant under L, then L|Uis a normal (resp. self-adjoint) operator on U.
This simple fact is assumed in the proof of the following momentous theorem.
Theorem 8.27 (Spectral Theorem ).Let(V,âŸ¨ âŸ©)be an inner product space over Fof dimension
nâˆˆN.
1.LetF=R. Then Lâˆˆ L(V)is a self-adjoint operator if and only if Vhas an orthonormal
basis consisting of the eigenvectors of L.
2.LetF=C. Then Lâˆˆ L(V)is a normal operator if and only if Vhas an orthonormal basis
consisting of the eigenvectors of L.
Proof.
Proof of Part (1). We will first apply induction on dim(V) to prove that Vmust have an
orthogonal basis consisting of eigenvectors if Lâˆˆ L(V) is self-adjoint, whereupon it will be easy
to see that Vhas an orthonormal basis consisting of eigenvectors.
Letdim(V) = 1, and suppose Lis self-adjoint. Then Lhas an eigenvector wby Theorem
8.23(2), and thus B={w}is a basis for Vby Theorem 3.54(1) that is clearly orthogonal.
Suppose Part (1) of the statement of the theorem is true for some nâˆˆN. Let ( V,âŸ¨ âŸ©) be an
inner product space over Rof dimension n+ 1, and let L:Vâ†’Vbe a self-adjoint operator.
Again, Lhas at least one eigenvector w0, so that L(w0) =Î»w0for some Î»âˆˆR. By the
Gram-Schmidt Process there exist vectors u1, . . . ,unâˆˆVsuch that B={w0,u1, . . . ,un}is an
orthogonal basis for V.
LetW=Span{w0}andU=Span{u1, . . . ,un}. For any vâˆˆWthere exists some câˆˆR
such that v=cw0, whereupon we obtain
L(v) =L(cw0) =cL(w0) =c(Î»w0) = (cÎ»)w0âˆˆW (8.15)
and we see that Wis invariant under L. Since U=WâŠ¥by Proposition 7.18, it follows by
Propositions 8.20 and 8.22 that Uis also invariant under L.
Now, dim(U) =nbecause {u1, . . . ,un}is a basis for U, and since ( U,âŸ¨ âŸ©) is an n-dimensional
inner product space over RandL:Uâ†’Uis a self-adjoint operator, it follows by the inductive
hypothesis that Uhas an orthogonal basis {w1, . . . ,wn}consisting of eigenvectors of Lâˆˆ L(U).281
Thus U=Span{w1, . . . ,wn}, where the vectors w1, . . . ,wnare mutually ortho-gonal. Moreover,
for each 1 â‰¤kâ‰¤n,
wkâˆˆUâ‡’wkâˆˆWâŠ¥â‡’ âŸ¨wk,w0âŸ©= 0,
and so w1, . . . ,wnare all orthogonal to w0. Hence O={w0, . . . ,wn}is a set of mutu-
ally orthogonal vectors, and by Lemma 7.13 we conclude that the vectors in Oare linearly
independent. Observing that |O|=n+ 1 = dim(V), Theorem 3.54(1) implies that Ois a basis
forV. That is, Ois an orthogonal basis for Vconsisting of eigenvectors of Lâˆˆ L(V).
So by induction we find that, for any nâˆˆN, ifVis an inner product space over Rof
dimension nandLis a self-adjoint operator on V, then Vhas an orthogonal basis {w1, . . . ,wn}
consisting of eigenvectors of L. Defining
Ë†wk=wk
âˆ¥wkâˆ¥
for 1â‰¤kâ‰¤n, then {Ë†w1, . . . , Ë†wn}is an orthonormal basis consisting of eigenvectors of L.
For the converse, suppose that Vhas an orthonormal basis O={w1, . . . ,wn}consisting
of the eigenvectors of Lâˆˆ L(V). Since Vis a vector space over R, it follows that there exist
Î»kâˆˆRsuch that L(wk) =Î»kwkfor all 1 â‰¤kâ‰¤n. Let u,vâˆˆV, so that
u=nX
k=1akwkand v=nX
k=1bkwk
for some ak, bkâˆˆR, 1â‰¤kâ‰¤n. Now, since Î»k=Î»k,
âŸ¨L(u),vâŸ©=DX
kakÎ»kwk,vE
=X
kÎ»kâŸ¨akwk,vâŸ©=X
kÎ»kD
akwk,X
â„“bâ„“wâ„“E
=X
kX
â„“Î»kâŸ¨akwk, bâ„“wâ„“âŸ©=X
kÎ»kâŸ¨akwk, bkwkâŸ©=X
kâŸ¨akwk, Î»kbkwkâŸ©
=X
kX
â„“âŸ¨aâ„“wâ„“, Î»kbkwkâŸ©=DX
â„“aâ„“wâ„“,X
kÎ»kbkwkE
=âŸ¨u, L(v)âŸ©
and therefore Lis self-adjoint.
Proof of Part (2). Letdim(V) = 1, and suppose Lis normal. Then Lhas an eigenvector wby
Theorem 8.23(1), and thus B={Ë†w}is an orthonormal basis for Vby Theorem 3.54(1).
Suppose Part (2) of the statement of the theorem is true for some nâˆˆN. Let ( V,âŸ¨ âŸ©) be
an inner product space over Cof dimension n+ 1, and let Lâˆˆ L(V) be normal. Again, Lhas
at least one eigenvector w0(which we can assume to be a unit vector), so that L(w0) =Î»w0
for some Î»âˆˆC. By the Gram-Schmidt Process there exist vectors u1, . . . ,unâˆˆVsuch that
B={w0,u1, . . . ,un}is an orthogonal basis for V.
LetW=Span{w0}andU=Span{u1, . . . ,un}. For any vâˆˆWthere exists some câˆˆC
such that v=cw0, whereupon (8.15) shows that Wis invariant under L. But Wis also
invariant under Lâˆ—, since by Proposition 8.25(1) we have
Lâˆ—(cw0) =cLâˆ—(w0) =c(Î»w0) = (cÎ»)w0âˆˆW.
Now, since Lâˆ—âˆˆ L(V) is normal, by Proposition 8.22 we conclude that WâŠ¥is invariant under
Lâˆ—âˆ—=L, where WâŠ¥=Uby Proposition 7.18.
Now, ( U,âŸ¨ âŸ©) is an n-dimensional inner product space over CandLâˆˆ L(U) is a normal
operator, so by the inductive hypothesis Uhas an orthonormal basis {w1, . . . ,wn}consisting282
of eigenvectors of L. Thus U= Span {w1, . . . ,wn}, where the vectors w1, . . . ,wnare mutually
orthogonal, and as with the proof of Part (1) we find that w1, . . . ,wnare each orthogonal to
w0. Hence O={w0, . . . ,wn}is a set of mutually orthogonal vectors which, as before, we find
to be a basis for V. In particular, Ois an orthonormal basis for Vconsisting of eigenvectors of
Lâˆˆ L(V). By induction we conclude that Part (2) of the theorem is true for all nâˆˆN.
Conversely, suppose that O={w1, . . . ,wn}is an orthonormal basis for Vconsisting of
eigenvectors of L. Thus there exist Î»kâˆˆCsuch that L(wk) =Î»kwkfor all 1 â‰¤kâ‰¤n, and by
Proposition 8.25(1) we also have Lâˆ—(wk) =Î»kwkfor all 1 â‰¤kâ‰¤n. Let vâˆˆV, so that
v=nX
k=1akwk
for some a1, . . . , a nâˆˆC. Now,
âŸ¨L(v), L(v)âŸ©=DX
kakÎ»kwk,X
â„“aâ„“Î»â„“wâ„“E
=X
kX
â„“âŸ¨akÎ»kwk, aâ„“Î»â„“wâ„“âŸ©
=X
kX
â„“Î»kÎ»â„“âŸ¨akwk, aâ„“wâ„“âŸ©=X
kX
â„“Î»kÎ»â„“âŸ¨aâ„“wâ„“, akwkâŸ©
=X
kX
â„“âŸ¨aâ„“Î»â„“wâ„“, akÎ»kwkâŸ©=X
kX
â„“âŸ¨aâ„“Lâˆ—(wâ„“), akLâˆ—(wk)âŸ©
=DX
â„“Lâˆ—(aâ„“wâ„“),X
kLâˆ—(akwk)E
=âŸ¨Lâˆ—(v), Lâˆ—(v)âŸ©,
where the fourth equality is justified since âŸ¨akwk, aâ„“wâ„“âŸ©is real-valued for all 1 â‰¤k, â„“â‰¤n:
âŸ¨akwk, aâ„“wâ„“âŸ©=(
0, ifkÌ¸=â„“
|ak|,ifk=â„“
Hence we have
âˆ¥L(v)âˆ¥=p
âŸ¨L(v), L(v)âŸ©=p
âŸ¨Lâˆ—(v), Lâˆ—(v)âŸ©=âˆ¥Lâˆ—(v)âˆ¥,
and so by Proposition 8.21 we conclude that Lis a normal operator. â– 
Corollary 8.28. Let(V,âŸ¨ âŸ©)be a nontrivial finite-dimensional inner product space over F, and
letÎ»1, . . . , Î» mbe the distinct eigenvalues of Lâˆˆ L(V). IfLis self-adjoint, or if Lis normal
andF=C, then
V=EL(Î»1)âŠ• Â·Â·Â· âŠ• EL(Î»m). (8.16)
Moreover, EL(Î»i)âŠ¥EL(Î»j)for all iÌ¸=j.
Proof. Suppose that Lis self-adjoint, or Lis normal and F=C. By the Spectral Theorem
there exists an orthonormal basis O={w1, . . . ,wn}consisting of the eigenvectors of L, and
thus (8.16) follows by Theorem 6.40.
Next, let uâˆˆEL(Î»i) and vâˆˆEL(Î»j) for 1 â‰¤i < j â‰¤m. If either u=0orv=0, we
obtain uâŠ¥v. Suppose that u,vÌ¸=0. Then uis an eigenvector of Lwith corresponding
eigenvalue Î»i, and vis an eigenvector of Lwith corresponding eigenvalue Î»j. Since Î»iÌ¸=Î»jand
Lis a normal operator, by Proposition 8.25(2) we conclude that uâŠ¥vonce again. Therefore
EL(Î»i)âŠ¥EL(Î»j). â– 283
Example 8.29. Let (V,âŸ¨ âŸ©) be an n-dimensional inner product space over F, and suppose that
Lâˆˆ L(V) is a self-adjoint operator. By Proposition 8.20 Lis also a normal operator, and so by
the Spectral Theorem (regardless of whether FisRorC) there exist eigenvectors v1, . . . ,vn
such that B= (v1, . . . ,vn) is an ordered basis for V. By Proposition 8.26(1) the corresponding
eigenvalues Î»1, . . . , Î» nmust be real numbers, and so for each 1 â‰¤kâ‰¤nwe have Î»kâˆˆRsuch
thatL(vk) =Î»kvk. By Corollary 4.21 the B-matrix of Lis
[L]B=h
L(v1)
BÂ·Â·Â·
L(vn)
Bi
=h
Î»1v1
BÂ·Â·Â·
Î»nvn
Bi
=h
Î»1
v1
BÂ·Â·Â·Î»n
vn
Bi
=ï£®
ï£¯ï£¯ï£¯ï£°Î»1ï£®
ï£¯ï£¯ï£°1
0
...
0ï£¹
ï£ºï£ºï£»Â·Â·Â· Î»nï£®
ï£¯ï£¯ï£°0
0
...
1ï£¹
ï£ºï£ºï£»ï£¹
ï£ºï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£°Î»10Â·Â·Â· 0
0Î»2Â·Â·Â· 0
............
0 0 Â·Â·Â· Î»nï£¹
ï£ºï£ºï£».
That is, [ L]Bis a diagonal matrix with real-valued entries, which makes it especially easy to
work with in applications.
We see, then, that the Spectral Theorem provides a means of diagonalizing self-adjoint
operators on nontrivial inner product spaces, and even normal operators if the underlying field
isC.
Proposition 8.30. IfAâˆˆFnÃ—nis self-adjoint, then there exists a unitary matrix Usuch that
Uâˆ’1AUis a diagonal matrix.
Proof. Suppose that AâˆˆFnÃ—nis self-adjoint. Let Ebe the standard basis for Fn, and let
Lâˆˆ L(Fn) be the operator given by [ L(x)]E=A[x]E, so that the matrix corresponding to L
with respect to Eis [L]E=A. Since Eis an orthonormal basis and [ L]Eis self-adjoint, by
Theorem 8.9 the operator Lis self-adjoint, and therefore Lis normal by Proposition 8.20. By
the Spectral Theorem there exists an ordered orthonormal basis Oconsisting of the eigenvectors
ofL, and so [ L]Ois found to be a diagonal matrix by Corollary 4.21.
Consider IEO, the change of basis matrix from EtoO. Both bases are orthonormal, so IEO
is a unitary matrix by Proposition 8.18, and
[L]O=IEO[L]EIâˆ’1
EO (8.17)
by Corollary 4.33. Now, the inverse of a unitary matrix is also unitary by Proposition 8.17, so if
we let U=Iâˆ’1
EO, then Uis unitary. Also we have Uâˆ’1=IEOis unitary. From (8.17) comes
Uâˆ’1AU= [L]O,
and the proof is done since [ L]Ois diagonal. â– 284
9
Canonical Forms
9.1 â€“ Generalized Eigenvectors
Recall that a vector vÌ¸=0is an eigenvector of a linear operator L:Vâ†’VifL(v) =Î»vfor
some scalar Î», where
L(v) =Î»vâ‡”L(v)âˆ’Î»v=0â‡”L(v)âˆ’Î»IV(v) =0â‡”(Lâˆ’Î»IV)(v) =0.(9.1)
We expand on this idea as follows.
Definition 9.1. LetVbe a vector space over F,Lâˆˆ L(V), and Î»âˆˆF. IfvâˆˆVis a nonzero
vector such that (Lâˆ’Î»IV)n(v) =0for some nâˆˆN, then vis ageneralized eigenvector of
Lcorresponding to Î».
From (9.1) is it clear that the set of eigenvectors of Lis included in the set of generalized
eigenvectors of L, and any eigenvalue corresponding to an eigenvector necessarily also corresponds
to a generalized eigenvector. Suppose that vÌ¸=0is a generalized eigenvector of Lcorresponding
toÎ». Let
n= min {kâˆˆN: (Lâˆ’Î»IV)k(v) =0}.
Ifnâ‰¥2, then w= (Lâˆ’Î»IV)nâˆ’1(v) is a nonzero vector in V, and
0= (Lâˆ’Î»IV)n(v) = (Lâˆ’Î»IV) 
(Lâˆ’Î»IV)nâˆ’1(v)
= (Lâˆ’Î»IV)(w) =L(w)âˆ’Î»w
implies that L(w) =Î»w. This result obtains immediately if n= 1, and so it follows that Î»is an
eigenvalue of Lwith ( Lâˆ’Î»IV)nâˆ’1(v) as a corresponding eigenvector. We see that any eigenvalue
corresponding to a generalized eigenvector necessarily also corresponds to an eigenvector. It is
because a scalar Î»corresponds to an eigenvector if and only if it corresponds to a generalized
eigenvector that we make no distinction between â€œeigenvaluesâ€ and â€œgeneralized eigenvalues.â€
Definition 9.2. LetVbe a vector space over FandLâˆˆ L(V). Suppose Î»âˆˆFis an eigenvalue
ofL. The set
KL(Î») ={vâˆˆV: (Lâˆ’Î»IV)n(v) =0for some nâˆˆN}
is the generalized eigenspace ofLcorresponding to Î».285
To prove the following proposition, note that if Wis an L-invariant subspace of a vector space
VoverF, then so too is Img(L), for the simple reason that L(W)âŠ†Wimplies L(L(W))âŠ†W,
and hence L(Img(L))âŠ†W. Also note that, for any fâˆˆ P(F), the L-invariance of Wimplies
thef(L)-invariance of W.
Lemma 9.3. LetVbe a vector space over F,Lâˆˆ L(V), and Î»âˆˆF. For any nâˆˆN,
(Lâˆ’Î»IV)nâ—¦L=Lâ—¦(Lâˆ’Î»IV)n.
Proof. When n= 1 we have, by Theorem 4.50
Lâ—¦(Lâˆ’Î»IV) =Lâ—¦Lâˆ’Î»Lâ—¦IV=Lâ—¦Lâˆ’Î»IVâ—¦L= (Lâˆ’Î»IV)â—¦L.
Suppose the conclusion of the lemma is true for some fixed nâˆˆN. That is, if M=Lâˆ’Î»IV,
then Lâ—¦Mn=Mnâ—¦L. Now, making use of Theorem 4.49,
Lâ—¦Mn+1= (Lâ—¦Mn)â—¦M= (Mnâ—¦L)â—¦M=Mnâ—¦(Lâ—¦M)
=Mnâ—¦(Mâ—¦L) = (Mnâ—¦M)â—¦L=Mn+1â—¦L,
and therefore Lâ—¦Mn=Mnâ—¦Lfor all nâˆˆNby induction. â– 
Proposition 9.4. LetVbe a vector space over FandLâˆˆ L(V). Suppose that Î»âˆˆFis an
eigenvalue of L. Then
1.KL(Î»)is an L-invariant subspace of Vsuch that EL(Î»)âŠ†KL(Î»).
2.For any ÂµâˆˆFsuch that ÂµÌ¸=Î», the operator Lâˆ’ÂµIV:KL(Î»)â†’Vis injective.
3.IfÂµis an eigenvalue of Lsuch that ÂµÌ¸=Î», then KL(Âµ)âˆ©KL(Î») ={0}.
Proof.
Proof of Part (1). We have KL(Î»)Ì¸=âˆ…since 0âˆˆKL(Î»). Let u,vâˆˆKL(Î»), so that
(Lâˆ’Î»IV)m(u) =0and ( Lâˆ’Î»IV)n(v) =0
for some m, nâˆˆN. Then
(Lâˆ’Î»IV)m+n(u+v) = (Lâˆ’Î»IV)m+n(u) + (Lâˆ’Î»IV)m+n(v)
= (Lâˆ’Î»IV)n 
(Lâˆ’Î»IV)m(u)
+ (Lâˆ’Î»IV)m 
(Lâˆ’Î»IV)n(v)
= (Lâˆ’Î»IV)n(0) + (Lâˆ’Î»IV)m(0) =0+0=0,
and we conclude that u+vâˆˆKL(Î»). IfcâˆˆF, then
(Lâˆ’Î»IV)m(cu) =c(Lâˆ’Î»IV)m(u) =c0=0
shows that cuâˆˆKL(Î»). Since KL(Î») is a nonempty subset of Vthat is closed under vector
addition and scalar multiplication, we conclude that it is a subspace of V. That EL(Î»)âŠ†KL(Î»)
is obvious.
Next, let vâˆˆL(KL(Î»)), so there is some uâˆˆKL(Î») such that L(u) =v. There exists some
nâˆˆNsuch that ( Lâˆ’Î»IV)n(u) =0, and hence by Lemma 9.3
(Lâˆ’Î»IV)n(v) = (Lâˆ’Î»IV)n(L(u)) =L 
(Lâˆ’Î»IV)n(u)
=L(0) =0.286
Therefore vâˆˆKL(Î»), and we conclude that L(KL(Î»))âŠ†KL(Î»).
Proof of Part (2). LetvâˆˆKL(Î») such that ( Lâˆ’ÂµIV)(v) =0. Let
n= min {kâˆˆN: (Lâˆ’Î»IV)k(v) =0}.
We have
(Lâˆ’Î»IV) 
(Lâˆ’Î»IV)nâˆ’1(v)
= (Lâˆ’Î»IV)n(v) =0,
so that ( Lâˆ’Î»IV)nâˆ’1(v)âˆˆEL(Î»). By Lemma 9.3,
(Lâˆ’ÂµIV) 
(Lâˆ’Î»IV)nâˆ’1(v)
= (Lâˆ’Î»IV)nâˆ’1 
(Lâˆ’ÂµIV)(v)
= (Lâˆ’Î»IV)nâˆ’1(0) =0,
so that ( Lâˆ’Î»IV)nâˆ’1(v)âˆˆEL(Âµ). Since EL(Î»)âˆ©EL(Âµ) ={0}by Proposition 6.7(2), it follows
that
(Lâˆ’Î»IV)nâˆ’1(v) =0.
Since nis the smallest positive integer for which ( Lâˆ’Î»IV)n(v) =0holds, we must conclude
thatnâˆ’1 = 0, and so
0= (Lâˆ’Î»IV)nâˆ’1(v) = (Lâˆ’Î»IV)0(v) =v.
Therefore the null space of Lâˆ’ÂµIVrestricted to KL(Î») is{0}, and so Lâˆ’ÂµIV:KL(Î»)â†’Vis
injective.
Proof of Part (3). Suppose that vâˆˆKL(Î»)âˆ©KL(Âµ), so in particular ( Lâˆ’ÂµIV)n(v) =0for some
nâˆˆN. Since KL(Î») isL-invariant by Part (1), it readily follows that KL(Î») is invariant under
Lâˆ’ÂµIV, and thus Lâˆ’ÂµIVis an injective operator on KL(Î») by Part (2). An easy induction
argument shows that ( Lâˆ’ÂµIV)nis likewise an injective operator on KL(Î»), and since vâˆˆKL(Î»)
is such that ( Lâˆ’ÂµIV)n(v) =0, it follows that v=0and therefore KL(Î»)âˆ©KL(Âµ) =âˆ….â– 
Proposition 9.5. LetVbe a finite-dimensional vector space over FandLâˆˆ L(V). Suppose
thatPLsplits over FandÎ»âˆˆÏƒ(L)has algebraic multiplicity m. Then
1. dim( KL(Î»))â‰¤m.
2.KL(Î») = Nul(( Lâˆ’Î»IV)m).
Proof.
Proof of Part (1). Letting dim( V) =n, and recalling Corollary 6.28 and Definition 6.30, there
exist a1, . . . , a nâˆ’mâˆˆFsuch that
PL(t) = (âˆ’1)n(tâˆ’Î»)mnâˆ’mY
k=1(tâˆ’ak),
where akÌ¸=Î»for all k. Let LK=L|KL(Î»). By Proposition 9.4(1), LKâˆˆ L(KL(Î»)) and Î»is an
eigenvalue of LK. From the latter fact it follows by Theorem 6.18 that PLK(Î») = 0, so that
tâˆ’Î»is a factor of PLK(t).
Suppose that PLK(t) has a factor tâˆ’Âµfor some ÂµÌ¸=Î», so that PLK(Âµ) = 0. Then Âµis
an eigenvalue of LKby Theorem 6.18 again, so there exists some vÌ¸=0inKL(Î») such that
LK(v) =Âµv, whence ( LKâˆ’ÂµI)(v) =0. But LKâˆ’ÂµI:KL(Î»)â†’KL(Î») is injective by287
Proposition 9.4(2), so that Nul(LKâˆ’ÂµI) ={0}and hence v=0, which is a contradiction.
Thus there exists no ÂµÌ¸=Î»such that tâˆ’Âµis a factor of PLK(t), and so
PLK(t) = (âˆ’1)r(tâˆ’Î»)r(9.2)
for some 1 â‰¤râ‰¤n. However, PLK(t) divides PL(t) by Proposition 4.55, and since PL(t) has
precisely mfactors of the form tâˆ’Î», we conclude that râ‰¤m. Observing that deg(PLK) =r,
we finally obtain
dim(KL(Î»)) = deg( PLK)â‰¤m
by Corollary 6.28.
Proof of Part (2). Since KL(Î») is a finite-dimensional vector space and LKâˆˆ L(KL(Î»)), by the
Cayley-Hamilton Theorem and (9.2) we have
PLK(L) = (âˆ’1)r(LKâˆ’Î»IK)r=OK,
where IKandOKrepresent the identity and zero operators on KL(Î»), and 1 â‰¤râ‰¤m. Thus
(LKâˆ’Î»IK)r=OK, and so for any vâˆˆKL(Î»),
(LKâˆ’Î»IK)r(v) =0, (9.3)
and then
(LKâˆ’Î»IK)m(v) = (LKâˆ’Î»IK)mâˆ’r 
(LKâˆ’Î»IK)r(v)
= (LKâˆ’Î»IK)mâˆ’r(0) =0
shows that
vâˆˆNul(( LKâˆ’Î»IK)m)âŠ†Nul(( Lâˆ’Î»IV)m).
(Observe that if r=m, then (9.3) delivers the desired outcome right away.) On the other hand
ifvâˆˆNul(( Lâˆ’Î»IV)m), then it is immediate that vâˆˆKL(Î»).
Therefore KL(Î») = Nul(( Lâˆ’Î»IV)m). â– 
9.2 â€“ Jordan Form288
10
The Geometry of Vector Spaces
10.1 â€“ Convex Sets
Recall that, if Vis a vector space over Randu,vâˆˆV, then the line segment joining uand
vis the set
Luv={(1âˆ’t)u+tv: 0â‰¤tâ‰¤1}.
A set CâŠ†Vthat always contains the line segment joining two of its elements is of special
interest.
Definition 10.1. LetVbe a vector space over R. A set CâŠ†Visconvex ifLuvâŠ†Cfor every
u,vâˆˆC.
Notice that any vector space Vis convex: if uandvare in V, then any linear combination
c1u+c2vis also in V, which certainly includes any linear combination of the form (1 âˆ’t)u+tv,
0â‰¤tâ‰¤1, and therefore LuvâŠ†V.
Theorem 10.2. LetVbe a vector space over R. For any v1, . . . ,vnâˆˆVthe set
S=(nX
i=1tivit1, . . . , t nâ‰¥0andnX
i=1ti= 1)
(10.1)
is convex.
Proof. Letv1, . . . ,vnâˆˆV. Fix u,wâˆˆS, so that
u=u1v1+Â·Â·Â·+unvnand w=w1v1+Â·Â·Â·+wnvn
for some ui, wiâˆˆRsuch that ui, wiâ‰¥0 for all 1 â‰¤iâ‰¤n, and
nX
i=1ui=nX
i=1wi= 1.
LetxâˆˆLuwbe arbitrary, so x= (1âˆ’s)u+swfor some sâˆˆ[0,1]. It must be shown that
xâˆˆS. Now,
x= (1âˆ’s)(u1v1+Â·Â·Â·+unvn) +s(w1v1+Â·Â·Â·+wnvn)289
= [(1âˆ’s)u1+sw1]v1+Â·Â·Â·+ [(1âˆ’s)un+swn]vn,
where for each iwe clearly have (1 âˆ’s)ui+swiâ‰¥0, and
nX
i=1[(1âˆ’s)ui+swi] = (1 âˆ’s)nX
i=1ui+snX
i=1wi= (1âˆ’s)(1) + ( s)(1) = (1 âˆ’s) +s= 1.
Thus if we let xi= (1âˆ’s)ui+swifor each 1 â‰¤iâ‰¤n, then
x=x1v1+Â·Â·Â·+xnvn
with xiâ‰¥0 for all iandx1+Â·Â·Â·+xn= 1. Hence xâˆˆS, and since xâˆˆLuwis arbitrary it
follows that LuwâŠ†S. Since u,wâˆˆSare arbitrary we conclude that LuwâŠ†Sfor all u,wâˆˆS,
and therefore Sis convex. â– 
Proposition 10.3. LetVbe a vector space over RandCâŠ†Va convex set. If v1, . . . ,vnâˆˆC
andt1, . . . , t nâ‰¥0withPn
i=1ti= 1, thenPn
i=1tiviâˆˆC.
The proof of this proposition will be done by induction.
Proof. In the case when n= 1, the statement of the proposition reads as: â€œIf v1âˆˆCand
t1â‰¥0 with t1= 1, then t1v1âˆˆCâ€. This is obviously true, and so the base case of the inductive
argument is established.
Now assume the statement of the proposition is true for some arbitrary integer nâ‰¥1.
Suppose that v1, . . . ,vn+1âˆˆCandt1, . . . , t n+1â‰¥0 with t1+Â·Â·Â·+tn+1= 1. It must be shown
thatt1v1+Â·Â·Â·+tn+1vn+1âˆˆC.
Iftn+1= 1 then we must have ti= 0 for all 1 â‰¤iâ‰¤n, whence
n+1X
i=1tivi=vn+1âˆˆC
and weâ€™re done.
Assuming that tn+1Ì¸= 1, observe that fromPn+1
i=1ti= 1 we have
nX
i=1ti= 1âˆ’tn+1,
and so
nX
i=1ti
1âˆ’tn+1= 1 (10.2)
obtains since 1 âˆ’tn+1Ì¸= 0. Now,
n+1X
i=1tivi=nX
i=1tivi+tn+1vn+1= (1âˆ’tn+1)nX
i=1ti
1âˆ’tn+1vi+tn+1vn+1,
where by the inductive hypothesis
u=nX
i=1ti
1âˆ’tn+1vi290
Figure 11. The convex hull of some points in R2.
is an element of Cbecause of (10.2) and the observation that
ti
1âˆ’tn+1â‰¥0
for all 1 â‰¤iâ‰¤n. Thus, since u,vn+1âˆˆC,
n+1X
i=1tivi= (1âˆ’tn+1)u+tn+1vn+1
for some 0 â‰¤tn+1<1, and Cis convex, we conclude thatPn+1
i=1tiviâˆˆC.
Therefore the statement of the proposition is true for n+ 1, and the proof is done. â– 
We say that Câ€²is the smallest convex set containing v1, . . . ,vnif, for any convex set C
such that v1, . . . ,vnâˆˆC, we have Câ€²âŠ†C.
Corollary 10.4. LetVbe a vector space and v1, . . . ,vnâˆˆV. Then the set Sgiven by (10.1)
is the smallest convex set that contains v1, . . . ,vn.
Proof. LetCbe a convex set containing v1, . . . ,vn. For any xâˆˆSwe have
x=nX
i=1tivi
Figure 12. Stereoscopic image of the convex hull of some points in R3.291
v1
v2
Figure 13. The parallelogram spanned by v1,v2.
for some t1, . . . , t nâ‰¥0 such thatPn
i=1ti= 1. But by Proposition 10.3 it then follows that
xâˆˆC. Therefore SâŠ†C. â– 
Theconvex hull of a set A, denoted here by Conv (A), is defined to be the smallest convex
set that contains A. It is easy to show that Conv (A) is equal to the intersection of all convex
setsCthat contain A:
Conv( A) =\
{C:AâŠ†CandCis convex }
Thus Corollary 10.3 states that
Conv({v1, . . . ,vn}) =(nX
i=1tivit1, . . . , t nâ‰¥0 andnX
i=1ti= 1)
.
See Figures 11 and 12.
Suppose that v1andv2are two linearly independent vectors in a vector space V. Then the
parallelogram spanned by v 1and v 2is the set of vectors (or points, if preferred)
{t1v1+t2v2: 0â‰¤t1, t2â‰¤1}.
Note that 0belongs to this set, as well as v1,v2,v1+v2. To see how the set forms a parallelogram
in the geometric sense, we return to the practice introduced in Chapter 1 of representing vectors
by arrows, which can still be done even if Vis not a Euclidean space (i.e. a vector space
consisting of Euclidean vectors). See Figure 13.
Definition 10.5. Let{v1, . . . ,vn}be a linearly independent set of vectors in V. The n-
dimensional box spanned by v1, . . . ,vnis the set
Bn=(nX
i=1tivi0â‰¤t1, . . . , t nâ‰¤1)
.
v1v2
v3v1v2
v3
Figure 14. The parallelepiped spanned by v1,v2,v3.292
In particular B1is the line segment spanned by v1,B2theparallelogram spanned by v1
andv2andB3theparallelepiped spanned by v1,v2, and v3.
For a depiction of a parallelepiped (or box) spanned by v1,v2, and v3, see Figure 14. It can
be shown that the box Bnis a convex set for any nâˆˆN.293
S
Symbol Glossary
A The matrix A.
AâŠ¤The transpose of A= [aij]: the ij-entry of AâŠ¤isaji.
A The conjugate of A= [aij]: the ij-entry of Aisaij.
Aâˆ—The adjoint of A:Aâˆ—= 
AâŠ¤.
|A| The determinant of the matrix A.
Aij The submatrix of Aobtained by deleting the ith row and jth column of A.
Aiâ‹† The submatrix of Aobtained by deleting the ith row of A.
Aâ‹†j The submatrix of Aobtained by deleting the jth column of A.
(A)ij Same as Aij. Used for such expressions as ( AâŠ¤)ijfor clarity.
[A]ij Theij-entry of matrix A.
[aij]m,n AnmÃ—nmatrix with ij-entry aij.
[aij]n AnnÃ—nsquare matrix with ij-entry aij.
[aij] A matrix with dimensions either unspecified or understood from context.
C The set of complex numbers.
Î´ij The Kronecker delta: Î´ij= 0 if iÌ¸=j,Î´ij= 1 if i=j.
ej Thejth standard basis element of RnorCn:ej= [Î´ij]nÃ—1.
EA(Î») The eigenspace corresponding to the eigenvalue Î»of matrix A.
EL(Î») The eigenspace corresponding to the eigenvalue Î»of operator L.294
F An unspecified field, the elements of which are called scalars.
FnF1Ã—nin Chapter 1, otherwise FnÃ—1.
FmÃ—nSet of all mÃ—nmatrices with entries in F.
Ï†B The coordinate map, where Ï†B(v) = [v]B.
Î³A(Î») The geometric multiplicity of eigenvalue Î»of matrix A.
Î³L(Î») The geometric multiplicity of eigenvalue Î»of operator L.
Img(L) Image of linear mapping L:Vâ†’W. Img( L) ={L(v) :vâˆˆV}.
IBBâ€² The change of basis matrix from basis Bto basis Bâ€².
In ThenÃ—nidentity matrix.
I An identity matrix, dimensions unspecified or understood from context.
IV The identity operator on vector space V:IV(v) =vfor all vâˆˆV.
[L] Matrix corresponding to linear mapping Lwith respect to any basis.
[L]B Matrix corresponding to linear operator Lwith respect to the basis B.
[L]BC Matrix corresponding to linear mapping Lwith respect to bases BandC.
L(V) Image of Vunder L:Vâ†’W.L(V) = Img( L).
L(V) Set of all linear operators L:Vâ†’Von some vector space V.
L(V, W ) Set of all linear mappings L:Vâ†’Won given vector spaces VandW.
ÂµA(Î») The algebraic multiplicity of eigenvalue Î»of matrix A.
ÂµL(Î») The algebraic multiplicity of eigenvalue Î»of operator L.
Nul(L) Null space of linear mapping L:Vâ†’W. Nul( L) ={vâˆˆV:L(v) =0}.
N The set of natural numbers (i.e. positive integers): N={1,2,3, . . .}.
O The zero mapping in L(V, W ):O(v) =0for all vâˆˆV.
OV The zero operator on vector space V:OV(v) =0for all vâˆˆV.
0 The zero vector.
F(S,F) The set of all functions Sâ†’F.
0V The zero vector of vector space V.295
Om,n ThemÃ—nzero matrix (all entries are 0).
On ThenÃ—nzero matrix (all entries are 0).
O A zero matrix, dimensions unspecified or understood from context.
O An orthonormal basis for an inner product space.
Pn(F) The vector space of all polynomials of degree at most nwith coefficients in F.
Q The set of rational numbers.
R The set of real numbers.
|S| The number of elements in the set S(i.e. the cardinality of S)
Symn(F) Set of all nÃ—nsymmetric matrices with entries in F.
Skw n(F) Set of all nÃ—nskew-symmetric matrices with entries in F.
sâˆ¼ The similar matrix relation.
Ïƒ(A) Set of all eigenvalues in Fof a matrix AâˆˆFnÃ—n.
Ïƒ(L) Set of all eigenvalues in Fof an operator Lâˆˆ L(V).
v The vector v.
âˆ¥vâˆ¥ The norm or magnitude of v; that is, âˆ¥vâˆ¥=âˆšvÂ·vorâˆ¥vâˆ¥=p
âŸ¨v,vâŸ©
Ë†v The normalization of vector v; that is, Ë†v=v/âˆ¥vâˆ¥
[v]B TheB-coordinates of vector v.
W The set of whole numbers: W={0,1,2,3, . . .}.
Z The set of integers: Z={1,âˆ’1,2,âˆ’2,3,âˆ’3, . . .}.
âŸ¨ âŸ© The inner product function.
âŸ¨ âŸ©V The inner product function of vector space V.
â‡’ Symbol for logical implication. Read as â€œimpliesâ€ or â€œimplies that.â€
â‡” Symbol for logical equivalence. Read as â€œis equivalent toâ€ or â€œif and only if.â€
>>>>>>> tailwind
